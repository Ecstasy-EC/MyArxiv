{"2025-05-02T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2505.01425v1","updated":"2025-05-02T17:59:55Z","published":"2025-05-02T17:59:55Z","title":"GENMO: A GENeralist Model for Human MOtion","summary":"  Human motion modeling traditionally separates motion generation and\nestimation into distinct tasks with specialized models. Motion generation\nmodels focus on creating diverse, realistic motions from inputs like text,\naudio, or keyframes, while motion estimation models aim to reconstruct accurate\nmotion trajectories from observations like videos. Despite sharing underlying\nrepresentations of temporal dynamics and kinematics, this separation limits\nknowledge transfer between tasks and requires maintaining separate models. We\npresent GENMO, a unified Generalist Model for Human Motion that bridges motion\nestimation and generation in a single framework. Our key insight is to\nreformulate motion estimation as constrained motion generation, where the\noutput motion must precisely satisfy observed conditioning signals. Leveraging\nthe synergy between regression and diffusion, GENMO achieves accurate global\nmotion estimation while enabling diverse motion generation. We also introduce\nan estimation-guided training objective that exploits in-the-wild videos with\n2D annotations and text descriptions to enhance generative diversity.\nFurthermore, our novel architecture handles variable-length motions and mixed\nmultimodal conditions (text, audio, video) at different time intervals,\noffering flexible control. This unified approach creates synergistic benefits:\ngenerative priors improve estimated motions under challenging conditions like\nocclusions, while diverse video data enhances generation capabilities.\nExtensive experiments demonstrate GENMO's effectiveness as a generalist\nframework that successfully handles multiple human motion tasks within a single\nmodel.\n","authors":["Jiefeng Li","Jinkun Cao","Haotian Zhang","Davis Rempe","Jan Kautz","Umar Iqbal","Ye Yuan"],"pdf_url":"https://arxiv.org/pdf/2505.01425v1.pdf","comment":"Project page: https://research.nvidia.com/labs/dair/genmo/"},{"id":"http://arxiv.org/abs/2502.01828v3","updated":"2025-05-02T17:53:34Z","published":"2025-02-03T21:11:02Z","title":"From Foresight to Forethought: VLM-In-the-Loop Policy Steering via\n  Latent Alignment","summary":"  While generative robot policies have demonstrated significant potential in\nlearning complex, multimodal behaviors from demonstrations, they still exhibit\ndiverse failures at deployment-time. Policy steering offers an elegant solution\nto reducing the chance of failure by using an external verifier to select from\nlow-level actions proposed by an imperfect generative policy. Here, one might\nhope to use a Vision Language Model (VLM) as a verifier, leveraging its\nopen-world reasoning capabilities. However, off-the-shelf VLMs struggle to\nunderstand the consequences of low-level robot actions as they are represented\nfundamentally differently than the text and images the VLM was trained on. In\nresponse, we propose FOREWARN, a novel framework to unlock the potential of\nVLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is\nto decouple the VLM's burden of predicting action outcomes (foresight) from\nevaluation (forethought). For foresight, we leverage a latent world model to\nimagine future latent states given diverse low-level action plans. For\nforethought, we align the VLM with these predicted latent states to reason\nabout the consequences of actions in its native representation--natural\nlanguage--and effectively filter proposed plans. We validate our framework\nacross diverse robotic manipulation tasks, demonstrating its ability to bridge\nrepresentational gaps and provide robust, generalizable policy steering. Videos\ncan be found on the project website: https://yilin-wu98.github.io/forewarn/.\n","authors":["Yilin Wu","Ran Tian","Gokul Swamy","Andrea Bajcsy"],"pdf_url":"https://arxiv.org/pdf/2502.01828v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15753v2","updated":"2025-05-02T17:49:12Z","published":"2024-11-24T08:23:23Z","title":"FoAR: Force-Aware Reactive Policy for Contact-Rich Robotic Manipulation","summary":"  Contact-rich tasks present significant challenges for robotic manipulation\npolicies due to the complex dynamics of contact and the need for precise\ncontrol. Vision-based policies often struggle with the skill required for such\ntasks, as they typically lack critical contact feedback modalities like\nforce/torque information. To address this issue, we propose FoAR, a force-aware\nreactive policy that combines high-frequency force/torque sensing with visual\ninputs to enhance the performance in contact-rich manipulation. Built upon the\nRISE policy, FoAR incorporates a multimodal feature fusion mechanism guided by\na future contact predictor, enabling dynamic adjustment of force/torque data\nusage between non-contact and contact phases. Its reactive control strategy\nalso allows FoAR to accomplish contact-rich tasks accurately through simple\nposition control. Experimental results demonstrate that FoAR significantly\noutperforms all baselines across various challenging contact-rich tasks while\nmaintaining robust performance under unexpected dynamic disturbances. Project\nwebsite: https://tonyfang.net/FoAR/\n","authors":["Zihao He","Hongjie Fang","Jingjing Chen","Hao-Shu Fang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2411.15753v2.pdf","comment":"Accepted to Robotics and Automation Letters. 9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.17662v2","updated":"2025-05-02T17:36:47Z","published":"2024-11-26T18:26:17Z","title":"RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through\n  Embedding Predictive Pre-Training","summary":"  Vision-based pose estimation of articulated robots with unknown joint angles\nhas applications in collaborative robotics and human-robot interaction tasks.\nCurrent frameworks use neural network encoders to extract image features and\ndownstream layers to predict joint angles and robot pose. While images of\nrobots inherently contain rich information about the robot's physical\nstructures, existing methods often fail to leverage it fully; therefore,\nlimiting performance under occlusions and truncations. To address this, we\nintroduce RoboPEPP, a method that fuses information about the robot's physical\nmodel into the encoder using a masking-based self-supervised\nembedding-predictive architecture. Specifically, we mask the robot's joints and\npre-train an encoder-predictor model to infer the joints' embeddings from\nsurrounding unmasked regions, enhancing the encoder's understanding of the\nrobot's physical model. The pre-trained encoder-predictor pair, along with\njoint angle and keypoint prediction networks, is then fine-tuned for pose and\njoint angle estimation. Random masking of input during fine-tuning and keypoint\nfiltering during evaluation further improves robustness. Our method, evaluated\non several datasets, achieves the best results in robot pose and joint angle\nestimation while being the least sensitive to occlusions and requiring the\nlowest execution time.\n","authors":["Raktim Gautam Goswami","Prashanth Krishnamurthy","Yann LeCun","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2411.17662v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03081v2","updated":"2025-05-02T17:36:36Z","published":"2025-03-05T00:44:12Z","title":"AirExo-2: Scaling up Generalizable Robotic Imitation Learning with\n  Low-Cost Exoskeletons","summary":"  Scaling up robotic imitation learning for real-world applications requires\nefficient and scalable demonstration collection methods. While teleoperation is\neffective, it depends on costly and inflexible robot platforms. In-the-wild\ndemonstrations offer a promising alternative, but existing collection devices\nhave key limitations: handheld setups offer limited observational coverage, and\nwhole-body systems often require fine-tuning with robot data due to domain\ngaps. To address these challenges, we present AirExo-2, a low-cost exoskeleton\nsystem for large-scale in-the-wild data collection, along with several adaptors\nthat transform collected data into pseudo-robot demonstrations suitable for\npolicy learning. We further introduce RISE-2, a generalizable imitation\nlearning policy that fuses 3D spatial and 2D semantic perception for robust\nmanipulations. Experiments show that RISE-2 outperforms prior state-of-the-art\nmethods on both in-domain and generalization evaluations. Trained solely on\nadapted in-the-wild data produced by AirExo-2, the RISE-2 policy achieves\ncomparable performance to the policy trained with teleoperated data,\nhighlighting the effectiveness and potential of AirExo-2 for scalable and\ngeneralizable imitation learning.\n","authors":["Hongjie Fang","Chenxi Wang","Yiming Wang","Jingjing Chen","Shangning Xia","Jun Lv","Zihao He","Xiyan Yi","Yunhan Guo","Xinyu Zhan","Lixin Yang","Weiming Wang","Cewu Lu","Hao-Shu Fang"],"pdf_url":"https://arxiv.org/pdf/2503.03081v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01399v1","updated":"2025-05-02T17:20:46Z","published":"2025-05-02T17:20:46Z","title":"Dynamic Robot Tool Use with Vision Language Models","summary":"  Tool use enhances a robot's task capabilities. Recent advances in\nvision-language models (VLMs) have equipped robots with sophisticated cognitive\ncapabilities for tool-use applications. However, existing methodologies focus\non elementary quasi-static tool manipulations or high-level tool selection\nwhile neglecting the critical aspect of task-appropriate tool grasping. To\naddress this limitation, we introduce inverse Tool-Use Planning (iTUP), a novel\nVLM-driven framework that enables grounded fine-grained planning for versatile\nrobotic tool use. Through an integrated pipeline of VLM-based tool and contact\npoint grounding, position-velocity trajectory planning, and physics-informed\ngrasp generation and selection, iTUP demonstrates versatility across (1)\nquasi-static and more challenging (2) dynamic and (3) cluster tool-use tasks.\nTo ensure robust planning, our framework integrates stable and safe task-aware\ngrasping by reasoning over semantic affordances and physical constraints. We\nevaluate iTUP and baselines on a comprehensive range of realistic tool use\ntasks including precision hammering, object scooping, and cluster sweeping.\nExperimental results demonstrate that iTUP ensures a thorough grounding of\ncognition and planning for challenging robot tool use across diverse\nenvironments.\n","authors":["Noah Trupin","Zixing Wang","Ahmed H. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2505.01399v1.pdf","comment":"In submission and under review"},{"id":"http://arxiv.org/abs/2505.01396v1","updated":"2025-05-02T17:13:03Z","published":"2025-05-02T17:13:03Z","title":"SIME: Enhancing Policy Self-Improvement with Modal-level Exploration","summary":"  Self-improvement requires robotic systems to initially learn from\nhuman-provided data and then gradually enhance their capabilities through\ninteraction with the environment. This is similar to how humans improve their\nskills through continuous practice. However, achieving effective\nself-improvement is challenging, primarily because robots tend to repeat their\nexisting abilities during interactions, often failing to generate new, valuable\ndata for learning. In this paper, we identify the key to successful\nself-improvement: modal-level exploration and data selection. By incorporating\na modal-level exploration mechanism during policy execution, the robot can\nproduce more diverse and multi-modal interactions. At the same time, we select\nthe most valuable trials and high-quality segments from these interactions for\nlearning. We successfully demonstrate effective robot self-improvement on both\nsimulation benchmarks and real-world experiments. The capability for\nself-improvement will enable us to develop more robust and high-success-rate\nrobotic control strategies at a lower cost. Our code and experiment scripts are\navailable at https://ericjin2002.github.io/SIME/\n","authors":["Yang Jin","Jun Lv","Wenye Yu","Hongjie Fang","Yong-Lu Li","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2505.01396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01383v1","updated":"2025-05-02T16:47:05Z","published":"2025-05-02T16:47:05Z","title":"FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft\n  Research","summary":"  We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing\nplatform for autonomy research. The hardware platform integrates a small\ncamera, a standard airframe, offboard computation, and radio communication for\nmanual overrides. We demonstrate FalconWing's capabilities by developing and\ndeploying a purely vision-based control policy for autonomous landing (without\nIMU or motion capture) using a novel real-to-sim-to-real learning approach. Our\nlearning approach: (1) constructs a photorealistic simulation environment via\n3D Gaussian splatting trained on real-world images; (2) identifies nonlinear\ndynamics from vision-estimated real-flight data; and (3) trains a multi-modal\nVision Transformer (ViT) policy through simulation-only imitation learning. The\nViT architecture fuses single RGB image with the history of control actions via\nself-attention, preserving temporal context while maintaining real-time 20 Hz\ninference. When deployed zero-shot on the hardware platform, this policy\nachieves an 80% success rate in vision-based autonomous landings. Together with\nthe hardware specifications, we also open-source the system dynamics, the\nsoftware for photorealistic simulator and the learning approach.\n","authors":["Yan Miao","Will Shen","Hang Cui","Sayan Mitra"],"pdf_url":"https://arxiv.org/pdf/2505.01383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01380v1","updated":"2025-05-02T16:41:38Z","published":"2025-05-02T16:41:38Z","title":"An Efficient Real-Time Planning Method for Swarm Robotics Based on an\n  Optimal Virtual Tube","summary":"  Swarm robotics navigating through unknown obstacle environments is an\nemerging research area that faces challenges. Performing tasks in such\nenvironments requires swarms to achieve autonomous localization, perception,\ndecision-making, control, and planning. The limited computational resources of\nonboard platforms present significant challenges for planning and control.\nReactive planners offer low computational demands and high re-planning\nfrequencies but lack predictive capabilities, often resulting in local minima.\nLong-horizon planners, on the other hand, can perform multi-step predictions to\nreduce deadlocks but cost much computation, leading to lower re-planning\nfrequencies. This paper proposes a real-time optimal virtual tube planning\nmethod for swarm robotics in unknown environments, which generates approximate\nsolutions for optimal trajectories through affine functions. As a result, the\ncomputational complexity of approximate solutions is $O(n_t)$, where $n_t$ is\nthe number of parameters in the trajectory, thereby significantly reducing the\noverall computational burden. By integrating reactive methods, the proposed\nmethod enables low-computation, safe swarm motion in unknown environments. The\neffectiveness of the proposed method is validated through several simulations\nand experiments.\n","authors":["Pengda Mao","Shuli Lv","Chen Min","Zhaolong Shen","Quan Quan"],"pdf_url":"https://arxiv.org/pdf/2505.01380v1.pdf","comment":"18 pages, 21 figures"},{"id":"http://arxiv.org/abs/2505.01339v1","updated":"2025-05-02T15:11:22Z","published":"2025-05-02T15:11:22Z","title":"Toward Teach and Repeat Across Seasonal Deep Snow Accumulation","summary":"  Teach and repeat is a rapid way to achieve autonomy in challenging terrain\nand off-road environments. A human operator pilots the vehicles to create a\nnetwork of paths that are mapped and associated with odometry. Immediately\nafter teaching, the system can drive autonomously within its tracks. This\nprecision lets operators remain confident that the robot will follow a\ntraversable route. However, this operational paradigm has rarely been explored\nin off-road environments that change significantly through seasonal variation.\nThis paper presents preliminary field trials using lidar and radar\nimplementations of teach and repeat. Using a subset of the data from the\nupcoming FoMo dataset, we attempted to repeat routes that were 4 days, 44 days,\nand 113 days old. Lidar teach and repeat demonstrated a stronger ability to\nlocalize when the ground points were removed. FMCW radar was often able to\nlocalize on older maps, but only with small deviations from the taught path.\nAdditionally, we highlight specific cases where radar localization failed with\nrecent maps due to the high pitch or roll of the vehicle. We highlight lessons\nlearned during the field deployment and highlight areas to improve to achieve\nreliable teach and repeat with seasonal changes in the environment. Please\nfollow the dataset at https://norlab-ulaval.github.io/FoMo-website for updates\nand information on the data release.\n","authors":["Matěj Boxan","Alexander Krawciw","Timothy D. Barfoot","François Pomerleau"],"pdf_url":"https://arxiv.org/pdf/2505.01339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01308v1","updated":"2025-05-02T14:37:06Z","published":"2025-05-02T14:37:06Z","title":"Desired Impedance Allocation for Robotic Systems","summary":"  Virtual Decomposition Control (VDC) has emerged as a powerful modular\nframework for real-world robotic control, particularly in contact-rich tasks.\nDespite its widespread use, VDC has been fundamentally limited to first-order\nimpedance allocation, inherently neglecting the desired inertia due to the\nmathematical complexity of second-order behavior allocation. However, inertia\nis crucial, not only for shaping dynamic responses during contact phases, but\nalso for enabling smooth acceleration and deceleration in trajectory tracking.\nMotivated by the growing demand for high-fidelity interaction control, this\nwork introduces, for the first time in the VDC framework, a method to realize\nsecond-order impedance behavior. By redefining the required end-effector\nvelocity and introducing a required acceleration and a pseudo-impedance term,\nwe achieve second-order impedance control while preserving the modularity of\nVDC. Rigorous stability analysis confirms the robustness of the proposed\ncontroller. Experimental validation on a 7-degree-of-freedom haptic exoskeleton\ndemonstrates superior tracking and contact performance compared to first-order\nmethods. Notably, incorporating inertia enables stable interaction with\nenvironments up to 70% stiffer, highlighting the effectiveness of the approach\nin real-world contact-rich scenarios.\n","authors":["Mahdi Hejrati","Jouni Mattila"],"pdf_url":"https://arxiv.org/pdf/2505.01308v1.pdf","comment":"This work has been submitted for possible publication in IEEE"},{"id":"http://arxiv.org/abs/2505.01288v1","updated":"2025-05-02T14:03:06Z","published":"2025-05-02T14:03:06Z","title":"ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video\n  Semantic Action Flow","summary":"  One of the central challenges preventing robots from acquiring complex\nmanipulation skills is the prohibitive cost of collecting large-scale robot\ndemonstrations. In contrast, humans are able to learn efficiently by watching\nothers interact with their environment. To bridge this gap, we introduce\nsemantic action flow as a core intermediate representation capturing the\nessential spatio-temporal manipulator-object interactions, invariant to\nsuperficial visual differences. We present ViSA-Flow, a framework that learns\nthis representation self-supervised from unlabeled large-scale video data.\nFirst, a generative model is pre-trained on semantic action flows automatically\nextracted from large-scale human-object interaction video data, learning a\nrobust prior over manipulation structure. Second, this prior is efficiently\nadapted to a target robot by fine-tuning on a small set of robot demonstrations\nprocessed through the same semantic abstraction pipeline. We demonstrate\nthrough extensive experiments on the CALVIN benchmark and real-world tasks that\nViSA-Flow achieves state-of-the-art performance, particularly in low-data\nregimes, outperforming prior methods by effectively transferring knowledge from\nhuman video observation to robotic execution. Videos are available at\nhttps://visaflow-web.github.io/ViSAFLOW.\n","authors":["Changhe Chen","Quantao Yang","Xiaohao Xu","Nima Fazeli","Olov Andersson"],"pdf_url":"https://arxiv.org/pdf/2505.01288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01179v1","updated":"2025-05-02T10:47:21Z","published":"2025-05-02T10:47:21Z","title":"Fast Flow-based Visuomotor Policies via Conditional Optimal Transport\n  Couplings","summary":"  Diffusion and flow matching policies have recently demonstrated remarkable\nperformance in robotic applications by accurately capturing multimodal robot\ntrajectory distributions. However, their computationally expensive inference,\ndue to the numerical integration of an ODE or SDE, limits their applicability\nas real-time controllers for robots. We introduce a methodology that utilizes\nconditional Optimal Transport couplings between noise and samples to enforce\nstraight solutions in the flow ODE for robot action generation tasks. We show\nthat naively coupling noise and samples fails in conditional tasks and propose\nincorporating condition variables into the coupling process to improve few-step\nperformance. The proposed few-step policy achieves a 4% higher success rate\nwith a 10x speed-up compared to Diffusion Policy on a diverse set of simulation\ntasks. Moreover, it produces high-quality and diverse action trajectories\nwithin 1-2 steps on a set of real-world robot tasks. Our method also retains\nthe same training complexity as Diffusion Policy and vanilla Flow Matching, in\ncontrast to distillation-based approaches.\n","authors":["Andreas Sochopoulos","Nikolay Malkin","Nikolaos Tsagkas","João Moura","Michael Gienger","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2505.01179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09762v2","updated":"2025-05-02T10:33:06Z","published":"2025-02-13T20:45:48Z","title":"AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit","summary":"  Adaptive teaming-the capability of agents to effectively collaborate with\nunfamiliar teammates without prior coordination-is widely explored in virtual\nvideo games but overlooked in real-world multi-robot contexts. Yet, such\nadaptive collaboration is crucial for real-world applications, including border\nsurveillance, search-and-rescue, and counter-terrorism operations. To address\nthis gap, we introduce AT-Drone, the first dedicated benchmark explicitly\ndesigned to facilitate comprehensive training and evaluation of adaptive\nteaming strategies in multi-drone pursuit scenarios. AT-Drone makes the\nfollowing key contributions: (1) An adaptable simulation environment\nconfigurator that enables intuitive and rapid setup of adaptive teaming\nmulti-drone pursuit tasks, including four predefined pursuit environments. (2)\nA streamlined real-world deployment pipeline that seamlessly translates\nsimulation insights into practical drone evaluations using edge devices and\nCrazyflie drones. (3) A novel algorithm zoo integrated with a distributed\ntraining framework, featuring diverse algorithms explicitly tailored, for the\nfirst time, to multi-pursuer and multi-evader settings. (4) Standardized\nevaluation protocols with newly designed unseen drone zoos, explicitly designed\nto rigorously assess the performance of adaptive teaming. Comprehensive\nexperimental evaluations across four progressively challenging multi-drone\npursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive\nteaming research. Real-world drone experiments further validate its practical\nfeasibility and utility for realistic robotic operations. Videos, code and\nweights are available at \\url{https://sites.google.com/view/at-drone}.\n","authors":["Yang Li","Junfan Chen","Feng Xue","Jiabin Qiu","Wenbin Li","Qingrui Zhang","Ying Wen","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2502.09762v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2503.14331v3","updated":"2025-05-02T09:17:40Z","published":"2025-03-18T15:03:28Z","title":"ADAPT: An Autonomous Forklift for Construction Site Operation","summary":"  Efficient material logistics play a critical role in controlling costs and\nschedules in the construction industry. However, manual material handling\nremains prone to inefficiencies, delays, and safety risks. Autonomous forklifts\noffer a promising solution to streamline on-site logistics, reducing reliance\non human operators and mitigating labor shortages. This paper presents the\ndevelopment and evaluation of ADAPT (Autonomous Dynamic All-terrain Pallet\nTransporter), a fully autonomous off-road forklift designed for construction\nenvironments. Unlike structured warehouse settings, construction sites pose\nsignificant challenges, including dynamic obstacles, unstructured terrain, and\nvarying weather conditions. To address these challenges, our system integrates\nAI-driven perception techniques with traditional approaches for decision\nmaking, planning, and control, enabling reliable operation in complex\nenvironments. We validate the system through extensive real-world testing,\ncomparing its continuous performance against an experienced human operator\nacross various weather conditions. Our findings demonstrate that autonomous\noutdoor forklifts can operate near human-level performance, offering a viable\npath toward safer and more efficient construction logistics.\n","authors":["Johannes Huemer","Markus Murschitz","Matthias Schörghuber","Lukas Reisinger","Thomas Kadiofsky","Christoph Weidinger","Mario Niedermeyer","Benedikt Widy","Marcel Zeilinger","Csaba Beleznai","Tobias Glück","Andreas Kugi","Patrik Zips"],"pdf_url":"https://arxiv.org/pdf/2503.14331v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02853v3","updated":"2025-05-02T08:53:38Z","published":"2025-02-05T03:13:04Z","title":"Rethinking Latent Redundancy in Behavior Cloning: An Information\n  Bottleneck Approach for Robot Manipulation","summary":"  Behavior Cloning (BC) is a widely adopted visual imitation learning method in\nrobot manipulation. Current BC approaches often enhance generalization by\nleveraging large datasets and incorporating additional visual and textual\nmodalities to capture more diverse information. However, these methods overlook\nwhether the learned representations contain redundant information and lack a\nsolid theoretical foundation to guide the learning process. To address these\nlimitations, we adopt an information-theoretic perspective and introduce mutual\ninformation to quantify and mitigate redundancy in latent representations.\nBuilding on this, we incorporate the Information Bottleneck (IB) principle into\nBC, which extends the idea of reducing redundancy by providing a structured\nframework for compressing irrelevant information while preserving task-relevant\nfeatures. This work presents the first comprehensive study on redundancy in\nlatent representations across various methods, backbones, and experimental\nsettings, while extending the generalizability of the IB to BC. Extensive\nexperiments and analyses on the CortexBench and LIBERO benchmarks demonstrate\nsignificant performance improvements with IB, underscoring the importance of\nreducing input data redundancy and highlighting its practical value for more\npractical applications. Project Page:\nhttps://baishuanghao.github.io/BC-IB.github.io.\n","authors":["Shuanghao Bai","Wanqi Zhou","Pengxiang Ding","Wei Zhao","Donglin Wang","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.02853v3.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.01113v1","updated":"2025-05-02T08:47:31Z","published":"2025-05-02T08:47:31Z","title":"NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization","summary":"  Recently, camera localization has been widely adopted in autonomous robotic\nnavigation due to its efficiency and convenience. However, autonomous\nnavigation in unknown environments often suffers from scene ambiguity,\nenvironmental disturbances, and dynamic object transformation in camera\nlocalization. To address this problem, inspired by the biological brain\nnavigation mechanism (such as grid cells, place cells, and head direction\ncells), we propose a novel neurobiological camera location method, namely\nNeuroLoc. Firstly, we designed a Hebbian learning module driven by place cells\nto save and replay historical information, aiming to restore the details of\nhistorical representations and solve the issue of scene fuzziness. Secondly, we\nutilized the head direction cell-inspired internal direction learning as\nmulti-head attention embedding to help restore the true orientation in similar\nscenes. Finally, we added a 3D grid center prediction in the pose regression\nmodule to reduce the final wrong prediction. We evaluate the proposed NeuroLoc\non commonly used benchmark indoor and outdoor datasets. The experimental\nresults show that our NeuroLoc can enhance the robustness in complex\nenvironments and improve the performance of pose regression by using only a\nsingle image.\n","authors":["Xun Li","Jian Yang","Fenli Jia","Muyu Wang","Qi Wu","Jun Wu","Jinpeng Mi","Jilin Hu","Peidong Liang","Xuan Tang","Ke Li","Xiong You","Xian Wei"],"pdf_url":"https://arxiv.org/pdf/2505.01113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01083v1","updated":"2025-05-02T07:42:41Z","published":"2025-05-02T07:42:41Z","title":"DexFlow: A Unified Approach for Dexterous Hand Pose Retargeting and\n  Interaction","summary":"  Despite advances in hand-object interaction modeling, generating realistic\ndexterous manipulation data for robotic hands remains a challenge. Retargeting\nmethods often suffer from low accuracy and fail to account for hand-object\ninteractions, leading to artifacts like interpenetration. Generative methods,\nlacking human hand priors, produce limited and unnatural poses. We propose a\ndata transformation pipeline that combines human hand and object data from\nmultiple sources for high-precision retargeting. Our approach uses a\ndifferential loss constraint to ensure temporal consistency and generates\ncontact maps to refine hand-object interactions. Experiments show our method\nsignificantly improves pose accuracy, naturalness, and diversity, providing a\nrobust solution for hand-object interaction modeling.\n","authors":["Xiaoyi Lin","Kunpeng Yao","Lixin Xu","Xueqiang Wang","Xuetao Li","Yuchen Wang","Miao Li"],"pdf_url":"https://arxiv.org/pdf/2505.01083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00512v2","updated":"2025-05-02T07:20:07Z","published":"2025-05-01T13:30:28Z","title":"InterLoc: LiDAR-based Intersection Localization using Road Segmentation\n  with Automated Evaluation Method","summary":"  Online localization of road intersections is beneficial for autonomous\nvehicle localization, mapping and motion planning. Intersections offer strong\nlandmarks to correct vehicle pose estimation in GNSS dropouts and anchor new\nsensor data in up-to-date maps. They are also decisive routing nodes in road\nnetwork graphs. Despite that importance, intersection localization has not been\nwidely studied, with existing methods either ignore the rich semantic\ninformation already computed onboard or depend on scarce, hand-labeled\nintersection datasets. To close that gap, this paper presents a LiDAR-based\nmethod for online vehicle-centric intersection localization. We fuse semantic\nroad segmentation with vehicle local pose to detect intersection candidates in\na bird's eye view (BEV) representation. We then refine those candidates by\nanalyzing branch topology and correcting the intersection point in a least\nsquares formulation. To evaluate our method, we introduce an automated\nbenchmarking pipeline that pairs localized intersection points with\nOpenStreetMap (OSM) intersection nodes using precise GNSS/INS ground-truth\nposes. Experiments on SemanticKITTI show that the method outperforms the latest\nlearning-based baseline in accuracy and reliability. Moreover, sensitivity\ntests demonstrate that our method is robust to challenging segmentation error\nlevels, highlighting its applicability in the real world.\n","authors":["Nguyen Hoang Khoi Tran","Julie Stephany Berrio","Mao Shan","Zhenxing Ming","Stewart Worrall"],"pdf_url":"https://arxiv.org/pdf/2505.00512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01059v1","updated":"2025-05-02T07:09:38Z","published":"2025-05-02T07:09:38Z","title":"Model Tensor Planning","summary":"  Sampling-based model predictive control (MPC) offers strong performance in\nnonlinear and contact-rich robotic tasks, yet often suffers from poor\nexploration due to locally greedy sampling schemes. We propose \\emph{Model\nTensor Planning} (MTP), a novel sampling-based MPC framework that introduces\nhigh-entropy control trajectory generation through structured tensor sampling.\nBy sampling over randomized multipartite graphs and interpolating control\ntrajectories with B-splines and Akima splines, MTP ensures smooth and globally\ndiverse control candidates. We further propose a simple $\\beta$-mixing strategy\nthat blends local exploitative and global exploratory samples within the\nmodified Cross-Entropy Method (CEM) update, balancing control refinement and\nexploration. Theoretically, we show that MTP achieves asymptotic path coverage\nand maximum entropy in the control trajectory space in the limit of infinite\ntensor depth and width.\n  Our implementation is fully vectorized using JAX and compatible with MuJoCo\nXLA, supporting \\emph{Just-in-time} (JIT) compilation and batched rollouts for\nreal-time control with online domain randomization. Through experiments on\nvarious challenging robotic tasks, ranging from dexterous in-hand manipulation\nto humanoid locomotion, we demonstrate that MTP outperforms standard MPC and\nevolutionary strategy baselines in task success and control robustness. Design\nand sensitivity ablations confirm the effectiveness of MTP tensor sampling\nstructure, spline interpolation choices, and mixing strategy. Altogether, MTP\noffers a scalable framework for robust exploration in model-based planning and\ncontrol.\n","authors":["An T. Le","Khai Nguyen","Minh Nhat Vu","João Carvalho","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2505.01059v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2409.00362v2","updated":"2025-05-02T06:25:41Z","published":"2024-08-31T06:18:46Z","title":"UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM","summary":"  Recent advancements in monocular neural depth estimation, particularly those\nachieved by the UniDepth network, have prompted the investigation of\nintegrating UniDepth within a Gaussian splatting framework for monocular SLAM.\nThis study presents UDGS-SLAM, a novel approach that eliminates the necessity\nof RGB-D sensors for depth estimation within Gaussian splatting framework.\nUDGS-SLAM employs statistical filtering to ensure local consistency of the\nestimated depth and jointly optimizes camera trajectory and Gaussian scene\nrepresentation parameters. The proposed method achieves high-fidelity rendered\nimages and low ATERMSE of the camera trajectory. The performance of UDGS-SLAM\nis rigorously evaluated using the TUM RGB-D dataset and benchmarked against\nseveral baseline methods, demonstrating superior performance across various\nscenarios. Additionally, an ablation study is conducted to validate design\nchoices and investigate the impact of different network backbone encoders on\nsystem performance.\n","authors":["Mostafa Mansour","Ahmed Abdelsalam","Ari Happonen","Jari Porras","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2409.00362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14810v2","updated":"2025-05-02T05:58:09Z","published":"2025-03-19T00:51:14Z","title":"A Study on Human-Swarm Interaction: A Framework for Assessing Situation\n  Awareness and Task Performance","summary":"  This paper introduces a framework for human swarm interaction studies that\nmeasures situation awareness in dynamic environments. A tablet-based interface\nwas developed for a user study by implementing the concepts introduced in the\nframework, where operators guided a robotic swarm in a single-target search\ntask, marking hazardous cells unknown to the swarm. Both subjective and\nobjective situation awareness measures were used, with task performance\nevaluated based on how close the robots were to the target. The framework\nenabled a structured investigation of the role of situation awareness in human\nswarm interaction, leading to key findings such as improved task performance\nacross attempts, showing the interface was learnable, centroid active robot\nposition proved to be a useful task performance metric for assessing situation\nawareness, perception and projection played a key role in task performance,\nhighlighting their importance in interface design and objective situation\nawareness influenced both subjective situation awareness and task performance,\nemphasizing the need for interfaces that emphasise objective situation\nawareness. These findings validate our framework as a structured approach for\nintegrating situation awareness concepts into human swarm interaction studies,\noffering a systematic way to assess situation awareness and task performance.\nThe framework can be applied to other swarming studies to evaluate interface\nlearnability, identify meaningful task performance metrics, and refine\ninterface designs to enhance situation awareness, ultimately improving human\nswarm interaction in dynamic environments.\n","authors":["Wasura D. Wattearachchi","Erandi Lakshika","Kathryn Kasmarik","Michael Barlow"],"pdf_url":"https://arxiv.org/pdf/2503.14810v2.pdf","comment":"10 pages, 8 figures, 2 tables, 2 equations"},{"id":"http://arxiv.org/abs/2505.01017v1","updated":"2025-05-02T05:27:28Z","published":"2025-05-02T05:27:28Z","title":"Tightly Coupled Range Inertial Odometry and Mapping with Exact Point\n  Cloud Downsampling","summary":"  In this work, to facilitate the real-time processing of multi-scan\nregistration error minimization on factor graphs, we devise a point cloud\ndownsampling algorithm based on coreset extraction. This algorithm extracts a\nsubset of the residuals of input points such that the subset yields exactly the\nsame quadratic error function as that of the original set for a given pose.\nThis enables a significant reduction in the number of residuals to be evaluated\nwithout approximation errors at the sampling point. Using this algorithm, we\ndevise a complete SLAM framework that consists of odometry estimation based on\nsliding window optimization and global trajectory optimization based on\nregistration error minimization over the entire map, both of which can run in\nreal time on a standard CPU. The experimental results demonstrate that the\nproposed framework outperforms state-of-the-art CPU-based SLAM frameworks\nwithout the use of GPU acceleration.\n","authors":["Kenji Koide","Aoki Takanose","Shuji Oishi","Masashi Yokozuka"],"pdf_url":"https://arxiv.org/pdf/2505.01017v1.pdf","comment":"IEEE International Conference on Robotics and Automation (ICRA2025)"},{"id":"http://arxiv.org/abs/2505.00995v1","updated":"2025-05-02T04:41:57Z","published":"2025-05-02T04:41:57Z","title":"Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation\n  in a GNSS-Denied Cherry Tomato Greenhouse","summary":"  As the agricultural workforce declines and labor costs rise, robotic yield\nestimation has become increasingly important. While unmanned ground vehicles\n(UGVs) are commonly used for indoor farm monitoring, their deployment in\ngreenhouses is often constrained by infrastructure limitations, sensor\nplacement challenges, and operational inefficiencies. To address these issues,\nwe develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D\ncamera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial\nodometry algorithm for precise navigation in GNSS-denied environments and\nutilizes a 3D multi-object tracking algorithm to estimate the count and weight\nof cherry tomatoes. We evaluate the system using two dataset: one from a\nharvesting row and another from a growing row. In the harvesting-row dataset,\nthe proposed system achieves 94.4\\% counting accuracy and 87.5\\% weight\nestimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For\nthe growing-row dataset, which consists of occluded unripened fruits, we\nqualitatively analyze tracking performance and highlight future research\ndirections for improving perception in greenhouse with strong occlusions. Our\nfindings demonstrate the potential of UAVs for efficient robotic yield\nestimation in commercial greenhouses.\n","authors":["Taewook Park","Jinwoo Lee","Hyondong Oh","Won-Jae Yun","Kyu-Wha Lee"],"pdf_url":"https://arxiv.org/pdf/2505.00995v1.pdf","comment":"Accepted at 2025 ICRA workshop on field robotics"},{"id":"http://arxiv.org/abs/2505.00991v1","updated":"2025-05-02T04:29:16Z","published":"2025-05-02T04:29:16Z","title":"DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning","summary":"  Dexterous manipulation has seen remarkable progress in recent years, with\npolicies capable of executing many complex and contact-rich tasks in\nsimulation. However, transferring these policies from simulation to real world\nremains a significant challenge. One important issue is the mismatch in\nlow-level controller dynamics, where identical trajectories can lead to vastly\ndifferent contact forces and behaviors when control parameters vary. Existing\napproaches often rely on manual tuning or controller randomization, which can\nbe labor-intensive, task-specific, and introduce significant training\ndifficulty. In this work, we propose a framework that jointly learns actions\nand controller parameters based on the historical information of both\ntrajectory and controller. This adaptive controller adjustment mechanism allows\nthe policy to automatically tune control parameters during execution, thereby\nmitigating the sim-to-real gap without extensive manual tuning or excessive\nrandomization. Moreover, by explicitly providing controller parameters as part\nof the observation, our approach facilitates better reasoning over force\ninteractions and improves robustness in real-world scenarios. Experimental\nresults demonstrate that our method achieves improved transfer performance\nacross a variety of dexterous tasks involving variable force conditions.\n","authors":["Shuqi Zhao","Ke Yang","Yuxin Chen","Chenran Li","Yichen Xie","Xiang Zhang","Changhao Wang","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2505.00991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00972v1","updated":"2025-05-02T03:22:00Z","published":"2025-05-02T03:22:00Z","title":"Seeking to Collide: Online Safety-Critical Scenario Generation for\n  Autonomous Driving with Retrieval Augmented Large Language Models","summary":"  Simulation-based testing is crucial for validating autonomous vehicles (AVs),\nyet existing scenario generation methods either overfit to common driving\npatterns or operate in an offline, non-interactive manner that fails to expose\nrare, safety-critical corner cases. In this paper, we introduce an online,\nretrieval-augmented large language model (LLM) framework for generating\nsafety-critical driving scenarios. Our method first employs an LLM-based\nbehavior analyzer to infer the most dangerous intent of the background vehicle\nfrom the observed state, then queries additional LLM agents to synthesize\nfeasible adversarial trajectories. To mitigate catastrophic forgetting and\naccelerate adaptation, we augment the framework with a dynamic memorization and\nretrieval bank of intent-planner pairs, automatically expanding its behavioral\nlibrary when novel intents arise. Evaluations using the Waymo Open Motion\nDataset demonstrate that our model reduces the mean minimum time-to-collision\nfrom 1.62 to 1.08 s and incurs a 75% collision rate, substantially\noutperforming baselines.\n","authors":["Yuewen Mei","Tong Nie","Jian Sun","Ye Tian"],"pdf_url":"https://arxiv.org/pdf/2505.00972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00969v1","updated":"2025-05-02T03:08:30Z","published":"2025-05-02T03:08:30Z","title":"Real-time Two-tape Control System in Vine robots","summary":"  This paper focuses on how to make a growing Vine robot steer in different\ndirections with a novel approach to real-time steering control by autonomously\napplying adhesive tape to induce a surface wrinkles. This enabling real-time\ndirectional control with arbitrary many turns while maintaining the robot's\nsoft structure. This system feeds growing material external to the tube. The\ndesign achieves fixed-angle turns in 2D space. Through experimental validation,\nwe demonstrate repeated 21-degree turns using a Dubins path planner with\nminimal error, establishing a foundation for more versatile Vine robot\napplications. This approach combines real-time control, multi-degree-of-freedom\nsteering, and structural flexibility, addressing key challenges in soft\nrobotics.\n","authors":["Hanmo Liu","Kayleen Smith","Zimu Yang","Mark Yim"],"pdf_url":"https://arxiv.org/pdf/2505.00969v1.pdf","comment":"6 pages 8 figures; submitted to IROS2025"},{"id":"http://arxiv.org/abs/2412.14415v3","updated":"2025-05-02T01:02:47Z","published":"2024-12-19T00:06:09Z","title":"DriveGPT: Scaling Autoregressive Behavior Models for Driving","summary":"  We present DriveGPT, a scalable behavior model for autonomous driving. We\nmodel driving as a sequential decision-making task, and learn a transformer\nmodel to predict future agent states as tokens in an autoregressive fashion. We\nscale up our model parameters and training data by multiple orders of\nmagnitude, enabling us to explore the scaling properties in terms of dataset\nsize, model parameters, and compute. We evaluate DriveGPT across different\nscales in a planning task, through both quantitative metrics and qualitative\nexamples, including closed-loop driving in complex real-world scenarios. In a\nseparate prediction task, DriveGPT outperforms state-of-the-art baselines and\nexhibits improved performance by pretraining on a large-scale dataset, further\nvalidating the benefits of data scaling.\n","authors":["Xin Huang","Eric M. Wolff","Paul Vernaza","Tung Phan-Minh","Hongge Chen","David S. Hayden","Mark Edmonds","Brian Pierce","Xinxin Chen","Pratik Elias Jacob","Xiaobai Chen","Chingiz Tairbekov","Pratik Agarwal","Tianshi Gao","Yuning Chai","Siddhartha Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2412.14415v3.pdf","comment":"ICML 2025. 14 pages, 17 figures, 8 tables, and 1 video link"},{"id":"http://arxiv.org/abs/2505.00935v1","updated":"2025-05-02T00:43:28Z","published":"2025-05-02T00:43:28Z","title":"Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning","summary":"  The increase in available computing power and the Deep Learning revolution\nhave allowed the exploration of new topics and frontiers in Artificial\nIntelligence research. A new field called Embodied Artificial Intelligence,\nwhich places at the intersection of Computer Vision, Robotics, and Decision\nMaking, has been gaining importance during the last few years, as it aims to\nfoster the development of smart autonomous robots and their deployment in\nsociety. The recent availability of large collections of 3D models for\nphotorealistic robotic simulation has allowed faster and safe training of\nlearning-based agents for millions of frames and a careful evaluation of their\nbehavior before deploying the models on real robotic platforms. These\nintelligent agents are intended to perform a certain task in a possibly unknown\nenvironment. To this end, during the training in simulation, the agents learn\nto perform continuous interactions with the surroundings, such as gathering\ninformation from the environment, encoding and extracting useful cues for the\ntask, and performing actions towards the final goal; where every action of the\nagent influences the interactions. This dissertation follows the complete\ncreation process of embodied agents for indoor environments, from their concept\nto their implementation and deployment. We aim to contribute to research in\nEmbodied AI and autonomous agents, in order to foster future work in this\nfield. We present a detailed analysis of the procedure behind implementing an\nintelligent embodied agent, comprehending a thorough description of the current\nstate-of-the-art in literature, technical explanations of the proposed methods,\nand accurate experimental studies on relevant robotic tasks.\n","authors":["Roberto Bigazzi"],"pdf_url":"https://arxiv.org/pdf/2505.00935v1.pdf","comment":"Ph.D. Dissertation"},{"id":"http://arxiv.org/abs/2505.00924v1","updated":"2025-05-02T00:00:02Z","published":"2025-05-02T00:00:02Z","title":"MARS: Defending Unmanned Aerial Vehicles From Attacks on Inertial\n  Sensors with Model-based Anomaly Detection and Recovery","summary":"  Unmanned Aerial Vehicles (UAVs) rely on measurements from Inertial\nMeasurement Units (IMUs) to maintain stable flight. However, IMUs are\nsusceptible to physical attacks, including acoustic resonant and\nelectromagnetic interference attacks, resulting in immediate UAV crashes.\nConsequently, we introduce a Model-based Anomaly detection and Recovery System\n(MARS) that enables UAVs to quickly detect adversarial attacks on inertial\nsensors and achieve dynamic flight recovery. MARS features an attack-resilient\nstate estimator based on the Extended Kalman Filter, which incorporates\nposition, velocity, heading, and rotor speed measurements to reconstruct\naccurate attitude and angular velocity information for UAV control. Moreover, a\nstatistical anomaly detection system monitors IMU sensor data, raising a\nsystem-level alert if an attack is detected. Upon receiving the alert, a\nmulti-stage dynamic flight recovery strategy suspends the ongoing mission,\nstabilizes the drone in a hovering condition, and then resumes tasks under the\nresilient control. Experimental results in PX4 software-in-the-loop\nenvironments as well as real-world MARS-PX4 autopilot-equipped drones\ndemonstrate the superiority of our approach over existing IMU-defense\nframeworks, showcasing the ability of the UAVs to survive attacks and complete\nthe missions.\n","authors":["Haocheng Meng","Shaocheng Luo","Zhenyuan Liang","Qing Huang","Amir Khazraei","Miroslav Pajic"],"pdf_url":"https://arxiv.org/pdf/2505.00924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01630v1","updated":"2025-05-02T23:37:29Z","published":"2025-05-02T23:37:29Z","title":"Deformable Cargo Transport in Microgravity with Astrobee","summary":"  We present pyastrobee: a simulation environment and control stack for\nAstrobee in Python, with an emphasis on cargo manipulation and transport tasks.\nWe also demonstrate preliminary success from a sampling-based MPC controller,\nusing reduced-order models of NASA's cargo transfer bag (CTB) to control a\nhigh-order deformable finite element model. Our code is open-source, fully\ndocumented, and available at https://danielpmorton.github.io/pyastrobee\n","authors":["Daniel Morton","Rika Antonova","Brian Coltin","Marco Pavone","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2505.01630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01624v1","updated":"2025-05-02T23:11:02Z","published":"2025-05-02T23:11:02Z","title":"Triangle-Decomposable Graphs for Isoperimetric Robots","summary":"  Isoperimetric robots are large scale, untethered inflatable robots that can\nundergo large shape changes, but have only been demonstrated in one 3D shape --\nan octahedron. These robots consist of independent triangles that can change\nshape while maintaining their perimeter by moving the relative position of\ntheir joints. We introduce an optimization routine that determines if an\narbitrary graph can be partitioned into unique triangles, and thus be\nconstructed as an isoperimetric robotic system. We enumerate all minimally\nrigid graphs that can be constructed with unique triangles up to 9 nodes (7\ntriangles), and characterize the workspace of one node of each these robots. We\nalso present a method for constructing larger graphs that can be partitioned by\nassembling subgraphs that are already partitioned into triangles. This enables\na wide variety of isoperimetric robot configurations.\n","authors":["Nathan Usevitch","Isaac Weaver","James Usevitch"],"pdf_url":"https://arxiv.org/pdf/2505.01624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01617v1","updated":"2025-05-02T22:40:40Z","published":"2025-05-02T22:40:40Z","title":"High Speed Robotic Table Tennis Swinging Using Lightweight Hardware with\n  Model Predictive Control","summary":"  We present a robotic table tennis platform that achieves a variety of hit\nstyles and ball-spins with high precision, power, and consistency. This is\nenabled by a custom lightweight, high-torque, low rotor inertia, five\ndegree-of-freedom arm capable of high acceleration. To generate swing\ntrajectories, we formulate an optimal control problem (OCP) that constrains the\nstate of the paddle at the time of the strike. The terminal position is given\nby a predicted ball trajectory, and the terminal orientation and velocity of\nthe paddle are chosen to match various possible styles of hits: loops\n(topspin), drives (flat), and chops (backspin). Finally, we construct a\nfixed-horizon model predictive controller (MPC) around this OCP to allow the\nhardware to quickly react to changes in the predicted ball trajectory. We\nvalidate on hardware that the system is capable of hitting balls with an\naverage exit velocity of 11 m/s at an 88% success rate across the three swing\ntypes.\n","authors":["David Nguyen","Kendrick D. Cancio","Sangbae Kim"],"pdf_url":"https://arxiv.org/pdf/2505.01617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01589v1","updated":"2025-05-02T21:18:47Z","published":"2025-05-02T21:18:47Z","title":"Phasing Through the Flames: Rapid Motion Planning with the AGHF PDE for\n  Arbitrary Objective Functions and Constraints","summary":"  The generation of optimal trajectories for high-dimensional robotic systems\nunder constraints remains computationally challenging due to the need to\nsimultaneously satisfy dynamic feasibility, input limits, and task-specific\nobjectives while searching over high-dimensional spaces. Recent approaches\nusing the Affine Geometric Heat Flow (AGHF) Partial Differential Equation (PDE)\nhave demonstrated promising results, generating dynamically feasible\ntrajectories for complex systems like the Digit V3 humanoid within seconds.\nThese methods efficiently solve trajectory optimization problems over a\ntwo-dimensional domain by evolving an initial trajectory to minimize control\neffort. However, these AGHF approaches are limited to a single type of optimal\ncontrol problem (i.e., minimizing the integral of squared control norms) and\ntypically require initial guesses that satisfy constraints to ensure\nsatisfactory convergence. These limitations restrict the potential utility of\nthe AGHF PDE especially when trying to synthesize trajectories for robotic\nsystems. This paper generalizes the AGHF formulation to accommodate arbitrary\ncost functions, significantly expanding the classes of trajectories that can be\ngenerated. This work also introduces a Phase1 - Phase 2 Algorithm that enables\nthe use of constraint-violating initial guesses while guaranteeing satisfactory\nconvergence. The effectiveness of the proposed method is demonstrated through\ncomparative evaluations against state-of-the-art techniques across various\ndynamical systems and challenging trajectory generation problems. Project Page:\nhttps://roahmlab.github.io/BLAZE/\n","authors":["Challen Enninful Adu","César E. Ramos Chuquiure","Yutong Zhou","Pearl Lin","Ruikai Yang","Bohao Zhang","Shubham Singh","Ram Vasudevan"],"pdf_url":"https://arxiv.org/pdf/2505.01589v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.08033v2","updated":"2025-05-02T19:45:46Z","published":"2025-02-12T00:26:01Z","title":"Predictive Planner for Autonomous Driving with Consistency Models","summary":"  Trajectory prediction and planning are essential for autonomous vehicles to\nnavigate safely and efficiently in dynamic environments. Traditional approaches\noften treat them separately, limiting the ability for interactive planning.\nWhile recent diffusion-based generative models have shown promise in\nmulti-agent trajectory generation, their slow sampling is less suitable for\nhigh-frequency planning tasks. In this paper, we leverage the consistency model\nto build a predictive planner that samples from a joint distribution of ego and\nsurrounding agents, conditioned on the ego vehicle's navigational goal. Trained\non real-world human driving datasets, our consistency model generates\nhigher-quality trajectories with fewer sampling steps than standard diffusion\nmodels, making it more suitable for real-time deployment. To enforce multiple\nplanning constraints simultaneously on the ego trajectory, a novel online\nguided sampling approach inspired by the Alternating Direction Method of\nMultipliers (ADMM) is introduced. Evaluated on the Waymo Open Motion Dataset\n(WOMD), our method enables proactive behavior such as nudging and yielding, and\nalso demonstrates smoother, safer, and more efficient trajectories and\nsatisfaction of multiple constraints under a limited computational budget.\n","authors":["Anjian Li","Sangjae Bae","David Isele","Ryne Beeson","Faizan M. Tariq"],"pdf_url":"https://arxiv.org/pdf/2502.08033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06985v3","updated":"2025-05-02T19:33:30Z","published":"2024-12-09T20:49:26Z","title":"Ground Perturbation Detection via Lower-Limb Kinematic States During\n  Locomotion","summary":"  Falls during daily ambulation activities are a leading cause of injury in\nolder adults due to delayed physiological responses to disturbances of balance.\nLower-limb exoskeletons have the potential to mitigate fall incidents by\ndetecting and reacting to perturbations before the user. Although commonly\nused, the standard metric for perturbation detection, whole-body angular\nmomentum, is poorly suited for exoskeleton applications due to computational\ndelays and additional tunings. To address this, we developed a novel ground\nperturbation detector using lower-limb kinematic states during locomotion. To\nidentify perturbations, we tracked deviations in the kinematic states from\ntheir nominal steady-state trajectories. Using a data-driven approach, we\nfurther optimized our detector with an open-source ground perturbation\nbiomechanics dataset. A pilot experimental validation with five able-bodied\nsubjects demonstrated that our model distinguished perturbed from unperturbed\ngait cycles with 98.8% accuracy and only a delay of 23.1% within the gait\ncycle, outperforming the benchmark by 47.7% in detection accuracy. The results\nof our study offer exciting promise for our detector and its potential utility\nto enhance the controllability of robotic assistive exoskeletons.\n","authors":["Maria T. Tagliaferri","Leonardo Campeggi","Owen N. Beck","Inseung Kang"],"pdf_url":"https://arxiv.org/pdf/2412.06985v3.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.01547v1","updated":"2025-05-02T19:19:40Z","published":"2025-05-02T19:19:40Z","title":"ASAP-MO:Advanced Situational Awareness and Perception for\n  Mission-critical Operations","summary":"  Deploying robotic missions can be challenging due to the complexity of\ncontrolling robots with multiple degrees of freedom, fusing diverse sensory\ninputs, and managing communication delays and interferences. In nuclear\ninspection, robots can be crucial in assessing environments where human\npresence is limited, requiring precise teleoperation and coordination.\nTeleoperation requires extensive training, as operators must process multiple\noutputs while ensuring safe interaction with critical assets. These challenges\nare amplified when operating a fleet of heterogeneous robots across multiple\nenvironments, as each robot may have distinct control interfaces, sensory\nsystems, and operational constraints. Efficient coordination in such settings\nremains an open problem. This paper presents a field report on how we\nintegrated robot fleet capabilities - including mapping, localization, and\ntelecommunication - toward a joint mission. We simulated a nuclear inspection\nscenario for exposed areas, using lights to represent a radiation source. We\ndeployed two Unmanned Ground Vehicles (UGVs) tasked with mapping indoor and\noutdoor environments while remotely controlled from a single base station.\nDespite having distinct operational goals, the robots produced a unified map\noutput, demonstrating the feasibility of coordinated multi-robot missions. Our\nresults highlight key operational challenges and provide insights into\nimproving adaptability and situational awareness in remote robotic deployments.\n","authors":["Veronica Vannini","William Dubois","Olivier Gamache","Jean-Michel Fortin","Nicolas Samson","Effie Daum","François Pomerleau","Edith Brotherton"],"pdf_url":"https://arxiv.org/pdf/2505.01547v1.pdf","comment":"6 pages + references, 7 figures, accepted for IEEE ICRA Workshop on\n  Field Robotics 2025"},{"id":"http://arxiv.org/abs/2505.01515v1","updated":"2025-05-02T18:04:20Z","published":"2025-05-02T18:04:20Z","title":"Comparison of Waymo Rider-Only Crash Rates by Crash Type to Human\n  Benchmarks at 56.7 Million Miles","summary":"  SAE Level 4 Automated Driving Systems (ADSs) are deployed on public roads,\nincluding Waymo's Rider-Only (RO) ride-hailing service (without a driver behind\nthe steering wheel). The objective of this study was to perform a retrospective\nsafety assessment of Waymo's RO crash rate compared to human benchmarks,\nincluding disaggregated by crash type.\n  Eleven crash type groups were identified from commonly relied upon crash\ntypologies that are derived from human crash databases. Human benchmarks were\naligned to the same vehicle types, road types, and locations as where the Waymo\nDriver operated. Waymo crashes were extracted from the NHTSA Standing General\nOrder (SGO). RO mileage was provided by the company via a public website.\nAny-injury-reported, Airbag Deployment, and Suspected Serious Injury+ crash\noutcomes were examined because they represented previously established,\nsafety-relevant benchmarks where statistical testing could be performed at the\ncurrent mileage.\n  Data was examined over 56.7 million RO miles through the end of January 2025,\nresulting in a statistically significant lower crashed vehicle rate for all\ncrashes compared to the benchmarks in Any-Injury-Reported and Airbag\nDeployment, and Suspected Serious Injury+ crashes. Of the crash types, V2V\nIntersection crash events represented the largest total crash reduction, with a\n96% reduction in Any-injury-reported (87%-99% CI) and a 91% reduction in Airbag\nDeployment (76%-98% CI) events. Cyclist, Motorcycle, Pedestrian, Secondary\nCrash, and Single Vehicle crashes were also statistically reduced for the\nAny-Injury-Reported outcome. There was no statistically significant disbenefit\nfound in any of the 11 crash type groups.\n  This study represents the first retrospective safety assessment of an RO ADS\nthat made statistical conclusions about more serious crash outcomes and\nanalyzed crash rates on a crash type basis.\n","authors":["Kristofer D. Kusano","John M. Scanlon","Yin-Hsiu Chen","Timothy L. McMurry","Tilia Gode","Trent Victor"],"pdf_url":"https://arxiv.org/pdf/2505.01515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01486v1","updated":"2025-05-02T16:45:01Z","published":"2025-05-02T16:45:01Z","title":"Aerial Path Online Planning for Urban Scene Updation","summary":"  We present the first scene-update aerial path planning algorithm specifically\ndesigned for detecting and updating change areas in urban environments. While\nexisting methods for large-scale 3D urban scene reconstruction focus on\nachieving high accuracy and completeness, they are inefficient for scenarios\nrequiring periodic updates, as they often re-explore and reconstruct entire\nscenes, wasting significant time and resources on unchanged areas. To address\nthis limitation, our method leverages prior reconstructions and change\nprobability statistics to guide UAVs in detecting and focusing on areas likely\nto have changed. Our approach introduces a novel changeability heuristic to\nevaluate the likelihood of changes, driving the planning of two flight paths: a\nprior path informed by static priors and a dynamic real-time path that adapts\nto newly detected changes. The framework integrates surface sampling and\ncandidate view generation strategies, ensuring efficient coverage of change\nareas with minimal redundancy. Extensive experiments on real-world urban\ndatasets demonstrate that our method significantly reduces flight time and\ncomputational overhead, while maintaining high-quality updates comparable to\nfull-scene re-exploration and reconstruction. These contributions pave the way\nfor efficient, scalable, and adaptive UAV-based scene updates in complex urban\nenvironments.\n","authors":["Mingfeng Tang","Ziyuan Xie","Ke Xie","Hui Huang","Jianwei Hu","Ningna Wang","Xiaohu Guo"],"pdf_url":"https://arxiv.org/pdf/2505.01486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03820v1","updated":"2025-05-02T22:47:43Z","published":"2025-05-02T22:47:43Z","title":"Satellite Autonomous Clock Fault Monitoring with Inter-Satellite Ranges\n  Using Euclidean Distance Matrices","summary":"  To address the need for robust positioning, navigation, and timing services\nin lunar environments, this paper proposes a novel onboard clock phase jump\ndetection framework for satellite constellations using range measurements\nobtained from dual one-way inter-satellite links. Our approach leverages vertex\nredundantly rigid graphs to detect faults without relying on prior knowledge of\nsatellite positions or clock biases, providing flexibility for lunar satellite\nnetworks with diverse satellite types and operators. We model satellite\nconstellations as graphs, where satellites are vertices and inter-satellite\nlinks are edges. The proposed algorithm detects and identifies satellites with\nclock jumps by monitoring the singular values of the geometric-centered\nEuclidean distance matrix (GCEDM) of 5-clique sub-graphs. The proposed method\nis validated through simulations of a GPS constellation and a notional\nconstellation around the Moon, demonstrating its effectiveness in various\nconfigurations.\n","authors":["Keidai Iiyama","Daniel Neamati","Grace Gao"],"pdf_url":"https://arxiv.org/pdf/2505.03820v1.pdf","comment":"This manuscript was submitted to the NAVIGATION: Journal of the\n  Institute of Navigation"},{"id":"http://arxiv.org/abs/2505.03815v1","updated":"2025-05-02T17:08:23Z","published":"2025-05-02T17:08:23Z","title":"Towards Cognitive Collaborative Robots: Semantic-Level Integration and\n  Explainable Control for Human-Centric Cooperation","summary":"  This is a preprint of a review article that has not yet undergone peer\nreview. The content is intended for early dissemination and academic\ndiscussion. The final version may differ upon formal publication. As the Fourth\nIndustrial Revolution reshapes industrial paradigms, human-robot collaboration\n(HRC) has transitioned from a desirable capability to an operational necessity.\nIn response, collaborative robots (Cobots) are evolving beyond repetitive tasks\ntoward adaptive, semantically informed interaction with humans and\nenvironments. This paper surveys five foundational pillars enabling this\ntransformation: semantic-level perception, cognitive action planning,\nexplainable learning and control, safety-aware motion design, and multimodal\nhuman intention recognition. We examine the role of semantic mapping in\ntransforming spatial data into meaningful context, and explore cognitive\nplanning frameworks that leverage this context for goal-driven decision-making.\nAdditionally, we analyze explainable reinforcement learning methods, including\npolicy distillation and attention mechanisms, which enhance interpretability\nand trust. Safety is addressed through force-adaptive control and risk-aware\ntrajectory planning, while seamless human interaction is supported via gaze and\ngesture-based intent recognition. Despite these advancements, challenges such\nas perception-action disjunction, real-time explainability limitations, and\nincomplete human trust persist. To address these, we propose a unified\nCognitive Synergy Architecture, integrating all modules into a cohesive\nframework for truly human-centric cobot collaboration.\n","authors":["Jaehong Oh"],"pdf_url":"https://arxiv.org/pdf/2505.03815v1.pdf","comment":"Preprint, 16 pages, 10 figures, 9 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2505.01425v1","updated":"2025-05-02T17:59:55Z","published":"2025-05-02T17:59:55Z","title":"GENMO: A GENeralist Model for Human MOtion","summary":"  Human motion modeling traditionally separates motion generation and\nestimation into distinct tasks with specialized models. Motion generation\nmodels focus on creating diverse, realistic motions from inputs like text,\naudio, or keyframes, while motion estimation models aim to reconstruct accurate\nmotion trajectories from observations like videos. Despite sharing underlying\nrepresentations of temporal dynamics and kinematics, this separation limits\nknowledge transfer between tasks and requires maintaining separate models. We\npresent GENMO, a unified Generalist Model for Human Motion that bridges motion\nestimation and generation in a single framework. Our key insight is to\nreformulate motion estimation as constrained motion generation, where the\noutput motion must precisely satisfy observed conditioning signals. Leveraging\nthe synergy between regression and diffusion, GENMO achieves accurate global\nmotion estimation while enabling diverse motion generation. We also introduce\nan estimation-guided training objective that exploits in-the-wild videos with\n2D annotations and text descriptions to enhance generative diversity.\nFurthermore, our novel architecture handles variable-length motions and mixed\nmultimodal conditions (text, audio, video) at different time intervals,\noffering flexible control. This unified approach creates synergistic benefits:\ngenerative priors improve estimated motions under challenging conditions like\nocclusions, while diverse video data enhances generation capabilities.\nExtensive experiments demonstrate GENMO's effectiveness as a generalist\nframework that successfully handles multiple human motion tasks within a single\nmodel.\n","authors":["Jiefeng Li","Jinkun Cao","Haotian Zhang","Davis Rempe","Jan Kautz","Umar Iqbal","Ye Yuan"],"pdf_url":"https://arxiv.org/pdf/2505.01425v1.pdf","comment":"Project page: https://research.nvidia.com/labs/dair/genmo/"},{"id":"http://arxiv.org/abs/2411.17662v2","updated":"2025-05-02T17:36:47Z","published":"2024-11-26T18:26:17Z","title":"RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through\n  Embedding Predictive Pre-Training","summary":"  Vision-based pose estimation of articulated robots with unknown joint angles\nhas applications in collaborative robotics and human-robot interaction tasks.\nCurrent frameworks use neural network encoders to extract image features and\ndownstream layers to predict joint angles and robot pose. While images of\nrobots inherently contain rich information about the robot's physical\nstructures, existing methods often fail to leverage it fully; therefore,\nlimiting performance under occlusions and truncations. To address this, we\nintroduce RoboPEPP, a method that fuses information about the robot's physical\nmodel into the encoder using a masking-based self-supervised\nembedding-predictive architecture. Specifically, we mask the robot's joints and\npre-train an encoder-predictor model to infer the joints' embeddings from\nsurrounding unmasked regions, enhancing the encoder's understanding of the\nrobot's physical model. The pre-trained encoder-predictor pair, along with\njoint angle and keypoint prediction networks, is then fine-tuned for pose and\njoint angle estimation. Random masking of input during fine-tuning and keypoint\nfiltering during evaluation further improves robustness. Our method, evaluated\non several datasets, achieves the best results in robot pose and joint angle\nestimation while being the least sensitive to occlusions and requiring the\nlowest execution time.\n","authors":["Raktim Gautam Goswami","Prashanth Krishnamurthy","Yann LeCun","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2411.17662v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01406v1","updated":"2025-05-02T17:35:03Z","published":"2025-05-02T17:35:03Z","title":"VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in\n  Video Diffusion Models","summary":"  The rapid rise of video diffusion models has enabled the generation of highly\nrealistic and temporally coherent videos, raising critical concerns about\ncontent authenticity, provenance, and misuse. Existing watermarking approaches,\nwhether passive, post-hoc, or adapted from image-based techniques, often\nstruggle to withstand video-specific manipulations such as frame insertion,\ndropping, or reordering, and typically degrade visual quality. In this work, we\nintroduce VIDSTAMP, a watermarking framework that embeds per-frame or\nper-segment messages directly into the latent space of temporally-aware video\ndiffusion models. By fine-tuning the model's decoder through a two-stage\npipeline, first on static image datasets to promote spatial message separation,\nand then on synthesized video sequences to restore temporal consistency,\nVIDSTAMP learns to embed high-capacity, flexible watermarks with minimal\nperceptual impact. Leveraging architectural components such as 3D convolutions\nand temporal attention, our method imposes no additional inference cost and\noffers better perceptual quality than prior methods, while maintaining\ncomparable robustness against common distortions and tampering. VIDSTAMP embeds\n768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a\nlog P-value of -166.65 (lower is better), and maintains a video quality score\nof 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior\nmethods in capacity-quality tradeoffs. Code: Code:\n\\url{https://github.com/SPIN-UMass/VidStamp}\n","authors":["Mohammadreza Teymoorianfard","Shiqing Ma","Amir Houmansadr"],"pdf_url":"https://arxiv.org/pdf/2505.01406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16276v2","updated":"2025-05-02T17:04:43Z","published":"2025-04-22T21:21:41Z","title":"An Automated Pipeline for Few-Shot Bird Call Classification: A Case\n  Study with the Tooth-Billed Pigeon","summary":"  This paper presents an automated one-shot bird call classification pipeline\ndesigned for rare species absent from large publicly available classifiers like\nBirdNET and Perch. While these models excel at detecting common birds with\nabundant training data, they lack options for species with only 1-3 known\nrecordings-a critical limitation for conservationists monitoring the last\nremaining individuals of endangered birds. To address this, we leverage the\nembedding space of large bird classification networks and develop a classifier\nusing cosine similarity, combined with filtering and denoising preprocessing\ntechniques, to optimize detection with minimal training data. We evaluate\nvarious embedding spaces using clustering metrics and validate our approach in\nboth a simulated scenario with Xeno-Canto recordings and a real-world test on\nthe critically endangered tooth-billed pigeon (Didunculus strigirostris), which\nhas no existing classifiers and only three confirmed recordings. The final\nmodel achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon\ncalls, making it practical for use in the field. This open-source system\nprovides a practical tool for conservationists seeking to detect and monitor\nrare species on the brink of extinction.\n","authors":["Abhishek Jana","Moeumu Uili","James Atherton","Mark O'Brien","Joe Wood","Leandra Brickson"],"pdf_url":"https://arxiv.org/pdf/2504.16276v2.pdf","comment":"16 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.01390v1","updated":"2025-05-02T16:57:37Z","published":"2025-05-02T16:57:37Z","title":"Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework\n  for Predicting Pathological Response in Non-Small Cell Lung Cancer","summary":"  This study proposes a novel approach combining Multimodal Deep Learning with\nintrinsic eXplainable Artificial Intelligence techniques to predict\npathological response in non-small cell lung cancer patients undergoing\nneoadjuvant therapy. Due to the limitations of existing radiomics and unimodal\ndeep learning approaches, we introduce an intermediate fusion strategy that\nintegrates imaging and clinical data, enabling efficient interaction between\ndata modalities. The proposed Multimodal Doctor-in-the-Loop method further\nenhances clinical relevance by embedding clinicians' domain knowledge directly\ninto the training process, guiding the model's focus gradually from broader\nlung regions to specific lesions. Results demonstrate improved predictive\naccuracy and explainability, providing insights into optimal data integration\nstrategies for clinical applications.\n","authors":["Alice Natalina Caragliano","Claudia Tacconi","Carlo Greco","Lorenzo Nibid","Edy Ippolito","Michele Fiore","Giuseppe Perrone","Sara Ramella","Paolo Soda","Valerio Guarrasi"],"pdf_url":"https://arxiv.org/pdf/2505.01390v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2502.17503"},{"id":"http://arxiv.org/abs/2505.01385v1","updated":"2025-05-02T16:49:07Z","published":"2025-05-02T16:49:07Z","title":"Global Collinearity-aware Polygonizer for Polygonal Building Mapping in\n  Remote Sensing","summary":"  This paper addresses the challenge of mapping polygonal buildings from remote\nsensing images and introduces a novel algorithm, the Global Collinearity-aware\nPolygonizer (GCP). GCP, built upon an instance segmentation framework,\nprocesses binary masks produced by any instance segmentation model. The\nalgorithm begins by collecting polylines sampled along the contours of the\nbinary masks. These polylines undergo a refinement process using a\ntransformer-based regression module to ensure they accurately fit the contours\nof the targeted building instances. Subsequently, a collinearity-aware polygon\nsimplification module simplifies these refined polylines and generate the final\npolygon representation. This module employs dynamic programming technique to\noptimize an objective function that balances the simplicity and fidelity of the\npolygons, achieving globally optimal solutions. Furthermore, the optimized\ncollinearity-aware objective is seamlessly integrated into network training,\nenhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has\nbeen validated on two public benchmarks for polygonal building mapping. Further\nexperiments reveal that applying the collinearity-aware polygon simplification\nmodule to arbitrary polylines, without prior knowledge, enhances accuracy over\ntraditional methods such as the Douglas-Peucker algorithm. This finding\nunderscores the broad applicability of GCP. The code for the proposed method\nwill be made available at https://github.com/zhu-xlab.\n","authors":["Fahong Zhang","Yilei Shi","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.01385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01364v1","updated":"2025-05-02T16:04:00Z","published":"2025-05-02T16:04:00Z","title":"Monitoring morphometric drift in lifelong learning segmentation of the\n  spinal cord","summary":"  Morphometric measures derived from spinal cord segmentations can serve as\ndiagnostic and prognostic biomarkers in neurological diseases and injuries\naffecting the spinal cord. While robust, automatic segmentation methods to a\nwide variety of contrasts and pathologies have been developed over the past few\nyears, whether their predictions are stable as the model is updated using new\ndatasets has not been assessed. This is particularly important for deriving\nnormative values from healthy participants. In this study, we present a spinal\ncord segmentation model trained on a multisite $(n=75)$ dataset, including 9\ndifferent MRI contrasts and several spinal cord pathologies. We also introduce\na lifelong learning framework to automatically monitor the morphometric drift\nas the model is updated using additional datasets. The framework is triggered\nby an automatic GitHub Actions workflow every time a new model is created,\nrecording the morphometric values derived from the model's predictions over\ntime. As a real-world application of the proposed framework, we employed the\nspinal cord segmentation model to update a recently-introduced normative\ndatabase of healthy participants containing commonly used measures of spinal\ncord morphometry. Results showed that: (i) our model outperforms previous\nversions and pathology-specific models on challenging lumbar spinal cord cases,\nachieving an average Dice score of $0.95 \\pm 0.03$; (ii) the automatic workflow\nfor monitoring morphometric drift provides a quick feedback loop for developing\nfuture segmentation models; and (iii) the scaling factor required to update the\ndatabase of morphometric measures is nearly constant among slices across the\ngiven vertebral levels, showing minimum drift between the current and previous\nversions of the model monitored by the framework. The model is freely available\nin Spinal Cord Toolbox v7.0.\n","authors":["Enamundram Naga Karthik","Sandrine Bédard","Jan Valošek","Christoph S. Aigner","Elise Bannier","Josef Bednařík","Virginie Callot","Anna Combes","Armin Curt","Gergely David","Falk Eippert","Lynn Farner","Michael G Fehlings","Patrick Freund","Tobias Granberg","Cristina Granziera","RHSCIR Network Imaging Group","Ulrike Horn","Tomáš Horák","Suzanne Humphreys","Markus Hupp","Anne Kerbrat","Nawal Kinany","Shannon Kolind","Petr Kudlička","Anna Lebret","Lisa Eunyoung Lee","Caterina Mainero","Allan R. Martin","Megan McGrath","Govind Nair","Kristin P. O'Grady","Jiwon Oh","Russell Ouellette","Nikolai Pfender","Dario Pfyffer","Pierre-François Pradat","Alexandre Prat","Emanuele Pravatà","Daniel S. Reich","Ilaria Ricchi","Naama Rotem-Kohavi","Simon Schading-Sassenhausen","Maryam Seif","Andrew Smith","Seth A Smith","Grace Sweeney","Roger Tam","Anthony Traboulsee","Constantina Andrada Treaba","Charidimos Tsagkas","Zachary Vavasour","Dimitri Van De Ville","Kenneth Arnold Weber II","Sarath Chandar","Julien Cohen-Adad"],"pdf_url":"https://arxiv.org/pdf/2505.01364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14432v2","updated":"2025-05-02T16:03:31Z","published":"2024-11-21T18:59:55Z","title":"Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models","summary":"  Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks.\n","authors":["Yuhao Dong","Zuyan Liu","Hai-Long Sun","Jingkang Yang","Winston Hu","Yongming Rao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18851v3","updated":"2025-05-02T15:52:03Z","published":"2025-01-31T02:24:13Z","title":"Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph\n  Convolution Networks","summary":"  Most existing RGB-D semantic segmentation methods focus on the feature level\nfusion, including complex cross-modality and cross-scale fusion modules.\nHowever, these methods may cause misalignment problem in the feature fusion\nprocess and counter-intuitive patches in the segmentation results. Inspired by\nthe popular pixel-node-pixel pipeline, we propose to 1) fuse features from two\nmodalities in a late fusion style, during which the geometric feature injection\nis guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on\nthe fused feature to alleviate the emergence of irregular patches by inferring\npatch relationship. At the 3D feature extraction stage, we argue that\ntraditional CNNs are not efficient enough for depth maps. So, we encode depth\nmap into normal map, after which CNNs can easily extract object surface\ntendencies.At projection matrix generation stage, we find the existence of\nBiased-Assignment and Ambiguous-Locality issues in the original pipeline.\nTherefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no\nmissing important pixel features, which can be viewed as hard pixel mining\nprocess; 2) connect regions that are close to each other in the Euclidean space\nas well as in the semantic space with larger edge weights so that location\ninformations can been considered. Extensive experiments on two public datasets,\nNYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost\nthe performance of RGB-D semantic segmentation task.\n","authors":["Xiaoyan Jiang","Bohan Wang","Xinlong Wan","Shanshan Chen","Hamido Fujita","Hanan Abd. Al Juaid"],"pdf_url":"https://arxiv.org/pdf/2501.18851v3.pdf","comment":"I have decided to withdraw this paper because I have recently\n  obtained some new data and insights during my ongoing research"},{"id":"http://arxiv.org/abs/2503.01284v3","updated":"2025-05-02T15:05:31Z","published":"2025-03-03T08:12:09Z","title":"Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating\n  MobileNetV2 and GraphSAGE with Cross-Modal Attention","summary":"  Soybean leaf disease detection is critical for agricultural productivity but\nfaces challenges due to visually similar symptoms and limited interpretability\nin conventional methods. While Convolutional Neural Networks (CNNs) excel in\nspatial feature extraction, they often neglect inter-image relational\ndependencies, leading to misclassifications. This paper proposes an\ninterpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that\nsynergizes MobileNetV2 for localized feature extraction and GraphSAGE for\nrelational modeling. The framework constructs a graph where nodes represent\nleaf images, with edges defined by cosine similarity-based adjacency matrices\nand adaptive neighborhood sampling. This design captures fine-grained lesion\nfeatures and global symptom patterns, addressing inter-class similarity\nchallenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM\nvisualizations, generating heatmaps to highlight disease-influential regions.\nEvaluated on a dataset of ten soybean leaf diseases, the model achieves\n$97.16\\%$ accuracy, surpassing standalone CNNs ($\\le95.04\\%$) and traditional\nmachine learning models ($\\le77.05\\%$). Ablation studies validate the\nsequential architecture's superiority over parallel or single-model\nconfigurations. With only 2.3 million parameters, the lightweight\nMobileNetV2-GraphSAGE combination ensures computational efficiency, enabling\nreal-time deployment in resource-constrained environments. The proposed\napproach bridges the gap between accurate classification and practical\napplicability, offering a robust, interpretable tool for agricultural\ndiagnostics while advancing CNN-GNN integration in plant pathology research.\n","authors":["Md Abrar Jahin","Soudeep Shahriar","M. F. Mridha","Md. Jakir Hossen","Nilanjan Dey"],"pdf_url":"https://arxiv.org/pdf/2503.01284v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01322v1","updated":"2025-05-02T14:53:56Z","published":"2025-05-02T14:53:56Z","title":"FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian\n  Scene without Spatial Priors","summary":"  Text-driven object insertion in 3D scenes is an emerging task that enables\nintuitive scene editing through natural language. However, existing 2D\nediting-based methods often rely on spatial priors such as 2D masks or 3D\nbounding boxes, and they struggle to ensure consistency of the inserted object.\nThese limitations hinder flexibility and scalability in real-world\napplications. In this paper, we propose FreeInsert, a novel framework that\nleverages foundation models including MLLMs, LGMs, and diffusion models to\ndisentangle object generation from spatial placement. This enables unsupervised\nand flexible object insertion in 3D scenes without spatial priors. FreeInsert\nstarts with an MLLM-based parser that extracts structured semantics, including\nobject types, spatial relationships, and attachment regions, from user\ninstructions. These semantics guide both the reconstruction of the inserted\nobject for 3D consistency and the learning of its degrees of freedom. We\nleverage the spatial reasoning capabilities of MLLMs to initialize object pose\nand scale. A hierarchical, spatially aware refinement stage further integrates\nspatial semantics and MLLM-inferred priors to enhance placement. Finally, the\nappearance of the object is improved using the inserted-object image to enhance\nvisual fidelity. Experimental results demonstrate that FreeInsert achieves\nsemantically coherent, spatially precise, and visually realistic 3D insertions\nwithout relying on spatial priors, offering a user-friendly and flexible\nediting experience.\n","authors":["Chenxi Li","Weijie Wang","Qiang Li","Bruno Lepri","Nicu Sebe","Weizhi Nie"],"pdf_url":"https://arxiv.org/pdf/2505.01322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01313v1","updated":"2025-05-02T14:39:44Z","published":"2025-05-02T14:39:44Z","title":"A Neural Architecture Search Method using Auxiliary Evaluation Metric\n  based on ResNet Architecture","summary":"  This paper proposes a neural architecture search space using ResNet as a\nframework, with search objectives including parameters for convolution,\npooling, fully connected layers, and connectivity of the residual network. In\naddition to recognition accuracy, this paper uses the loss value on the\nvalidation set as a secondary objective for optimization. The experimental\nresults demonstrate that the search space of this paper together with the\noptimisation approach can find competitive network architectures on the MNIST,\nFashion-MNIST and CIFAR100 datasets.\n","authors":["Shang Wang","Huanrong Tang","Jianquan Ouyang"],"pdf_url":"https://arxiv.org/pdf/2505.01313v1.pdf","comment":"GECCO 2023"},{"id":"http://arxiv.org/abs/2504.09149v3","updated":"2025-05-02T14:16:40Z","published":"2025-04-12T09:28:12Z","title":"MASH: Masked Anchored SpHerical Distances for 3D Shape Representation\n  and Generation","summary":"  We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view\nand parametrized representation of 3D shapes. Inspired by multi-view geometry\nand motivated by the importance of perceptual shape understanding for learning\n3D shapes, MASH represents a 3D shape as a collection of observable local\nsurface patches, each defined by a spherical distance function emanating from\nan anchor point. We further leverage the compactness of spherical harmonics to\nencode the MASH functions, combined with a generalized view cone with a\nparameterized base that masks the spatial extent of the spherical function to\nattain locality. We develop a differentiable optimization algorithm capable of\nconverting any point cloud into a MASH representation accurately approximating\nground-truth surfaces with arbitrary geometry and topology. Extensive\nexperiments demonstrate that MASH is versatile for multiple applications\nincluding surface reconstruction, shape generation, completion, and blending,\nachieving superior performance thanks to its unique representation encompassing\nboth implicit and explicit features.\n","authors":["Changhao Li","Yu Xin","Xiaowei Zhou","Ariel Shamir","Hao Zhang","Ligang Liu","Ruizhen Hu"],"pdf_url":"https://arxiv.org/pdf/2504.09149v3.pdf","comment":"11 pages, 11 figures, SIGGRAPH 2025 Accept - Conference"},{"id":"http://arxiv.org/abs/2505.01267v1","updated":"2025-05-02T13:41:14Z","published":"2025-05-02T13:41:14Z","title":"Diffusion-based Adversarial Purification from the Perspective of the\n  Frequency Domain","summary":"  The diffusion-based adversarial purification methods attempt to drown\nadversarial perturbations into a part of isotropic noise through the forward\nprocess, and then recover the clean images through the reverse process. Due to\nthe lack of distribution information about adversarial perturbations in the\npixel domain, it is often unavoidable to damage normal semantics. We turn to\nthe frequency domain perspective, decomposing the image into amplitude spectrum\nand phase spectrum. We find that for both spectra, the damage caused by\nadversarial perturbations tends to increase monotonically with frequency. This\nmeans that we can extract the content and structural information of the\noriginal clean sample from the frequency components that are less damaged.\nMeanwhile, theoretical analysis indicates that existing purification methods\nindiscriminately damage all frequency components, leading to excessive damage\nto the image. Therefore, we propose a purification method that can eliminate\nadversarial perturbations while maximizing the preservation of the content and\nstructure of the original image. Specifically, at each time step during the\nreverse process, for the amplitude spectrum, we replace the low-frequency\ncomponents of the estimated image's amplitude spectrum with the corresponding\nparts of the adversarial image. For the phase spectrum, we project the phase of\nthe estimated image into a designated range of the adversarial image's phase\nspectrum, focusing on the low frequencies. Empirical evidence from extensive\nexperiments demonstrates that our method significantly outperforms most current\ndefense methods.\n","authors":["Gaozheng Pei","Ke Ma","Yingfei Sun","Qianqian Xu","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2505.01267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01263v1","updated":"2025-05-02T13:30:19Z","published":"2025-05-02T13:30:19Z","title":"FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and\n  Flow Matching based Voice Enhancing","summary":"  Movie Dubbing aims to convert scripts into speeches that align with the given\nmovie clip in both temporal and emotional aspects while preserving the vocal\ntimbre of a given brief reference audio. Existing methods focus primarily on\nreducing the word error rate while ignoring the importance of lip-sync and\nacoustic quality. To address these issues, we propose a large language model\n(LLM) based flow matching architecture for dubbing, named FlowDubber, which\nachieves high-quality audio-visual sync and pronunciation by incorporating a\nlarge speech language model and dual contrastive aligning while achieving\nbetter acoustic quality via the proposed voice-enhanced flow matching than\nprevious works. First, we introduce Qwen2.5 as the backbone of LLM to learn the\nin-context sequence from movie scripts and reference audio. Then, the proposed\nsemantic-aware learning focuses on capturing LLM semantic knowledge at the\nphoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment\nwith lip movement, reducing ambiguities where similar phonemes might be\nconfused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves\nacoustic quality in two aspects, which introduces an LLM-based acoustics flow\nmatching guidance to strengthen clarity and uses affine style prior to enhance\nidentity when recovering noise into mel-spectrograms via gradient vector field\nprediction. Extensive experiments demonstrate that our method outperforms\nseveral state-of-the-art methods on two primary benchmarks. The demos are\navailable at\n{\\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.\n","authors":["Gaoxiang Cong","Liang Li","Jiadong Pan","Zhedong Zhang","Amin Beheshti","Anton van den Hengel","Yuankai Qi","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2505.01263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01257v1","updated":"2025-05-02T13:26:23Z","published":"2025-05-02T13:26:23Z","title":"CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object\n  Tracking","summary":"  Online multi-object tracking has been recently dominated by\ntracking-by-detection (TbD) methods, where recent advances rely on increasingly\nsophisticated heuristics for tracklet representation, feature fusion, and\nmulti-stage matching. The key strength of TbD lies in its modular design,\nenabling the integration of specialized off-the-shelf models like motion\npredictors and re-identification. However, the extensive usage of human-crafted\nrules for temporal associations makes these methods inherently limited in their\nability to capture the complex interplay between various tracking cues. In this\nwork, we introduce CAMEL, a novel association module for Context-Aware\nMulti-Cue ExpLoitation, that learns resilient association strategies directly\nfrom data, breaking free from hand-crafted heuristics while maintaining TbD's\nvaluable modularity. At its core, CAMEL employs two transformer-based modules\nand relies on a novel association-centric training scheme to effectively model\nthe complex interactions between tracked targets and their various association\ncues. Unlike end-to-end detection-by-tracking approaches, our method remains\nlightweight and fast to train while being able to leverage external\noff-the-shelf models. Our proposed online tracking pipeline, CAMELTrack,\nachieves state-of-the-art performance on multiple tracking benchmarks. Our code\nis available at https://github.com/TrackingLaboratory/CAMELTrack.\n","authors":["Vladimir Somers","Baptiste Standaert","Victor Joos","Alexandre Alahi","Christophe De Vleeschouwer"],"pdf_url":"https://arxiv.org/pdf/2505.01257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01249v1","updated":"2025-05-02T13:17:08Z","published":"2025-05-02T13:17:08Z","title":"Fusing Foveal Fixations Using Linear Retinal Transformations and\n  Bayesian Experimental Design","summary":"  Humans (and many vertebrates) face the problem of fusing together multiple\nfixations of a scene in order to obtain a representation of the whole, where\neach fixation uses a high-resolution fovea and decreasing resolution in the\nperiphery. In this paper we explicitly represent the retinal transformation of\na fixation as a linear downsampling of a high-resolution latent image of the\nscene, exploiting the known geometry. This linear transformation allows us to\ncarry out exact inference for the latent variables in factor analysis (FA) and\nmixtures of FA models of the scene. Further, this allows us to formulate and\nsolve the choice of \"where to look next\" as a Bayesian experimental design\nproblem using the Expected Information Gain criterion. Experiments on the Frey\nfaces and MNIST datasets demonstrate the effectiveness of our models.\n","authors":["Christopher K. I. Williams"],"pdf_url":"https://arxiv.org/pdf/2505.01249v1.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.01239v1","updated":"2025-05-02T13:04:01Z","published":"2025-05-02T13:04:01Z","title":"Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in\n  Lung CT Imaging","summary":"  Accurate lung tumor segmentation is crucial for improving diagnosis,\ntreatment planning, and patient outcomes in oncology. However, the complexity\nof tumor morphology, size, and location poses significant challenges for\nautomated segmentation. This study presents a comprehensive benchmarking\nanalysis of deep learning-based segmentation models, comparing traditional\narchitectures such as U-Net and DeepLabV3, self-configuring models like nnUNet,\nand foundation models like MedSAM, and MedSAM~2. Evaluating performance across\ntwo lung tumor segmentation datasets, we assess segmentation accuracy and\ncomputational efficiency under various learning paradigms, including few-shot\nlearning and fine-tuning. The results reveal that while traditional models\nstruggle with tumor delineation, foundation models, particularly MedSAM~2,\noutperform them in both accuracy and computational efficiency. These findings\nunderscore the potential of foundation models for lung tumor segmentation,\nhighlighting their applicability in improving clinical workflows and patient\noutcomes.\n","authors":["Elena Mulero Ayllón","Massimiliano Mantegna","Linlin Shen","Paolo Soda","Valerio Guarrasi","Matteo Tortora"],"pdf_url":"https://arxiv.org/pdf/2505.01239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01237v1","updated":"2025-05-02T12:59:58Z","published":"2025-05-02T12:59:58Z","title":"CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via\n  Fine-Grained Alignment","summary":"  Recent advances in audio-visual learning have shown promising results in\nlearning representations across modalities. However, most approaches rely on\nglobal audio representations that fail to capture fine-grained temporal\ncorrespondences with visual frames. Additionally, existing methods often\nstruggle with conflicting optimization objectives when trying to jointly learn\nreconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync\nas a simple yet effective extension of the original CAV-MAE framework for\nself-supervised audio-visual learning. We address three key challenges: First,\nwe tackle the granularity mismatch between modalities by treating audio as a\ntemporal sequence aligned with video frames, rather than using global\nrepresentations. Second, we resolve conflicting optimization goals by\nseparating contrastive and reconstruction objectives through dedicated global\ntokens. Third, we improve spatial localization by introducing learnable\nregister tokens that reduce semantic load on patch tokens. We evaluate the\nproposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on\nzero-shot retrieval, classification and localization tasks demonstrating\nstate-of-the-art performance and outperforming more complex architectures.\n","authors":["Edson Araujo","Andrew Rouditchenko","Yuan Gong","Saurabhchand Bhati","Samuel Thomas","Brian Kingsbury","Leonid Karlinsky","Rogerio Feris","James R. Glass"],"pdf_url":"https://arxiv.org/pdf/2505.01237v1.pdf","comment":"To be published at CVPR 2025, code available at\n  https://github.com/edsonroteia/cav-mae-sync"},{"id":"http://arxiv.org/abs/2505.01235v1","updated":"2025-05-02T12:50:24Z","published":"2025-05-02T12:50:24Z","title":"Compensating Spatiotemporally Inconsistent Observations for Online\n  Dynamic 3D Gaussian Splatting","summary":"  Online reconstruction of dynamic scenes is significant as it enables learning\nscenes from live-streaming video inputs, while existing offline dynamic\nreconstruction methods rely on recorded video inputs. However, previous online\nreconstruction approaches have primarily focused on efficiency and rendering\nquality, overlooking the temporal consistency of their results, which often\ncontain noticeable artifacts in static regions. This paper identifies that\nerrors such as noise in real-world recordings affect temporal inconsistency in\nonline reconstruction. We propose a method that enhances temporal consistency\nin online reconstruction from observations with temporal inconsistency which is\ninevitable in cameras. We show that our method restores the ideal observation\nby subtracting the learned error. We demonstrate that applying our method to\nvarious baselines significantly enhances both temporal consistency and\nrendering quality across datasets. Code, video results, and checkpoints are\navailable at https://bbangsik13.github.io/OR2.\n","authors":["Youngsik Yun","Jeongmin Bae","Hyunseung Son","Seoha Kim","Hahyun Lee","Gun Bang","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2505.01235v1.pdf","comment":"SIGGRAPH 2025, Project page: https://bbangsik13.github.io/OR2"},{"id":"http://arxiv.org/abs/2505.01225v1","updated":"2025-05-02T12:22:08Z","published":"2025-05-02T12:22:08Z","title":"Core-Set Selection for Data-efficient Land Cover Segmentation","summary":"  The increasing accessibility of remotely sensed data and the potential of\nsuch data to inform large-scale decision-making has driven the development of\ndeep learning models for many Earth Observation tasks. Traditionally, such\nmodels must be trained on large datasets. However, the common assumption that\nbroadly larger datasets lead to better outcomes tends to overlook the\ncomplexities of the data distribution, the potential for introducing biases and\nnoise, and the computational resources required for processing and storing vast\ndatasets. Therefore, effective solutions should consider both the quantity and\nquality of data. In this paper, we propose six novel core-set selection methods\nfor selecting important subsets of samples from remote sensing image\nsegmentation datasets that rely on imagery only, labels only, and a combination\nof each. We benchmark these approaches against a random-selection baseline on\nthree commonly used land cover classification datasets: DFC2022, Vaihingen, and\nPotsdam. In each of the datasets, we demonstrate that training on a subset of\nsamples outperforms the random baseline, and some approaches outperform\ntraining on all available data. This result shows the importance and potential\nof data-centric learning for the remote sensing domain. The code is available\nat https://github.com/keillernogueira/data-centric-rs-classification/.\n","authors":["Keiller Nogueira","Akram Zaytar","Wanli Ma","Ribana Roscher","Ronny Hänsch","Caleb Robinson","Anthony Ortiz","Simone Nsutezo","Rahul Dodhia","Juan M. Lavista Ferres","Oktay Karakuş","Paul L. Rosin"],"pdf_url":"https://arxiv.org/pdf/2505.01225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01224v1","updated":"2025-05-02T12:21:44Z","published":"2025-05-02T12:21:44Z","title":"RD-UIE: Relation-Driven State Space Modeling for Underwater Image\n  Enhancement","summary":"  Underwater image enhancement (UIE) is a critical preprocessing step for\nmarine vision applications, where wavelength-dependent attenuation causes\nsevere content degradation and color distortion. While recent state space\nmodels like Mamba show potential for long-range dependency modeling, their\nunfolding operations and fixed scan paths on 1D sequences fail to adapt to\nlocal object semantics and global relation modeling, limiting their efficacy in\ncomplex underwater environments. To address this, we enhance conventional Mamba\nwith the sorting-based scanning mechanism that dynamically reorders scanning\nsequences based on statistical distribution of spatial correlation of all\npixels. In this way, it encourages the network to prioritize the most\ninformative components--structural and semantic features. Upon building this\nmechanism, we devise a Visually Self-adaptive State Block (VSSB) that\nharmonizes dynamic sorting of Mamba with input-dependent dynamic convolution,\nenabling coherent integration of global context and local relational cues. This\nexquisite design helps eliminate global focus bias, especially for widely\ndistributed contents, which greatly weakens the statistical frequency. For\nrobust feature extraction and refinement, we design a cross-feature bridge\n(CFB) to adaptively fuse multi-scale representations. These efforts compose the\nnovel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive\nexperiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms\nthe state-of-the-art approach WMamba in both quantitative metrics and visual\nfidelity, averagely achieving 0.55 dB performance gain on the three benchmarks.\nOur code is available at https://github.com/kkoucy/RD-UIE/tree/main\n","authors":["Kui Jiang","Yan Luo","Junjun Jiang","Xin Xu","Fei Ma","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2505.01224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01212v1","updated":"2025-05-02T12:04:38Z","published":"2025-05-02T12:04:38Z","title":"High Dynamic Range Novel View Synthesis with Single Exposure","summary":"  High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D\nscene HDR model from Low Dynamic Range (LDR) imagery. Typically,\nmultiple-exposure LDR images are employed to capture a wider range of\nbrightness levels in a scene, as a single LDR image cannot represent both the\nbrightest and darkest regions simultaneously. While effective, this\nmultiple-exposure HDR-NVS approach has significant limitations, including\nsusceptibility to motion artifacts (e.g., ghosting and blurring), high capture\nand storage costs. To overcome these challenges, we introduce, for the first\ntime, the single-exposure HDR-NVS problem, where only single exposure LDR\nimages are available during training. We further introduce a novel approach,\nMono-HDR-3D, featuring two dedicated modules formulated by the LDR image\nformation principles, one for converting LDR colors to HDR counterparts, and\nthe other for transforming HDR images to LDR format so that unsupervised\nlearning is enabled in a closed loop. Designed as a meta-algorithm, our\napproach can be seamlessly integrated with existing NVS models. Extensive\nexperiments show that Mono-HDR-3D significantly outperforms previous methods.\nSource code will be released.\n","authors":["Kaixuan Zhang","Hu Wang","Minxian Li","Mingwu Ren","Mao Ye","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.01212v1.pdf","comment":"It has been accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.01207v1","updated":"2025-05-02T11:50:48Z","published":"2025-05-02T11:50:48Z","title":"T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise\n  Translation Graph","summary":"  Sparse-view camera pose estimation, which aims to estimate the\n6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from\ndifferent viewpoints, is a fundamental yet challenging problem in remote\nsensing applications. Existing methods often overlook the translation\ninformation between each pair of viewpoints, leading to suboptimal performance\nin sparse-view scenarios. To address this limitation, we introduce T-Graph, a\nlightweight, plug-and-play module to enhance camera pose estimation in\nsparse-view settings. T-graph takes paired image features as input and maps\nthem through a Multilayer Perceptron (MLP). It then constructs a fully\nconnected translation graph, where nodes represent cameras and edges encode\ntheir translation relationships. It can be seamlessly integrated into existing\nmodels as an additional branch in parallel with the original prediction,\nmaintaining efficiency and ease of use. Furthermore, we introduce two pairwise\ntranslation representations, relative-t and pair-t, formulated under different\nlocal coordinate systems. While relative-t captures intuitive spatial\nrelationships, pair-t offers a rotation-disentangled alternative. The two\nrepresentations contribute to enhanced adaptability across diverse application\nscenarios, further improving our module's robustness. Extensive experiments on\ntwo state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D\nand IMC PhotoTourism) validate both the effectiveness and generalizability of\nT-Graph. The results demonstrate consistent improvements across various\nmetrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8\nviewpoints.\n","authors":["Qingyu Xian","Weiqin Jiao","Hao Cheng","Berend Jan van der Zwaag","Yanqiu Huang"],"pdf_url":"https://arxiv.org/pdf/2505.01207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01203v1","updated":"2025-05-02T11:48:11Z","published":"2025-05-02T11:48:11Z","title":"Efficient Vision-based Vehicle Speed Estimation","summary":"  This paper presents a computationally efficient method for vehicle speed\nestimation from traffic camera footage. Building upon previous work that\nutilizes 3D bounding boxes derived from 2D detections and vanishing point\ngeometry, we introduce several improvements to enhance real-time performance.\nWe evaluate our method in several variants on the BrnoCompSpeed dataset in\nterms of vehicle detection and speed estimation accuracy. Our extensive\nevaluation across various hardware platforms, including edge devices,\ndemonstrates significant gains in frames per second (FPS) compared to the prior\nstate-of-the-art, while maintaining comparable or improved speed estimation\naccuracy. We analyze the trade-off between accuracy and computational cost,\nshowing that smaller models utilizing post-training quantization offer the best\nbalance for real-world deployment. Our best performing model beats previous\nstate-of-the-art in terms of median vehicle speed estimation error (0.58 km/h\nvs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs.\n83.32%) while also being 5.5 times faster.\n","authors":["Andrej Macko","Lukáš Gajdošech","Viktor Kocur"],"pdf_url":"https://arxiv.org/pdf/2505.01203v1.pdf","comment":"Submitted to Journal of Real-Time Image Processing (JRTIP)"},{"id":"http://arxiv.org/abs/2410.01723v4","updated":"2025-05-02T11:29:31Z","published":"2024-10-02T16:34:29Z","title":"HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration","summary":"  Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.\n","authors":["Yushi Huang","Zining Wang","Ruihao Gong","Jing Liu","Xinjie Zhang","Jinyang Guo","Xianglong Liu","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01723v4.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2504.11485v2","updated":"2025-05-02T11:28:54Z","published":"2025-04-14T07:20:21Z","title":"Deciphering scrolls with tomography: A training experiment","summary":"  The recovery of severely damaged ancient written documents has proven to be a\nmajor challenge for many scientists, mainly due to the impracticality of\nphysical unwrapping them. Non-destructive techniques, such as X-ray computed\ntomography (CT), combined with computer vision algorithms, have emerged as a\nmeans of facilitating the virtual reading of the hidden contents of the damaged\ndocuments. This paper proposes an educational laboratory aimed at simulating\nthe entire process of acquisition and virtual recovery of the ancient works. We\nhave developed an experimental setup that uses visible light to replace the\ndetrimental X-rays, and a didactic software pipeline that allows students to\nvirtually reconstruct a transparent rolled sheet with printed text on it, the\nwrapped scroll.\n","authors":["Sonia Foschiatti","Axel Kittenberger","Otmar Scherzer"],"pdf_url":"https://arxiv.org/pdf/2504.11485v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01182v1","updated":"2025-05-02T10:50:04Z","published":"2025-05-02T10:50:04Z","title":"TSTMotion: Training-free Scene-awarenText-to-motion Generation","summary":"  Text-to-motion generation has recently garnered significant research\ninterest, primarily focusing on generating human motion sequences in blank\nbackgrounds. However, human motions commonly occur within diverse 3D scenes,\nwhich has prompted exploration into scene-aware text-to-motion generation\nmethods. Yet, existing scene-aware methods often rely on large-scale\nground-truth motion sequences in diverse 3D scenes, which poses practical\nchallenges due to the expensive cost. To mitigate this challenge, we are the\nfirst to propose a \\textbf{T}raining-free \\textbf{S}cene-aware\n\\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that\nefficiently empowers pre-trained blank-background motion generators with the\nscene-aware capability. Specifically, conditioned on the given 3D scene and\ntext description, we adopt foundation models together to reason, predict and\nvalidate a scene-aware motion guidance. Then, the motion guidance is\nincorporated into the blank-background motion generators with two\nmodifications, resulting in scene-aware text-driven motion sequences. Extensive\nexperiments demonstrate the efficacy and generalizability of our proposed\nframework. We release our code in \\href{https://tstmotion.github.io/}{Project\nPage}.\n","authors":["Ziyan Guo","Haoxuan Qu","Hossein Rahmani","Dewen Soh","Ping Hu","Qiuhong Ke","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2505.01182v1.pdf","comment":"Accepted by ICME2025"},{"id":"http://arxiv.org/abs/2505.01172v1","updated":"2025-05-02T10:27:58Z","published":"2025-05-02T10:27:58Z","title":"FreePCA: Integrating Consistency Information across Long-short Frames in\n  Training-free Long Video Generation via Principal Component Analysis","summary":"  Long video generation involves generating extended videos using models\ntrained on short videos, suffering from distribution shifts due to varying\nframe counts. It necessitates the use of local information from the original\nshort frames to enhance visual and motion quality, and global information from\nthe entire long frames to ensure appearance consistency. Existing training-free\nmethods struggle to effectively integrate the benefits of both, as appearance\nand motion in videos are closely coupled, leading to motion inconsistency and\nvisual quality. In this paper, we reveal that global and local information can\nbe precisely decoupled into consistent appearance and motion intensity\ninformation by applying Principal Component Analysis (PCA), allowing for\nrefined complementary integration of global consistency and local quality. With\nthis insight, we propose FreePCA, a training-free long video generation\nparadigm based on PCA that simultaneously achieves high consistency and\nquality. Concretely, we decouple consistent appearance and motion intensity\nfeatures by measuring cosine similarity in the principal component space.\nCritically, we progressively integrate these features to preserve original\nquality and ensure smooth transitions, while further enhancing consistency by\nreusing the mean statistics of the initial noise. Experiments demonstrate that\nFreePCA can be applied to various video diffusion models without requiring\ntraining, leading to substantial improvements. Code is available at\nhttps://github.com/JosephTiTan/FreePCA.\n","authors":["Jiangtong Tan","Hu Yu","Jie Huang","Jie Xiao","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.01172v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2412.06204v2","updated":"2025-05-02T09:57:16Z","published":"2024-12-09T04:55:18Z","title":"You KAN Do It in a Single Shot: Plug-and-Play Methods with\n  Single-Instance Priors","summary":"  The use of Plug-and-Play (PnP) methods has become a central approach for\nsolving inverse problems, with denoisers serving as regularising priors that\nguide optimisation towards a clean solution. In this work, we introduce\nKAN-PnP, an optimisation framework that incorporates Kolmogorov-Arnold Networks\n(KANs) as denoisers within the Plug-and-Play (PnP) paradigm. KAN-PnP is\nspecifically designed to solve inverse problems with single-instance priors,\nwhere only a single noisy observation is available, eliminating the need for\nlarge datasets typically required by traditional denoising methods. We show\nthat KANs, based on the Kolmogorov-Arnold representation theorem, serve\neffectively as priors in such settings, providing a robust approach to\ndenoising. We prove that the KAN denoiser is Lipschitz continuous, ensuring\nstability and convergence in optimisation algorithms like PnP-ADMM, even in the\ncontext of single-shot learning. Additionally, we provide theoretical\nguarantees for KAN-PnP, demonstrating its convergence under key conditions: the\nconvexity of the data fidelity term, Lipschitz continuity of the denoiser, and\nboundedness of the regularisation functional. These conditions are crucial for\nstable and reliable optimisation. Our experimental results show, on\nsuper-resolution and joint optimisation, that KAN-PnP outperforms exiting\nmethods, delivering superior performance in single-shot learning with minimal\ndata. The method exhibits strong convergence properties, achieving high\naccuracy with fewer iterations.\n","authors":["Yanqi Cheng","Carola-Bibiane Schönlieb","Angelica I Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2412.06204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14331v3","updated":"2025-05-02T09:17:40Z","published":"2025-03-18T15:03:28Z","title":"ADAPT: An Autonomous Forklift for Construction Site Operation","summary":"  Efficient material logistics play a critical role in controlling costs and\nschedules in the construction industry. However, manual material handling\nremains prone to inefficiencies, delays, and safety risks. Autonomous forklifts\noffer a promising solution to streamline on-site logistics, reducing reliance\non human operators and mitigating labor shortages. This paper presents the\ndevelopment and evaluation of ADAPT (Autonomous Dynamic All-terrain Pallet\nTransporter), a fully autonomous off-road forklift designed for construction\nenvironments. Unlike structured warehouse settings, construction sites pose\nsignificant challenges, including dynamic obstacles, unstructured terrain, and\nvarying weather conditions. To address these challenges, our system integrates\nAI-driven perception techniques with traditional approaches for decision\nmaking, planning, and control, enabling reliable operation in complex\nenvironments. We validate the system through extensive real-world testing,\ncomparing its continuous performance against an experienced human operator\nacross various weather conditions. Our findings demonstrate that autonomous\noutdoor forklifts can operate near human-level performance, offering a viable\npath toward safer and more efficient construction logistics.\n","authors":["Johannes Huemer","Markus Murschitz","Matthias Schörghuber","Lukas Reisinger","Thomas Kadiofsky","Christoph Weidinger","Mario Niedermeyer","Benedikt Widy","Marcel Zeilinger","Csaba Beleznai","Tobias Glück","Andreas Kugi","Patrik Zips"],"pdf_url":"https://arxiv.org/pdf/2503.14331v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06126v3","updated":"2025-05-02T08:48:37Z","published":"2024-10-08T15:28:33Z","title":"$X^2$-DFD: A framework for eXplainable and eXtendable Deepfake Detection","summary":"  Detecting deepfakes has become an important task. Most existing detection\nmethods provide only real/fake predictions without offering\nhuman-comprehensible explanations. Recent studies leveraging MLLMs for deepfake\ndetection have shown improvements in explainability. However, the performance\nof pre-trained MLLMs (e.g., LLaVA) remains limited due to a lack of\nunderstanding of their capabilities for this task and strategies to enhance\nthem. In this work, we empirically assess the strengths and weaknesses of MLLMs\nspecifically in deepfake detection via forgery features analysis. Building on\nthese assessments, we propose a novel framework called ${X}^2$-DFD, consisting\nof three core modules. The first module, Model Feature Assessment (MFA),\nmeasures the detection capabilities of forgery features intrinsic to MLLMs, and\ngives a descending ranking of these features. The second module, Strong Feature\nStrengthening (SFS), enhances the detection and explanation capabilities by\nfine-tuning the MLLM on a dataset constructed based on the top-ranked features.\nThe third module, Weak Feature Supplementing (WFS), improves the fine-tuned\nMLLM's capabilities on lower-ranked features by integrating external dedicated\ndeepfake detectors. To verify the effectiveness of this framework, we further\npresent a practical implementation, where an automated forgery features\ngeneration, evaluation, and ranking procedure is designed for MFA module; an\nautomated generation procedure of the fine-tuning dataset containing real and\nfake images with explanations based on top-ranked features is developed for SFS\nmodel; an external conventional deepfake detector focusing on blending\nartifact, which corresponds to a low detection capability in the pre-trained\nMLLM, is integrated for WFS module. Experiments show that our approach enhances\nboth detection and explanation performance.\n","authors":["Yize Chen","Zhiyuan Yan","Siwei Lyu","Baoyuan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.06126v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01113v1","updated":"2025-05-02T08:47:31Z","published":"2025-05-02T08:47:31Z","title":"NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization","summary":"  Recently, camera localization has been widely adopted in autonomous robotic\nnavigation due to its efficiency and convenience. However, autonomous\nnavigation in unknown environments often suffers from scene ambiguity,\nenvironmental disturbances, and dynamic object transformation in camera\nlocalization. To address this problem, inspired by the biological brain\nnavigation mechanism (such as grid cells, place cells, and head direction\ncells), we propose a novel neurobiological camera location method, namely\nNeuroLoc. Firstly, we designed a Hebbian learning module driven by place cells\nto save and replay historical information, aiming to restore the details of\nhistorical representations and solve the issue of scene fuzziness. Secondly, we\nutilized the head direction cell-inspired internal direction learning as\nmulti-head attention embedding to help restore the true orientation in similar\nscenes. Finally, we added a 3D grid center prediction in the pose regression\nmodule to reduce the final wrong prediction. We evaluate the proposed NeuroLoc\non commonly used benchmark indoor and outdoor datasets. The experimental\nresults show that our NeuroLoc can enhance the robustness in complex\nenvironments and improve the performance of pose regression by using only a\nsingle image.\n","authors":["Xun Li","Jian Yang","Fenli Jia","Muyu Wang","Qi Wu","Jun Wu","Jinpeng Mi","Jilin Hu","Peidong Liang","Xuan Tang","Ke Li","Xiong You","Xian Wei"],"pdf_url":"https://arxiv.org/pdf/2505.01113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01109v1","updated":"2025-05-02T08:43:50Z","published":"2025-05-02T08:43:50Z","title":"Self-Supervision Enhances Instance-based Multiple Instance Learning\n  Methods in Digital Pathology: A Benchmark Study","summary":"  Multiple Instance Learning (MIL) has emerged as the best solution for Whole\nSlide Image (WSI) classification. It consists of dividing each slide into\npatches, which are treated as a bag of instances labeled with a global label.\nMIL includes two main approaches: instance-based and embedding-based. In the\nformer, each patch is classified independently, and then the patch scores are\naggregated to predict the bag label. In the latter, bag classification is\nperformed after aggregating patch embeddings. Even if instance-based methods\nare naturally more interpretable, embedding-based MILs have usually been\npreferred in the past due to their robustness to poor feature extractors.\nHowever, recently, the quality of feature embeddings has drastically increased\nusing self-supervised learning (SSL). Nevertheless, many authors continue to\nendorse the superiority of embedding-based MIL. To investigate this further, we\nconduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6\nself-supervised methods with 4 backbones, 4 foundation models, and various\npathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL\nmethods never used before in the pathology domain. Through these extensive\nexperiments, we show that with a good SSL feature extractor, simple\ninstance-based MILs, with very few parameters, obtain similar or better\nperformance than complex, state-of-the-art (SOTA) embedding-based MIL methods,\nsetting new SOTA results on the BRACS and Camelyon16 datasets. Since simple\ninstance-based MIL methods are naturally more interpretable and explainable to\nclinicians, our results suggest that more effort should be put into\nwell-adapted SSL methods for WSI rather than into complex embedding-based MIL\nmethods.\n","authors":["Ali Mammadov","Loic Le Folgoc","Julien Adam","Anne Buronfosse","Gilles Hayem","Guillaume Hocquet","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2505.01109v1.pdf","comment":"Accepted for publication in the Journal of Medical Imaging (SPIE)"},{"id":"http://arxiv.org/abs/2504.18317v3","updated":"2025-05-02T08:32:14Z","published":"2025-04-25T12:49:14Z","title":"Task-Oriented Communications for Visual Navigation with Edge-Aerial\n  Collaboration in Low Altitude Economy","summary":"  To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles\n(UAVs) localization in urban areas where global positioning system (GPS)\nsignals are unavailable. Vision-based methods offer a viable alternative but\nface severe bandwidth, memory and processing constraints on lightweight UAVs.\nInspired by mammalian spatial cognition, we propose a task-oriented\ncommunication framework, where UAVs equipped with multi-camera systems extract\ncompact multi-view features and offload localization tasks to edge servers. We\nintroduce the Orthogonally-constrained Variational Information Bottleneck\nencoder (O-VIB), which incorporates automatic relevance determination (ARD) to\nprune non-informative features while enforcing orthogonality to minimize\nredundancy. This enables efficient and accurate localization with minimal\ntransmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows\nthat O-VIB achieves high-precision localization under stringent bandwidth\nbudgets. Code and dataset will be made publicly available:\ngithub.com/fangzr/TOC-Edge-Aerial.\n","authors":["Zhengru Fang","Zhenghao Liu","Jingjing Wang","Senkang Hu","Yu Guo","Yiqin Deng","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2504.18317v3.pdf","comment":"Code and dataset will be made publicly available:\n  https://github.com/fangzr/TOC-Edge-Aerial"},{"id":"http://arxiv.org/abs/2505.01104v1","updated":"2025-05-02T08:31:43Z","published":"2025-05-02T08:31:43Z","title":"VSC: Visual Search Compositional Text-to-Image Diffusion Model","summary":"  Text-to-image diffusion models have shown impressive capabilities in\ngenerating realistic visuals from natural-language prompts, yet they often\nstruggle with accurately binding attributes to corresponding objects,\nespecially in prompts containing multiple attribute-object pairs. This\nchallenge primarily arises from the limitations of commonly used text encoders,\nsuch as CLIP, which can fail to encode complex linguistic relationships and\nmodifiers effectively. Existing approaches have attempted to mitigate these\nissues through attention map control during inference and the use of layout\ninformation or fine-tuning during training, yet they face performance drops\nwith increased prompt complexity. In this work, we introduce a novel\ncompositional generation method that leverages pairwise image embeddings to\nimprove attribute-object binding. Our approach decomposes complex prompts into\nsub-prompts, generates corresponding images, and computes visual prototypes\nthat fuse with text embeddings to enhance representation. By applying\nsegmentation-based localization training, we address cross-attention\nmisalignment, achieving improved accuracy in binding multiple attributes to\nobjects. Our approaches outperform existing compositional text-to-image\ndiffusion models on the benchmark T2I CompBench, achieving better image\nquality, evaluated by humans, and emerging robustness under scaling number of\nbinding pairs in the prompt.\n","authors":["Do Huu Dat","Nam Hyeonu","Po-Yuan Mao","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2505.01104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01096v1","updated":"2025-05-02T08:14:03Z","published":"2025-05-02T08:14:03Z","title":"Evaluating Vision Language Model Adaptations for Radiology Report\n  Generation in Low-Resource Languages","summary":"  The integration of artificial intelligence in healthcare has opened new\nhorizons for improving medical diagnostics and patient care. However,\nchallenges persist in developing systems capable of generating accurate and\ncontextually relevant radiology reports, particularly in low-resource\nlanguages. In this study, we present a comprehensive benchmark to evaluate the\nperformance of instruction-tuned Vision-Language Models (VLMs) in the\nspecialized task of radiology report generation across three low-resource\nlanguages: Italian, German, and Spanish. Employing the LLaVA architectural\nframework, we conducted a systematic evaluation of pre-trained models utilizing\ngeneral datasets, domain-specific datasets, and low-resource language-specific\ndatasets. In light of the unavailability of models that possess prior knowledge\nof both the medical domain and low-resource languages, we analyzed various\nadaptations to determine the most effective approach for these contexts. The\nresults revealed that language-specific models substantially outperformed both\ngeneral and domain-specific models in generating radiology reports, emphasizing\nthe critical role of linguistic adaptation. Additionally, models fine-tuned\nwith medical terminology exhibited enhanced performance across all languages\ncompared to models with generic knowledge, highlighting the importance of\ndomain-specific training. We also explored the influence of the temperature\nparameter on the coherence of report generation, providing insights for optimal\nmodel settings. Our findings highlight the importance of tailored language and\ndomain-specific training for improving the quality and accuracy of radiological\nreports in multilingual settings. This research not only advances our\nunderstanding of VLMs adaptability in healthcare but also points to significant\navenues for future investigations into model tuning and language-specific\nadaptations.\n","authors":["Marco Salmè","Rosa Sicilia","Paolo Soda","Valerio Guarrasi"],"pdf_url":"https://arxiv.org/pdf/2505.01096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01091v1","updated":"2025-05-02T08:07:24Z","published":"2025-05-02T08:07:24Z","title":"Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and\n  Radiological Report Generation","summary":"  Generative models have revolutionized Artificial Intelligence (AI),\nparticularly in multimodal applications. However, adapting these models to the\nmedical domain poses unique challenges due to the complexity of medical data\nand the stringent need for clinical accuracy. In this work, we introduce a\nframework specifically designed for multimodal medical data generation. By\nenabling the generation of multi-view chest X-rays and their associated\nclinical report, it bridges the gap between general-purpose vision-language\nmodels and the specialized requirements of healthcare. Leveraging the MIMIC-CXR\ndataset, the proposed framework shows superior performance in generating\nhigh-fidelity images and semantically coherent reports. Our quantitative\nevaluation reveals significant results in terms of FID and BLEU scores,\nshowcasing the quality of the generated data. Notably, our framework achieves\ncomparable or even superior performance compared to real data on downstream\ndisease classification tasks, underlining its potential as a tool for medical\nresearch and diagnostics. This study highlights the importance of\ndomain-specific adaptations in enhancing the relevance and utility of\ngenerative models for clinical applications, paving the way for future\nadvancements in synthetic multimodal medical data generation.\n","authors":["Daniele Molino","Francesco di Feola","Linlin Shen","Paolo Soda","Valerio Guarrasi"],"pdf_url":"https://arxiv.org/pdf/2505.01091v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2501.04614"},{"id":"http://arxiv.org/abs/2505.00568v2","updated":"2025-05-02T08:02:39Z","published":"2025-05-01T14:51:30Z","title":"Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor\n  Analysis with Missing Modalities","summary":"  Multimodal magnetic resonance imaging (MRI) constitutes the first line of\ninvestigation for clinicians in the care of brain tumors, providing crucial\ninsights for surgery planning, treatment monitoring, and biomarker\nidentification. Pre-training on large datasets have been shown to help models\nlearn transferable representations and adapt with minimal labeled data. This\nbehavior is especially valuable in medical imaging, where annotations are often\nscarce. However, applying this paradigm to multimodal medical data introduces a\nchallenge: most existing approaches assume that all imaging modalities are\navailable during both pre-training and fine-tuning. In practice, missing\nmodalities often occur due to acquisition issues, specialist unavailability, or\nspecific experimental designs on small in-house datasets. Consequently, a\ncommon approach involves training a separate model for each desired modality\ncombination, making the process both resource-intensive and impractical for\nclinical use. Therefore, we introduce BM-MAE, a masked image modeling\npre-training strategy tailored for multimodal MRI data. The same pre-trained\nmodel seamlessly adapts to any combination of available modalities, extracting\nrich representations that capture both intra- and inter-modal information. This\nallows fine-tuning on any subset of modalities without requiring architectural\nchanges, while still benefiting from a model pre-trained on the full set of\nmodalities. Extensive experiments show that the proposed pre-training strategy\noutperforms or remains competitive with baselines that require separate\npre-training for each modality subset, while substantially surpassing training\nfrom scratch on several downstream tasks. Additionally, it can quickly and\nefficiently reconstruct missing modalities, highlighting its practical value.\nCode and trained models are available at: https://github.com/Lucas-rbnt/BM-MAE\n","authors":["Lucas Robinet","Ahmad Berjaoui","Elizabeth Cohen-Jonathan Moyal"],"pdf_url":"https://arxiv.org/pdf/2505.00568v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21331v2","updated":"2025-05-02T07:38:57Z","published":"2025-04-30T05:36:31Z","title":"Towards Space Group Determination from EBSD Patterns: The Role of Deep\n  Learning and High-throughput Dynamical Simulations","summary":"  The design of novel materials hinges on the understanding of\nstructure-property relationships. However, in recent times, our capability to\nsynthesize a large number of materials has outpaced our speed at characterizing\nthem. While the overall chemical constituents can be readily known during\nsynthesis, the structural evolution and characterization of newly synthesized\nsamples remains a bottleneck for the ultimate goal of high throughput\nnanomaterials discovery. Thus, scalable methods for crystal symmetry\ndetermination that can analyze a large volume of material samples within a\nshort time-frame are especially needed. Kikuchi diffraction in the SEM is a\npromising technique for this due to its sensitivity to dynamical scattering,\nwhich may provide information beyond just the seven crystal systems and\nfourteen Bravais lattices. After diffraction patterns are collected from\nmaterial samples, deep learning methods may be able to classify the space group\nsymmetries using the patterns as input, which paired with the elemental\ncomposition, would help enable the determination of the crystal structure. To\ninvestigate the feasibility of this solution, neural networks were trained to\npredict the space group type of background corrected EBSD patterns. Our\nnetworks were first trained and tested on an artificial dataset of EBSD\npatterns of 5,148 different cubic phases, created through physics-based\ndynamical simulations. Next, Maximum Classifier Discrepancy, an unsupervised\ndeep learning-based domain adaptation method, was utilized to train neural\nnetworks to make predictions for experimental EBSD patterns. We introduce a\nrelabeling scheme, which enables our models to achieve accuracy scores higher\nthan 90% on simulated and experimental data, suggesting that neural networks\nare capable of making predictions of crystal symmetry from an EBSD pattern.\n","authors":["Alfred Yan","Muhammad Nur Talha Kilic","Gert Nolze","Ankit Agrawal","Alok Choudhary","Roberto dos Reis","Vinayak Dravid"],"pdf_url":"https://arxiv.org/pdf/2504.21331v2.pdf","comment":"33 pages, preliminary version"},{"id":"http://arxiv.org/abs/2505.01079v1","updated":"2025-05-02T07:36:49Z","published":"2025-05-02T07:36:49Z","title":"Improving Editability in Image Generation with Layer-wise Memory","summary":"  Most real-world image editing tasks require multiple sequential edits to\nachieve desired results. Current editing approaches, primarily designed for\nsingle-object modifications, struggle with sequential editing: especially with\nmaintaining previous edits along with adapting new objects naturally into the\nexisting content. These limitations significantly hinder complex editing\nscenarios where multiple objects need to be modified while preserving their\ncontextual relationships. We address this fundamental challenge through two key\nproposals: enabling rough mask inputs that preserve existing content while\nnaturally integrating new elements and supporting consistent editing across\nmultiple modifications. Our framework achieves this through layer-wise memory,\nwhich stores latent representations and prompt embeddings from previous edits.\nWe propose Background Consistency Guidance that leverages memorized latents to\nmaintain scene coherence and Multi-Query Disentanglement in cross-attention\nthat ensures natural adaptation to existing content. To evaluate our method, we\npresent a new benchmark dataset incorporating semantic alignment metrics and\ninteractive editing scenarios. Through comprehensive experiments, we\ndemonstrate superior performance in iterative image editing tasks with minimal\nuser effort, requiring only rough masks while maintaining high-quality results\nthroughout multiple editing steps.\n","authors":["Daneul Kim","Jaeah Lee","Jaesik Park"],"pdf_url":"https://arxiv.org/pdf/2505.01079v1.pdf","comment":"CVPR 2025. Project page :\n  https://carpedkm.github.io/projects/improving_edit/index.html"},{"id":"http://arxiv.org/abs/2505.00512v2","updated":"2025-05-02T07:20:07Z","published":"2025-05-01T13:30:28Z","title":"InterLoc: LiDAR-based Intersection Localization using Road Segmentation\n  with Automated Evaluation Method","summary":"  Online localization of road intersections is beneficial for autonomous\nvehicle localization, mapping and motion planning. Intersections offer strong\nlandmarks to correct vehicle pose estimation in GNSS dropouts and anchor new\nsensor data in up-to-date maps. They are also decisive routing nodes in road\nnetwork graphs. Despite that importance, intersection localization has not been\nwidely studied, with existing methods either ignore the rich semantic\ninformation already computed onboard or depend on scarce, hand-labeled\nintersection datasets. To close that gap, this paper presents a LiDAR-based\nmethod for online vehicle-centric intersection localization. We fuse semantic\nroad segmentation with vehicle local pose to detect intersection candidates in\na bird's eye view (BEV) representation. We then refine those candidates by\nanalyzing branch topology and correcting the intersection point in a least\nsquares formulation. To evaluate our method, we introduce an automated\nbenchmarking pipeline that pairs localized intersection points with\nOpenStreetMap (OSM) intersection nodes using precise GNSS/INS ground-truth\nposes. Experiments on SemanticKITTI show that the method outperforms the latest\nlearning-based baseline in accuracy and reliability. Moreover, sensitivity\ntests demonstrate that our method is robust to challenging segmentation error\nlevels, highlighting its applicability in the real world.\n","authors":["Nguyen Hoang Khoi Tran","Julie Stephany Berrio","Mao Shan","Zhenxing Ming","Stewart Worrall"],"pdf_url":"https://arxiv.org/pdf/2505.00512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12623v2","updated":"2025-05-02T07:17:44Z","published":"2025-03-16T19:32:32Z","title":"MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network","summary":"  Dynamic emotion recognition in the wild remains challenging due to the\ntransient nature of emotional expressions and temporal misalignment of\nmulti-modal cues. Traditional approaches predict valence and arousal and often\noverlook the inherent correlation between these two dimensions. The proposed\nMulti-modal Attention for Valence-Arousal Emotion Network (MAVEN) integrates\nvisual, audio, and textual modalities through a bi-directional cross-modal\nattention mechanism. MAVEN uses modality-specific encoders to extract features\nfrom synchronized video frames, audio segments, and transcripts, predicting\nemotions in polar coordinates following Russell's circumplex model. The\nevaluation of the Aff-Wild2 dataset using MAVEN achieved a concordance\ncorrelation coefficient (CCC) of 0.3061, surpassing the ResNet-50 baseline\nmodel with a CCC of 0.22. The multistage architecture captures the subtle and\ntransient nature of emotional expressions in conversational videos and improves\nemotion recognition in real-world situations. The code is available at:\nhttps://github.com/Vrushank-Ahire/MAVEN_8th_ABAW\n","authors":["Vrushank Ahire","Kunal Shah","Mudasir Nazir Khan","Nikhil Pakhale","Lownish Rai Sookha","M. A. Ganaie","Abhinav Dhall"],"pdf_url":"https://arxiv.org/pdf/2503.12623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01064v1","updated":"2025-05-02T07:14:58Z","published":"2025-05-02T07:14:58Z","title":"Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of\n  Multimodal LLMs","summary":"  Fine-grained Visual Recognition (FGVR) involves distinguishing between\nvisually similar categories, which is inherently challenging due to subtle\ninter-class differences and the need for large, expert-annotated datasets. In\ndomains like medical imaging, such curated datasets are unavailable due to\nissues like privacy concerns and high annotation costs. In such scenarios\nlacking labeled data, an FGVR model cannot rely on a predefined set of training\nlabels, and hence has an unconstrained output space for predictions. We refer\nto this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict\nlabels from an unconstrained output space without prior label information.\nWhile recent Multimodal Large Language Models (MLLMs) show potential for\nVF-FGVR, querying these models for each test input is impractical because of\nhigh costs and prohibitive inference times. To address these limitations, we\nintroduce \\textbf{Nea}rest-Neighbor Label \\textbf{R}efinement (NeaR), a novel\napproach that fine-tunes a downstream CLIP model using labels generated by an\nMLLM. Our approach constructs a weakly supervised dataset from a small,\nunlabeled training set, leveraging MLLMs for label generation. NeaR is designed\nto handle the noise, stochasticity, and open-endedness inherent in labels\ngenerated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.\n","authors":["Hari Chandana Kuchibhotla","Sai Srinivas Kancheti","Abbavaram Gowtham Reddy","Vineeth N Balasubramanian"],"pdf_url":"https://arxiv.org/pdf/2505.01064v1.pdf","comment":"preprint; earlier version accepted at NeurIPS 2024 Workshop on\n  Adaptive Foundation Models"},{"id":"http://arxiv.org/abs/2505.01057v1","updated":"2025-05-02T07:07:00Z","published":"2025-05-02T07:07:00Z","title":"GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual\n  Feature Extraction in Image Segmentation","summary":"  This paper introduces GeloVec, a new CNN-based attention smoothing framework\nfor semantic segmentation that addresses critical limitations in conventional\napproaches. While existing attention-backed segmentation methods suffer from\nboundary instability and contextual discontinuities during feature mapping, our\nframework implements a higher-dimensional geometric smoothing method to\nestablish a robust manifold relationships between visually coherent regions.\nGeloVec combines modified Chebyshev distance metrics with multispatial\ntransformations to enhance segmentation accuracy through stabilized feature\nextraction. The core innovation lies in the adaptive sampling weights system\nthat calculates geometric distances in n-dimensional feature space, achieving\nsuperior edge preservation while maintaining intra-class homogeneity. The\nmultispatial transformation matrix incorporates tensorial projections with\northogonal basis vectors, creating more discriminative feature representations\nwithout sacrificing computational efficiency. Experimental validation across\nmultiple benchmark datasets demonstrates significant improvements in\nsegmentation performance, with mean Intersection over Union (mIoU) gains of\n2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets\nrespectively compared to state-of-the-art methods. GeloVec's mathematical\nfoundation in Riemannian geometry provides theoretical guarantees on\nsegmentation stability. Importantly, our framework maintains computational\nefficiency through parallelized implementation of geodesic transformations and\nexhibits strong generalization capabilities across disciplines due to the\nabsence of information loss during transformations.\n","authors":["Boris Kriuk","Matey Yordanov"],"pdf_url":"https://arxiv.org/pdf/2505.01057v1.pdf","comment":"13 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2505.01050v1","updated":"2025-05-02T06:51:11Z","published":"2025-05-02T06:51:11Z","title":"Transferable Adversarial Attacks on Black-Box Vision-Language Models","summary":"  Vision Large Language Models (VLLMs) are increasingly deployed to offer\nadvanced capabilities on inputs comprising both text and images. While prior\nresearch has shown that adversarial attacks can transfer from open-source to\nproprietary black-box models in text-only and vision-only contexts, the extent\nand effectiveness of such vulnerabilities remain underexplored for VLLMs. We\npresent a comprehensive analysis demonstrating that targeted adversarial\nexamples are highly transferable to widely-used proprietary VLLMs such as\nGPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to\ninduce specific attacker-chosen interpretations of visual information, such as\nmisinterpreting hazardous content as safe, overlooking sensitive or restricted\nmaterial, or generating detailed incorrect responses aligned with the\nattacker's intent. Furthermore, we discover that universal perturbations --\nmodifications applicable to a wide set of images -- can consistently induce\nthese misinterpretations across multiple proprietary VLLMs. Our experimental\nresults on object recognition, visual question answering, and image captioning\nshow that this vulnerability is common across current state-of-the-art models,\nand underscore an urgent need for robust mitigations to ensure the safe and\nsecure deployment of VLLMs.\n","authors":["Kai Hu","Weichen Yu","Li Zhang","Alexander Robey","Andy Zou","Chengming Xu","Haoqi Hu","Matt Fredrikson"],"pdf_url":"https://arxiv.org/pdf/2505.01050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01040v1","updated":"2025-05-02T06:30:21Z","published":"2025-05-02T06:30:21Z","title":"Edge Detection based on Channel Attention and Inter-region Independence\n  Test","summary":"  Existing edge detection methods often suffer from noise amplification and\nexcessive retention of non-salient details, limiting their applicability in\nhigh-precision industrial scenarios. To address these challenges, we propose\nCAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM)\nand Edge Detection via Independence Testing (EDIT). The CAM module adaptively\nenhances discriminative edge features through multi-channel fusion, while the\nEDIT module employs region-wise statistical independence analysis (using\nFisher's exact test and chi-square test) to suppress uncorrelated\nnoise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate\nstate-of-the-art performance. Among the nine comparison algorithms, the\nF-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of\n19.2\\% to 26.5\\% over traditional methods (Canny, CannySR), and better than the\nlatest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations\nfurther reveal a 2.2\\% PSNR improvement under Gaussian noise compared to\nbaseline methods. Qualitative results exhibit cleaner edge maps with reduced\nartifacts, demonstrating its potential for high-precision industrial\napplications.\n","authors":["Ru-yu Yan","Da-Qing Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.01040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00362v2","updated":"2025-05-02T06:25:41Z","published":"2024-08-31T06:18:46Z","title":"UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM","summary":"  Recent advancements in monocular neural depth estimation, particularly those\nachieved by the UniDepth network, have prompted the investigation of\nintegrating UniDepth within a Gaussian splatting framework for monocular SLAM.\nThis study presents UDGS-SLAM, a novel approach that eliminates the necessity\nof RGB-D sensors for depth estimation within Gaussian splatting framework.\nUDGS-SLAM employs statistical filtering to ensure local consistency of the\nestimated depth and jointly optimizes camera trajectory and Gaussian scene\nrepresentation parameters. The proposed method achieves high-fidelity rendered\nimages and low ATERMSE of the camera trajectory. The performance of UDGS-SLAM\nis rigorously evaluated using the TUM RGB-D dataset and benchmarked against\nseveral baseline methods, demonstrating superior performance across various\nscenarios. Additionally, an ablation study is conducted to validate design\nchoices and investigate the impact of different network backbone encoders on\nsystem performance.\n","authors":["Mostafa Mansour","Ahmed Abdelsalam","Ari Happonen","Jari Porras","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2409.00362v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01032v1","updated":"2025-05-02T06:09:32Z","published":"2025-05-02T06:09:32Z","title":"Edge-preserving Image Denoising via Multi-scale Adaptive Statistical\n  Independence Testing","summary":"  Edge detection is crucial in image processing, but existing methods often\nproduce overly detailed edge maps, affecting clarity. Fixed-window statistical\ntesting faces issues like scale mismatch and computational redundancy. To\naddress these, we propose a novel Multi-scale Adaptive Independence\nTesting-based Edge Detection and Denoising (EDD-MAIT), a Multi-scale Adaptive\nStatistical Testing-based edge detection and denoising method that integrates a\nchannel attention mechanism with independence testing. A gradient-driven\nadaptive window strategy adjusts window sizes dynamically, improving detail\npreservation and noise suppression. EDD-MAIT achieves better robustness,\naccuracy, and efficiency, outperforming traditional and learning-based methods\non BSDS500 and BIPED datasets, with improvements in F-score, MSE, PSNR, and\nreduced runtime. It also shows robustness against Gaussian noise, generating\naccurate and clean edge maps in noisy environments.\n","authors":["Ruyu Yan","Da-Qing Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.01032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01016v1","updated":"2025-05-02T05:27:14Z","published":"2025-05-02T05:27:14Z","title":"Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO\n  Performance","summary":"  The success of large pre-trained object detectors hinges on their\nadaptability to diverse downstream tasks. While fine-tuning is the standard\nadaptation method, specializing these models for challenging fine-grained\ndomains necessitates careful consideration of feature granularity. The critical\nquestion remains: how deeply should the pre-trained backbone be fine-tuned to\noptimize for the specialized task without incurring catastrophic forgetting of\nthe original general capabilities? Addressing this, we present a systematic\nempirical study evaluating the impact of fine-tuning depth. We adapt a standard\nYOLOv8n model to a custom, fine-grained fruit detection dataset by\nprogressively unfreezing backbone layers (freeze points at layers 22, 15, and\n10) and training. Performance was rigorously evaluated on both the target fruit\ndataset and, using a dual-head evaluation architecture, on the original COCO\nvalidation set. Our results demonstrate unequivocally that deeper fine-tuning\n(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\\%\nabsolute mAP50) on the fine-grained fruit task compared to only training the\nhead. Strikingly, this significant adaptation and specialization resulted in\nnegligible performance degradation (<0.1\\% absolute mAP difference) on the COCO\nbenchmark across all tested freeze levels. We conclude that adapting\nmid-to-late backbone features is highly effective for fine-grained\nspecialization. Critically, our results demonstrate this adaptation can be\nachieved without the commonly expected penalty of catastrophic forgetting,\npresenting a compelling case for exploring deeper fine-tuning strategies,\nparticularly when targeting complex domains or when maximizing specialized\nperformance is paramount.\n","authors":["Vishal Gandhi","Sagar Gandhi"],"pdf_url":"https://arxiv.org/pdf/2505.01016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06806v2","updated":"2025-05-02T05:24:06Z","published":"2024-11-25T06:49:59Z","title":"A Physics-Inspired Deep Learning Framework with Polar Coordinate\n  Attention for Ptychographic Imaging","summary":"  Ptychographic imaging confronts inherent challenges in applying deep learning\nfor phase retrieval from diffraction patterns. Conventional neural\narchitectures, both convolutional neural networks and Transformer-based\nmethods, are optimized for natural images with Euclidean spatial\nneighborhood-based inductive biases that exhibit geometric mismatch with the\nconcentric coherent patterns characteristic of diffraction data in reciprocal\nspace. In this paper, we present PPN, a physics-inspired deep learning network\nwith Polar Coordinate Attention (PoCA) for ptychographic imaging, that aligns\nneural inductive biases with diffraction physics through a dual-branch\narchitecture separating local feature extraction from non-local coherence\nmodeling. It consists of a PoCA mechanism that replaces Euclidean spatial\npriors with physically consistent radial-angular correlations. PPN outperforms\nexisting end-to-end models, with spectral and spatial analysis confirming its\ngreater preservation of high-frequency details. Notably, PPN maintains robust\nperformance compared to iterative methods even at low overlap ratios, making it\nwell suited for high-throughput imaging in real-world acquisition scenarios for\nsamples with consistent structural characteristics.\n","authors":["Han Yue","Jun Cheng","Yu-Xuan Ren","Chien-Chun Chen","Grant A. van Riessen","Philip Heng Wai Leong","Steve Feng Shu"],"pdf_url":"https://arxiv.org/pdf/2412.06806v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.01007v1","updated":"2025-05-02T05:11:17Z","published":"2025-05-02T05:11:17Z","title":"Towards the Resistance of Neural Network Watermarking to Fine-tuning","summary":"  This paper proves a new watermarking method to embed the ownership\ninformation into a deep neural network (DNN), which is robust to fine-tuning.\nSpecifically, we prove that when the input feature of a convolutional layer\nonly contains low-frequency components, specific frequency components of the\nconvolutional filter will not be changed by gradient descent during the\nfine-tuning process, where we propose a revised Fourier transform to extract\nfrequency components from the convolutional filter. Additionally, we also prove\nthat these frequency components are equivariant to weight scaling and weight\npermutations. In this way, we design a watermark module to encode the watermark\ninformation to specific frequency components in a convolutional filter.\nPreliminary experiments demonstrate the effectiveness of our method.\n","authors":["Ling Tang","Yuefeng Chen","Hui Xue","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.01007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01003v1","updated":"2025-05-02T04:58:04Z","published":"2025-05-02T04:58:04Z","title":"3D Human Pose Estimation via Spatial Graph Order Attention and Temporal\n  Body Aware Transformer","summary":"  Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the\nprevailing techniques for 3D human pose estimation. However, Transformer-based\nmethods either ignore the spatial neighborhood relationships between the joints\nwhen used for skeleton representations or disregard the local temporal patterns\nof the local joint movements in skeleton sequence modeling, while GCN-based\nmethods often neglect the need for pose-specific representations. To address\nthese problems, we propose a new method that exploits the graph modeling\ncapability of GCN to represent each skeleton with multiple graphs of different\norders, incorporated with a newly introduced Graph Order Attention module that\ndynamically emphasizes the most representative orders for each joint. The\nresulting spatial features of the sequence are further processed using a\nproposed temporal Body Aware Transformer that models the global body feature\ndependencies in the sequence with awareness of the local inter-skeleton feature\ndependencies of joints. Given that our 3D pose output aligns with the central\n2D pose in the sequence, we improve the self-attention mechanism to be aware of\nthe central pose while diminishing its focus gradually towards the first and\nthe last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I\ndatasets demonstrate the effectiveness of the proposed method. Code and models\nare made available on Github.\n","authors":["Kamel Aouaidjia","Aofan Li","Wenhao Zhang","Chongsheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.01003v1.pdf","comment":"16 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2504.15122v3","updated":"2025-05-02T04:57:20Z","published":"2025-04-21T14:19:19Z","title":"MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry\n  Monocular Video","summary":"  We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)\nframework capable of reconstructing sharp and high-quality novel\nspatio-temporal views from blurry monocular videos in an end-to-end manner.\nExisting dynamic novel view synthesis (NVS) methods are highly sensitive to\nmotion blur in casually captured videos, resulting in significant degradation\nof rendering quality. While recent approaches address motion-blurred inputs for\nNVS, they primarily focus on static scene reconstruction and lack dedicated\nmotion modeling for dynamic objects. To overcome these limitations, our MoBGS\nintroduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for\neffective latent camera trajectory estimation, improving global camera motion\ndeblurring. In addition, we propose a physically-inspired Latent Camera-induced\nExposure Estimation (LCEE) method to ensure consistent deblurring of both\nglobal camera and local object motion. Our MoBGS framework ensures the temporal\nconsistency of unseen latent timestamps and robust motion decomposition of\nstatic and dynamic regions. Extensive experiments on the Stereo Blur dataset\nand real-world blurry videos show that our MoBGS significantly outperforms the\nvery recent advanced methods (DyBluRF and Deblur4DGS), achieving\nstate-of-the-art performance for dynamic NVS under motion blur.\n","authors":["Minh-Quan Viet Bui","Jongmin Park","Juan Luis Gonzalez Bello","Jaeho Moon","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2504.15122v3.pdf","comment":"The first two authors contributed equally to this work (equal\n  contribution). The last two authors are co-corresponding authors. Please\n  visit our project page at https://kaist-viclab.github.io/mobgs-site/"},{"id":"http://arxiv.org/abs/2404.01330v2","updated":"2025-05-02T04:51:15Z","published":"2024-03-29T15:27:28Z","title":"P-Hologen: An End-to-End Generative Framework for Phase-Only Holograms","summary":"  Holography stands at the forefront of visual technology, offering immersive,\nthree-dimensional visualizations through the manipulation of light wave\namplitude and phase. Although generative models have been extensively explored\nin the image domain, their application to holograms remains relatively\nunderexplored due to the inherent complexity of phase learning. Exploiting\ngenerative models for holograms offers exciting opportunities for advancing\ninnovation and creativity, such as semantic-aware hologram generation and\nediting. Currently, the most viable approach for utilizing generative models in\nthe hologram domain involves integrating an image-based generative model with\nan image-to-hologram conversion model, which comes at the cost of increased\ncomputational complexity and inefficiency. To tackle this problem, we introduce\nP-Hologen, the first end-to-end generative framework designed for phase-only\nholograms (POHs). P-Hologen employs vector quantized variational autoencoders\nto capture the complex distributions of POHs. It also integrates the angular\nspectrum method into the training process, constructing latent spaces for\ncomplex phase data using strategies from the image processing domain. Extensive\nexperiments demonstrate that P-Hologen achieves superior quality and\ncomputational efficiency compared to the existing methods. Furthermore, our\nmodel generates high-quality unseen, diverse holographic content from its\nlearned latent space without requiring pre-existing images. Our work paves the\nway for new applications and methodologies in holographic content creation,\nopening a new era in the exploration of generative holographic content. The\ncode for our paper is publicly available on\nhttps://github.com/james0223/P-Hologen.\n","authors":["JooHyun Park","YuJin Jeon","HuiYong Kim","SeungHwan Baek","HyeongYeop Kang"],"pdf_url":"https://arxiv.org/pdf/2404.01330v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00998v1","updated":"2025-05-02T04:48:28Z","published":"2025-05-02T04:48:28Z","title":"Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human\n  Motion Synthesis","summary":"  Human motion synthesis aims to generate plausible human motion sequences,\nwhich has raised widespread attention in computer animation. Recent score-based\ngenerative models (SGMs) have demonstrated impressive results on this task.\nHowever, their training process involves complex curvature trajectories,\nleading to unstable training process. In this paper, we propose a\nDeterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for\nhuman motion synthesis. DSDFM consists of two stages. The first human motion\nreconstruction stage aims to learn the latent space distribution of human\nmotions. The second diverse motion generation stage aims to build connections\nbetween the Gaussian distribution and the latent space distribution of human\nmotions, thereby enhancing the diversity and accuracy of the generated human\nmotions. This stage is achieved by the designed deterministic feature mapping\nprocedure with DerODE and stochastic diverse output generation procedure with\nDivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can\nenhance diversity without introducing additional training parameters.Through\nqualitative and quantitative experiments, DSDFM achieves state-of-the-art\nresults surpassing the latest methods, validating its superiority in human\nmotion synthesis.\n","authors":["Yu Hua","Weiming Liu","Gui Xu","Yaqing Hou","Yew-Soon Ong","Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.00998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02782v3","updated":"2025-05-02T04:42:06Z","published":"2025-04-03T17:23:16Z","title":"GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation","summary":"  The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.\n","authors":["Zhiyuan Yan","Junyan Ye","Weijia Li","Zilong Huang","Shenghai Yuan","Xiangyang He","Kaiqing Lin","Jun He","Conghui He","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2504.02782v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00995v1","updated":"2025-05-02T04:41:57Z","published":"2025-05-02T04:41:57Z","title":"Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation\n  in a GNSS-Denied Cherry Tomato Greenhouse","summary":"  As the agricultural workforce declines and labor costs rise, robotic yield\nestimation has become increasingly important. While unmanned ground vehicles\n(UGVs) are commonly used for indoor farm monitoring, their deployment in\ngreenhouses is often constrained by infrastructure limitations, sensor\nplacement challenges, and operational inefficiencies. To address these issues,\nwe develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D\ncamera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial\nodometry algorithm for precise navigation in GNSS-denied environments and\nutilizes a 3D multi-object tracking algorithm to estimate the count and weight\nof cherry tomatoes. We evaluate the system using two dataset: one from a\nharvesting row and another from a growing row. In the harvesting-row dataset,\nthe proposed system achieves 94.4\\% counting accuracy and 87.5\\% weight\nestimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For\nthe growing-row dataset, which consists of occluded unripened fruits, we\nqualitatively analyze tracking performance and highlight future research\ndirections for improving perception in greenhouse with strong occlusions. Our\nfindings demonstrate the potential of UAVs for efficient robotic yield\nestimation in commercial greenhouses.\n","authors":["Taewook Park","Jinwoo Lee","Hyondong Oh","Won-Jae Yun","Kyu-Wha Lee"],"pdf_url":"https://arxiv.org/pdf/2505.00995v1.pdf","comment":"Accepted at 2025 ICRA workshop on field robotics"},{"id":"http://arxiv.org/abs/2505.00986v1","updated":"2025-05-02T04:19:07Z","published":"2025-05-02T04:19:07Z","title":"On-demand Test-time Adaptation for Edge Devices","summary":"  Continual Test-time adaptation (CTTA) continuously adapts the deployed model\non every incoming batch of data. While achieving optimal accuracy, existing\nCTTA approaches present poor real-world applicability on resource-constrained\nedge devices, due to the substantial memory overhead and energy consumption. In\nthis work, we first introduce a novel paradigm -- on-demand TTA -- which\ntriggers adaptation only when a significant domain shift is detected. Then, we\npresent OD-TTA, an on-demand TTA framework for accurate and efficient\nadaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a\nlightweight domain shift detection mechanism to activate TTA only when it is\nneeded, drastically reducing the overall computation overhead, 2) a source\ndomain selection module that chooses an appropriate source model for\nadaptation, ensuring high and robust accuracy, 3) a decoupled Batch\nNormalization (BN) update scheme to enable memory-efficient adaptation with\nsmall batch sizes. Extensive experiments show that OD-TTA achieves comparable\nand even better performance while reducing the energy and computation overhead\nremarkably, making TTA a practical reality.\n","authors":["Xiao Ma","Young D. Kwon","Dong Ma"],"pdf_url":"https://arxiv.org/pdf/2505.00986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00980v1","updated":"2025-05-02T04:00:03Z","published":"2025-05-02T04:00:03Z","title":"LMDepth: Lightweight Mamba-based Monocular Depth Estimation for\n  Real-World Deployment","summary":"  Monocular depth estimation provides an additional depth dimension to RGB\nimages, making it widely applicable in various fields such as virtual reality,\nautonomous driving and robotic navigation. However, existing depth estimation\nalgorithms often struggle to effectively balance performance and computational\nefficiency, which poses challenges for deployment on resource-constrained\ndevices. To address this, we propose LMDepth, a lightweight Mamba-based\nmonocular depth estimation network, designed to reconstruct high-precision\ndepth information while maintaining low computational overhead. Specifically,\nwe propose a modified pyramid spatial pooling module that serves as a\nmulti-scale feature aggregator and context extractor, ensuring global spatial\ninformation for accurate depth estimation. Moreover, we integrate multiple\ndepth Mamba blocks into the decoder. Designed with linear computations, the\nMamba Blocks enable LMDepth to efficiently decode depth information from global\nfeatures, providing a lightweight alternative to Transformer-based\narchitectures that depend on complex attention mechanisms. Extensive\nexperiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of\nour proposed LMDepth. Compared to previous lightweight depth estimation\nmethods, LMDepth achieves higher performance with fewer parameters and lower\ncomputational complexity (measured by GFLOPs). We further deploy LMDepth on an\nembedded platform with INT8 quantization, validating its practicality for\nreal-world edge applications.\n","authors":["Jiahuan Long","Xin Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.00980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11891v3","updated":"2025-05-02T03:55:42Z","published":"2023-06-02T17:47:29Z","title":"VitalVideos-Europe: A dataset of face videos with PPG and blood pressure\n  ground truths","summary":"  We collected a large dataset consisting of 850 unique participants. For every\nparticipant we recorded two 30 second uncompressed videos, synchronized PPG\nwaveforms and a single blood pressure measurement. Gender, age and skin color\nwere also registered for every participant. The dataset includes roughly equal\nnumbers of males and females, as well as participants of all ages. While the\nskin color distribution could have been more balanced, the dataset contains\nindividuals from every skin color. The data was collected in a diverse set of\nlocations to ensure a wide variety of backgrounds and lighting conditions. In\nan effort to assist in the research and development of remote vital sign\nmeasurement we are now opening up access to this dataset.\n  vitalvideos.org for all datasets.\n","authors":["Pieter-Jan Toye"],"pdf_url":"https://arxiv.org/pdf/2306.11891v3.pdf","comment":"vitalvideos.org"},{"id":"http://arxiv.org/abs/2505.00254v2","updated":"2025-05-02T03:40:03Z","published":"2025-05-01T02:40:23Z","title":"Empowering Agentic Video Analytics Systems with Video Language Models","summary":"  AI-driven video analytics has become increasingly pivotal across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Video-Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVAS, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVAS incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively, significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an\naccuracy of 75.8%.\n","authors":["Yuxuan Yan","Shiqi Jiang","Ting Cao","Yifan Yang","Qianqian Yang","Yuanchao Shu","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.00254v2.pdf","comment":"15 pages, AVAS"},{"id":"http://arxiv.org/abs/2505.00975v1","updated":"2025-05-02T03:37:09Z","published":"2025-05-02T03:37:09Z","title":"Generating Animated Layouts as Structured Text Representations","summary":"  Despite the remarkable progress in text-to-video models, achieving precise\ncontrol over text elements and animated graphics remains a significant\nchallenge, especially in applications such as video advertisements. To address\nthis limitation, we introduce Animated Layout Generation, a novel approach to\nextend static graphic layouts with temporal dynamics. We propose a Structured\nText Representation for fine-grained video control through hierarchical visual\nelements. To demonstrate the effectiveness of our approach, we present VAKER\n(Video Ad maKER), a text-to-video advertisement generation pipeline that\ncombines a three-stage generation process with Unstructured Text Reasoning for\nseamless integration with LLMs. VAKER fully automates video advertisement\ngeneration by incorporating dynamic layout trajectories for objects and\ngraphics across specific video frames. Through extensive evaluations, we\ndemonstrate that VAKER significantly outperforms existing methods in generating\nvideo advertisements. Project Page:\nhttps://yeonsangshin.github.io/projects/Vaker\n","authors":["Yeonsang Shin","Jihwan Kim","Yumin Song","Kyungseung Lee","Hyunhee Chung","Taeyoung Na"],"pdf_url":"https://arxiv.org/pdf/2505.00975v1.pdf","comment":"AI for Content Creation (AI4CC) Workshop at CVPR 2025"},{"id":"http://arxiv.org/abs/2408.08518v2","updated":"2025-05-02T02:32:39Z","published":"2024-08-16T04:14:28Z","title":"Visual-Friendly Concept Protection via Selective Adversarial\n  Perturbations","summary":"  Personalized concept generation by tuning diffusion models with a few images\nraises potential legal and ethical concerns regarding privacy and intellectual\nproperty rights. Researchers attempt to prevent malicious personalization using\nadversarial perturbations. However, previous efforts have mainly focused on the\neffectiveness of protection while neglecting the visibility of perturbations.\nThey utilize global adversarial perturbations, which introduce noticeable\nalterations to original images and significantly degrade visual quality. In\nthis work, we propose the Visual-Friendly Concept Protection (VCPro) framework,\nwhich prioritizes the protection of key concepts chosen by the image owner\nthrough adversarial perturbations with lower perceptibility. To ensure these\nperturbations are as inconspicuous as possible, we introduce a relaxed\noptimization objective to identify the least perceptible yet effective\nadversarial perturbations, solved using the Lagrangian multiplier method.\nQualitative and quantitative experiments validate that VCPro achieves a better\ntrade-off between the visibility of perturbations and protection effectiveness,\neffectively prioritizing the protection of target concepts in images with less\nperceptible perturbations.\n","authors":["Xiaoyue Mi","Fan Tang","Juan Cao","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.08518v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.14415v3","updated":"2025-05-02T01:02:47Z","published":"2024-12-19T00:06:09Z","title":"DriveGPT: Scaling Autoregressive Behavior Models for Driving","summary":"  We present DriveGPT, a scalable behavior model for autonomous driving. We\nmodel driving as a sequential decision-making task, and learn a transformer\nmodel to predict future agent states as tokens in an autoregressive fashion. We\nscale up our model parameters and training data by multiple orders of\nmagnitude, enabling us to explore the scaling properties in terms of dataset\nsize, model parameters, and compute. We evaluate DriveGPT across different\nscales in a planning task, through both quantitative metrics and qualitative\nexamples, including closed-loop driving in complex real-world scenarios. In a\nseparate prediction task, DriveGPT outperforms state-of-the-art baselines and\nexhibits improved performance by pretraining on a large-scale dataset, further\nvalidating the benefits of data scaling.\n","authors":["Xin Huang","Eric M. Wolff","Paul Vernaza","Tung Phan-Minh","Hongge Chen","David S. Hayden","Mark Edmonds","Brian Pierce","Xinxin Chen","Pratik Elias Jacob","Xiaobai Chen","Chingiz Tairbekov","Pratik Agarwal","Tianshi Gao","Yuning Chai","Siddhartha Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2412.14415v3.pdf","comment":"ICML 2025. 14 pages, 17 figures, 8 tables, and 1 video link"},{"id":"http://arxiv.org/abs/2505.00938v1","updated":"2025-05-02T00:46:25Z","published":"2025-05-02T00:46:25Z","title":"CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against\n  Feature Confusion","summary":"  Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects\nacross different domains with limited class instances. Feature confusion,\nincluding object-background confusion and object-object confusion, presents\nsignificant challenges in both cross-domain and few-shot settings. In this\nwork, we introduce CDFormer, a cross-domain few-shot object detection\ntransformer against feature confusion, to address these challenges. The method\nspecifically tackles feature confusion through two key modules:\nobject-background distinguishing (OBD) and object-object distinguishing (OOD).\nThe OBD module leverages a learnable background token to differentiate between\nobjects and background, while the OOD module enhances the distinction between\nobjects of different classes. Experimental results demonstrate that CDFormer\noutperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0%\nmAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively,\nwhen fine-tuned.\n","authors":["Boyuan Meng","Xiaohan Zhang","Peilin Li","Zhe Wu","Yiming Li","Wenkai Zhao","Beinan Yu","Hui-Liang Shen"],"pdf_url":"https://arxiv.org/pdf/2505.00938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11487v3","updated":"2025-05-02T00:44:08Z","published":"2024-02-18T07:28:37Z","title":"Visual Concept-driven Image Generation with Text-to-Image Diffusion\n  Model","summary":"  Text-to-image (TTI) diffusion models have demonstrated impressive results in\ngenerating high-resolution images of complex and imaginative scenes. Recent\napproaches have further extended these methods with personalization techniques\nthat allow them to integrate user-illustrated concepts (e.g., the user\nhim/herself) using a few sample image illustrations. However, the ability to\ngenerate images with multiple interacting concepts, such as human subjects, as\nwell as concepts that may be entangled in one, or across multiple, image\nillustrations remains illusive. In this work, we propose a concept-driven TTI\npersonalization framework that addresses these core challenges. We build on\nexisting works that learn custom tokens for user-illustrated concepts, allowing\nthose to interact with existing text tokens in the TTI model. However,\nimportantly, to disentangle and better learn the concepts in question, we\njointly learn (latent) segmentation masks that disentangle these concepts in\nuser-provided image illustrations. We do so by introducing an Expectation\nMaximization (EM)-like optimization procedure where we alternate between\nlearning the custom tokens and estimating (latent) masks encompassing\ncorresponding concepts in user-supplied images. We obtain these masks based on\ncross-attention, from within the U-Net parameterized latent diffusion model and\nsubsequent DenseCRF optimization. We illustrate that such joint alternating\nrefinement leads to the learning of better tokens for concepts and, as a\nby-product, latent masks. We illustrate the benefits of the proposed approach\nqualitatively and quantitatively with several examples and use cases that can\ncombine three or more entangled concepts.\n","authors":["Tanzila Rahman","Shweta Mahajan","Hsin-Ying Lee","Jian Ren","Sergey Tulyakov","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2402.11487v3.pdf","comment":"11 Figures, 14 Pages, 2 tables"},{"id":"http://arxiv.org/abs/2505.00935v1","updated":"2025-05-02T00:43:28Z","published":"2025-05-02T00:43:28Z","title":"Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning","summary":"  The increase in available computing power and the Deep Learning revolution\nhave allowed the exploration of new topics and frontiers in Artificial\nIntelligence research. A new field called Embodied Artificial Intelligence,\nwhich places at the intersection of Computer Vision, Robotics, and Decision\nMaking, has been gaining importance during the last few years, as it aims to\nfoster the development of smart autonomous robots and their deployment in\nsociety. The recent availability of large collections of 3D models for\nphotorealistic robotic simulation has allowed faster and safe training of\nlearning-based agents for millions of frames and a careful evaluation of their\nbehavior before deploying the models on real robotic platforms. These\nintelligent agents are intended to perform a certain task in a possibly unknown\nenvironment. To this end, during the training in simulation, the agents learn\nto perform continuous interactions with the surroundings, such as gathering\ninformation from the environment, encoding and extracting useful cues for the\ntask, and performing actions towards the final goal; where every action of the\nagent influences the interactions. This dissertation follows the complete\ncreation process of embodied agents for indoor environments, from their concept\nto their implementation and deployment. We aim to contribute to research in\nEmbodied AI and autonomous agents, in order to foster future work in this\nfield. We present a detailed analysis of the procedure behind implementing an\nintelligent embodied agent, comprehending a thorough description of the current\nstate-of-the-art in literature, technical explanations of the proposed methods,\nand accurate experimental studies on relevant robotic tasks.\n","authors":["Roberto Bigazzi"],"pdf_url":"https://arxiv.org/pdf/2505.00935v1.pdf","comment":"Ph.D. Dissertation"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.01425v1","updated":"2025-05-02T17:59:55Z","published":"2025-05-02T17:59:55Z","title":"GENMO: A GENeralist Model for Human MOtion","summary":"  Human motion modeling traditionally separates motion generation and\nestimation into distinct tasks with specialized models. Motion generation\nmodels focus on creating diverse, realistic motions from inputs like text,\naudio, or keyframes, while motion estimation models aim to reconstruct accurate\nmotion trajectories from observations like videos. Despite sharing underlying\nrepresentations of temporal dynamics and kinematics, this separation limits\nknowledge transfer between tasks and requires maintaining separate models. We\npresent GENMO, a unified Generalist Model for Human Motion that bridges motion\nestimation and generation in a single framework. Our key insight is to\nreformulate motion estimation as constrained motion generation, where the\noutput motion must precisely satisfy observed conditioning signals. Leveraging\nthe synergy between regression and diffusion, GENMO achieves accurate global\nmotion estimation while enabling diverse motion generation. We also introduce\nan estimation-guided training objective that exploits in-the-wild videos with\n2D annotations and text descriptions to enhance generative diversity.\nFurthermore, our novel architecture handles variable-length motions and mixed\nmultimodal conditions (text, audio, video) at different time intervals,\noffering flexible control. This unified approach creates synergistic benefits:\ngenerative priors improve estimated motions under challenging conditions like\nocclusions, while diverse video data enhances generation capabilities.\nExtensive experiments demonstrate GENMO's effectiveness as a generalist\nframework that successfully handles multiple human motion tasks within a single\nmodel.\n","authors":["Jiefeng Li","Jinkun Cao","Haotian Zhang","Davis Rempe","Jan Kautz","Umar Iqbal","Ye Yuan"],"pdf_url":"https://arxiv.org/pdf/2505.01425v1.pdf","comment":"Project page: https://research.nvidia.com/labs/dair/genmo/"},{"id":"http://arxiv.org/abs/2403.04577v2","updated":"2025-05-02T17:52:05Z","published":"2024-03-07T15:22:07Z","title":"Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables","summary":"  Interest in solving table interpretation tasks has grown over the years, yet\nit still relies on existing datasets that may be overly simplified. This is\npotentially reducing the effectiveness of the dataset for thorough evaluation\nand failing to accurately represent tables as they appear in the real-world. To\nenrich the existing benchmark datasets, we extract and annotate a new, more\nchallenging dataset. The proposed Wiki-TabNER dataset features complex tables\ncontaining several entities per cell, with named entities labeled using DBpedia\nclasses. This dataset is specifically designed to address named entity\nrecognition (NER) task within tables, but it can also be used as a more\nchallenging dataset for evaluating the entity linking task. In this paper we\ndescribe the distinguishing features of the Wiki-TabNER dataset and the\nlabeling process. In addition, we propose a prompting framework for evaluating\nthe new large language models on the within tables NER task. Finally, we\nperform qualitative analysis to gain insights into the challenges encountered\nby the models and to understand the limitations of the proposed~dataset.\n","authors":["Aneta Koleva","Martin Ringsquandl","Ahmed Hatem","Thomas Runkler","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2403.04577v2.pdf","comment":"Accepted at SIGIR 2025 conference"},{"id":"http://arxiv.org/abs/2505.01396v1","updated":"2025-05-02T17:13:03Z","published":"2025-05-02T17:13:03Z","title":"SIME: Enhancing Policy Self-Improvement with Modal-level Exploration","summary":"  Self-improvement requires robotic systems to initially learn from\nhuman-provided data and then gradually enhance their capabilities through\ninteraction with the environment. This is similar to how humans improve their\nskills through continuous practice. However, achieving effective\nself-improvement is challenging, primarily because robots tend to repeat their\nexisting abilities during interactions, often failing to generate new, valuable\ndata for learning. In this paper, we identify the key to successful\nself-improvement: modal-level exploration and data selection. By incorporating\na modal-level exploration mechanism during policy execution, the robot can\nproduce more diverse and multi-modal interactions. At the same time, we select\nthe most valuable trials and high-quality segments from these interactions for\nlearning. We successfully demonstrate effective robot self-improvement on both\nsimulation benchmarks and real-world experiments. The capability for\nself-improvement will enable us to develop more robust and high-success-rate\nrobotic control strategies at a lower cost. Our code and experiment scripts are\navailable at https://ericjin2002.github.io/SIME/\n","authors":["Yang Jin","Jun Lv","Wenye Yu","Hongjie Fang","Yong-Lu Li","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2505.01396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16276v2","updated":"2025-05-02T17:04:43Z","published":"2025-04-22T21:21:41Z","title":"An Automated Pipeline for Few-Shot Bird Call Classification: A Case\n  Study with the Tooth-Billed Pigeon","summary":"  This paper presents an automated one-shot bird call classification pipeline\ndesigned for rare species absent from large publicly available classifiers like\nBirdNET and Perch. While these models excel at detecting common birds with\nabundant training data, they lack options for species with only 1-3 known\nrecordings-a critical limitation for conservationists monitoring the last\nremaining individuals of endangered birds. To address this, we leverage the\nembedding space of large bird classification networks and develop a classifier\nusing cosine similarity, combined with filtering and denoising preprocessing\ntechniques, to optimize detection with minimal training data. We evaluate\nvarious embedding spaces using clustering metrics and validate our approach in\nboth a simulated scenario with Xeno-Canto recordings and a real-world test on\nthe critically endangered tooth-billed pigeon (Didunculus strigirostris), which\nhas no existing classifiers and only three confirmed recordings. The final\nmodel achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon\ncalls, making it practical for use in the field. This open-source system\nprovides a practical tool for conservationists seeking to detect and monitor\nrare species on the brink of extinction.\n","authors":["Abhishek Jana","Moeumu Uili","James Atherton","Mark O'Brien","Joe Wood","Leandra Brickson"],"pdf_url":"https://arxiv.org/pdf/2504.16276v2.pdf","comment":"16 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.01390v1","updated":"2025-05-02T16:57:37Z","published":"2025-05-02T16:57:37Z","title":"Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework\n  for Predicting Pathological Response in Non-Small Cell Lung Cancer","summary":"  This study proposes a novel approach combining Multimodal Deep Learning with\nintrinsic eXplainable Artificial Intelligence techniques to predict\npathological response in non-small cell lung cancer patients undergoing\nneoadjuvant therapy. Due to the limitations of existing radiomics and unimodal\ndeep learning approaches, we introduce an intermediate fusion strategy that\nintegrates imaging and clinical data, enabling efficient interaction between\ndata modalities. The proposed Multimodal Doctor-in-the-Loop method further\nenhances clinical relevance by embedding clinicians' domain knowledge directly\ninto the training process, guiding the model's focus gradually from broader\nlung regions to specific lesions. Results demonstrate improved predictive\naccuracy and explainability, providing insights into optimal data integration\nstrategies for clinical applications.\n","authors":["Alice Natalina Caragliano","Claudia Tacconi","Carlo Greco","Lorenzo Nibid","Edy Ippolito","Michele Fiore","Giuseppe Perrone","Sara Ramella","Paolo Soda","Valerio Guarrasi"],"pdf_url":"https://arxiv.org/pdf/2505.01390v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2502.17503"},{"id":"http://arxiv.org/abs/2505.01383v1","updated":"2025-05-02T16:47:05Z","published":"2025-05-02T16:47:05Z","title":"FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft\n  Research","summary":"  We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing\nplatform for autonomy research. The hardware platform integrates a small\ncamera, a standard airframe, offboard computation, and radio communication for\nmanual overrides. We demonstrate FalconWing's capabilities by developing and\ndeploying a purely vision-based control policy for autonomous landing (without\nIMU or motion capture) using a novel real-to-sim-to-real learning approach. Our\nlearning approach: (1) constructs a photorealistic simulation environment via\n3D Gaussian splatting trained on real-world images; (2) identifies nonlinear\ndynamics from vision-estimated real-flight data; and (3) trains a multi-modal\nVision Transformer (ViT) policy through simulation-only imitation learning. The\nViT architecture fuses single RGB image with the history of control actions via\nself-attention, preserving temporal context while maintaining real-time 20 Hz\ninference. When deployed zero-shot on the hardware platform, this policy\nachieves an 80% success rate in vision-based autonomous landings. Together with\nthe hardware specifications, we also open-source the system dynamics, the\nsoftware for photorealistic simulator and the learning approach.\n","authors":["Yan Miao","Will Shen","Hang Cui","Sayan Mitra"],"pdf_url":"https://arxiv.org/pdf/2505.01383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01372v1","updated":"2025-05-02T16:18:40Z","published":"2025-05-02T16:18:40Z","title":"Evaluating Explanations: An Explanatory Virtues Framework for\n  Mechanistic Interpretability -- The Strange Science Part I.ii","summary":"  Mechanistic Interpretability (MI) aims to understand neural networks through\ncausal explanations. Though MI has many explanation-generating methods,\nprogress has been limited by the lack of a universal approach to evaluating\nexplanations. Here we analyse the fundamental question \"What makes a good\nexplanation?\" We introduce a pluralist Explanatory Virtues Framework drawing on\nfour perspectives from the Philosophy of Science - the Bayesian, Kuhnian,\nDeutschian, and Nomological - to systematically evaluate and improve\nexplanations in MI. We find that Compact Proofs consider many explanatory\nvirtues and are hence a promising approach. Fruitful research directions\nimplied by our framework include (1) clearly defining explanatory simplicity,\n(2) focusing on unifying explanations and (3) deriving universal principles for\nneural networks. Improved MI methods enhance our ability to monitor, predict,\nand steer AI systems.\n","authors":["Kola Ayonrinde","Louis Jaburi"],"pdf_url":"https://arxiv.org/pdf/2505.01372v1.pdf","comment":"13 pages (plus appendices), 5 figures"},{"id":"http://arxiv.org/abs/2410.11502v3","updated":"2025-05-02T15:46:08Z","published":"2024-10-15T11:15:03Z","title":"Offline Model-Based Optimization by Learning to Rank","summary":"  Offline model-based optimization (MBO) aims to identify a design that\nmaximizes a black-box function using only a fixed, pre-collected dataset of\ndesigns and their corresponding scores. A common approach in offline MBO is to\ntrain a regression-based surrogate model by minimizing mean squared error (MSE)\nand then find the best design within this surrogate model by different\noptimizers (e.g., gradient ascent). However, a critical challenge is the risk\nof out-of-distribution errors, i.e., the surrogate model may typically\noverestimate the scores and mislead the optimizers into suboptimal regions.\nPrior works have attempted to address this issue in various ways, such as using\nregularization techniques and ensemble learning to enhance the robustness of\nthe model, but it still remains. In this paper, we argue that regression models\ntrained with MSE are not well-aligned with the primary goal of offline MBO,\nwhich is to select promising designs rather than to predict their scores\nprecisely. Notably, if a surrogate model can maintain the order of candidate\ndesigns based on their relative score relationships, it can produce the best\ndesigns even without precise predictions. To validate it, we conduct\nexperiments to compare the relationship between the quality of the final\ndesigns and MSE, finding that the correlation is really very weak. In contrast,\na metric that measures order-maintaining quality shows a significantly stronger\ncorrelation. Based on this observation, we propose learning a ranking-based\nmodel that leverages learning to rank techniques to prioritize promising\ndesigns based on their relative scores. We show that the generalization error\non ranking loss can be well bounded. Empirical results across diverse tasks\ndemonstrate the superior performance of our proposed ranking-based models than\ntwenty existing methods.\n","authors":["Rong-Xi Tan","Ke Xue","Shen-Huan Lyu","Haopu Shang","Yao Wang","Yaoyuan Wang","Sheng Fu","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2410.11502v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2505.01353v1","updated":"2025-05-02T15:43:37Z","published":"2025-05-02T15:43:37Z","title":"Differentiable Nonlinear Model Predictive Control","summary":"  The efficient computation of parametric solution sensitivities is a key\nchallenge in the integration of learning-enhanced methods with nonlinear model\npredictive control (MPC), as their availability is crucial for many learning\nalgorithms. While approaches presented in the machine learning community are\nlimited to convex or unconstrained formulations, this paper discusses the\ncomputation of solution sensitivities of general nonlinear programs (NLPs)\nusing the implicit function theorem (IFT) and smoothed optimality conditions\ntreated in interior-point methods (IPM). We detail sensitivity computation\nwithin a sequential quadratic programming (SQP) method which employs an IPM for\nthe quadratic subproblems. The publication is accompanied by an efficient\nopen-source implementation within the framework, providing both forward and\nadjoint sensitivities for general optimal control problems, achieving speedups\nexceeding 3x over the state-of-the-art solver mpc.pytorch.\n","authors":["Jonathan Frey","Katrin Baumgärtner","Gianluca Frison","Dirk Reinhardt","Jasper Hoffmann","Leonard Fichtner","Sebastien Gros","Moritz Diehl"],"pdf_url":"https://arxiv.org/pdf/2505.01353v1.pdf","comment":"19 page, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.01343v1","updated":"2025-05-02T15:31:32Z","published":"2025-05-02T15:31:32Z","title":"BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in\n  Multi-modal Model Editing","summary":"  Large multi-modal models inevitably decay over time as facts change and\npreviously learned information becomes outdated. Traditional approaches such as\nfine-tuning are often impractical for updating these models due to their size\nand complexity. Instead, direct knowledge editing within the models presents a\nmore viable solution. Current model editing techniques, however, typically\noverlook the unique influence ranges of different facts, leading to compromised\nmodel performance in terms of both generality and locality. To address this\nissue, we introduce the concept of the generality-locality trade-off in\nmulti-modal model editing. We develop a new model editing dataset named OKEDIT,\nspecifically designed to effectively evaluate this trade-off. Building on this\nfoundation, we propose BalancEdit, a novel method for balanced model editing\nthat dynamically achieves an optimal balance between generality and locality.\nBalancEdit utilizes a unique mechanism that generates both positive and\nnegative samples for each fact to accurately determine its influence scope and\nincorporates these insights into the model's latent space using a discrete,\nlocalized codebook of edits, without modifying the underlying model weights. To\nour knowledge, this is the first approach explicitly addressing the\ngenerality-locality trade-off in multi-modal model editing. Our comprehensive\nresults confirm the effectiveness of BalancEdit, demonstrating minimal\ntrade-offs while maintaining robust editing capabilities. Our code and dataset\nwill be available.\n","authors":["Dongliang Guo","Mengxuan Hu","Zihan Guan","Thomas Hartvigsen","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2505.01343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01328v1","updated":"2025-05-02T15:01:42Z","published":"2025-05-02T15:01:42Z","title":"Constrained Network Adversarial Attacks: Validity, Robustness, and\n  Transferability","summary":"  While machine learning has significantly advanced Network Intrusion Detection\nSystems (NIDS), particularly within IoT environments where devices generate\nlarge volumes of data and are increasingly susceptible to cyber threats, these\nmodels remain vulnerable to adversarial attacks. Our research reveals a\ncritical flaw in existing adversarial attack methodologies: the frequent\nviolation of domain-specific constraints, such as numerical and categorical\nlimits, inherent to IoT and network traffic. This leads to up to 80.3% of\nadversarial examples being invalid, significantly overstating real-world\nvulnerabilities. These invalid examples, though effective in fooling models, do\nnot represent feasible attacks within practical IoT deployments. Consequently,\nrelying on these results can mislead resource allocation for defense, inflating\nthe perceived susceptibility of IoT-enabled NIDS models to adversarial\nmanipulation. Furthermore, we demonstrate that simpler surrogate models like\nMulti-Layer Perceptron (MLP) generate more valid adversarial examples compared\nto complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate,\nwe analyze the transferability of adversarial severity to other ML/DL models\ncommonly used in IoT contexts. This work underscores the importance of\nconsidering both domain constraints and model architecture when evaluating and\ndesigning robust ML/DL models for security-critical IoT and network\napplications.\n","authors":["Anass Grini","Oumaima Taheri","Btissam El Khamlichi","Amal El Fallah-Seghrouchni"],"pdf_url":"https://arxiv.org/pdf/2505.01328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01315v1","updated":"2025-05-02T14:42:26Z","published":"2025-05-02T14:42:26Z","title":"Helping Big Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System","summary":"  The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.\n","authors":["Sheikh Samit Muhaimin","Spyridon Mastorakis"],"pdf_url":"https://arxiv.org/pdf/2505.01315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01309v1","updated":"2025-05-02T14:38:13Z","published":"2025-05-02T14:38:13Z","title":"Enhancing SPARQL Query Rewriting for Complex Ontology Alignments","summary":"  SPARQL query rewriting is a fundamental mechanism for uniformly querying\nheterogeneous ontologies in the Linked Data Web. However, the complexity of\nontology alignments, particularly rich correspondences (c : c), makes this\nprocess challenging. Existing approaches primarily focus on simple (s : s) and\npartially complex ( s : c) alignments, thereby overlooking the challenges posed\nby more expressive alignments. Moreover, the intricate syntax of SPARQL\npresents a barrier for non-expert users seeking to fully exploit the knowledge\nencapsulated in ontologies. This article proposes an innovative approach for\nthe automatic rewriting of SPARQL queries from a source ontology to a target\nontology, based on a user's need expressed in natural language. It leverages\nthe principles of equivalence transitivity as well as the advanced capabilities\nof large language models such as GPT-4. By integrating these elements, this\napproach stands out for its ability to efficiently handle complex alignments,\nparticularly (c : c) correspondences , by fully exploiting their\nexpressiveness. Additionally, it facilitates access to aligned ontologies for\nusers unfamiliar with SPARQL, providing a flexible solution for querying\nheterogeneous data.\n","authors":["Anicet Lepetit Ondo","Laurence Capus","Mamadou Bousso"],"pdf_url":"https://arxiv.org/pdf/2505.01309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01307v1","updated":"2025-05-02T14:34:33Z","published":"2025-05-02T14:34:33Z","title":"Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical\n  software assessments","summary":"  Safety critical software assessment requires robust assessment against\ncomplex regulatory frameworks, a process traditionally limited by manual\nevaluation. This paper presents Document Retrieval-Augmented Fine-Tuning\n(DRAFT), a novel approach that enhances the capabilities of a large language\nmodel (LLM) for safety-critical compliance assessment. DRAFT builds upon\nexisting Retrieval-Augmented Generation (RAG) techniques by introducing a novel\nfine-tuning framework that accommodates our dual-retrieval architecture, which\nsimultaneously accesses both software documentation and applicable reference\nstandards. To fine-tune DRAFT, we develop a semi-automated dataset generation\nmethodology that incorporates variable numbers of relevant documents with\nmeaningful distractors, closely mirroring real-world assessment scenarios.\nExperiments with GPT-4o-mini demonstrate a 7% improvement in correctness over\nthe baseline model, with qualitative improvements in evidence handling,\nresponse structure, and domain-specific reasoning. DRAFT represents a practical\napproach to improving compliance assessment systems while maintaining the\ntransparency and evidence-based reasoning essential in regulatory domains.\n","authors":["Regan Bolton","Mohammadreza Sheikhfathollahi","Simon Parkinson","Vanessa Vulovic","Gary Bamford","Dan Basher","Howard Parkinson"],"pdf_url":"https://arxiv.org/pdf/2505.01307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01305v1","updated":"2025-05-02T14:32:44Z","published":"2025-05-02T14:32:44Z","title":"Early Detection of Patient Deterioration from Real-Time Wearable\n  Monitoring System","summary":"  Early detection of patient deterioration is crucial for reducing mortality\nrates. Heart rate data has shown promise in assessing patient health, and\nwearable devices offer a cost-effective solution for real-time monitoring.\nHowever, extracting meaningful insights from diverse heart rate data and\nhandling missing values in wearable device data remain key challenges. To\naddress these challenges, we propose TARL, an innovative approach that models\nthe structural relationships of representative subsequences, known as\nshapelets, in heart rate time series. TARL creates a shapelet-transition\nknowledge graph to model shapelet dynamics in heart rate time series,\nindicating illness progression and potential future changes. We further\nintroduce a transition-aware knowledge embedding to reinforce relationships\namong shapelets and quantify the impact of missing values, enabling the\nformulation of comprehensive heart rate representations. These representations\ncapture explanatory structures and predict future heart rate trends, aiding\nearly illness detection. We collaborate with physicians and nurses to gather\nICU patient heart rate data from wearables and diagnostic metrics assessing\nillness severity for evaluating deterioration. Experiments on real-world ICU\ndata demonstrate that TARL achieves both high reliability and early detection.\nA case study further showcases TARL's explainable detection process,\nhighlighting its potential as an AI-driven tool to assist clinicians in\nrecognizing early signs of patient deterioration.\n","authors":["Lo Pang-Yun Ting","Hong-Pei Chen","An-Shan Liu","Chun-Yin Yeh","Po-Lin Chen","Kun-Ta Chuang"],"pdf_url":"https://arxiv.org/pdf/2505.01305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17412v4","updated":"2025-05-02T14:24:42Z","published":"2024-05-27T17:57:12Z","title":"Towards One Model for Classical Dimensionality Reduction: A\n  Probabilistic Perspective on UMAP and t-SNE","summary":"  This paper shows that dimensionality reduction methods such as UMAP and\nt-SNE, can be approximately recast as MAP inference methods corresponding to a\nmodel introduced in Ravuri et al. (2023), that describes the graph Laplacian\n(an estimate of the data precision matrix) using a Wishart distribution, with a\nmean given by a non-linear covariance function evaluated on the latents. This\ninterpretation offers deeper theoretical and semantic insights into such\nalgorithms, and forging a connection to Gaussian process latent variable models\nby showing that well-known kernels can be used to describe covariances implied\nby graph Laplacians. We also introduce tools with which similar dimensionality\nreduction methods can be studied.\n","authors":["Aditya Ravuri","Neil D. Lawrence"],"pdf_url":"https://arxiv.org/pdf/2405.17412v4.pdf","comment":"AABI version"},{"id":"http://arxiv.org/abs/2411.14995v2","updated":"2025-05-02T14:12:10Z","published":"2024-11-22T15:09:50Z","title":"Learning Lifted STRIPS Models from Action Traces Alone: A Simple,\n  General, and Scalable Solution","summary":"  Learning STRIPS action models from action traces alone is a challenging\nproblem as it involves learning the domain predicates as well. In this work, a\nnovel approach is introduced which, like the well-known LOCM systems, is\nscalable, but like SAT approaches, is sound and complete. Furthermore, the\napproach is general and imposes no restrictions on the hidden domain or the\nnumber or arity of the predicates. The new learning method is based on an\n\\emph{efficient, novel test} that checks whether the assumption that a\npredicate is affected by a set of action patterns, namely, actions with\nspecific argument positions, is consistent with the traces. The predicates and\naction patterns that pass the test provide the basis for the learned domain\nthat is then easily completed with preconditions and static predicates. The new\nmethod is studied theoretically and experimentally. For the latter, the method\nis evaluated on traces and graphs obtained from standard classical domains like\nthe 8-puzzle, which involve hundreds of thousands of states and transitions.\nThe learned representations are then verified on larger instances.\n","authors":["Jonas Gösgens","Niklas Jansen","Hector Geffner"],"pdf_url":"https://arxiv.org/pdf/2411.14995v2.pdf","comment":"accepted at ICAPS 2025"},{"id":"http://arxiv.org/abs/2403.07404v3","updated":"2025-05-02T14:03:34Z","published":"2024-03-12T08:33:26Z","title":"Improving Continual Learning Performance and Efficiency with Auxiliary\n  Classifiers","summary":"  Continual learning is crucial for applying machine learning in challenging,\ndynamic, and often resource-constrained environments. However, catastrophic\nforgetting - overwriting previously learned knowledge when new information is\nacquired - remains a major challenge. In this work, we examine the intermediate\nrepresentations in neural network layers during continual learning and find\nthat such representations are less prone to forgetting, highlighting their\npotential to accelerate computation. Motivated by these findings, we propose to\nuse auxiliary classifiers(ACs) to enhance performance and demonstrate that\nintegrating ACs into various continual learning methods consistently improves\naccuracy across diverse evaluation settings, yielding an average 10% relative\ngain. We also leverage the ACs to reduce the average cost of the inference by\n10-60% without compromising accuracy, enabling the model to return the\npredictions before computing all the layers. Our approach provides a scalable\nand efficient solution for continual learning.\n","authors":["Filip Szatkowski","Yaoyue Zheng","Fei Yang","Bartłomiej Twardowski","Tomasz Trzciński","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2403.07404v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.01288v1","updated":"2025-05-02T14:03:06Z","published":"2025-05-02T14:03:06Z","title":"ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video\n  Semantic Action Flow","summary":"  One of the central challenges preventing robots from acquiring complex\nmanipulation skills is the prohibitive cost of collecting large-scale robot\ndemonstrations. In contrast, humans are able to learn efficiently by watching\nothers interact with their environment. To bridge this gap, we introduce\nsemantic action flow as a core intermediate representation capturing the\nessential spatio-temporal manipulator-object interactions, invariant to\nsuperficial visual differences. We present ViSA-Flow, a framework that learns\nthis representation self-supervised from unlabeled large-scale video data.\nFirst, a generative model is pre-trained on semantic action flows automatically\nextracted from large-scale human-object interaction video data, learning a\nrobust prior over manipulation structure. Second, this prior is efficiently\nadapted to a target robot by fine-tuning on a small set of robot demonstrations\nprocessed through the same semantic abstraction pipeline. We demonstrate\nthrough extensive experiments on the CALVIN benchmark and real-world tasks that\nViSA-Flow achieves state-of-the-art performance, particularly in low-data\nregimes, outperforming prior methods by effectively transferring knowledge from\nhuman video observation to robotic execution. Videos are available at\nhttps://visaflow-web.github.io/ViSAFLOW.\n","authors":["Changhe Chen","Quantao Yang","Xiaohao Xu","Nima Fazeli","Olov Andersson"],"pdf_url":"https://arxiv.org/pdf/2505.01288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01286v1","updated":"2025-05-02T14:00:48Z","published":"2025-05-02T14:00:48Z","title":"2DXformer: Dual Transformers for Wind Power Forecasting with Dual\n  Exogenous Variables","summary":"  Accurate wind power forecasting can help formulate scientific dispatch plans,\nwhich is of great significance for maintaining the safety, stability, and\nefficient operation of the power system. In recent years, wind power\nforecasting methods based on deep learning have focused on extracting the\nspatiotemporal correlations among data, achieving significant improvements in\nforecasting accuracy. However, they exhibit two limitations. First, there is a\nlack of modeling for the inter-variable relationships, which limits the\naccuracy of the forecasts. Second, by treating endogenous and exogenous\nvariables equally, it leads to unnecessary interactions between the endogenous\nand exogenous variables, increasing the complexity of the model. In this paper,\nwe propose the 2DXformer, which, building upon the previous work's focus on\nspatiotemporal correlations, addresses the aforementioned two limitations.\nSpecifically, we classify the inputs of the model into three types: exogenous\nstatic variables, exogenous dynamic variables, and endogenous variables. First,\nwe embed these variables as variable tokens in a channel-independent manner.\nThen, we use the attention mechanism to capture the correlations among\nexogenous variables. Finally, we employ a multi-layer perceptron with residual\nconnections to model the impact of exogenous variables on endogenous variables.\nExperimental results on two real-world large-scale datasets indicate that our\nproposed 2DXformer can further improve the performance of wind power\nforecasting. The code is available in this repository:\n\\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.\n","authors":["Yajuan Zhang","Jiahai Jiang","Yule Yan","Liang Yang","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.01286v1.pdf","comment":"Accepted by ICDM 2024"},{"id":"http://arxiv.org/abs/2505.01283v1","updated":"2025-05-02T13:58:47Z","published":"2025-05-02T13:58:47Z","title":"Reduced-order structure-property linkages for stochastic metamaterials","summary":"  The capabilities of additive manufacturing have facilitated the design and\nproduction of mechanical metamaterials with diverse unit cell geometries.\nEstablishing linkages between the vast design space of unit cells and their\neffective mechanical properties is critical for the efficient design and\nperformance evaluation of such metamaterials. However, physics-based\nsimulations of metamaterial unit cells across the entire design space are\ncomputationally expensive, necessitating a materials informatics framework to\nefficiently capture complex structure-property relationships. In this work,\nprincipal component analysis of 2-point correlation functions is performed to\nextract the salient features from a large dataset of randomly generated 2D\nmetamaterials. Physics-based simulations are performed using a fast Fourier\ntransform (FFT)-based homogenization approach to efficiently compute the\nhomogenized effective elastic stiffness across the extensive unit cell designs.\nSubsequently, Gaussian process regression is used to generate reduced-order\nsurrogates, mapping unit cell designs to their homogenized effective elastic\nconstant. It is demonstrated that the adopted workflow enables a high-value\nlow-dimensional representation of the voluminous stochastic metamaterial\ndataset, facilitating the construction of robust structure-property maps.\nFinally, an uncertainty-based active learning framework is utilized to train a\nsurrogate model with a significantly smaller number of data points compared to\nthe original full dataset. It is shown that a dataset as small as $0.61\\%$ of\nthe entire dataset is sufficient to generate accurate and robust\nstructure-property maps.\n","authors":["Hooman Danesh","Maruthi Annamaraju","Tim Brepols","Stefanie Reese","Surya R. Kalidindi"],"pdf_url":"https://arxiv.org/pdf/2505.01283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01281v1","updated":"2025-05-02T13:58:36Z","published":"2025-05-02T13:58:36Z","title":"A Physics-preserved Transfer Learning Method for Differential Equations","summary":"  While data-driven methods such as neural operator have achieved great success\nin solving differential equations (DEs), they suffer from domain shift problems\ncaused by different learning environments (with data bias or equation changes),\nwhich can be alleviated by transfer learning (TL). However, existing TL methods\nadopted in DEs problems lack either generalizability in general DEs problems or\nphysics preservation during training. In this work, we focus on a general\ntransfer learning method that adaptively correct the domain shift and preserve\nphysical information. Mathematically, we characterize the data domain as\nproduct distribution and the essential problems as distribution bias and\noperator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that\nsimultaneously admits generalizability to common DEs and physics preservation\nof specific problem is proposed to adapt the data-driven model to target domain\nutilizing the push-forward distribution induced by the POTT map. Extensive\nexperiments demonstrate the superior performance, generalizability and physics\npreservation of the proposed POTT method.\n","authors":["Hao-Ran Yang","Chuan-Xian Ren"],"pdf_url":"https://arxiv.org/pdf/2505.01281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01261v1","updated":"2025-05-02T13:28:50Z","published":"2025-05-02T13:28:50Z","title":"Enhancing Obsolescence Forecasting with Deep Generative Data\n  Augmentation: A Semi-Supervised Framework for Low-Data Industrial\n  Applications","summary":"  The challenge of electronic component obsolescence is particularly critical\nin systems with long life cycles. Various obsolescence management methods are\nemployed to mitigate its impact, with obsolescence forecasting being a highly\nsought-after and prominent approach. As a result, numerous machine\nlearning-based forecasting methods have been proposed. However, machine\nlearning models require a substantial amount of relevant data to achieve high\nprecision, which is lacking in the current obsolescence landscape in some\nsituations. This work introduces a novel framework for obsolescence forecasting\nbased on deep learning. The proposed framework solves the lack of available\ndata through deep generative modeling, where new obsolescence cases are\ngenerated and used to augment the training dataset. The augmented dataset is\nthen used to train a classical machine learning-based obsolescence forecasting\nmodel. To train classical forecasting models using augmented datasets, existing\nclassical supervised-learning classifiers are adapted for semi-supervised\nlearning within this framework. The proposed framework demonstrates\nstate-of-the-art results on benchmarking datasets.\n","authors":["Elie Saad","Mariem Besbes","Marc Zolghadri","Victor Czmil","Claude Baron","Vincent Bourgeois"],"pdf_url":"https://arxiv.org/pdf/2505.01261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15106v6","updated":"2025-05-02T13:21:13Z","published":"2024-01-25T16:21:37Z","title":"Underspecified Human Decision Experiments Considered Harmful","summary":"  Decision-making with information displays is a key focus of research in areas\nlike human-AI collaboration and data visualization. However, what constitutes a\ndecision problem, and what is required for an experiment to conclude that\ndecisions are flawed, remain imprecise. We present a widely applicable\ndefinition of a decision problem synthesized from statistical decision theory\nand information economics. We claim that to attribute loss in human performance\nto bias, an experiment must provide the information that a rational agent would\nneed to identify the normative decision. We evaluate whether recent empirical\nresearch on AI-assisted decisions achieves this standard. We find that only 10\n(26%) of 39 studies that claim to identify biased behavior presented\nparticipants with sufficient information to make this claim in at least one\ntreatment condition. We motivate the value of studying well-defined decision\nproblems by describing a characterization of performance losses they allow to\nbe conceived.\n","authors":["Jessica Hullman","Alex Kale","Jason Hartline"],"pdf_url":"https://arxiv.org/pdf/2401.15106v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01238v1","updated":"2025-05-02T13:00:05Z","published":"2025-05-02T13:00:05Z","title":"EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods\n  on NLP Models","summary":"  As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP.\n","authors":["Mahdi Dhaini","Kafaite Zahra Hussain","Efstratios Zaradoukas","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2505.01238v1.pdf","comment":"Accepted to the xAI World Conference (2025) - System Demonstration"},{"id":"http://arxiv.org/abs/2505.01198v1","updated":"2025-05-02T11:41:25Z","published":"2025-05-02T11:41:25Z","title":"Gender Bias in Explainability: Investigating Performance Disparity in\n  Post-hoc Methods","summary":"  While research on applications and evaluations of explanation methods\ncontinues to expand, fairness of the explanation methods concerning disparities\nin their performance across subgroups remains an often overlooked aspect. In\nthis paper, we address this gap by showing that, across three tasks and five\nlanguage models, widely used post-hoc feature attribution methods exhibit\nsignificant gender disparity with respect to their faithfulness, robustness,\nand complexity. These disparities persist even when the models are pre-trained\nor fine-tuned on particularly unbiased datasets, indicating that the\ndisparities we observe are not merely consequences of biased training data. Our\nresults highlight the importance of addressing disparities in explanations when\ndeveloping and applying explainability methods, as these can lead to biased\noutcomes against certain subgroups, with particularly critical implications in\nhigh-stakes contexts. Furthermore, our findings underscore the importance of\nincorporating the fairness of explanations, alongside overall model fairness\nand explainability, as a requirement in regulatory frameworks.\n","authors":["Mahdi Dhaini","Ege Erdogan","Nils Feldhus","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2505.01198v1.pdf","comment":"Accepted to ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) 2025"},{"id":"http://arxiv.org/abs/2505.00016v2","updated":"2025-05-02T11:34:00Z","published":"2025-04-23T19:02:04Z","title":"Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning","summary":"  This work reframes the Text-to-SQL task as a pathway for teaching large\nlanguage models (LLMs) to reason over and manipulate tabular data--moving\nbeyond the traditional focus on query generation. We propose a two-stage\nframework that leverages SQL supervision to develop transferable table\nreasoning capabilities. First, we synthesize detailed chain-of-thought (CoT)\ntraces from real-world SQL queries, providing step-by-step, clause-level\nsupervision that teaches the model how to traverse, filter, and aggregate table\nfields. Second, we introduce a Group Relative Policy Optimization (GRPO)\nreinforcement learning objective that connects SQL execution accuracy to\ngeneralizable reasoning by encouraging steps that extend beyond task-specific\nsyntax and transfer across datasets. Empirically, our approach improves\nperformance on standard Text-to-SQL benchmarks and achieves substantial gains\non reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced\ngeneralization and interpretability. Specifically, the distilled-quantized\nLLaMA model achieved a relative 33.9\\% increase in accuracy when trained on\nText-to-SQL tasks, while Qwen achieved a relative 14.5\\% increase. These\nresults suggest that SQL can serve not only as a target formalism but also as\nan effective scaffold for learning robust, transferable reasoning over\nstructured data.\n","authors":["Josefa Lia Stoisser","Marc Boubnovski Martell","Julien Fauqueur"],"pdf_url":"https://arxiv.org/pdf/2505.00016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07446v3","updated":"2025-05-02T11:32:37Z","published":"2024-12-10T12:05:03Z","title":"A Causal World Model Underlying Next Token Prediction: Exploring GPT in\n  a Controlled Environment","summary":"  Do generative pre-trained transformer (GPT) models, trained only to predict\nthe next token, implicitly learn a world model from which a sequence is\ngenerated one token at a time? We address this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that GPT\nmodels, at inference time, can be utilized for zero-shot causal structure\nlearning for input sequences and present a confidence score. Empirical\nevaluation is conducted in a controlled environment using the setup and rules\nof the Othello and Chess strategy games. A GPT, pre-trained on real-world games\nplayed with the intention of winning, is tested on out-of-distribution\nsynthetic data consisting of sequences of random legal moves. We find that the\nGPT model is likely to generate legal next moves for out-of-distribution\nsequences for which a causal structure is encoded in the attention mechanism\nwith high confidence. In cases for which the GPT model generates illegal moves\nit also fails to capture any causal structure.\n","authors":["Raanan Y. Rohekar","Yaniv Gurwicz","Sungduk Yu","Estelle Aflalo","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2412.07446v3.pdf","comment":"International Conference on Machine Learning (ICML), 2025"},{"id":"http://arxiv.org/abs/2505.01192v1","updated":"2025-05-02T11:30:53Z","published":"2025-05-02T11:30:53Z","title":"Exploring the Impact of Explainable AI and Cognitive Capabilities on\n  Users' Decisions","summary":"  Artificial Intelligence (AI) systems are increasingly used for\ndecision-making across domains, raising debates over the information and\nexplanations they should provide. Most research on Explainable AI (XAI) has\nfocused on feature-based explanations, with less attention on alternative\nstyles. Personality traits like the Need for Cognition (NFC) can also lead to\ndifferent decision-making outcomes among low and high NFC individuals. We\ninvestigated how presenting AI information (prediction, confidence, and\naccuracy) and different explanation styles (example-based, feature-based,\nrule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive\nload in a loan application scenario. We also examined low and high NFC\nindividuals' differences in prioritizing XAI interface elements (loan\nattributes, AI information, and explanations), accuracy, and cognitive load.\nOur findings show that high AI confidence significantly increases reliance on\nAI while reducing cognitive load. Feature-based explanations did not enhance\naccuracy compared to other conditions. Although counterfactual explanations\nwere less understandable, they enhanced overall accuracy, increasing reliance\non AI and reducing cognitive load when AI predictions were correct. Both low\nand high NFC individuals prioritized explanations after loan attributes,\nleaving AI information as the least important. However, we found no significant\ndifferences between low and high NFC groups in accuracy or cognitive load,\nraising questions about the role of personality traits in AI-assisted\ndecision-making. These findings highlight the need for user-centric\npersonalization in XAI interfaces, incorporating diverse explanation styles and\nexploring multiple personality traits and other user characteristics to\noptimize human-AI collaboration.\n","authors":["Federico Maria Cau","Lucio Davide Spano"],"pdf_url":"https://arxiv.org/pdf/2505.01192v1.pdf","comment":"30 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.02636v2","updated":"2025-05-02T11:07:54Z","published":"2025-03-04T14:01:10Z","title":"YARE-GAN: Yet Another Resting State EEG-GAN","summary":"  In this study, we implement a Wasserstein GAN with Gradient Penalty (WGAN-GP)\nto generate multi-channel resting-state EEG data and assess the quality of the\nsynthesized signals through both visual and feature-based evaluations. Our\nresults indicate that the model effectively captures the statistical and\nspectral characteristics of real EEG data, although challenges remain in\nreplicating high-frequency oscillations in the frontal region. Additionally, we\ndemonstrate that the Critic's learned representations can be reused for gender\nclassification task, achieving an out-of-sample accuracy, significantly better\nthan a shuffled-label baseline and a model trained directly on EEG data. These\nfindings suggest that generative models can serve not only as EEG data\ngenerators but also as unsupervised feature extractors, reducing the need for\nmanual feature engineering. This study highlights the potential of GAN-based\nunsupervised learning for EEG analysis, suggesting avenues for more\ndata-efficient deep learning applications in neuroscience.\n","authors":["Yeganeh Farahzadi","Morteza Ansarinia","Zoltan Kekecs"],"pdf_url":"https://arxiv.org/pdf/2503.02636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01186v1","updated":"2025-05-02T11:01:00Z","published":"2025-05-02T11:01:00Z","title":"Secure Cluster-Based Hierarchical Federated Learning in Vehicular\n  Networks","summary":"  Hierarchical Federated Learning (HFL) has recently emerged as a promising\nsolution for intelligent decision-making in vehicular networks, helping to\naddress challenges such as limited communication resources, high vehicle\nmobility, and data heterogeneity. However, HFL remains vulnerable to\nadversarial and unreliable vehicles, whose misleading updates can significantly\ncompromise the integrity and convergence of the global model. To address these\nchallenges, we propose a novel defense framework that integrates dynamic\nvehicle selection with robust anomaly detection within a cluster-based HFL\narchitecture, specifically designed to counter Gaussian noise and gradient\nascent attacks. The framework performs a comprehensive reliability assessment\nfor each vehicle by evaluating historical accuracy, contribution frequency, and\nanomaly records. Anomaly detection combines Z-score and cosine similarity\nanalyses on model updates to identify both statistical outliers and directional\ndeviations in model updates. To further refine detection, an adaptive\nthresholding mechanism is incorporated into the cosine similarity metric,\ndynamically adjusting the threshold based on the historical accuracy of each\nvehicle to enforce stricter standards for consistently high-performing\nvehicles. In addition, a weighted gradient averaging mechanism is implemented,\nwhich assigns higher weights to gradient updates from more trustworthy\nvehicles. To defend against coordinated attacks, a cross-cluster consistency\ncheck is applied to identify collaborative attacks in which multiple\ncompromised clusters coordinate misleading updates. Together, these mechanisms\nform a multi-level defense strategy to filter out malicious contributions\neffectively. Simulation results show that the proposed algorithm significantly\nreduces convergence time compared to benchmark methods across both 1-hop and\n3-hop topologies.\n","authors":["M. Saeid HaghighiFard","Sinem Coleri"],"pdf_url":"https://arxiv.org/pdf/2505.01186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01185v1","updated":"2025-05-02T11:00:40Z","published":"2025-05-02T11:00:40Z","title":"EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an\n  Environmental-Aware Path Loss and Adaptive RSSI Smoothing","summary":"  LoRaWAN technology's extensive coverage positions it as a strong contender\nfor large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor\nlocalization remains challenging due to complex environmental conditions,\nmultipath fading, and transient obstructions. This paper proposes a lightweight\nbut robust approach combining adaptive filtering with an extended log-distance,\nmulti-wall path loss and shadowing (PLS) model. Our methodology augments\nconventional models with critical LoRaWAN parameters (received signal strength\nindicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic\nenvironmental indicators (temperature, humidity, carbon dioxide, particulate\nmatter, and barometric pressure). An adaptive Kalman filter reduces RSSI\nfluctuations, isolating persistent trends from momentary noise. Using a\nsix-month dataset of 1,328,334 field measurements, we evaluate three models:\nthe baseline COST 231 multi-wall model (MWM), the baseline model augmented with\nenvironmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered\nRSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF\nachieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP\n(10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation\nreduces systematic errors by 41.22%, while Kalman filtering significantly\nenhances robustness under high RSSI volatility by 42.63%, on average across all\ndevices. These findings present an interpretable, efficient solution for\nprecise indoor LoRaWAN localization in dynamically changing environments.\n","authors":["Nahshon Mokua Obiri","Kristof Van Laerhoven"],"pdf_url":"https://arxiv.org/pdf/2505.01185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01182v1","updated":"2025-05-02T10:50:04Z","published":"2025-05-02T10:50:04Z","title":"TSTMotion: Training-free Scene-awarenText-to-motion Generation","summary":"  Text-to-motion generation has recently garnered significant research\ninterest, primarily focusing on generating human motion sequences in blank\nbackgrounds. However, human motions commonly occur within diverse 3D scenes,\nwhich has prompted exploration into scene-aware text-to-motion generation\nmethods. Yet, existing scene-aware methods often rely on large-scale\nground-truth motion sequences in diverse 3D scenes, which poses practical\nchallenges due to the expensive cost. To mitigate this challenge, we are the\nfirst to propose a \\textbf{T}raining-free \\textbf{S}cene-aware\n\\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that\nefficiently empowers pre-trained blank-background motion generators with the\nscene-aware capability. Specifically, conditioned on the given 3D scene and\ntext description, we adopt foundation models together to reason, predict and\nvalidate a scene-aware motion guidance. Then, the motion guidance is\nincorporated into the blank-background motion generators with two\nmodifications, resulting in scene-aware text-driven motion sequences. Extensive\nexperiments demonstrate the efficacy and generalizability of our proposed\nframework. We release our code in \\href{https://tstmotion.github.io/}{Project\nPage}.\n","authors":["Ziyan Guo","Haoxuan Qu","Hossein Rahmani","Dewen Soh","Ping Hu","Qiuhong Ke","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2505.01182v1.pdf","comment":"Accepted by ICME2025"},{"id":"http://arxiv.org/abs/2505.01181v1","updated":"2025-05-02T10:48:40Z","published":"2025-05-02T10:48:40Z","title":"Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary\n  Swarms","summary":"  Swarming systems, such as for example multi-drone networks, excel at\ncooperative tasks like monitoring, surveillance, or disaster assistance in\ncritical environments, where autonomous agents make decentralized decisions in\norder to fulfill team-level objectives in a robust and efficient manner.\nUnfortunately, team-level coordinated strategies in the wild are vulnerable to\ndata poisoning attacks, resulting in either inaccurate coordination or\nadversarial behavior among the agents. To address this challenge, we contribute\na framework that investigates the effects of such data poisoning attacks, using\nexplainable AI methods. We model the interaction among agents using\nevolutionary intelligence, where an optimal coalition strategically emerges to\nperform coordinated tasks. Then, through a rigorous evaluation, the swarm model\nis systematically poisoned using data manipulation attacks. We showcase the\napplicability of explainable AI methods to quantify the effects of poisoning on\nthe team strategy and extract footprint characterizations that enable\ndiagnosing. Our findings indicate that when the model is poisoned above 10%,\nnon-optimal strategies resulting in inefficient cooperation can be identified.\n","authors":["Mehrdad Asadi","Roxana Rădulescu","Ann Nowé"],"pdf_url":"https://arxiv.org/pdf/2505.01181v1.pdf","comment":"To appear in short form in Genetic and Evolutionary Computation\n  Conference (GECCO '25 Companion), 2025"},{"id":"http://arxiv.org/abs/2311.09830v4","updated":"2025-05-02T10:41:18Z","published":"2023-11-16T11:55:27Z","title":"Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning","summary":"  Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL. Our NL prompts yield better performance than PDDL\nprompts and simple template-based NL prompts. Compared to symbolic planners,\nLLM planning lags far behind; but in some domains, our best LLM configuration\nscales up further than A$^\\star$ using LM-cut.\n","authors":["Katharina Stein","Daniel Fišer","Jörg Hoffmann","Alexander Koller"],"pdf_url":"https://arxiv.org/pdf/2311.09830v4.pdf","comment":"Extended version of the paper from the ICAPS'25 proceedings (same\n  main part + additional appendix)"},{"id":"http://arxiv.org/abs/2405.14606v4","updated":"2025-05-02T10:39:23Z","published":"2024-05-23T14:19:21Z","title":"Logical Characterizations of Recurrent Graph Neural Networks with Reals\n  and Floats","summary":"  In pioneering work from 2019, Barcel\\'o and coauthors identified logics that\nprecisely match the expressive power of constant iteration-depth graph neural\nnetworks (GNNs) relative to properties definable in first-order logic. In this\narticle, we give exact logical characterizations of recurrent GNNs in two\nscenarios: (1) in the setting with floating-point numbers and (2) with reals.\nFor floats, the formalism matching recurrent GNNs is a rule-based modal logic\nwith counting, while for reals we use a suitable infinitary modal logic, also\nwith counting. These results give exact matches between logics and GNNs in the\nrecurrent setting without relativising to a background logic in either case,\nbut using some natural assumptions about floating-point arithmetic. Applying\nour characterizations, we also prove that, relative to graph properties\ndefinable in monadic second-order logic (MSO), our infinitary and rule-based\nlogics are equally expressive. This implies that recurrent GNNs with reals and\nfloats have the same expressive power over MSO-definable properties and shows\nthat, for such properties, also recurrent GNNs with reals are characterized by\na (finitary!) rule-based modal logic. In the general case, in contrast, the\nexpressive power with floats is weaker than with reals. In addition to\nlogic-oriented results, we also characterize recurrent GNNs, with both reals\nand floats, via distributed automata, drawing links to distributed computing\nmodels.\n","authors":["Veeti Ahvonen","Damian Heiman","Antti Kuusisto","Carsten Lutz"],"pdf_url":"https://arxiv.org/pdf/2405.14606v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01177v1","updated":"2025-05-02T10:35:26Z","published":"2025-05-02T10:35:26Z","title":"LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures","summary":"  As large language models (LLMs) continue to evolve, it is critical to assess\nthe security threats and vulnerabilities that may arise both during their\ntraining phase and after models have been deployed. This survey seeks to define\nand categorize the various attacks targeting LLMs, distinguishing between those\nthat occur during the training phase and those that affect already trained\nmodels. A thorough analysis of these attacks is presented, alongside an\nexploration of defense mechanisms designed to mitigate such threats. Defenses\nare classified into two primary categories: prevention-based and\ndetection-based defenses. Furthermore, our survey summarizes possible attacks\nand their corresponding defense strategies. It also provides an evaluation of\nthe effectiveness of the known defense mechanisms for the different security\nthreats. Our survey aims to offer a structured framework for securing LLMs,\nwhile also identifying areas that require further research to improve and\nstrengthen defenses against emerging security challenges.\n","authors":["Francisco Aguilera-Martínez","Fernando Berzal"],"pdf_url":"https://arxiv.org/pdf/2505.01177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09762v2","updated":"2025-05-02T10:33:06Z","published":"2025-02-13T20:45:48Z","title":"AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit","summary":"  Adaptive teaming-the capability of agents to effectively collaborate with\nunfamiliar teammates without prior coordination-is widely explored in virtual\nvideo games but overlooked in real-world multi-robot contexts. Yet, such\nadaptive collaboration is crucial for real-world applications, including border\nsurveillance, search-and-rescue, and counter-terrorism operations. To address\nthis gap, we introduce AT-Drone, the first dedicated benchmark explicitly\ndesigned to facilitate comprehensive training and evaluation of adaptive\nteaming strategies in multi-drone pursuit scenarios. AT-Drone makes the\nfollowing key contributions: (1) An adaptable simulation environment\nconfigurator that enables intuitive and rapid setup of adaptive teaming\nmulti-drone pursuit tasks, including four predefined pursuit environments. (2)\nA streamlined real-world deployment pipeline that seamlessly translates\nsimulation insights into practical drone evaluations using edge devices and\nCrazyflie drones. (3) A novel algorithm zoo integrated with a distributed\ntraining framework, featuring diverse algorithms explicitly tailored, for the\nfirst time, to multi-pursuer and multi-evader settings. (4) Standardized\nevaluation protocols with newly designed unseen drone zoos, explicitly designed\nto rigorously assess the performance of adaptive teaming. Comprehensive\nexperimental evaluations across four progressively challenging multi-drone\npursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive\nteaming research. Real-world drone experiments further validate its practical\nfeasibility and utility for realistic robotic operations. Videos, code and\nweights are available at \\url{https://sites.google.com/view/at-drone}.\n","authors":["Yang Li","Junfan Chen","Feng Xue","Jiabin Qiu","Wenbin Li","Qingrui Zhang","Ying Wen","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2502.09762v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2504.14204v2","updated":"2025-05-02T10:25:23Z","published":"2025-04-19T06:35:06Z","title":"DConAD: A Differencing-based Contrastive Representation Learning\n  Framework for Time Series Anomaly Detection","summary":"  Time series anomaly detection holds notable importance for risk\nidentification and fault detection across diverse application domains.\nUnsupervised learning methods have become popular because they have no\nrequirement for labels. However, due to the challenges posed by the\nmultiplicity of abnormal patterns, the sparsity of anomalies, and the growth of\ndata scale and complexity, these methods often fail to capture robust and\nrepresentative dependencies within the time series for identifying anomalies.\nTo enhance the ability of models to capture normal patterns of time series and\navoid the retrogression of modeling ability triggered by the dependencies on\nhigh-quality prior knowledge, we propose a differencing-based contrastive\nrepresentation learning framework for time series anomaly detection (DConAD).\nSpecifically, DConAD generates differential data to provide additional\ninformation about time series and utilizes transformer-based architecture to\ncapture spatiotemporal dependencies, which enhances the robustness of unbiased\nrepresentation learning ability. Furthermore, DConAD implements a novel KL\ndivergence-based contrastive learning paradigm that only uses positive samples\nto avoid deviation from reconstruction and deploys the stop-gradient strategy\nto compel convergence. Extensive experiments on five public datasets show the\nsuperiority and effectiveness of DConAD compared with nine baselines. The code\nis available at https://github.com/shaieesss/DConAD.\n","authors":["Wenxin Zhang","Xiaojian Lin","Wenjun Yu","Guangzhen Yao","jingxiang Zhong","Yu Li","Renda Han","Songcheng Xu","Hao Shi","Cuicui Luo"],"pdf_url":"https://arxiv.org/pdf/2504.14204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01169v1","updated":"2025-05-02T10:17:49Z","published":"2025-05-02T10:17:49Z","title":"Distilling Two-Timed Flow Models by Separately Matching Initial and\n  Terminal Velocities","summary":"  A flow matching model learns a time-dependent vector field $v_t(x)$ that\ngenerates a probability path $\\{ p_t \\}_{0 \\leq t \\leq 1}$ that interpolates\nbetween a well-known noise distribution ($p_0$) and the data distribution\n($p_1$). It can be distilled into a \\emph{two-timed flow model} (TTFM)\n$\\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an\ninitial time $s$ to another belonging to the distribution at a terminal time\n$t$ in one function evaluation. We present a new loss function for TTFM\ndistillation called the \\emph{initial/terminal velocity matching} (ITVM) loss\nthat extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi\net al. by adding redundant terms to match the initial velocities at time $s$,\nremoving the derivative from the terminal velocity term at time $t$, and using\na version of the model under training, stabilized by exponential moving\naveraging (EMA), to compute the target terminal average velocity. Preliminary\nexperiments show that our loss leads to better few-step generation performance\non multiple types of datasets and model architectures over baselines.\n","authors":["Pramook Khungurn","Pratch Piyawongwisal","Sira Sriswadi","Supasorn Suwajanakorn"],"pdf_url":"https://arxiv.org/pdf/2505.01169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01168v1","updated":"2025-05-02T10:17:33Z","published":"2025-05-02T10:17:33Z","title":"Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for\n  Adversarial Transferability","summary":"  The development of model ensemble attacks has significantly improved the\ntransferability of adversarial examples, but this progress also poses severe\nthreats to the security of deep neural networks. Existing methods, however,\nface two critical challenges: insufficient capture of shared gradient\ndirections across models and a lack of adaptive weight allocation mechanisms.\nTo address these issues, we propose a novel method Harmonized Ensemble for\nAdversarial Transferability (HEAT), which introduces domain generalization into\nadversarial example generation for the first time. HEAT consists of two key\nmodules: Consensus Gradient Direction Synthesizer, which uses Singular Value\nDecomposition to synthesize shared gradient directions; and Dual-Harmony Weight\nOrchestrator which dynamically balances intra-domain coherence, stabilizing\ngradients within individual models, and inter-domain diversity, enhancing\ntransferability across models. Experimental results demonstrate that HEAT\nsignificantly outperforms existing methods across various datasets and\nsettings, offering a new perspective and direction for adversarial attack\nresearch.\n","authors":["Zhaoyang Ma","Zhihao Wu","Wang Lu","Xin Gao","Jinghang Yue","Taolin Zhang","Lipo Wang","Youfang Lin","Jing Wang"],"pdf_url":"https://arxiv.org/pdf/2505.01168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01162v1","updated":"2025-05-02T10:08:34Z","published":"2025-05-02T10:08:34Z","title":"On the Limitations of Steering in Language Model Alignment","summary":"  Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models.\n","authors":["Chebrolu Niranjan","Kokil Jaidka","Gerard Christopher Yeo"],"pdf_url":"https://arxiv.org/pdf/2505.01162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04744v2","updated":"2025-05-02T09:58:28Z","published":"2024-09-07T07:40:43Z","title":"Reward Guidance for Reinforcement Learning Tasks Based on Large Language\n  Models: The LMGT Framework","summary":"  The inherent uncertainty in the environmental transition model of\nReinforcement Learning (RL) necessitates a delicate balance between exploration\nand exploitation. This balance is crucial for optimizing computational\nresources to accurately estimate expected rewards for the agent. In scenarios\nwith sparse rewards, such as robotic control systems, achieving this balance is\nparticularly challenging. However, given that many environments possess\nextensive prior knowledge, learning from the ground up in such contexts may be\nredundant. To address this issue, we propose Language Model Guided reward\nTuning (LMGT), a novel, sample-efficient framework. LMGT leverages the\ncomprehensive prior knowledge embedded in Large Language Models (LLMs) and\ntheir proficiency in processing non-standard data forms, such as wiki\ntutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances\nexploration and exploitation, thereby guiding the agent's exploratory behavior\nand enhancing sample efficiency. We have rigorously evaluated LMGT across\nvarious RL tasks and evaluated it in the embodied robotic environment\nHousekeep. Our results demonstrate that LMGT consistently outperforms baseline\nmethods. Furthermore, the findings suggest that our framework can substantially\nreduce the computational resources required during the RL training phase.\n","authors":["Yongxin Deng","Xihe Qiu","Jue Chen","Xiaoyu Tan"],"pdf_url":"https://arxiv.org/pdf/2409.04744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00598v2","updated":"2025-05-02T09:34:03Z","published":"2025-05-01T15:31:09Z","title":"Fast and Low-Cost Genomic Foundation Models via Outlier Removal","summary":"  To address the challenge of scarce computational resources in genomic\nmodeling, we introduce GERM, a genomic foundation model with strong compression\nperformance and fast adaptability. GERM improves upon models like DNABERT-2 by\neliminating outliers that hinder low-rank adaptation and post-training\nquantization, enhancing both efficiency and robustness. We replace the vanilla\nattention layer with an outlier-free mechanism inspired by associative memory\nmodels. By removing outliers during both pre-training and fine-tuning, this\napproach accelerates adaptation, reduces computational costs, and enhances\nquantization robustness within acceptable loss margins. Additionally, we\npropose GERM-T, a strategy that employs small-step continual learning within\nthe outlier-free framework, leveraging original checkpoints to avoid retraining\nfrom scratch. Empirically, GERM improves fine-tuning performance by 37.98% and\nquantization by 64.34% over the baseline model. It also reduces average\nkurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leading\nmethods, GERM consistently delivers superior performance, offering a practical\nsolution for genomic modeling in resource-constrained settings. Code is\navailable at https://github.com/MAGICS-LAB/GERM.\n","authors":["Haozheng Luo","Chenghao Qiu","Maojiang Su","Zhihan Zhou","Zoe Mehta","Guo Ye","Jerry Yao-Chieh Hu","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2505.00598v2.pdf","comment":"International Conference on Machine Learning (ICML) 2025"},{"id":"http://arxiv.org/abs/2505.01130v1","updated":"2025-05-02T09:16:44Z","published":"2025-05-02T09:16:44Z","title":"Risk Analysis and Design Against Adversarial Actions","summary":"  Learning models capable of providing reliable predictions in the face of\nadversarial actions has become a central focus of the machine learning\ncommunity in recent years. This challenge arises from observing that data\nencountered at deployment time often deviate from the conditions under which\nthe model was trained. In this paper, we address deployment-time adversarial\nactions and propose a versatile, well-principled framework to evaluate the\nmodel's robustness against attacks of diverse types and intensities. While we\ninitially focus on Support Vector Regression (SVR), the proposed approach\nextends naturally to the broad domain of learning via relaxed optimization\ntechniques. Our results enable an assessment of the model vulnerability without\nrequiring additional test data and operate in a distribution-free setup. These\nresults not only provide a tool to enhance trust in the model's applicability\nbut also aid in selecting among competing alternatives. Later in the paper, we\nshow that our findings also offer useful insights for establishing new results\nwithin the out-of-distribution framework.\n","authors":["Marco C. Campi","Algo Carè","Luis G. Crespo","Simone Garatti","Federico A. Ramponi"],"pdf_url":"https://arxiv.org/pdf/2505.01130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14012v3","updated":"2025-05-02T09:04:45Z","published":"2025-01-23T18:44:25Z","title":"Transfer Learning of Surrogate Models via Domain Affine Transformation\n  Across Synthetic and Real-World Benchmarks","summary":"  Surrogate models are frequently employed as efficient substitutes for the\ncostly execution of real-world processes. However, constructing a high-quality\nsurrogate model often demands extensive data acquisition. A solution to this\nissue is to transfer pre-trained surrogate models for new tasks, provided that\ncertain invariances exist between tasks. This study focuses on transferring\nnon-differentiable surrogate models (e.g., random forests) from a source\nfunction to a target function, where we assume their domains are related by an\nunknown affine transformation, using only a limited amount of transfer data\npoints evaluated on the target. Previous research attempts to tackle this\nchallenge for differentiable models, e.g., Gaussian process regression, which\nminimizes the empirical loss on the transfer data by tuning the affine\ntransformations. In this paper, we extend the previous work to the random\nforest and assess its effectiveness on a widely-used artificial problem set -\nBlack-Box Optimization Benchmark (BBOB) testbed, and on four real-world\ntransfer learning problems. The results highlight the significant practical\nadvantages of the proposed method, particularly in reducing both the data\nrequirements and computational costs of training surrogate models for complex\nreal-world scenarios.\n","authors":["Shuaiqun Pan","Diederick Vermetten","Manuel López-Ibáñez","Thomas Bäck","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2501.14012v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01109v1","updated":"2025-05-02T08:43:50Z","published":"2025-05-02T08:43:50Z","title":"Self-Supervision Enhances Instance-based Multiple Instance Learning\n  Methods in Digital Pathology: A Benchmark Study","summary":"  Multiple Instance Learning (MIL) has emerged as the best solution for Whole\nSlide Image (WSI) classification. It consists of dividing each slide into\npatches, which are treated as a bag of instances labeled with a global label.\nMIL includes two main approaches: instance-based and embedding-based. In the\nformer, each patch is classified independently, and then the patch scores are\naggregated to predict the bag label. In the latter, bag classification is\nperformed after aggregating patch embeddings. Even if instance-based methods\nare naturally more interpretable, embedding-based MILs have usually been\npreferred in the past due to their robustness to poor feature extractors.\nHowever, recently, the quality of feature embeddings has drastically increased\nusing self-supervised learning (SSL). Nevertheless, many authors continue to\nendorse the superiority of embedding-based MIL. To investigate this further, we\nconduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6\nself-supervised methods with 4 backbones, 4 foundation models, and various\npathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL\nmethods never used before in the pathology domain. Through these extensive\nexperiments, we show that with a good SSL feature extractor, simple\ninstance-based MILs, with very few parameters, obtain similar or better\nperformance than complex, state-of-the-art (SOTA) embedding-based MIL methods,\nsetting new SOTA results on the BRACS and Camelyon16 datasets. Since simple\ninstance-based MIL methods are naturally more interpretable and explainable to\nclinicians, our results suggest that more effort should be put into\nwell-adapted SSL methods for WSI rather than into complex embedding-based MIL\nmethods.\n","authors":["Ali Mammadov","Loic Le Folgoc","Julien Adam","Anne Buronfosse","Gilles Hayem","Guillaume Hocquet","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2505.01109v1.pdf","comment":"Accepted for publication in the Journal of Medical Imaging (SPIE)"},{"id":"http://arxiv.org/abs/2502.12354v2","updated":"2025-05-02T08:24:50Z","published":"2025-02-17T22:42:53Z","title":"Human-centered explanation does not fit all: The interplay of\n  sociotechnical, cognitive, and individual factors in the effect AI\n  explanations in algorithmic decision-making","summary":"  Recent XAI studies have investigated what constitutes a \\textit{good}\nexplanation in AI-assisted decision-making. Despite the widely accepted\nhuman-friendly properties of explanations, such as contrastive and selective,\nexisting studies have yielded inconsistent findings. To address these gaps, our\nstudy focuses on the cognitive dimensions of explanation evaluation, by\nevaluating six explanations with different contrastive strategies and\ninformation selectivity and scrutinizing factors behind their valuation\nprocess. Our analysis results find that contrastive explanations are not the\nmost preferable or understandable in general; Rather, different contrastive and\nselective explanations were appreciated to a different extent based on who they\nare, when, how, and what to explain -- with different level of cognitive load\nand engagement and sociotechnical contexts. Given these findings, we call for a\nnuanced view of explanation strategies, with implications for designing AI\ninterfaces to accommodate individual and contextual differences in AI-assisted\ndecision-making.\n","authors":["Yongsu Ahn","Yu-Ru Lin","Malihe Alikhani","Eunjeong Cheon"],"pdf_url":"https://arxiv.org/pdf/2502.12354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19139v2","updated":"2025-05-02T08:16:08Z","published":"2025-04-27T07:27:17Z","title":"Fast and Robust: Task Sampling with Posterior and Diversity Synergies\n  for Adaptive Decision-Makers in Randomized Environments","summary":"  Task robust adaptation is a long-standing pursuit in sequential\ndecision-making. Some risk-averse strategies, e.g., the conditional\nvalue-at-risk principle, are incorporated in domain randomization or meta\nreinforcement learning to prioritize difficult tasks in optimization, which\ndemand costly intensive evaluations. The efficiency issue prompts the\ndevelopment of robust active task sampling to train adaptive policies, where\nrisk-predictive models are used to surrogate policy evaluation. This work\ncharacterizes the optimization pipeline of robust active task sampling as a\nMarkov decision process, posits theoretical and practical insights, and\nconstitutes robustness concepts in risk-averse scenarios. Importantly, we\npropose an easy-to-implement method, referred to as Posterior and Diversity\nSynergized Task Sampling (PDTS), to accommodate fast and robust sequential\ndecision-making. Extensive experiments show that PDTS unlocks the potential of\nrobust active task sampling, significantly improves the zero-shot and few-shot\nadaptation robustness in challenging tasks, and even accelerates the learning\nprocess under certain scenarios. Our project website is at\nhttps://thu-rllab.github.io/PDTS_project_page.\n","authors":["Yun Qu","Qi Cheems Wang","Yixiu Mao","Yiqin Lv","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2504.19139v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01094v1","updated":"2025-05-02T08:14:01Z","published":"2025-05-02T08:14:01Z","title":"Multi-Objective Reinforcement Learning for Water Management","summary":"  Many real-world problems (e.g., resource management, autonomous driving, drug\ndiscovery) require optimizing multiple, conflicting objectives. Multi-objective\nreinforcement learning (MORL) extends classic reinforcement learning to handle\nmultiple objectives simultaneously, yielding a set of policies that capture\nvarious trade-offs. However, the MORL field lacks complex, realistic\nenvironments and benchmarks. We introduce a water resource (Nile river basin)\nmanagement case study and model it as a MORL environment. We then benchmark\nexisting MORL algorithms on this task. Our results show that specialized water\nmanagement methods outperform state-of-the-art MORL approaches, underscoring\nthe scalability challenges MORL algorithms face in real-world scenarios.\n","authors":["Zuzanna Osika","Roxana Radelescu","Jazmin Zatarain Salazar","Frans Oliehoek","Pradeep K. Murukannaiah"],"pdf_url":"https://arxiv.org/pdf/2505.01094v1.pdf","comment":"Accepted to AAMAS 2025"},{"id":"http://arxiv.org/abs/2408.11433v2","updated":"2025-05-02T08:12:59Z","published":"2024-08-21T08:42:21Z","title":"Towards Aligned Data Removal via Twin Machine Unlearning","summary":"  Modern privacy regulations have spurred the evolution of machine unlearning,\na technique that enables the removal of data from an already trained ML model\nwithout requiring retraining from scratch. Previous unlearning methods tend to\ninduce the model to achieve lowest classification accuracy on the removal data.\nNonetheless, the authentic objective of machine unlearning is to align the\nunlearned model with the gold model, i.e., achieving the same classification\naccuracy as the gold model. For this purpose, we present a Twin Machine\nUnlearning (TMU) approach, where a twin unlearning problem is defined\ncorresponding to the original unlearning problem. As a results, the\ngeneralization-label predictor trained on the twin problem can be transferred\nto the original problem, facilitating aligned data removal. Comprehensive\nempirical experiments illustrate that our approach significantly enhances the\nalignment between the unlearned model and the gold model. Meanwhile, our method\nallows data removal without compromising the model accuracy.\n","authors":["Haoxuan Ji","Zheng Lin","Yuyao Sun","Gao Fei","Yuhang Wang","Haichang Gao","Zhenxing Niu"],"pdf_url":"https://arxiv.org/pdf/2408.11433v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01091v1","updated":"2025-05-02T08:07:24Z","published":"2025-05-02T08:07:24Z","title":"Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and\n  Radiological Report Generation","summary":"  Generative models have revolutionized Artificial Intelligence (AI),\nparticularly in multimodal applications. However, adapting these models to the\nmedical domain poses unique challenges due to the complexity of medical data\nand the stringent need for clinical accuracy. In this work, we introduce a\nframework specifically designed for multimodal medical data generation. By\nenabling the generation of multi-view chest X-rays and their associated\nclinical report, it bridges the gap between general-purpose vision-language\nmodels and the specialized requirements of healthcare. Leveraging the MIMIC-CXR\ndataset, the proposed framework shows superior performance in generating\nhigh-fidelity images and semantically coherent reports. Our quantitative\nevaluation reveals significant results in terms of FID and BLEU scores,\nshowcasing the quality of the generated data. Notably, our framework achieves\ncomparable or even superior performance compared to real data on downstream\ndisease classification tasks, underlining its potential as a tool for medical\nresearch and diagnostics. This study highlights the importance of\ndomain-specific adaptations in enhancing the relevance and utility of\ngenerative models for clinical applications, paving the way for future\nadvancements in synthetic multimodal medical data generation.\n","authors":["Daniele Molino","Francesco di Feola","Linlin Shen","Paolo Soda","Valerio Guarrasi"],"pdf_url":"https://arxiv.org/pdf/2505.01091v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2501.04614"},{"id":"http://arxiv.org/abs/2505.00568v2","updated":"2025-05-02T08:02:39Z","published":"2025-05-01T14:51:30Z","title":"Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor\n  Analysis with Missing Modalities","summary":"  Multimodal magnetic resonance imaging (MRI) constitutes the first line of\ninvestigation for clinicians in the care of brain tumors, providing crucial\ninsights for surgery planning, treatment monitoring, and biomarker\nidentification. Pre-training on large datasets have been shown to help models\nlearn transferable representations and adapt with minimal labeled data. This\nbehavior is especially valuable in medical imaging, where annotations are often\nscarce. However, applying this paradigm to multimodal medical data introduces a\nchallenge: most existing approaches assume that all imaging modalities are\navailable during both pre-training and fine-tuning. In practice, missing\nmodalities often occur due to acquisition issues, specialist unavailability, or\nspecific experimental designs on small in-house datasets. Consequently, a\ncommon approach involves training a separate model for each desired modality\ncombination, making the process both resource-intensive and impractical for\nclinical use. Therefore, we introduce BM-MAE, a masked image modeling\npre-training strategy tailored for multimodal MRI data. The same pre-trained\nmodel seamlessly adapts to any combination of available modalities, extracting\nrich representations that capture both intra- and inter-modal information. This\nallows fine-tuning on any subset of modalities without requiring architectural\nchanges, while still benefiting from a model pre-trained on the full set of\nmodalities. Extensive experiments show that the proposed pre-training strategy\noutperforms or remains competitive with baselines that require separate\npre-training for each modality subset, while substantially surpassing training\nfrom scratch on several downstream tasks. Additionally, it can quickly and\nefficiently reconstruct missing modalities, highlighting its practical value.\nCode and trained models are available at: https://github.com/Lucas-rbnt/BM-MAE\n","authors":["Lucas Robinet","Ahmad Berjaoui","Elizabeth Cohen-Jonathan Moyal"],"pdf_url":"https://arxiv.org/pdf/2505.00568v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01085v1","updated":"2025-05-02T07:46:41Z","published":"2025-05-02T07:46:41Z","title":"Artificial Intelligence in Government: Why People Feel They Lose Control","summary":"  The use of Artificial Intelligence (AI) in public administration is expanding\nrapidly, moving from automating routine tasks to deploying generative and\nagentic systems that autonomously act on goals. While AI promises greater\nefficiency and responsiveness, its integration into government functions raises\nconcerns about fairness, transparency, and accountability. This article applies\nprincipal-agent theory (PAT) to conceptualize AI adoption as a special case of\ndelegation, highlighting three core tensions: assessability (can decisions be\nunderstood?), dependency (can the delegation be reversed?), and contestability\n(can decisions be challenged?). These structural challenges may lead to a\n\"failure-by-success\" dynamic, where early functional gains obscure long-term\nrisks to democratic legitimacy. To test this framework, we conducted a\npre-registered factorial survey experiment across tax, welfare, and law\nenforcement domains. Our findings show that although efficiency gains initially\nbolster trust, they simultaneously reduce citizens' perceived control. When the\nstructural risks come to the foreground, institutional trust and perceived\ncontrol both drop sharply, suggesting that hidden costs of AI adoption\nsignificantly shape public attitudes. The study demonstrates that PAT offers a\npowerful lens for understanding the institutional and political implications of\nAI in government, emphasizing the need for policymakers to address delegation\nrisks transparently to maintain public trust.\n","authors":["Alexander Wuttke","Adrian Rauchfleisch","Andreas Jungherr"],"pdf_url":"https://arxiv.org/pdf/2505.01085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01081v1","updated":"2025-05-02T07:39:08Z","published":"2025-05-02T07:39:08Z","title":"MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC\n  Benchmark","summary":"  Artificial Intelligence (AI) has achieved remarkable success in specialized\ntasks but struggles with efficient skill acquisition and generalization. The\nAbstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based\non minimal training requirements. While Large Language Models (LLMs) have\nrecently improved ARC performance, they rely on extensive pre-training and high\ncomputational costs. We introduce MADIL (MDL-based AI), a novel approach\nleveraging the Minimum Description Length (MDL) principle for efficient\ninductive learning. MADIL performs pattern-based decomposition, enabling\nstructured generalization. While its performance (7% at ArcPrize 2024) remains\nbelow LLM-based methods, it offers greater efficiency and interpretability.\nThis paper details MADIL's methodology, its application to ARC, and\nexperimental evaluations.\n","authors":["Sébastien Ferré"],"pdf_url":"https://arxiv.org/pdf/2505.01081v1.pdf","comment":"54 pages"},{"id":"http://arxiv.org/abs/2505.01073v1","updated":"2025-05-02T07:25:01Z","published":"2025-05-02T07:25:01Z","title":"Retrieval Augmented Learning: A Retrial-based Large Language Model\n  Self-Supervised Learning and Autonomous Knowledge Generation","summary":"  The lack of domain-specific data in the pre-training of Large Language Models\n(LLMs) severely limits LLM-based decision systems in specialized applications,\nwhile post-training a model in the scenarios requires significant computational\nresources. In this paper, we present Retrial-Augmented Learning (RAL), a\nreward-free self-supervised learning framework for LLMs that operates without\nmodel training. By developing Retrieval-Augmented Generation (RAG) into a\nmodule for organizing intermediate data, we realized a three-stage autonomous\nknowledge generation of proposing a hypothesis, validating the hypothesis, and\ngenerating the knowledge. The method is evaluated in the LLM-PySC2 environment,\na representative decision-making platform that combines sufficient complexity\nwith domain-specific knowledge requirements. Experiments demonstrate that the\nproposed method effectively reduces hallucination by generating and utilizing\nvalidated knowledge, and increases decision-making performance at an extremely\nlow cost. Meanwhile, the approach exhibits potential in\nout-of-distribution(OOD) tasks, robustness, and transferability, making it a\ncost-friendly but effective solution for decision-making problems and\nautonomous knowledge generation.\n","authors":["Zongyuan Li","Pengfei Li","Runnan Qi","Yanan Ni","Lumin Jiang","Hui Wu","Xuebo Zhang","Kuihua Huang","Xian Guo"],"pdf_url":"https://arxiv.org/pdf/2505.01073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05348v2","updated":"2025-05-02T07:20:36Z","published":"2024-11-08T06:04:22Z","title":"LLM-PySC2: Starcraft II learning environment for Large Language Models","summary":"  The tremendous potential has been demonstrated by large language models\n(LLMs) in intelligent decision-making problems, with unprecedented capabilities\nshown across diverse applications ranging from gaming AI systems to complex\nstrategic planning frameworks. However, the StarCraft II platform, which has\nbeen widely adopted for validating decision-making algorithms in the past\ndecade, has not yet provided substantial support for this emerging domain. To\naddress issues that LLMs cannot interface with the hundreds of actions of the\npysc2 backend and the lack of native support for multi-agent (MA)\ncollaboration, we propose the LLM-PySC2 environment. This is the first\nenvironment that offers LLMs the complete pysc2 action space with sufficient\nmulti-modal information and game Wiki knowledge. With an asynchronous query\narchitecture, the environment efficiently interacts with LLMs that maintain a\nconstant latency regardless of the scale of the agents' population. In the\nexperiments, we evaluated LLMs' decision-making performance in both the\nmacro-decision and micro-operation scenarios, with traditional StarCraft II\nMulti-Agent Challenge (SMAC) tasks and a series of new proposed. Results\nindicate that LLMs possess the potential to achieve victories in complex\nscenarios but cannot constantly generate correct decisions, especially in the\nrecovered pysc2 action space and MA settings. Without task-relevant\ninstructions, the pre-trained models suffer from issues such as hallucinations\nand inefficient collaboration. Our findings suggest that StarCraft II still\nchallenges in the era of large models, revealing that there is a lot to do to\ndevelop an advanced LLM decision-making system, and the proposed LLM-PySC2\nenvironment will support future development of LLM-based decision-making\nsolutions.\n","authors":["Zongyuan Li","Yanan Ni","Runnan Qi","Lumin Jiang","Chang Lu","Xiaojie Xu","Xiangbei Liu","Pengfei Li","Yunzheng Guo","Zhe Ma","Huanyu Li","Hui Wu","Xian Guo","Kuihua Huang","Xuebo Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.05348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01070v1","updated":"2025-05-02T07:18:52Z","published":"2025-05-02T07:18:52Z","title":"Improving Group Fairness in Knowledge Distillation via Laplace\n  Approximation of Early Exits","summary":"  Knowledge distillation (KD) has become a powerful tool for training compact\nstudent models using larger, pretrained teacher models, often requiring less\ndata and computational resources. Teacher models typically possess more layers\nand thus exhibit richer feature representations compared to their student\ncounterparts. Furthermore, student models tend to learn simpler, surface-level\nfeatures in their early layers. This discrepancy can increase errors in groups\nwhere labels spuriously correlate with specific input attributes, leading to a\ndecline in group fairness even when overall accuracy remains comparable to the\nteacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs),\nwhich enable predictions at multiple intermediate layers, have been employed.\nConfidence margins derived from these early exits have been utilized to\nreweight both cross-entropy and distillation losses on a per-instance basis. In\nthis paper, we propose that leveraging Laplace approximation-based methods to\nobtain well-calibrated uncertainty estimates can also effectively reweight\nchallenging instances and improve group fairness. We hypothesize that Laplace\napproximation offers a more robust identification of difficult or ambiguous\ninstances compared to margin-based approaches. To validate our claims, we\nbenchmark our approach using a Bert-based model on the MultiNLI dataset.\n","authors":["Edvin Fasth","Sagar Singh"],"pdf_url":"https://arxiv.org/pdf/2505.01070v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2505.01068v1","updated":"2025-05-02T07:18:00Z","published":"2025-05-02T07:18:00Z","title":"Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs","summary":"  Multimodal Sentiment Analysis (MSA) is a rapidly developing field that\nintegrates multimodal information to recognize sentiments, and existing models\nhave made significant progress in this area. The central challenge in MSA is\nmultimodal fusion, which is predominantly addressed by Multimodal Transformers\n(MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns.\nIn this work, from the perspective of efficiency optimization, we propose and\nprove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and\nwe introduce the graph-structured representation pattern of MulTs. Based on\nthis pattern, we propose an Interlaced Mask (IM) mechanism to design the\nGraph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is\nformally equivalent to MulTs which achieves an efficient weight-sharing\nmechanism without information disorder through IM, enabling All-Modal-In-One\nfusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called\nDecomposition is implemented to ensure avoiding additional computational\noverhead. Moreover, it achieves significantly higher performance than\ntraditional MulTs. To further validate the effectiveness of GsiT itself and the\nHMHG concept, we integrate them into multiple state-of-the-art models and\ndemonstrate notable performance improvements and parameter reduction on widely\nused MSA datasets.\n","authors":["Yijie Jin","Junjie Peng","Xuanchao Lin","Haochen Yuan","Lan Wang","Cangzhi Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.01068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12623v2","updated":"2025-05-02T07:17:44Z","published":"2025-03-16T19:32:32Z","title":"MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network","summary":"  Dynamic emotion recognition in the wild remains challenging due to the\ntransient nature of emotional expressions and temporal misalignment of\nmulti-modal cues. Traditional approaches predict valence and arousal and often\noverlook the inherent correlation between these two dimensions. The proposed\nMulti-modal Attention for Valence-Arousal Emotion Network (MAVEN) integrates\nvisual, audio, and textual modalities through a bi-directional cross-modal\nattention mechanism. MAVEN uses modality-specific encoders to extract features\nfrom synchronized video frames, audio segments, and transcripts, predicting\nemotions in polar coordinates following Russell's circumplex model. The\nevaluation of the Aff-Wild2 dataset using MAVEN achieved a concordance\ncorrelation coefficient (CCC) of 0.3061, surpassing the ResNet-50 baseline\nmodel with a CCC of 0.22. The multistage architecture captures the subtle and\ntransient nature of emotional expressions in conversational videos and improves\nemotion recognition in real-world situations. The code is available at:\nhttps://github.com/Vrushank-Ahire/MAVEN_8th_ABAW\n","authors":["Vrushank Ahire","Kunal Shah","Mudasir Nazir Khan","Nikhil Pakhale","Lownish Rai Sookha","M. A. Ganaie","Abhinav Dhall"],"pdf_url":"https://arxiv.org/pdf/2503.12623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01067v1","updated":"2025-05-02T07:16:20Z","published":"2025-05-02T07:16:20Z","title":"A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in\n  Model Repositories","summary":"  Recent advancements in large language models (LLMs) have spurred the\ndevelopment of diverse AI applications from code generation and video editing\nto text generation; however, AI supply chains such as Hugging Face, which host\npretrained models and their associated configuration files contributed by the\npublic, face significant security challenges; in particular, configuration\nfiles originally intended to set up models by specifying parameters and initial\nsettings can be exploited to execute unauthorized code, yet research has\nlargely overlooked their security compared to that of the models themselves; in\nthis work, we present the first comprehensive study of malicious configurations\non Hugging Face, identifying three attack scenarios (file, website, and\nrepository operations) that expose inherent risks; to address these threats, we\nintroduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in\nthe context of their associated runtime code and critical libraries,\neffectively detecting suspicious elements with low false positive rates and\nhigh accuracy; our extensive evaluation uncovers thousands of suspicious\nrepositories and configuration files, underscoring the urgent need for enhanced\nsecurity validation in AI model hosting platforms.\n","authors":["Ziqi Ding","Qian Fu","Junchen Ding","Gelei Deng","Yi Liu","Yuekang Li"],"pdf_url":"https://arxiv.org/pdf/2505.01067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01065v1","updated":"2025-05-02T07:15:22Z","published":"2025-05-02T07:15:22Z","title":"Good News for Script Kiddies? Evaluating Large Language Models for\n  Automated Exploit Generation","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, raising concerns about their potential for automated\nexploit generation (AEG). This paper presents the first systematic study on\nLLMs' effectiveness in AEG, evaluating both their cooperativeness and technical\nproficiency. To mitigate dataset bias, we introduce a benchmark with refactored\nversions of five software security labs. Additionally, we design an LLM-based\nattacker to systematically prompt LLMs for exploit generation. Our experiments\nreveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to\nuncensored models, while Llama3 is the most resistant. However, no model\nsuccessfully generates exploits for refactored labs, though GPT-4o's minimal\nerrors highlight the potential for LLM-driven AEG advancements.\n","authors":["David Jin","Qian Fu","Yuekang Li"],"pdf_url":"https://arxiv.org/pdf/2505.01065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01059v1","updated":"2025-05-02T07:09:38Z","published":"2025-05-02T07:09:38Z","title":"Model Tensor Planning","summary":"  Sampling-based model predictive control (MPC) offers strong performance in\nnonlinear and contact-rich robotic tasks, yet often suffers from poor\nexploration due to locally greedy sampling schemes. We propose \\emph{Model\nTensor Planning} (MTP), a novel sampling-based MPC framework that introduces\nhigh-entropy control trajectory generation through structured tensor sampling.\nBy sampling over randomized multipartite graphs and interpolating control\ntrajectories with B-splines and Akima splines, MTP ensures smooth and globally\ndiverse control candidates. We further propose a simple $\\beta$-mixing strategy\nthat blends local exploitative and global exploratory samples within the\nmodified Cross-Entropy Method (CEM) update, balancing control refinement and\nexploration. Theoretically, we show that MTP achieves asymptotic path coverage\nand maximum entropy in the control trajectory space in the limit of infinite\ntensor depth and width.\n  Our implementation is fully vectorized using JAX and compatible with MuJoCo\nXLA, supporting \\emph{Just-in-time} (JIT) compilation and batched rollouts for\nreal-time control with online domain randomization. Through experiments on\nvarious challenging robotic tasks, ranging from dexterous in-hand manipulation\nto humanoid locomotion, we demonstrate that MTP outperforms standard MPC and\nevolutionary strategy baselines in task success and control robustness. Design\nand sensitivity ablations confirm the effectiveness of MTP tensor sampling\nstructure, spline interpolation choices, and mixing strategy. Altogether, MTP\noffers a scalable framework for robust exploration in model-based planning and\ncontrol.\n","authors":["An T. Le","Khai Nguyen","Minh Nhat Vu","João Carvalho","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2505.01059v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.00230v2","updated":"2025-05-02T06:39:57Z","published":"2024-10-31T22:02:32Z","title":"Reinforcement learning with learned gadgets to tackle hard quantum\n  problems on real hardware","summary":"  Designing quantum circuits for specific tasks is challenging due to the\nexponential growth of the state space. We introduce gadget reinforcement\nlearning (GRL), which integrates reinforcement learning with program synthesis\nto automatically generate and incorporate composite gates (gadgets) into the\naction space. This enhances the exploration of parameterized quantum circuits\n(PQCs) for complex tasks like approximating ground states of quantum\nHamiltonians, an NP-hard problem. We evaluate GRL using the transverse field\nIsing model under typical computational budgets (e.g., 2- 3 days of GPU\nruntime). Our results show improved accuracy, hardware compatibility and\nscalability. GRL exhibits robust performance as the size and complexity of the\nproblem increases, even with constrained computational resources. By\nintegrating gadget extraction, GRL facilitates the discovery of reusable\ncircuit components tailored for specific hardware, bridging the gap between\nalgorithmic design and practical implementation. This makes GRL a versatile\nframework for optimizing quantum circuits with applications in\nhardware-specific optimizations and variational quantum algorithms. The code is\navailable at: https://github.com/Aqasch/Gadget_RL\n","authors":["Akash Kundu","Leopoldo Sarra"],"pdf_url":"https://arxiv.org/pdf/2411.00230v2.pdf","comment":"23 pages, 13 figures. Comments are encouraged"},{"id":"http://arxiv.org/abs/2505.01043v1","updated":"2025-05-02T06:33:25Z","published":"2025-05-02T06:33:25Z","title":"Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities","summary":"  Large language models (LLMs) have achieved impressive performance across\nvarious domains. However, the substantial hardware resources required for their\ntraining present a significant barrier to efficiency and scalability. To\nmitigate this challenge, low-precision training techniques have been widely\nadopted, leading to notable advancements in training efficiency. Despite these\ngains, low-precision training involves several components$\\unicode{x2013}$such\nas weights, activations, and gradients$\\unicode{x2013}$each of which can be\nrepresented in different numerical formats. The resulting diversity has created\na fragmented landscape in low-precision training research, making it difficult\nfor researchers to gain a unified overview of the field. This survey provides a\ncomprehensive review of existing low-precision training methods. To\nsystematically organize these approaches, we categorize them into three primary\ngroups based on their underlying numerical formats, which is a key factor\ninfluencing hardware compatibility, computational efficiency, and ease of\nreference for readers. The categories are: (1) fixed-point and integer-based\nmethods, (2) floating-point-based methods, and (3) customized format-based\nmethods. Additionally, we discuss quantization-aware training approaches, which\nshare key similarities with low-precision training during forward propagation.\nFinally, we highlight several promising research directions to advance this\nfield. A collection of papers discussed in this survey is provided in\nhttps://github.com/Hao840/Awesome-Low-Precision-Training.\n","authors":["Zhiwei Hao","Jianyuan Guo","Li Shen","Yong Luo","Han Hu","Guoxia Wang","Dianhai Yu","Yonggang Wen","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2505.01043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01036v1","updated":"2025-05-02T06:19:09Z","published":"2025-05-02T06:19:09Z","title":"Stagnation in Evolutionary Algorithms: Convergence $\\neq$ Optimality","summary":"  In the evolutionary computation community, it is widely believed that\nstagnation impedes convergence in evolutionary algorithms, and that convergence\ninherently indicates optimality. However, this perspective is misleading. In\nthis study, it is the first to highlight that the stagnation of an individual\ncan actually facilitate the convergence of the entire population, and\nconvergence does not necessarily imply optimality, not even local optimality.\nConvergence alone is insufficient to ensure the effectiveness of evolutionary\nalgorithms. Several counterexamples are provided to illustrate this argument.\n","authors":["Xiaojun Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.01036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01028v1","updated":"2025-05-02T05:55:56Z","published":"2025-05-02T05:55:56Z","title":"Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active\n  Directory","summary":"  Security vulnerabilities in Windows Active Directory (AD) systems are\ntypically modeled using an attack graph and hardening AD systems involves an\niterative workflow: security teams propose an edge to remove, and IT operations\nteams manually review these fixes before implementing the removal. As\nverification requires significant manual effort, we formulate an Adaptive Path\nRemoval Problem to minimize the number of steps in this iterative removal\nprocess. In our model, a wizard proposes an attack path in each step and\npresents it as a set of multiple-choice options to the IT admin. The IT admin\nthen selects one edge from the proposed set to remove. This process continues\nuntil the target $t$ is disconnected from source $s$ or the number of proposed\npaths reaches $B$. The model aims to optimize the human effort by minimizing\nthe expected number of interactions between the IT admin and the security\nwizard. We first prove that the problem is $\\mathcal{\\#P}$-hard. We then\npropose a set of solutions including an exact algorithm, an approximate\nalgorithm, and several scalable heuristics. Our best heuristic, called DPR, can\noperate effectively on larger-scale graphs compared to the exact algorithm and\nconsistently outperforms the approximate algorithm across all graphs. We verify\nthe effectiveness of our algorithms on several synthetic AD graphs and an AD\nattack graph collected from a real organization.\n","authors":["Huy Q. Ngo","Mingyu Guo","Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2505.01028v1.pdf","comment":"To be appear in IJCAI 2025"},{"id":"http://arxiv.org/abs/2501.00070v2","updated":"2025-05-02T05:27:38Z","published":"2024-12-29T18:58:09Z","title":"ICLR: In-Context Learning of Representations","summary":"  Recent work has demonstrated that semantics specified by pretraining data\ninfluence how representations of different concepts are organized in a large\nlanguage model (LLM). However, given the open-ended nature of LLMs, e.g., their\nability to in-context learn, we can ask whether models alter these pretraining\nsemantics to adopt alternative, context-specified ones. Specifically, if we\nprovide in-context exemplars wherein a concept plays a different role than what\nthe pretraining data suggests, do models reorganize their representations in\naccordance with these novel semantics? To answer this question, we take\ninspiration from the theory of conceptual role semantics and define a toy\n\"graph tracing\" task wherein the nodes of the graph are referenced via concepts\nseen during training (e.g., apple, bird, etc.) and the connectivity of the\ngraph is defined via some predefined structure (e.g., a square grid). Given\nexemplars that indicate traces of random walks on the graph, we analyze\nintermediate representations of the model and find that as the amount of\ncontext is scaled, there is a sudden re-organization from pretrained semantic\nrepresentations to in-context representations aligned with the graph structure.\nFurther, we find that when reference concepts have correlations in their\nsemantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure\nis still present in the representations, but is unable to dominate the\npretrained structure. To explain these results, we analogize our task to energy\nminimization for a predefined graph topology, providing evidence towards an\nimplicit optimization process to infer context-specified semantics. Overall,\nour findings indicate scaling context-size can flexibly re-organize model\nrepresentations, possibly unlocking novel capabilities.\n","authors":["Core Francisco Park","Andrew Lee","Ekdeep Singh Lubana","Yongyi Yang","Maya Okawa","Kento Nishi","Martin Wattenberg","Hidenori Tanaka"],"pdf_url":"https://arxiv.org/pdf/2501.00070v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2505.01016v1","updated":"2025-05-02T05:27:14Z","published":"2025-05-02T05:27:14Z","title":"Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO\n  Performance","summary":"  The success of large pre-trained object detectors hinges on their\nadaptability to diverse downstream tasks. While fine-tuning is the standard\nadaptation method, specializing these models for challenging fine-grained\ndomains necessitates careful consideration of feature granularity. The critical\nquestion remains: how deeply should the pre-trained backbone be fine-tuned to\noptimize for the specialized task without incurring catastrophic forgetting of\nthe original general capabilities? Addressing this, we present a systematic\nempirical study evaluating the impact of fine-tuning depth. We adapt a standard\nYOLOv8n model to a custom, fine-grained fruit detection dataset by\nprogressively unfreezing backbone layers (freeze points at layers 22, 15, and\n10) and training. Performance was rigorously evaluated on both the target fruit\ndataset and, using a dual-head evaluation architecture, on the original COCO\nvalidation set. Our results demonstrate unequivocally that deeper fine-tuning\n(unfreezing down to layer 10) yields substantial performance gains (e.g., +10\\%\nabsolute mAP50) on the fine-grained fruit task compared to only training the\nhead. Strikingly, this significant adaptation and specialization resulted in\nnegligible performance degradation (<0.1\\% absolute mAP difference) on the COCO\nbenchmark across all tested freeze levels. We conclude that adapting\nmid-to-late backbone features is highly effective for fine-grained\nspecialization. Critically, our results demonstrate this adaptation can be\nachieved without the commonly expected penalty of catastrophic forgetting,\npresenting a compelling case for exploring deeper fine-tuning strategies,\nparticularly when targeting complex domains or when maximizing specialized\nperformance is paramount.\n","authors":["Vishal Gandhi","Sagar Gandhi"],"pdf_url":"https://arxiv.org/pdf/2505.01016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01015v1","updated":"2025-05-02T05:26:50Z","published":"2025-05-02T05:26:50Z","title":"Value Portrait: Understanding Values of LLMs with Human-aligned\n  Benchmark","summary":"  The importance of benchmarks for assessing the values of language models has\nbeen pronounced due to the growing need of more authentic, human-aligned\nresponses. However, existing benchmarks rely on human or machine annotations\nthat are vulnerable to value-related biases. Furthermore, the tested scenarios\noften diverge from real-world contexts in which models are commonly used to\ngenerate text and express values. To address these issues, we propose the Value\nPortrait benchmark, a reliable framework for evaluating LLMs' value\norientations with two key characteristics. First, the benchmark consists of\nitems that capture real-life user-LLM interactions, enhancing the relevance of\nassessment results to real-world LLM usage and thus ecological validity.\nSecond, each item is rated by human subjects based on its similarity to their\nown thoughts, and correlations between these ratings and the subjects' actual\nvalue scores are derived. This psychometrically validated approach ensures that\nitems strongly correlated with specific values serve as reliable items for\nassessing those values. Through evaluating 27 LLMs with our benchmark, we find\nthat these models prioritize Benevolence, Security, and Self-Direction values\nwhile placing less emphasis on Tradition, Power, and Achievement values. Also,\nour analysis reveals biases in how LLMs perceive various demographic groups,\ndeviating from real human data.\n","authors":["Jongwook Han","Dongmin Choi","Woojung Song","Eun-Ju Lee","Yohan Jo"],"pdf_url":"https://arxiv.org/pdf/2505.01015v1.pdf","comment":"32 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.01009v1","updated":"2025-05-02T05:16:17Z","published":"2025-05-02T05:16:17Z","title":"Improving Large Language Model Planning with Action Sequence Similarity","summary":"  Planning is essential for artificial intelligence systems to look ahead and\nproactively determine a course of actions to reach objectives in the virtual\nand real world. Recent work on large language models (LLMs) sheds light on\ntheir planning capability in various tasks. However, it remains unclear what\nsignals in the context influence the model performance. In this work, we\nexplore how to improve the model planning capability through in-context\nlearning (ICL), specifically, what signals can help select the exemplars.\nThrough extensive experiments, we observe that commonly used problem similarity\nmay result in false positives with drastically different plans, which can\nmislead the model. In response, we propose to sample and filter exemplars\nleveraging plan side action sequence similarity (AS). We propose GRASE-DC: a\ntwo-stage pipeline that first re-samples high AS exemplars and then curates the\nselected exemplars with dynamic clustering on AS to achieve a balance of\nrelevance and diversity. Our experimental result confirms that GRASE-DC\nachieves significant performance improvement on various planning tasks (up to\n~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on\naverage). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a\nvalidator, we are able to even boost the performance by 18.9% more.\n  Extensive analysis validates the consistent performance improvement of\nGRASE-DC with various backbone LLMs and on both classical planning and natural\nlanguage planning benchmarks. GRASE-DC can further boost the planning accuracy\nby ~24 absolute points on harder problems using simpler problems as exemplars\nover a random baseline. This demonstrates its ability to generalize to\nout-of-distribution problems.\n","authors":["Xinran Zhao","Hanie Sedghi","Bernd Bohnet","Dale Schuurmans","Azade Nova"],"pdf_url":"https://arxiv.org/pdf/2505.01009v1.pdf","comment":"25 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.01007v1","updated":"2025-05-02T05:11:17Z","published":"2025-05-02T05:11:17Z","title":"Towards the Resistance of Neural Network Watermarking to Fine-tuning","summary":"  This paper proves a new watermarking method to embed the ownership\ninformation into a deep neural network (DNN), which is robust to fine-tuning.\nSpecifically, we prove that when the input feature of a convolutional layer\nonly contains low-frequency components, specific frequency components of the\nconvolutional filter will not be changed by gradient descent during the\nfine-tuning process, where we propose a revised Fourier transform to extract\nfrequency components from the convolutional filter. Additionally, we also prove\nthat these frequency components are equivariant to weight scaling and weight\npermutations. In this way, we design a watermark module to encode the watermark\ninformation to specific frequency components in a convolutional filter.\nPreliminary experiments demonstrate the effectiveness of our method.\n","authors":["Ling Tang","Yuefeng Chen","Hui Xue","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.01007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13725v2","updated":"2025-05-02T04:45:34Z","published":"2024-06-19T17:40:11Z","title":"Tree-Sliced Wasserstein Distance: A Geometric Perspective","summary":"  Many variants of Optimal Transport (OT) have been developed to address its\nheavy computation. Among them, notably, Sliced Wasserstein (SW) is widely used\nfor application domains by projecting the OT problem onto one-dimensional\nlines, and leveraging the closed-form expression of the univariate OT to reduce\nthe computational burden. However, projecting measures onto low-dimensional\nspaces can lead to a loss of topological information. To mitigate this issue,\nin this work, we propose to replace one-dimensional lines with a more intricate\nstructure, called tree systems. This structure is metrizable by a tree metric,\nwhich yields a closed-form expression for OT problems on tree systems. We\nprovide an extensive theoretical analysis to formally define tree systems with\ntheir topological properties, introduce the concept of splitting maps, which\noperate as the projection mechanism onto these structures, then finally propose\na novel variant of Radon transform for tree systems and verify its injectivity.\nThis framework leads to an efficient metric between measures, termed\nTree-Sliced Wasserstein distance on Systems of Lines (TSW-SL). By conducting a\nvariety of experiments on gradient flows, image style transfer, and generative\nmodels, we illustrate that our proposed approach performs favorably compared to\nSW and its variants.\n","authors":["Viet-Hoang Tran","Trang Pham","Tho Tran","Minh Khoi Nguyen Nhat","Thanh Chu","Tam Le","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.13725v2.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2505.00983v1","updated":"2025-05-02T04:06:00Z","published":"2025-05-02T04:06:00Z","title":"Toward Data-centric Directed Graph Learning: An Entropy-driven Approach","summary":"  The directed graph (digraph), as a generalization of undirected graphs,\nexhibits superior representation capability in modeling complex topology\nsystems and has garnered considerable attention in recent years. Despite the\nnotable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage\ndirected edges, they still fail to comprehensively delve into the abundant data\nknowledge concealed in the digraphs. This data-level limitation results in\nmodel-level sub-optimal predictive performance and underscores the necessity of\nfurther exploring the potential correlations between the directed edges\n(topology) and node profiles (feature and labels) from a data-centric\nperspective, thereby empowering model-centric neural networks with stronger\nencoding capabilities.\n  In this paper, we propose \\textbf{E}ntropy-driven \\textbf{D}igraph\nknowl\\textbf{E}dge distillatio\\textbf{N} (EDEN), which can serve as a\ndata-centric digraph learning paradigm or a model-agnostic hot-and-plug\ndata-centric Knowledge Distillation (KD) module. The core idea is to achieve\ndata-centric ML, guided by our proposed hierarchical encoding theory for\nstructured data. Specifically, EDEN first utilizes directed structural\nmeasurements from a topology perspective to construct a coarse-grained\nHierarchical Knowledge Tree (HKT). Subsequently, EDEN quantifies the mutual\ninformation of node profiles to refine knowledge flow in the HKT, enabling\ndata-centric KD supervision within model training. As a general framework, EDEN\ncan also naturally extend to undirected scenarios and demonstrate satisfactory\nperformance. In our experiments, EDEN has been widely evaluated on 14 (di)graph\ndatasets (homophily and heterophily) and across 4 downstream tasks. The results\ndemonstrate that EDEN attains SOTA performance and exhibits strong improvement\nfor prevalent (Di)GNNs.\n","authors":["Xunkai Li","Zhengyu Wu","Kaichi Yu","Hongchao Qin","Guang Zeng","Rong-Hua Li","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2505.00983v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.00979v1","updated":"2025-05-02T03:40:39Z","published":"2025-05-02T03:40:39Z","title":"Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for\n  Continue Pre-training of Large Language Models","summary":"  Large Language Models (LLMs) have achieved remarkable success but remain\ndata-inefficient, especially when learning from small, specialized corpora with\nlimited and proprietary data. Existing synthetic data generation methods for\ncontinue pre-training focus on intra-document content and overlook\ncross-document knowledge associations, limiting content diversity and depth. We\npropose Synthetic-on-Graph (SoG), a synthetic data generation framework that\nincorporates cross-document knowledge associations for efficient corpus\nexpansion. SoG constructs a context graph by extracting entities and concepts\nfrom the original corpus, representing cross-document associations, and\nemploying a graph walk strategy for knowledge-associated sampling. This\nenhances synthetic data diversity and coherence, enabling models to learn\ncomplex knowledge structures and handle rare knowledge. To further improve\nsynthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive\nClarifying (CC) synthetic, enhancing reasoning processes and discriminative\npower. Experiments show that SoG outperforms the state-of-the-art (SOTA) method\nin a multi-hop document Q&A dataset while performing comparably to the SOTA\nmethod on the reading comprehension task datasets, which also underscores the\nbetter generalization capability of SoG. Our work advances synthetic data\ngeneration and provides practical solutions for efficient knowledge acquisition\nin LLMs, especially in domains with limited data availability.\n","authors":["Xuhui Jiang","Shengjie Ma","Chengjin Xu","Cehao Yang","Liyu Zhang","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2505.00979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00254v2","updated":"2025-05-02T03:40:03Z","published":"2025-05-01T02:40:23Z","title":"Empowering Agentic Video Analytics Systems with Video Language Models","summary":"  AI-driven video analytics has become increasingly pivotal across diverse\ndomains. However, existing systems are often constrained to specific,\npredefined tasks, limiting their adaptability in open-ended analytical\nscenarios. The recent emergence of Video-Language Models (VLMs) as\ntransformative technologies offers significant potential for enabling\nopen-ended video understanding, reasoning, and analytics. Nevertheless, their\nlimited context windows present challenges when processing ultra-long video\ncontent, which is prevalent in real-world applications. To address this, we\nintroduce AVAS, a VLM-powered system designed for open-ended, advanced video\nanalytics. AVAS incorporates two key innovations: (1) the near real-time\nconstruction of Event Knowledge Graphs (EKGs) for efficient indexing of long or\ncontinuous video streams, and (2) an agentic retrieval-generation mechanism\nthat leverages EKGs to handle complex and diverse queries. Comprehensive\nevaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that\nAVAS achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy,\nrespectively, significantly surpassing existing VLM and video\nRetrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video\nanalytics in ultra-long and open-world video scenarios, we introduce a new\nbenchmark, AVAS-100. This benchmark comprises 8 videos, each exceeding 10 hours\nin duration, along with 120 manually annotated, diverse, and complex\nquestion-answer pairs. On AVAS-100, AVAS achieves top-tier performance with an\naccuracy of 75.8%.\n","authors":["Yuxuan Yan","Shiqi Jiang","Ting Cao","Yifan Yang","Qianqian Yang","Yuanchao Shu","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.00254v2.pdf","comment":"15 pages, AVAS"},{"id":"http://arxiv.org/abs/2505.00976v1","updated":"2025-05-02T03:37:52Z","published":"2025-05-02T03:37:52Z","title":"Attack and defense techniques in large language models: A survey and new\n  perspectives","summary":"  Large Language Models (LLMs) have become central to numerous natural language\nprocessing tasks, but their vulnerabilities present significant security and\nethical challenges. This systematic survey explores the evolving landscape of\nattack and defense techniques in LLMs. We classify attacks into adversarial\nprompt attack, optimized attacks, model theft, as well as attacks on\napplication of LLMs, detailing their mechanisms and implications. Consequently,\nwe analyze defense strategies, including prevention-based and detection-based\ndefense methods. Although advances have been made, challenges remain to adapt\nto the dynamic threat landscape, balance usability with robustness, and address\nresource constraints in defense implementation. We highlight open problems,\nincluding the need for adaptive scalable defenses, explainable security\ntechniques, and standardized evaluation frameworks. This survey provides\nactionable insights and directions for developing secure and resilient LLMs,\nemphasizing the importance of interdisciplinary collaboration and ethical\nconsiderations to mitigate risks in real-world applications.\n","authors":["Zhiyu Liao","Kang Chen","Yuanguo Lin","Kangkang Li","Yunxuan Liu","Hefeng Chen","Xingwang Huang","Yuanhui Yu"],"pdf_url":"https://arxiv.org/pdf/2505.00976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00972v1","updated":"2025-05-02T03:22:00Z","published":"2025-05-02T03:22:00Z","title":"Seeking to Collide: Online Safety-Critical Scenario Generation for\n  Autonomous Driving with Retrieval Augmented Large Language Models","summary":"  Simulation-based testing is crucial for validating autonomous vehicles (AVs),\nyet existing scenario generation methods either overfit to common driving\npatterns or operate in an offline, non-interactive manner that fails to expose\nrare, safety-critical corner cases. In this paper, we introduce an online,\nretrieval-augmented large language model (LLM) framework for generating\nsafety-critical driving scenarios. Our method first employs an LLM-based\nbehavior analyzer to infer the most dangerous intent of the background vehicle\nfrom the observed state, then queries additional LLM agents to synthesize\nfeasible adversarial trajectories. To mitigate catastrophic forgetting and\naccelerate adaptation, we augment the framework with a dynamic memorization and\nretrieval bank of intent-planner pairs, automatically expanding its behavioral\nlibrary when novel intents arise. Evaluations using the Waymo Open Motion\nDataset demonstrate that our model reduces the mean minimum time-to-collision\nfrom 1.62 to 1.08 s and incurs a 75% collision rate, substantially\noutperforming baselines.\n","authors":["Yuewen Mei","Tong Nie","Jian Sun","Ye Tian"],"pdf_url":"https://arxiv.org/pdf/2505.00972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19982v3","updated":"2025-05-02T03:19:49Z","published":"2024-10-25T21:46:25Z","title":"Random Policy Enables In-Context Reinforcement Learning within Trust\n  Horizons","summary":"  Pretrained foundation models have exhibited extraordinary in-context learning\nperformance, allowing zero-shot generalization to new tasks not encountered\nduring pretraining. In the case of reinforcement learning (RL), in-context RL\n(ICRL) emerges when pretraining FMs on decision-making problems in an\nautoregressive-supervised manner. Nevertheless, current state-of-the-art ICRL\nalgorithms, like Algorithm Distillation, Decision Pretrained Transformer and\nDecision Importance Transformer, impose stringent requirements on the\npretraining dataset concerning the source policies, context information, and\naction labels. Notably, these algorithms either demand optimal policies or\nrequire varying degrees of well-trained behavior policies for all pretraining\nenvironments. This significantly hinders the application of ICRL to real-world\nscenarios, where acquiring optimal or well-trained policies for a substantial\nvolume of real-world training environments can be intractable. To overcome this\nchallenge, we introduce a novel approach, termed State-Action Distillation\n(SAD), that allows to generate an effective pretraining dataset guided solely\nby random policies. In particular, SAD selects query states and corresponding\naction labels by distilling outstanding state-action pairs from the entire\nstate and action spaces by using random policies within a trust horizon, and\nthen inherits the classical autoregressive-supervised mechanism during\npretraining. To the best of our knowledge, this is the first work that enables\neffective ICRL under random policies and random contexts. We also establish\nquantitative analysis of the trustworthiness as well as the performance\nguarantees of SAD. Moreover, our empirical results across multiple popular ICRL\nbenchmark environments demonstrate that, on average, SAD outperforms the best\nbaseline by 236.3% in the offline evaluation and by 135.2% in the online\nevaluation.\n","authors":["Weiqin Chen","Santiago Paternain"],"pdf_url":"https://arxiv.org/pdf/2410.19982v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08324v2","updated":"2025-05-02T03:07:14Z","published":"2025-01-14T18:56:33Z","title":"ADAM: An AI Reasoning and Bioinformatics Model for Alzheimer's Disease\n  Detection and Microbiome-Clinical Data Integration","summary":"  Alzheimer's Disease Analysis Model (ADAM) is a multi-agent reasoning large\nlanguage model (LLM) framework designed to integrate and analyze multimodal\ndata, including microbiome profiles, clinical datasets, and external knowledge\nbases, to enhance the understanding and classification of Alzheimer's disease\n(AD). By leveraging the agentic system with LLM, ADAM produces insights from\ndiverse data sources and contextualizes the findings with literature-driven\nevidence. A comparative evaluation with XGBoost revealed a significantly\nimproved mean F1 score and significantly reduced variance for ADAM,\nhighlighting its robustness and consistency, particularly when utilizing human\nbiological data. Although currently tailored for binary classification tasks\nwith two data modalities, future iterations will aim to incorporate additional\ndata types, such as neuroimaging and peripheral biomarkers, and expand them to\npredict disease progression, thereby broadening ADAM's scalability and\napplicability in AD research and diagnostic applications.\n","authors":["Ziyuan Huang","Vishaldeep Kaur Sekhon","Roozbeh Sadeghian","Maria L. Vaida","Cynthia Jo","Doyle Ward","Vanni Bucci","John P. Haran"],"pdf_url":"https://arxiv.org/pdf/2501.08324v2.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.00968v1","updated":"2025-05-02T03:06:25Z","published":"2025-05-02T03:06:25Z","title":"Tree-Sliced Wasserstein Distance with Nonlinear Projection","summary":"  Tree-Sliced methods have recently emerged as an alternative to the\ntraditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines\nwith tree-based metric spaces and incorporating a splitting mechanism for\nprojecting measures. This approach enhances the ability to capture the\ntopological structures of integration domains in Sliced Optimal Transport while\nmaintaining low computational costs. Building on this foundation, we propose a\nnovel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW)\ndistance, substituting the linear projections in earlier versions with general\nprojections, while ensuring the injectivity of the associated Radon Transform\nand preserving the well-definedness of the resulting metric. By designing\nappropriate projections, we construct efficient metrics for measures on both\nEuclidean spaces and spheres. Finally, we validate our proposed metric through\nextensive numerical experiments for Euclidean and spherical datasets.\nApplications include gradient flows, self-supervised learning, and generative\nmodels, where our methods demonstrate significant improvements over recent SW\nand TSW variants.\n","authors":["Thanh Tran","Viet-Hoang Tran","Thanh Chu","Trang Pham","Laurent El Ghaoui","Tam Le","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2505.00968v1.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2501.06382v3","updated":"2025-05-02T02:25:37Z","published":"2025-01-10T23:18:23Z","title":"Dynamics of Spontaneous Topic Changes in Next Token Prediction with\n  Self-Attention","summary":"  Human cognition is punctuated by abrupt, spontaneous shifts between\ntopics-driven by emotional, contextual, or associative cues-a phenomenon known\nas spontaneous thought in neuroscience. In contrast, self-attention based\nmodels depend on structured patterns over their inputs to predict each next\ntoken, lacking spontaneity. Motivated by this distinction, we characterize\nspontaneous topic changes in self-attention architectures, revealing both their\nsimilarities and their divergences from spontaneous human thought. First, we\nestablish theoretical results under a simplified, single-layer self-attention\nmodel with suitable conditions by defining the topic as a set of Token Priority\nGraphs (TPGs). Specifically, we demonstrate that (1) the model maintains the\npriority order of tokens related to the input topic, (2) a spontaneous topic\nchange can occur only if lower-priority tokens outnumber all higher-priority\ntokens of the input topic, and (3) unlike human cognition, the longer context\nlength or the more ambiguous input topic reduces the likelihood of spontaneous\nchange. Second, we empirically validate that these dynamics persist in modern,\nstate-of-the-art LLMs, underscoring a fundamental disparity between human\ncognition and AI behaviour in the context of spontaneous topic changes. To the\nbest of our knowledge, no prior work has explored these questions with a focus\nas closely aligned to human thought.\n","authors":["Mumin Jia","Jairo Diaz-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2501.06382v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12352v3","updated":"2025-05-02T02:07:05Z","published":"2025-01-21T18:32:31Z","title":"Test-time regression: a unifying framework for designing sequence models\n  with associative memory","summary":"  Sequence models lie at the heart of modern deep learning. However, rapid\nadvancements have produced a diversity of seemingly unrelated architectures,\nsuch as Transformers and recurrent alternatives. In this paper, we introduce a\nunifying framework to understand and derive these sequence models, inspired by\nthe empirical importance of associative recall, the capability to retrieve\ncontextually relevant tokens. We formalize associative recall as a two-step\nprocess, memorization and retrieval, casting memorization as a regression\nproblem. Layers that combine these two steps perform associative recall via\n``test-time regression'' over its input tokens. Prominent layers, including\nlinear attention, state-space models, fast-weight programmers, online learners,\nand softmax attention, arise as special cases defined by three design choices:\nthe regression weights, the regressor function class, and the test-time\noptimization algorithm. Our approach clarifies how linear attention fails to\ncapture inter-token correlations and offers a mathematical justification for\nthe empirical effectiveness of query-key normalization in softmax attention.\nFurther, it illuminates unexplored regions within the design space, which we\nuse to derive novel higher-order generalizations of softmax attention. Beyond\nunification, our work bridges sequence modeling with classic regression\nmethods, a field with extensive literature, paving the way for developing more\npowerful and theoretically principled architectures.\n","authors":["Ke Alexander Wang","Jiaxin Shi","Emily B. Fox"],"pdf_url":"https://arxiv.org/pdf/2501.12352v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00949v1","updated":"2025-05-02T01:35:35Z","published":"2025-05-02T01:35:35Z","title":"Llama-Nemotron: Efficient Reasoning Models","summary":"  We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.\n","authors":["Akhiad Bercovich","Itay Levy","Izik Golan","Mohammad Dabbah","Ran El-Yaniv","Omri Puny","Ido Galil","Zach Moshe","Tomer Ronen","Najeeb Nabwani","Ido Shahaf","Oren Tropp","Ehud Karpas","Ran Zilberstein","Jiaqi Zeng","Soumye Singhal","Alexander Bukharin","Yian Zhang","Tugrul Konuk","Gerald Shen","Ameya Sunil Mahabaleshwarkar","Bilal Kartal","Yoshi Suhara","Olivier Delalleau","Zijia Chen","Zhilin Wang","David Mosallanezhad","Adi Renduchintala","Haifeng Qian","Dima Rekesh","Fei Jia","Somshubra Majumdar","Vahid Noroozi","Wasi Uddin Ahmad","Sean Narenthiran","Aleksander Ficek","Mehrzad Samadi","Jocelyn Huang","Siddhartha Jain","Igor Gitman","Ivan Moshkov","Wei Du","Shubham Toshniwal","George Armstrong","Branislav Kisacanin","Matvei Novikov","Daria Gitman","Evelina Bakhturina","Jane Polak Scowcroft","John Kamalu","Dan Su","Kezhi Kong","Markus Kliegl","Rabeeh Karimi","Ying Lin","Sanjeev Satheesh","Jupinder Parmar","Pritam Gundecha","Brandon Norick","Joseph Jennings","Shrimai Prabhumoye","Syeda Nahida Akter","Mostofa Patwary","Abhinav Khattar","Deepak Narayanan","Roger Waleffe","Jimmy Zhang","Bor-Yiing Su","Guyue Huang","Terry Kong","Parth Chadha","Sahil Jain","Christine Harvey","Elad Segal","Jining Huang","Sergey Kashirsky","Robert McQueen","Izzy Putterman","George Lam","Arun Venkatesan","Sherry Wu","Vinh Nguyen","Manoj Kilaru","Andrew Wang","Anna Warno","Abhilash Somasamudramath","Sandip Bhaskar","Maka Dong","Nave Assaf","Shahar Mor","Omer Ullman Argov","Scot Junkin","Oleksandr Romanenko","Pedro Larroy","Monika Katariya","Marco Rovinelli","Viji Balas","Nicholas Edelman","Anahita Bhiwandiwalla","Muthu Subramaniam","Smita Ithape","Karthik Ramamoorthy","Yuting Wu","Suguna Varshini Velury","Omri Almog","Joyjit Daw","Denys Fridman","Erick Galinkin","Michael Evans","Katherine Luna","Leon Derczynski","Nikki Pope","Eileen Long","Seth Schneider","Guillermo Siman","Tomasz Grzegorzek","Pablo Ribalta","Monika Katariya","Joey Conway","Trisha Saar","Ann Guan","Krzysztof Pawelec","Shyamala Prayaga","Oleksii Kuchaiev","Boris Ginsburg","Oluwatobi Olabiyi","Kari Briski","Jonathan Cohen","Bryan Catanzaro","Jonah Alben","Yonatan Geifman","Eric Chung"],"pdf_url":"https://arxiv.org/pdf/2505.00949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08067v5","updated":"2025-05-02T01:10:28Z","published":"2024-10-10T16:01:51Z","title":"Reward-Augmented Data Enhances Direct Preference Alignment of LLMs","summary":"  Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses, despite having access\nto preference data that includes reward scores from judge models during AI\nfeedback. Striving to maximize the implicit reward gap between the chosen and\nthe slightly inferior rejected responses can cause overfitting and unnecessary\nunlearning of the high-quality rejected responses. The unawareness of the\nreward scores also drives the LLM to indiscriminately favor the low-quality\nchosen responses and fail to generalize to optimal responses that are sparse in\ndata. To overcome these shortcomings, our study introduces reward-conditioned\nLLM policies that discern and learn from the entire spectrum of response\nquality within the dataset, helping extrapolate to more optimal regions. We\npropose an effective yet simple data relabeling method that conditions the\npreference pairs on quality scores to construct a reward-augmented dataset. The\nexperiments across various benchmarks and diverse models demonstrate that our\napproach consistently boosts DPO by a considerable margin. Through\ncomprehensive ablation studies, we demonstrate that our method not only\nmaximizes the utility of preference data but also mitigates the issue of\nunlearning, demonstrating its broad effectiveness beyond mere data expansion.\nOur code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference.\n","authors":["Shenao Zhang","Zhihan Liu","Boyi Liu","Yufeng Zhang","Yingxiang Yang","Yongfei Liu","Liyu Chen","Tao Sun","Zhaoran Wang"],"pdf_url":"https://arxiv.org/pdf/2410.08067v5.pdf","comment":"Published at ICML 2025"},{"id":"http://arxiv.org/abs/2412.14415v3","updated":"2025-05-02T01:02:47Z","published":"2024-12-19T00:06:09Z","title":"DriveGPT: Scaling Autoregressive Behavior Models for Driving","summary":"  We present DriveGPT, a scalable behavior model for autonomous driving. We\nmodel driving as a sequential decision-making task, and learn a transformer\nmodel to predict future agent states as tokens in an autoregressive fashion. We\nscale up our model parameters and training data by multiple orders of\nmagnitude, enabling us to explore the scaling properties in terms of dataset\nsize, model parameters, and compute. We evaluate DriveGPT across different\nscales in a planning task, through both quantitative metrics and qualitative\nexamples, including closed-loop driving in complex real-world scenarios. In a\nseparate prediction task, DriveGPT outperforms state-of-the-art baselines and\nexhibits improved performance by pretraining on a large-scale dataset, further\nvalidating the benefits of data scaling.\n","authors":["Xin Huang","Eric M. Wolff","Paul Vernaza","Tung Phan-Minh","Hongge Chen","David S. Hayden","Mark Edmonds","Brian Pierce","Xinxin Chen","Pratik Elias Jacob","Xiaobai Chen","Chingiz Tairbekov","Pratik Agarwal","Tianshi Gao","Yuning Chai","Siddhartha Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2412.14415v3.pdf","comment":"ICML 2025. 14 pages, 17 figures, 8 tables, and 1 video link"},{"id":"http://arxiv.org/abs/2410.20027v2","updated":"2025-05-02T00:50:50Z","published":"2024-10-26T00:51:39Z","title":"Agentic Feedback Loop Modeling Improves Recommendation and User\n  Simulation","summary":"  Large language model-based agents are increasingly applied in the\nrecommendation field due to their extensive knowledge and strong planning\ncapabilities. While prior research has primarily focused on enhancing either\nthe recommendation agent or the user agent individually, the collaborative\ninteraction between the two has often been overlooked. Towards this research\ngap, we propose a novel framework that emphasizes the feedback loop process to\nfacilitate the collaboration between the recommendation agent and the user\nagent. Specifically, the recommendation agent refines its understanding of user\npreferences by analyzing the feedback from the user agent on the item\nrecommendation. Conversely, the user agent further identifies potential user\ninterests based on the items and recommendation reasons provided by the\nrecommendation agent. This iterative process enhances the ability of both\nagents to infer user behaviors, enabling more effective item recommendations\nand more accurate user simulations. Extensive experiments on three datasets\ndemonstrate the effectiveness of the agentic feedback loop: the agentic\nfeedback loop yields an average improvement of 11.52% over the single\nrecommendation agent and 21.12% over the single user agent. Furthermore, the\nresults show that the agentic feedback loop does not exacerbate popularity or\nposition bias, which are typically amplified by the real-world feedback loop,\nhighlighting its robustness. The source code is available at\nhttps://github.com/Lanyu0303/AFL.\n","authors":["Shihao Cai","Jizhi Zhang","Keqin Bao","Chongming Gao","Qifan Wang","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2410.20027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00938v1","updated":"2025-05-02T00:46:25Z","published":"2025-05-02T00:46:25Z","title":"CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against\n  Feature Confusion","summary":"  Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects\nacross different domains with limited class instances. Feature confusion,\nincluding object-background confusion and object-object confusion, presents\nsignificant challenges in both cross-domain and few-shot settings. In this\nwork, we introduce CDFormer, a cross-domain few-shot object detection\ntransformer against feature confusion, to address these challenges. The method\nspecifically tackles feature confusion through two key modules:\nobject-background distinguishing (OBD) and object-object distinguishing (OOD).\nThe OBD module leverages a learnable background token to differentiate between\nobjects and background, while the OOD module enhances the distinction between\nobjects of different classes. Experimental results demonstrate that CDFormer\noutperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0%\nmAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively,\nwhen fine-tuned.\n","authors":["Boyuan Meng","Xiaohan Zhang","Peilin Li","Zhe Wu","Yiming Li","Wenkai Zhao","Beinan Yu","Hui-Liang Shen"],"pdf_url":"https://arxiv.org/pdf/2505.00938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00935v1","updated":"2025-05-02T00:43:28Z","published":"2025-05-02T00:43:28Z","title":"Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning","summary":"  The increase in available computing power and the Deep Learning revolution\nhave allowed the exploration of new topics and frontiers in Artificial\nIntelligence research. A new field called Embodied Artificial Intelligence,\nwhich places at the intersection of Computer Vision, Robotics, and Decision\nMaking, has been gaining importance during the last few years, as it aims to\nfoster the development of smart autonomous robots and their deployment in\nsociety. The recent availability of large collections of 3D models for\nphotorealistic robotic simulation has allowed faster and safe training of\nlearning-based agents for millions of frames and a careful evaluation of their\nbehavior before deploying the models on real robotic platforms. These\nintelligent agents are intended to perform a certain task in a possibly unknown\nenvironment. To this end, during the training in simulation, the agents learn\nto perform continuous interactions with the surroundings, such as gathering\ninformation from the environment, encoding and extracting useful cues for the\ntask, and performing actions towards the final goal; where every action of the\nagent influences the interactions. This dissertation follows the complete\ncreation process of embodied agents for indoor environments, from their concept\nto their implementation and deployment. We aim to contribute to research in\nEmbodied AI and autonomous agents, in order to foster future work in this\nfield. We present a detailed analysis of the procedure behind implementing an\nintelligent embodied agent, comprehending a thorough description of the current\nstate-of-the-art in literature, technical explanations of the proposed methods,\nand accurate experimental studies on relevant robotic tasks.\n","authors":["Roberto Bigazzi"],"pdf_url":"https://arxiv.org/pdf/2505.00935v1.pdf","comment":"Ph.D. Dissertation"},{"id":"http://arxiv.org/abs/2505.00932v1","updated":"2025-05-02T00:20:38Z","published":"2025-05-02T00:20:38Z","title":"A Self-Supervised Transformer for Unusable Shared Bike Detection","summary":"  The rapid expansion of bike-sharing systems (BSS) has greatly improved urban\n\"last-mile\" connectivity, yet large-scale deployments face escalating\noperational challenges, particularly in detecting faulty bikes. Existing\ndetection approaches either rely on static model-based thresholds that overlook\ndynamic spatiotemporal (ST) usage patterns or employ supervised learning\nmethods that struggle with label scarcity and class imbalance. To address these\nlimitations, this paper proposes a novel Self-Supervised Transformer\n(SSTransformer) framework for automatically detecting unusable shared bikes,\nleveraging ST features extracted from GPS trajectories and trip records. The\nmodel incorporates a self-supervised pre-training strategy to enhance its\nfeature extraction capabilities, followed by fine-tuning for efficient status\nrecognition. In the pre-training phase, the Transformer encoder learns\ngeneralized representations of bike movement via a self-supervised objective;\nin the fine-tuning phase, the encoder is adapted to a downstream binary\nclassification task. Comprehensive experiments on a real-world dataset of\n10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate\nthat SSTransformer significantly outperforms traditional machine learning,\nensemble learning, and deep learning baselines, achieving the best accuracy\n(97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the\neffectiveness of self-supervised Transformer on ST data for capturing complex\nanomalies in BSS, paving the way toward more reliable and scalable maintenance\nsolutions for shared mobility.\n","authors":["Yin Huang","Yongqi Dong","Youhua Tang","Alvaro García Hernandez"],"pdf_url":"https://arxiv.org/pdf/2505.00932v1.pdf","comment":"6 pages, 5 figures, under review by the 2025 IEEE International\n  Conference on Intelligent Transportation Systems (IEEE ITSC 2025)"},{"id":"http://arxiv.org/abs/2505.00931v1","updated":"2025-05-02T00:19:50Z","published":"2025-05-02T00:19:50Z","title":"Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy\n  in English Language Learner Writing","summary":"  This study investigates the potential for Large Language Models (LLMs) to\nscale-up Dynamic Assessment (DA). To facilitate such an investigation, we first\ndeveloped DynaWrite-a modular, microservices-based grammatical tutoring\napplication which supports multiple LLMs to generate dynamic feedback to\nlearners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural\nchat to have the most potential to scale-up DA in the language learning\nclassroom. Further testing of these two candidates found both models performed\nsimilarly in their ability to accurately identify grammatical errors in user\nsentences. However, GPT-4o consistently outperformed neural chat in the quality\nof its DA by generating clear, consistent, and progressively explicit hints.\nReal-time responsiveness and system stability were also confirmed through\ndetailed performance testing, with GPT-4o exhibiting sufficient speed and\nstability. This study shows that LLMs can be used to scale-up dynamic\nassessment and thus enable dynamic assessment to be delivered to larger groups\nthan possible in traditional teacher-learner settings.\n","authors":["Timur Jaganov","John Blake","Julián Villegas","Nicholas Carr"],"pdf_url":"https://arxiv.org/pdf/2505.00931v1.pdf","comment":"15 pages, 8 Figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2412.10422v3","updated":"2025-05-02T00:11:48Z","published":"2024-12-10T11:03:49Z","title":"AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework","summary":"  Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\nderivation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation...\n","authors":["Meihao Fan","Ju Fan","Nan Tang","Lei Cao","Guoliang Li","Xiaoyong Du"],"pdf_url":"https://arxiv.org/pdf/2412.10422v3.pdf","comment":null}]},"2025-05-05T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2505.02833v1","updated":"2025-05-05T17:59:03Z","published":"2025-05-05T17:59:03Z","title":"TWIST: Teleoperated Whole-Body Imitation System","summary":"  Teleoperating humanoid robots in a whole-body manner marks a fundamental step\ntoward developing general-purpose robotic intelligence, with human motion\nproviding an ideal interface for controlling all degrees of freedom. Yet, most\ncurrent humanoid teleoperation systems fall short of enabling coordinated\nwhole-body behavior, typically limiting themselves to isolated locomotion or\nmanipulation tasks. We present the Teleoperated Whole-Body Imitation System\n(TWIST), a system for humanoid teleoperation through whole-body motion\nimitation. We first generate reference motion clips by retargeting human motion\ncapture data to the humanoid robot. We then develop a robust, adaptive, and\nresponsive whole-body controller using a combination of reinforcement learning\nand behavior cloning (RL+BC). Through systematic analysis, we demonstrate how\nincorporating privileged future motion frames and real-world motion capture\n(MoCap) data improves tracking accuracy. TWIST enables real-world humanoid\nrobots to achieve unprecedented, versatile, and coordinated whole-body motor\nskills--spanning whole-body manipulation, legged manipulation, locomotion, and\nexpressive movement--using a single unified neural network controller. Our\nproject website: https://humanoid-teleop.github.io\n","authors":["Yanjie Ze","Zixuan Chen","João Pedro Araújo","Zi-ang Cao","Xue Bin Peng","Jiajun Wu","C. Karen Liu"],"pdf_url":"https://arxiv.org/pdf/2505.02833v1.pdf","comment":"Project website: https://humanoid-teleop.github.io"},{"id":"http://arxiv.org/abs/2504.05287v2","updated":"2025-05-05T17:42:57Z","published":"2025-04-07T17:38:19Z","title":"RobustDexGrasp: Robust Dexterous Grasping of General Objects","summary":"  The ability to robustly grasp a variety of objects is essential for dexterous\nrobots. In this paper, we present a framework for zero-shot dynamic dexterous\ngrasping using single-view visual inputs, designed to be resilient to various\ndisturbances. Our approach utilizes a hand-centric object shape representation\nbased on dynamic distance vectors between finger joints and object surfaces.\nThis representation captures the local shape around potential contact regions\nrather than focusing on detailed global object geometry, thereby enhancing\ngeneralization to shape variations and uncertainties. To address perception\nlimitations, we integrate a privileged teacher policy with a mixed curriculum\nlearning approach, allowing the student policy to effectively distill grasping\ncapabilities and explore for adaptation to disturbances. Trained in simulation,\nour method achieves success rates of 97.0% across 247,786 simulated objects and\n94.6% across 512 real objects, demonstrating remarkable generalization.\nQuantitative and qualitative results validate the robustness of our policy\nagainst various disturbances.\n","authors":["Hui Zhang","Zijian Wu","Linyi Huang","Sammy Christen","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2504.05287v2.pdf","comment":"Project Page: https://zdchan.github.io/Robust_DexGrasp/"},{"id":"http://arxiv.org/abs/2504.18794v2","updated":"2025-05-05T17:21:55Z","published":"2025-04-26T04:30:10Z","title":"Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation\n  with Autonomous Mobile Robots","summary":"  Hierarchical reinforcement learning (HRL) is hypothesized to be able to take\nadvantage of the inherent hierarchy in robot learning tasks with sparse reward\nschemes, in contrast to more traditional reinforcement learning algorithms. In\nthis research, hierarchical reinforcement learning is evaluated and contrasted\nwith standard reinforcement learning in complex navigation tasks. We evaluate\nunique characteristics of HRL, including their ability to create sub-goals and\nthe termination function. We constructed experiments to test the differences\nbetween PPO and HRL, different ways of creating sub-goals, manual vs automatic\nsub-goal creation, and the effects of the frequency of termination on\nperformance. These experiments highlight the advantages of HRL and how it\nachieves these advantages.\n","authors":["Brendon Johnson","Alfredo Weitzenfeld"],"pdf_url":"https://arxiv.org/pdf/2504.18794v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.05153v2","updated":"2025-05-05T16:26:30Z","published":"2025-03-07T05:22:52Z","title":"Generative Trajectory Stitching through Diffusion Composition","summary":"  Effective trajectory stitching for long-horizon planning is a significant\nchallenge in robotic decision-making. While diffusion models have shown promise\nin planning, they are limited to solving tasks similar to those seen in their\ntraining data. We propose CompDiffuser, a novel generative approach that can\nsolve new tasks by learning to compositionally stitch together shorter\ntrajectory chunks from previously seen tasks. Our key insight is modeling the\ntrajectory distribution by subdividing it into overlapping chunks and learning\ntheir conditional relationships through a single bidirectional diffusion model.\nThis allows information to propagate between segments during generation,\nensuring physically consistent connections. We conduct experiments on benchmark\ntasks of various difficulties, covering different environment sizes, agent\nstate dimension, trajectory types, training data quality, and show that\nCompDiffuser significantly outperforms existing methods.\n","authors":["Yunhao Luo","Utkarsh A. Mishra","Yilun Du","Danfei Xu"],"pdf_url":"https://arxiv.org/pdf/2503.05153v2.pdf","comment":"Project page: https://comp-diffuser.github.io/"},{"id":"http://arxiv.org/abs/2505.02766v1","updated":"2025-05-05T16:21:46Z","published":"2025-05-05T16:21:46Z","title":"Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models\n  for Cellular Control","summary":"  Guiding biological systems toward desired states, such as morphogenetic\noutcomes, remains a fundamental challenge with far-reaching implications for\nmedicine and synthetic biology. While large language models (LLMs) have enabled\nnatural language as an interface for interpretable control in AI systems, their\nuse as mediators for steering biological or cellular dynamics remains largely\nunexplored.\n  In this work, we present a functional pipeline that translates natural\nlanguage prompts into spatial vector fields capable of directing simulated\ncellular collectives. Our approach combines a large language model with an\nevolvable neural controller (Prompt-to-Intervention, or P2I), optimized via\nevolutionary strategies to generate behaviors such as clustering or scattering\nin a simulated 2D environment.\n  We demonstrate that even with constrained vocabulary and simplified cell\nmodels, evolved P2I networks can successfully align cellular dynamics with\nuser-defined goals expressed in plain language. This work offers a complete\nloop from language input to simulated bioelectric-like intervention to\nbehavioral output, providing a foundation for future systems capable of natural\nlanguage-driven cellular control.\n","authors":["Nam H. Le","Patrick Erikson","Yanbo Zhang","Michael Levin","Josh Bongard"],"pdf_url":"https://arxiv.org/pdf/2505.02766v1.pdf","comment":"Accepted to GECCO Workshop on Bio-Inspired AI (ACM GECCO2025). 13\n  pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.19786v2","updated":"2025-05-05T16:02:57Z","published":"2024-09-29T21:17:49Z","title":"Spatio-Tempora Metric-Semantic Mapping for Persistent Orchard\n  Monitoring: Method and Dataset","summary":"  Monitoring orchards at the individual tree or fruit level throughout the\ngrowth season is crucial for plant phenotyping and horticultural resource\noptimization, such as chemical use and yield estimation. We present a 4D\nspatio-temporal metric-semantic mapping system that integrates multi-session\nmeasurements to track fruit growth over time. Our approach combines a LiDAR-RGB\nfusion module for 3D fruit localization with a 4D fruit association method\nleveraging positional, visual, and topology information for improved data\nassociation precision. Evaluated on real orchard data, our method achieves a\n96.9% fruit counting accuracy for 1,790 apples across 60 trees, a mean fruit\nsize estimation error of 1.1 cm, and a 23.7% improvement in 4D data association\nprecision over baselines. We publicly release a multimodal dataset covering\nfive fruit species across their growth seasons.\nhttps://4d-metric-semantic-mapping.org/\n","authors":["Jiuzhou Lei","Ankit Prabhu","Xu Liu","Fernando Cladera","Mehrad Mortazavi","Reza Ehsani","Pratik Chaudhari","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2409.19786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07242v2","updated":"2025-05-05T15:57:12Z","published":"2025-04-09T19:29:16Z","title":"Analysis of the Unscented Transform for Cooperative Localization with\n  Ranging-Only Information","summary":"  Cooperative localization in multi-agent robotic systems is challenging,\nespecially when agents rely on limited information, such as only peer-to-peer\nrange measurements. Two key challenges arise: utilizing this limited\ninformation to improve position estimation; handling uncertainties from sensor\nnoise, nonlinearity, and unknown correlations between agents measurements; and\navoiding information reuse. This paper examines the use of the Unscented\nTransform (UT) for state estimation for a case in which range measurement\nbetween agents and covariance intersection (CI) is used to handle unknown\ncorrelations. Unlike Kalman Filter approaches, CI methods fuse complete state\nand covariance estimates. This makes formulating a CI approach with\nranging-only measurements a challenge. To overcome this, UT is used to handle\nuncertainties and formulate a cooperative state update using range measurements\nand current cooperative state estimates. This introduces information reuse in\nthe measurement update. Therefore, this work aims to evaluate the limitations\nand utility of this formulation when faced with various levels of state\nmeasurement uncertainty and errors.\n","authors":["Uthman Olawoye","Cagri Kilic","Jason N Gross"],"pdf_url":"https://arxiv.org/pdf/2504.07242v2.pdf","comment":"8 pages, 8 figures. The paper will be presented at the 2025 IEEE/ION\n  Position, Location and Navigation Symposium (PLANS)"},{"id":"http://arxiv.org/abs/2505.02744v1","updated":"2025-05-05T15:52:14Z","published":"2025-05-05T15:52:14Z","title":"Re-purposing a modular origami manipulator into an adaptive physical\n  computer for machine learning and robotic perception","summary":"  Physical computing has emerged as a powerful tool for performing intelligent\ntasks directly in the mechanical domain of functional materials and robots,\nreducing our reliance on the more traditional COMS computers. However, no\nsystematic study explains how mechanical design can influence physical\ncomputing performance. This study sheds insights into this question by\nrepurposing an origami-inspired modular robotic manipulator into an adaptive\nphysical reservoir and systematically evaluating its computing capacity with\ndifferent physical configurations, input setups, and computing tasks. By\nchallenging this adaptive reservoir computer to complete the classical NARMA\nbenchmark tasks, this study shows that its time series emulation performance\ndirectly correlates to the Peak Similarity Index (PSI), which quantifies the\nfrequency spectrum correlation between the target output and reservoir\ndynamics. The adaptive reservoir also demonstrates perception capabilities,\naccurately extracting its payload weight and orientation information from the\nintrinsic dynamics. Importantly, such information extraction capability can be\nmeasured by the spatial correlation between nodal dynamics within the reservoir\nbody. Finally, by integrating shape memory alloy (SMA) actuation, this study\ndemonstrates how to exploit such computing power embodied in the physical body\nfor practical, robotic operations. This study provides a strategic framework\nfor harvesting computing power from soft robots and functional materials,\ndemonstrating how design parameters and input selection can be configured based\non computing task requirements. Extending this framework to bio-inspired\nadaptive materials, prosthetics, and self-adaptive soft robotic systems could\nenable next-generation embodied intelligence, where the physical structure can\ncompute and interact with their digital counterparts.\n","authors":["Jun Wang","Suyi Li"],"pdf_url":"https://arxiv.org/pdf/2505.02744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02664v1","updated":"2025-05-05T14:14:32Z","published":"2025-05-05T14:14:32Z","title":"Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp\n  Pose Detection in Clutter","summary":"  Grasp pose detection in cluttered, real-world environments remains a\nsignificant challenge due to noisy and incomplete sensory data combined with\ncomplex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0)\nmethod, a lightweight yet highly effective hypothesis-and-test robotics\ngrasping framework which leverages an ensemble of Graph Neural Networks for\nefficient geometric reasoning from point cloud data. Building on the success of\nGtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp\ndetection but was limited by assumptions of complete, noise-free point clouds\nand 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to\nefficiently produce 7-Dof grasp candidates. Candidates are assessed with an\nensemble Graph Neural Network model which includes points within the gripper\njaws (inside points) and surrounding contextual points (outside points). This\nimproved representation boosts grasp detection performance over previous\nmethods using the same generator. GtG 2.0 shows up to a 35% improvement in\nAverage Precision on the GraspNet-1Billion benchmark compared to\nhypothesis-and-test and Graph Neural Network-based methods, ranking it among\nthe top three frameworks. Experiments with a 3-Dof Delta Parallel robot and\nKinect-v1 camera show a success rate of 91% and a clutter completion rate of\n100%, demonstrating its flexibility and reliability.\n","authors":["Ali Rashidi Moghadam","Sayedmohammadreza Rastegari","Mehdi Tale Masouleh","Ahmad Kalhor"],"pdf_url":"https://arxiv.org/pdf/2505.02664v1.pdf","comment":"9 Pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.01250v2","updated":"2025-05-05T14:03:44Z","published":"2024-10-02T05:49:34Z","title":"High and Low Resolution Tradeoffs in Roadside Multimodal Sensing","summary":"  Balancing cost and performance is crucial when choosing high- versus\nlow-resolution point-cloud roadside sensors. For example, LiDAR delivers dense\npoint cloud, while 4D millimeter-wave radar, though spatially sparser, embeds\nvelocity cues that help distinguish objects and come at a lower price.\nUnfortunately, the sensor placement strategies will influence point cloud\ndensity and distribution across the coverage area. Compounding the first\nchallenge is the fact that different sensor mixtures often demand distinct\nneural network architectures to maximize their complementary strengths. Without\nan evaluation framework that establishes a benchmark for comparison, it is\nimprudent to make claims regarding whether marginal gains result from higher\nresolution and new sensing modalities or from the algorithms. We present an\nex-ante evaluation that addresses the two challenges. First, we realized a\nsimulation tool that builds on integer programming to automatically compare\ndifferent sensor placement strategies against coverage and cost jointly.\nAdditionally, inspired by human multi-sensory integration, we propose a modular\nframework to assess whether reductions in spatial resolution can be compensated\nby informational richness in detecting traffic participants. Extensive\nexperimental testing on the proposed framework shows that fusing\nvelocity-encoded radar with low-resolution LiDAR yields marked gains (14\npercent AP for pedestrians and an overall mAP improvement of 1.5 percent across\nsix categories) at lower cost than high-resolution LiDAR alone. Notably, these\nmarked gains hold regardless of the specific deep neural modules employed in\nour frame. The result challenges the prevailing assumption that high resolution\nare always superior to low-resolution alternatives.\n","authors":["Shaozu Ding","Yihong Tang","Marco De Vincenzi","Dajiang Suo"],"pdf_url":"https://arxiv.org/pdf/2410.01250v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.05873v2","updated":"2025-05-05T14:03:25Z","published":"2024-12-08T09:37:32Z","title":"AC-LIO: Towards Asymptotic and Consistent Convergence in LiDAR-Inertial\n  Odometry","summary":"  Existing LiDAR-Inertial Odometry (LIO) methods typically utilize the prior\nstate trajectory derived from the IMU integration to compensate for the motion\ndistortion within LiDAR frames. However, discrepancies between the prior and\nactual trajectory can lead to residual distortions that compromise the\nconsistency of the LiDAR frame with its corresponding geometric environment.\nThis imbalance may result in pointcloud registration becoming trapped in local\noptima, thereby exacerbating drift during long-term and large-scale\nlocalization. To address the issue, we propose a novel asymptotically and\nconsistently converging LIO framework dubbed AC-LIO. Our key idea is to back\npropagate current update term based on the prior state chain, and\nasymptotically compensate for the residual distortion during iteration.\nMoreover, considering the weak correlation between previous error and current\ndistortion, we establish convergence criteria based on the pointcloud\nconstraints to regulate the backpropagation. This method of guiding asymptotic\ndistortion compensation using convergence criteria subtly enhances the\nconsistent convergence of pointcloud registration, futher improving the\naccuracy and robustness of LIO system. Extensive experiments demonstrate that\nour AC-LIO framework significantly promotes consistent convergence in state\nestimation compared to prior arts, with about 30.4% reduction in average RMSE\nover the second best result, leading to marked improvements in the accuracy of\nlong-term and large-scale localization and mapping.\n","authors":["Tianxiang Zhang","Xuanxuan Zhang","Wenlei Fan","Xin Xia","Huai Yu","Lin Wang","You Li"],"pdf_url":"https://arxiv.org/pdf/2412.05873v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.12850v2","updated":"2025-05-05T13:48:35Z","published":"2025-03-17T06:03:49Z","title":"In vivo validation of Wireless Power Transfer System for Magnetically\n  Controlled Robotic Capsule Endoscopy","summary":"  This paper presents the in vivo validation of an inductive wireless power\ntransfer (WPT) system integrated for the first time into a magnetically\ncontrolled robotic capsule endoscopy platform. The proposed system enables\ncontinuous power delivery to the capsule without the need for onboard\nbatteries, thus extending operational time and reducing size constraints. The\nWPT system operates through a resonant inductive coupling mechanism, based on a\ntransmitting coil mounted on the end effector of a robotic arm that also houses\nan external permanent magnet and a localization coil for precise capsule\nmanipulation. To ensure robust and stable power transmission in the presence of\ncoil misalignment and rotation, a 3D receiving coil is integrated within the\ncapsule. Additionally, a closed-loop adaptive control system, based on\nload-shift keying (LSK) modulation, dynamically adjusts the transmitted power\nto optimize efficiency while maintaining compliance with specific absorption\nrate (SAR) safety limits. The system has been extensively characterized in\nlaboratory settings and validated through in vivo experiments using a porcine\nmodel, demonstrating reliable power transfer and effective robotic navigation\nin realistic gastrointestinal conditions: the average received power was 110 mW\nat a distance of 9 cm between the coils, with variable capsule rotation angles.\nThe results confirm the feasibility of the proposed WPT approach for\nautonomous, battery-free robotic capsule endoscopy, paving the way for enhanced\ndiagnostic in gastrointestinal medicine.\n","authors":["Alessandro Catania","Michele Bertozzi","Nikita J. Greenidge","Benjamin Calme","Gabriele Bandini","Christian Sbrana","Roberto Cecchi","Alice Buffi","Massimo Macucci","Sebastiano Strangio","Pietro Valdastri","Giuseppe Iannaccone"],"pdf_url":"https://arxiv.org/pdf/2503.12850v2.pdf","comment":"9 pages, 9 figures, regular paper"},{"id":"http://arxiv.org/abs/2309.01813v3","updated":"2025-05-05T13:36:39Z","published":"2023-09-04T21:05:59Z","title":"Inverse Dynamics Trajectory Optimization for Contact-Implicit Model\n  Predictive Control","summary":"  Robots must make and break contact with the environment to perform useful\ntasks, but planning and control through contact remains a formidable challenge.\nIn this work, we achieve real-time contact-implicit model predictive control\nwith a surprisingly simple method: inverse dynamics trajectory optimization.\nWhile trajectory optimization with inverse dynamics is not new, we introduce a\nseries of incremental innovations that collectively enable fast model\npredictive control on a variety of challenging manipulation and locomotion\ntasks. We implement these innovations in an open-source solver and present\nsimulation examples to support the effectiveness of the proposed approach.\nAdditionally, we demonstrate contact-implicit model predictive control on\nhardware at over 100 Hz for a 20-degree-of-freedom bi-manual manipulation task.\nVideo and code are available at https://idto.github.io.\n","authors":["Vince Kurtz","Alejandro Castro","Aykut Özgün Önol","Hai Lin"],"pdf_url":"https://arxiv.org/pdf/2309.01813v3.pdf","comment":"IJRR accepted version"},{"id":"http://arxiv.org/abs/2407.05979v3","updated":"2025-05-05T13:12:35Z","published":"2024-07-08T14:21:45Z","title":"Smoothing of Headland Path Edges and Headland-to-Mainfield Lane\n  Transitions Based on a Spatial Domain Transformation and Linear Programming","summary":"  Within the context of in-field path planning and under the assumption of\nnonholonomic vehicle models this paper addresses two tasks: smoothing of\nheadland path edges and smoothing of headland-to-mainfield lane transitions.\nBoth tasks are solved by a two-step hierarchical algorithm. The first step\ndiffers for the two tasks generating either a piecewise-affine or a Dubins\nreference path. The second step leverages a transformation of vehicle dynamics\nfrom the time domain into the spatial domain and linear programming. Benefits\nsuch as a hyperparameter-free objective function and spatial constraints useful\nfor area coverage gaps avoidance and precision path planning are discussed. The\nmethod, which is a deterministic optimisation-based method, is evaluated on 5\nreal-world fields solving 19 instances of the first task and 84 instances of\nthe second task.\n","authors":["Mogens Plessen"],"pdf_url":"https://arxiv.org/pdf/2407.05979v3.pdf","comment":"13 pages, 12 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.02598v1","updated":"2025-05-05T12:07:35Z","published":"2025-05-05T12:07:35Z","title":"LiDAR-Inertial SLAM-Based Navigation and Safety-Oriented AI-Driven\n  Control System for Skid-Steer Robots","summary":"  Integrating artificial intelligence (AI) and stochastic technologies into the\nmobile robot navigation and control (MRNC) framework while adhering to rigorous\nsafety standards presents significant challenges. To address these challenges,\nthis paper proposes a comprehensively integrated MRNC framework for skid-steer\nwheeled mobile robots (SSWMRs), in which all components are actively engaged in\nreal-time execution. The framework comprises: 1) a LiDAR-inertial simultaneous\nlocalization and mapping (SLAM) algorithm for estimating the current pose of\nthe robot within the built map; 2) an effective path-following control system\nfor generating desired linear and angular velocity commands based on the\ncurrent pose and the desired pose; 3) inverse kinematics for transferring\nlinear and angular velocity commands into left and right side velocity\ncommands; and 4) a robust AI-driven (RAID) control system incorporating a\nradial basis function network (RBFN) with a new adaptive algorithm to enforce\nin-wheel actuation systems to track each side motion commands. To further meet\nsafety requirements, the proposed RAID control within the MRNC framework of the\nSSWMR constrains AI-generated tracking performance within predefined overshoot\nand steady-state error limits, while ensuring robustness and system stability\nby compensating for modeling errors, unknown RBF weights, and external forces.\nExperimental results verify the proposed MRNC framework performance for a 4,836\nkg SSWMR operating on soft terrain.\n","authors":["Mehdi Heydari Shahna","Eemil Haaparanta","Pauli Mustalahti","Jouni Mattila"],"pdf_url":"https://arxiv.org/pdf/2505.02598v1.pdf","comment":"This paper has been submitted in the IEEE CDC 2025 for potential\n  presentation"},{"id":"http://arxiv.org/abs/2412.11829v2","updated":"2025-05-05T11:55:44Z","published":"2024-12-16T14:52:51Z","title":"Robust Contact-rich Manipulation through Implicit Motor Adaptation","summary":"  Contact-rich manipulation plays an important role in daily human activities.\nHowever, uncertain physical parameters often pose significant challenges for\nboth planning and control. A promising strategy is to develop policies that are\nrobust across a wide range of parameters. Domain adaptation and domain\nrandomization are widely used, but they tend to either limit generalization to\nnew instances or perform conservatively due to neglecting instance-specific\ninformation. \\textit{Explicit motor adaptation} addresses these issues by\nestimating system parameters online and then retrieving the\nparameter-conditioned policy from a parameter-augmented base policy. However,\nit typically requires precise system identification or additional training of a\nstudent policy, both of which are challenging in contact-rich manipulation\ntasks with diverse physical parameters. In this work, we propose\n\\textit{implicit motor adaptation}, which enables parameter-conditioned policy\nretrieval given a roughly estimated parameter distribution instead of a single\nestimate. We leverage tensor train as an implicit representation of the base\npolicy, facilitating efficient retrieval of the parameter-conditioned policy by\nexploiting the separable structure of tensor cores. This framework eliminates\nthe need for precise system estimation and policy retraining while preserving\noptimal behavior and strong generalization. We provide a theoretical analysis\nto validate the approach, supported by numerical evaluations on three\ncontact-rich manipulation primitives. Both simulation and real-world\nexperiments demonstrate its ability to generate robust policies across diverse\ninstances.\n  Project website:\n\\href{https://sites.google.com/view/implicit-ma}{https://sites.google.com/view/implicit-ma}.\n","authors":["Teng Xue","Amirreza Razmjoo","Suhan Shetty","Sylvain Calinon"],"pdf_url":"https://arxiv.org/pdf/2412.11829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02574v1","updated":"2025-05-05T11:23:51Z","published":"2025-05-05T11:23:51Z","title":"Learning and Online Replication of Grasp Forces from Electromyography\n  Signals for Prosthetic Finger Control","summary":"  Partial hand amputations significantly affect the physical and psychosocial\nwell-being of individuals, yet intuitive control of externally powered\nprostheses remains an open challenge. To address this gap, we developed a\nforce-controlled prosthetic finger activated by electromyography (EMG) signals.\nThe prototype, constructed around a wrist brace, functions as a supernumerary\nfinger placed near the index, allowing for early-stage evaluation on unimpaired\nsubjects. A neural network-based model was then implemented to estimate\nfingertip forces from EMG inputs, allowing for online adjustment of the\nprosthetic finger grip strength. The force estimation model was validated\nthrough experiments with ten participants, demonstrating its effectiveness in\npredicting forces. Additionally, online trials with four users wearing the\nprosthesis exhibited precise control over the device. Our findings highlight\nthe potential of using EMG-based force estimation to enhance the functionality\nof prosthetic fingers.\n","authors":["Robin Arbaud","Elisa Motta","Marco Domenico Avaro","Stefano Picinich","Marta Lorenzini","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2505.02574v1.pdf","comment":"7 pages, 6 figures, to be presented at ICRA 2025"},{"id":"http://arxiv.org/abs/2505.02569v1","updated":"2025-05-05T11:21:03Z","published":"2025-05-05T11:21:03Z","title":"HapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic\n  Interaction","summary":"  This paper introduces HapticVLM, a novel multimodal system that integrates\nvision-language reasoning with deep convolutional networks to enable real-time\nhaptic feedback. HapticVLM leverages a ConvNeXt-based material recognition\nmodule to generate robust visual embeddings for accurate identification of\nobject materials, while a state-of-the-art Vision-Language Model\n(Qwen2-VL-2B-Instruct) infers ambient temperature from environmental cues. The\nsystem synthesizes tactile sensations by delivering vibrotactile feedback\nthrough speakers and thermal cues via a Peltier module, thereby bridging the\ngap between visual perception and tactile experience. Experimental evaluations\ndemonstrate an average recognition accuracy of 84.67% across five distinct\nauditory-tactile patterns and a temperature estimation accuracy of 86.7% based\non a tolerance-based evaluation method with an 8{\\deg}C margin of error across\n15 scenarios. Although promising, the current study is limited by the use of a\nsmall set of prominent patterns and a modest participant pool. Future work will\nfocus on expanding the range of tactile patterns and increasing user studies to\nfurther refine and validate the system's performance. Overall, HapticVLM\npresents a significant step toward context-aware, multimodal haptic interaction\nwith potential applications in virtual reality, and assistive technologies.\n","authors":["Muhammad Haris Khan","Miguel Altamirano Cabrera","Dmitrii Iarchuk","Yara Mahmoud","Daria Trinitatova","Issatay Tokmurziyev","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2505.02569v1.pdf","comment":"Submitted to IEEE conf"},{"id":"http://arxiv.org/abs/2505.02543v1","updated":"2025-05-05T10:30:52Z","published":"2025-05-05T10:30:52Z","title":"Data-Driven Energy Modeling of Industrial IoT Systems: A Benchmarking\n  Approach","summary":"  The widespread adoption of IoT has driven the development of cyber-physical\nsystems (CPS) in industrial environments, leveraging Industrial IoTs (IIoTs) to\nautomate manufacturing processes and enhance productivity. The transition to\nautonomous systems introduces significant operational costs, particularly in\nterms of energy consumption. Accurate modeling and prediction of IIoT energy\nrequirements are critical, but traditional physics- and engineering-based\napproaches often fall short in addressing these challenges comprehensively. In\nthis paper, we propose a novel methodology for benchmarking and analyzing IIoT\ndevices and applications to uncover insights into their power demands, energy\nconsumption, and performance. To demonstrate this methodology, we develop a\ncomprehensive framework and apply it to study an industrial CPS comprising an\neducational robotic arm, a conveyor belt, a smart camera, and a compute node.\nBy creating micro-benchmarks and an end-to-end application within this\nframework, we create an extensive performance and power consumption dataset,\nwhich we use to train and analyze ML models for predicting energy usage from\nfeatures of the application and the CPS system. The proposed methodology and\nframework provide valuable insights into the energy dynamics of industrial CPS,\noffering practical implications for researchers and practitioners aiming to\nenhance the efficiency and sustainability of IIoT-driven automation.\n","authors":["Dimitris Kallis","Moysis Symeonides","Marios D. Dikaiakos"],"pdf_url":"https://arxiv.org/pdf/2505.02543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03270v2","updated":"2025-05-05T09:42:27Z","published":"2025-02-05T15:25:46Z","title":"When Pre-trained Visual Representations Fall Short: Limitations in\n  Visuo-Motor Robot Learning","summary":"  The integration of pre-trained visual representations (PVRs) into visuo-motor\nrobot learning has emerged as a promising alternative to training visual\nencoders from scratch. However, PVRs face critical challenges in the context of\npolicy learning, including temporal entanglement and an inability to generalise\neven in the presence of minor scene perturbations. These limitations hinder\nperformance in tasks requiring temporal awareness and robustness to scene\nchanges. This work identifies these shortcomings and proposes solutions to\naddress them. First, we augment PVR features with temporal perception and a\nsense of task completion, effectively disentangling them in time. Second, we\nintroduce a module that learns to selectively attend to task-relevant local\nfeatures, enhancing robustness when evaluated on out-of-distribution scenes.\nOur experiments demonstrate significant performance improvements, particularly\nin PVRs trained with masking objectives, and validate the effectiveness of our\nenhancements in addressing PVR-specific limitations.\n","authors":["Nikolaos Tsagkas","Andreas Sochopoulos","Duolikun Danier","Sethu Vijayakumar","Chris Xiaoxuan Lu","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2502.03270v2.pdf","comment":"Project Page: https://tsagkas.github.io/pvrobo/"},{"id":"http://arxiv.org/abs/2505.02501v1","updated":"2025-05-05T09:29:32Z","published":"2025-05-05T09:29:32Z","title":"Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict\n  Reliable 6D Pose Distributions","summary":"  We introduce Corr2Distrib, the first correspondence-based method which\nestimates a 6D camera pose distribution from an RGB image, explaining the\nobservations. Indeed, symmetries and occlusions introduce visual ambiguities,\nleading to multiple valid poses. While a few recent methods tackle this\nproblem, they do not rely on local correspondences which, according to the BOP\nChallenge, are currently the most effective way to estimate a single 6DoF pose\nsolution. Using correspondences to estimate a pose distribution is not\nstraightforward, since ambiguous correspondences induced by visual ambiguities\ndrastically decrease the performance of PnP. With Corr2Distrib, we turn these\nambiguities into an advantage to recover all valid poses. Corr2Distrib first\nlearns a symmetry-aware representation for each 3D point on the object's\nsurface, characterized by a descriptor and a local frame. This representation\nenables the generation of 3DoF rotation hypotheses from single 2D-3D\ncorrespondences. Next, we refine these hypotheses into a 6DoF pose distribution\nusing PnP and pose scoring. Our experimental evaluations on complex\nnon-synthetic scenes show that Corr2Distrib outperforms state-of-the-art\nsolutions for both pose distribution estimation and single pose estimation from\nan RGB image, demonstrating the potential of correspondences-based approaches.\n","authors":["Asma Brazi","Boris Meden","Fabrice Mayran de Chamisso","Steve Bourgeois","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2505.02501v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.02483v1","updated":"2025-05-05T09:06:17Z","published":"2025-05-05T09:06:17Z","title":"Automated Hybrid Reward Scheduling via Large Language Models for Robotic\n  Skill Learning","summary":"  Enabling a high-degree-of-freedom robot to learn specific skills is a\nchallenging task due to the complexity of robotic dynamics. Reinforcement\nlearning (RL) has emerged as a promising solution; however, addressing such\nproblems requires the design of multiple reward functions to account for\nvarious constraints in robotic motion. Existing approaches typically sum all\nreward components indiscriminately to optimize the RL value function and\npolicy. We argue that this uniform inclusion of all reward components in policy\noptimization is inefficient and limits the robot's learning performance. To\naddress this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework\nbased on Large Language Models (LLMs). This paradigm dynamically adjusts the\nlearning intensity of each reward component throughout the policy optimization\nprocess, enabling robots to acquire skills in a gradual and structured manner.\nSpecifically, we design a multi-branch value network, where each branch\ncorresponds to a distinct reward component. During policy optimization, each\nbranch is assigned a weight that reflects its importance, and these weights are\nautomatically computed based on rules designed by LLMs. The LLM generates a\nrule set in advance, derived from the task description, and during training, it\nselects a weight calculation rule from the library based on language prompts\nthat evaluate the performance of each branch. Experimental results demonstrate\nthat the AHRS method achieves an average 6.48% performance improvement across\nmultiple high-degree-of-freedom robotic tasks.\n","authors":["Changxin Huang","Junyang Liang","Yanbin Chang","Jingzhao Xu","Jianqiang Li"],"pdf_url":"https://arxiv.org/pdf/2505.02483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02476v1","updated":"2025-05-05T09:00:16Z","published":"2025-05-05T09:00:16Z","title":"Point Cloud Recombination: Systematic Real Data Augmentation Using\n  Robotic Targets for LiDAR Perception Validation","summary":"  The validation of LiDAR-based perception of intelligent mobile systems\noperating in open-world applications remains a challenge due to the variability\nof real environmental conditions. Virtual simulations allow the generation of\narbitrary scenes under controlled conditions but lack physical sensor\ncharacteristics, such as intensity responses or material-dependent effects. In\ncontrast, real-world data offers true sensor realism but provides less control\nover influencing factors, hindering sufficient validation. Existing approaches\naddress this problem with augmentation of real-world point cloud data by\ntransferring objects between scenes. However, these methods do not consider\nvalidation and remain limited in controllability because they rely on empirical\ndata. We solve these limitations by proposing Point Cloud Recombination, which\nsystematically augments captured point cloud scenes by integrating point clouds\nacquired from physical target objects measured in controlled laboratory\nenvironments. Thus enabling the creation of vast amounts and varieties of\nrepeatable, physically accurate test scenes with respect to phenomena-aware\nocclusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we\ndemonstrate the augmentation of real-world urban and rural scenes with humanoid\ntargets featuring varied clothing and poses, for repeatable positioning. We\nshow that the recombined scenes closely match real sensor outputs, enabling\ntargeted testing, scalable failure analysis, and improved system safety. By\nproviding controlled yet sensor-realistic data, our method enables trustworthy\nconclusions about the limitations of specific sensors in compound with their\nalgorithms, e.g., object detection.\n","authors":["Hubert Padusinski","Christian Steinhauser","Christian Scherl","Julian Gaal","Jacob Langner"],"pdf_url":"https://arxiv.org/pdf/2505.02476v1.pdf","comment":"Pre-print for IEEE IAVVC 2025"},{"id":"http://arxiv.org/abs/2505.02460v1","updated":"2025-05-05T08:44:52Z","published":"2025-05-05T08:44:52Z","title":"ZeloS -- A Research Platform for Early-Stage Validation of Research\n  Findings Related to Automated Driving","summary":"  This paper presents ZeloS, a research platform designed and built for\npractical validation of automated driving methods in an early stage of\nresearch. We overview ZeloS' hardware setup and automation architecture and\nfocus on motion planning and control. ZeloS weighs 69 kg, measures a length of\n117 cm, and is equipped with all-wheel steering, all-wheel drive, and various\nonboard sensors for localization. The hardware setup and the automation\narchitecture of ZeloS are designed and built with a focus on modularity and the\ngoal of being simple yet effective. The modular design allows the modification\nof individual automation modules without the need for extensive onboarding into\nthe automation architecture. As such, this design supports ZeloS in being a\nversatile research platform for validating various automated driving methods.\nThe motion planning component and control of ZeloS feature optimization-based\nmethods that allow for explicitly considering constraints. We demonstrate the\nhardware and automation setup by presenting experimental data.\n","authors":["Christopher Bohn","Florian Siebenrock","Janne Bosch","Tobias Hetzner","Samuel Mauch","Philipp Reis","Timo Staudt","Manuel Hess","Ben-Micha Piscol","Sören Hohmann"],"pdf_url":"https://arxiv.org/pdf/2505.02460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02414v1","updated":"2025-05-05T07:20:04Z","published":"2025-05-05T07:20:04Z","title":"Quadrupedal Spine Control Strategies: Exploring Correlations Between\n  System Dynamic Responses and Human Perspectives","summary":"  Unlike their biological cousins, the majority of existing quadrupedal robots\nare constructed with rigid chassis. This results in motion that is either\nbeetle-like or distinctly robotic, lacking the natural fluidity characteristic\nof mammalian movements. Existing literature on quadrupedal robots with spinal\nconfigurations primarily focuses on energy efficiency and does not consider the\neffects in human-robot interaction scenarios. Our contributions include an\ninitial investigation into various trajectory generation strategies for a\nquadrupedal robot with a four degree of freedom spine, and an analysis on the\neffect that such methods have on human perception of gait naturalness compared\nto a fixed spine baseline. The strategies were evaluated using videos of\nwalking, trotting and turning simulations. Among the four different strategies\ndeveloped, the optimised time varying and the foot-tracking strategies were\nperceived to be more natural than the baseline in a randomised trial with 50\nparticipants. Although none of the strategies demonstrated any energy\nefficiency improvements over the no-spine baseline, some showed greater\nfootfall consistency at higher speeds. Given the greater likeability drawn from\nthe more natural locomotion patterns, this type of robot displays potential for\napplications in social robot scenarios such as elderly care, where energy\nefficiency is not a primary concern.\n","authors":["Nicholas Hafner","Chaoran Liu","Carlos Ishi","Hiroshi Ishiguro"],"pdf_url":"https://arxiv.org/pdf/2505.02414v1.pdf","comment":"27 pages, 13 figures"},{"id":"http://arxiv.org/abs/2505.02405v1","updated":"2025-05-05T06:55:59Z","published":"2025-05-05T06:55:59Z","title":"Estimating Commonsense Scene Composition on Belief Scene Graphs","summary":"  This work establishes the concept of commonsense scene composition, with a\nfocus on extending Belief Scene Graphs by estimating the spatial distribution\nof unseen objects. Specifically, the commonsense scene composition capability\nrefers to the understanding of the spatial relationships among related objects\nin the scene, which in this article is modeled as a joint probability\ndistribution for all possible locations of the semantic object class. The\nproposed framework includes two variants of a Correlation Information (CECI)\nmodel for learning probability distributions: (i) a baseline approach based on\na Graph Convolutional Network, and (ii) a neuro-symbolic extension that\nintegrates a spatial ontology based on Large Language Models (LLMs).\nFurthermore, this article provides a detailed description of the dataset\ngeneration process for such tasks. Finally, the framework has been validated\nthrough multiple runs on simulated data, as well as in a real-world indoor\nenvironment, demonstrating its ability to spatially interpret scenes across\ndifferent room types.\n","authors":["Mario A. V. Saucedo","Vignesh Kottayam Viswanathan","Christoforos Kanellakis","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2505.02405v1.pdf","comment":"Accepted at ICRA25"},{"id":"http://arxiv.org/abs/2504.16062v3","updated":"2025-05-05T06:53:10Z","published":"2025-04-22T17:38:38Z","title":"ForesightNav: Learning Scene Imagination for Efficient Exploration","summary":"  Understanding how humans leverage prior knowledge to navigate unseen\nenvironments while making exploratory decisions is essential for developing\nautonomous robots with similar abilities. In this work, we propose\nForesightNav, a novel exploration strategy inspired by human imagination and\nreasoning. Our approach equips robotic agents with the capability to predict\ncontextual information, such as occupancy and semantic details, for unexplored\nregions. These predictions enable the robot to efficiently select meaningful\nlong-term navigation goals, significantly enhancing exploration in unseen\nenvironments. We validate our imagination-based approach using the Structured3D\ndataset, demonstrating accurate occupancy prediction and superior performance\nin anticipating unseen scene geometry. Our experiments show that the\nimagination module improves exploration efficiency in unseen environments,\nachieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav\non the Structured3D Validation split. These contributions demonstrate the power\nof imagination-driven reasoning for autonomous systems to enhance generalizable\nand efficient exploration.\n","authors":["Hardik Shah","Jiaxu Xing","Nico Messikommer","Boyang Sun","Marc Pollefeys","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2504.16062v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.13243v3","updated":"2025-05-05T06:42:58Z","published":"2023-07-25T04:11:47Z","title":"A Model Predictive Capture Point Control Framework for Robust Humanoid\n  Balancing via Ankle, Hip, and Stepping Strategies","summary":"  The robust balancing capability of humanoids is essential for mobility in\nreal environments. Many studies focus on implementing human-inspired ankle,\nhip, and stepping strategies to achieve human-level balance. In this paper, a\nrobust balance control framework for humanoids is proposed. Firstly, a Model\nPredictive Control (MPC) framework is proposed for Capture Point (CP) tracking\ncontrol, enabling the integration of ankle, hip, and stepping strategies within\na single framework. Additionally, a variable weighting method is introduced\nthat adjusts the weighting parameters of the Centroidal Angular Momentum\ndamping control. Secondly, a hierarchical structure of the MPC and a stepping\ncontroller was proposed, allowing for the step time optimization. The robust\nbalancing performance of the proposed method is validated through simulations\nand real robot experiments. Furthermore, a superior balancing performance is\ndemonstrated compared to a state-of-the-art Quadratic Programming-based CP\ncontroller that employs the ankle, hip, and stepping strategies.\n","authors":["Myeong-Ju Kim","Daegyu Lim","Gyeongjae Park","Kwanwoo Lee","Jaeheung Park"],"pdf_url":"https://arxiv.org/pdf/2307.13243v3.pdf","comment":"20 pages,13 figures"},{"id":"http://arxiv.org/abs/2505.02395v1","updated":"2025-05-05T06:36:26Z","published":"2025-05-05T06:36:26Z","title":"A Real-Time Control Barrier Function-Based Safety Filter for Motion\n  Planning with Arbitrary Road Boundary Constraints","summary":"  We present a real-time safety filter for motion planning, such as\nlearning-based methods, using Control Barrier Functions (CBFs), which provides\nformal guarantees for collision avoidance with road boundaries. A key feature\nof our approach is its ability to directly incorporate road geometries of\narbitrary shape without resorting to conservative overapproximations. We\nformulate the safety filter as a constrained optimization problem in the form\nof a Quadratic Program (QP). It achieves safety by making minimal, necessary\nadjustments to the control actions issued by the nominal motion planner. We\nvalidate our safety filter through extensive numerical experiments across a\nvariety of traffic scenarios featuring complex roads. The results confirm its\nreliable safety and high computational efficiency (execution frequency up to 40\nHz). Code & Video Demo: github.com/bassamlab/SigmaRL\n","authors":["Jianye Xu","Chang Che","Bassam Alrifaee"],"pdf_url":"https://arxiv.org/pdf/2505.02395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02388v1","updated":"2025-05-05T06:13:25Z","published":"2025-05-05T06:13:25Z","title":"MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans","summary":"  Embodied AI (EAI) research requires high-quality, diverse 3D scenes to\neffectively support skill acquisition, sim-to-real transfer, and\ngeneralization. Achieving these quality standards, however, necessitates the\nprecise replication of real-world object diversity. Existing datasets\ndemonstrate that this process heavily relies on artist-driven designs, which\ndemand substantial human effort and present significant scalability challenges.\nTo scalably produce realistic and interactive 3D scenes, we first present\nMetaScenes, a large-scale, simulatable 3D scene dataset constructed from\nreal-world scans, which includes 15366 objects spanning 831 fine-grained\ncategories. Then, we introduce Scan2Sim, a robust multi-modal alignment model,\nwhich enables the automated, high-quality replacement of assets, thereby\neliminating the reliance on artist-driven designs for scaling 3D scenes. We\nfurther propose two benchmarks to evaluate MetaScenes: a detailed scene\nsynthesis task focused on small item layouts for robotic manipulation and a\ndomain transfer task in vision-and-language navigation (VLN) to validate\ncross-domain transfer. Results confirm MetaScene's potential to enhance EAI by\nsupporting more generalizable agent learning and sim-to-real applications,\nintroducing new possibilities for EAI research. Project website:\nhttps://meta-scenes.github.io/.\n","authors":["Huangyue Yu","Baoxiong Jia","Yixin Chen","Yandan Yang","Puhao Li","Rongpeng Su","Jiaxin Li","Qing Li","Wei Liang","Song-Chun Zhu","Tengyu Liu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.02388v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2505.00503v2","updated":"2025-05-05T06:00:10Z","published":"2025-05-01T13:14:07Z","title":"Variational OOD State Correction for Offline Reinforcement Learning","summary":"  The performance of Offline reinforcement learning is significantly impacted\nby the issue of state distributional shift, and out-of-distribution (OOD) state\ncorrection is a popular approach to address this problem. In this paper, we\npropose a novel method named Density-Aware Safety Perception (DASP) for OOD\nstate correction. Specifically, our method encourages the agent to prioritize\nactions that lead to outcomes with higher data density, thereby promoting its\noperation within or the return to in-distribution (safe) regions. To achieve\nthis, we optimize the objective within a variational framework that\nconcurrently considers both the potential outcomes of decision-making and their\ndensity, thus providing crucial contextual information for safe\ndecision-making. Finally, we validate the effectiveness and feasibility of our\nproposed method through extensive experimental evaluations on the offline\nMuJoCo and AntMaze suites.\n","authors":["Ke Jiang","Wen Jiang","Masahiro Fujisawa","Xiaoyang Tan"],"pdf_url":"https://arxiv.org/pdf/2505.00503v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02323v1","updated":"2025-05-05T02:39:55Z","published":"2025-05-05T02:39:55Z","title":"Riemannian Direct Trajectory Optimization of Rigid Bodies on Matrix Lie\n  Groups","summary":"  Designing dynamically feasible trajectories for rigid bodies is a fundamental\nproblem in robotics. Although direct trajectory optimization is widely applied\nto solve this problem, inappropriate parameterizations of rigid body dynamics\noften result in slow convergence and violations of the intrinsic topological\nstructure of the rotation group. This paper introduces a Riemannian\noptimization framework for direct trajectory optimization of rigid bodies. We\nfirst use the Lie Group Variational Integrator to formulate the discrete rigid\nbody dynamics on matrix Lie groups. We then derive the closed-form first- and\nsecond-order Riemannian derivatives of the dynamics. Finally, this work applies\na line-search Riemannian Interior Point Method (RIPM) to perform trajectory\noptimization with general nonlinear constraints. As the optimization is\nperformed on matrix Lie groups, it is correct-by-construction to respect the\ntopological structure of the rotation group and be free of singularities. The\npaper demonstrates that both the derivative evaluations and Newton steps\nrequired to solve the RIPM exhibit linear complexity with respect to the\nplanning horizon and system degrees of freedom. Simulation results illustrate\nthat the proposed method is faster than conventional methods by an order of\nmagnitude in challenging robotics tasks.\n","authors":["Sangli Teng","Tzu-Yuan Lin","William A Clark","Ram Vasudevan","Maani Ghaffari"],"pdf_url":"https://arxiv.org/pdf/2505.02323v1.pdf","comment":"Accepted to Robotics: Science and Systems (RSS) 2025"},{"id":"http://arxiv.org/abs/2505.03046v1","updated":"2025-05-05T22:04:12Z","published":"2025-05-05T22:04:12Z","title":"Sim2Real Transfer for Vision-Based Grasp Verification","summary":"  The verification of successful grasps is a crucial aspect of robot\nmanipulation, particularly when handling deformable objects. Traditional\nmethods relying on force and tactile sensors often struggle with deformable and\nnon-rigid objects. In this work, we present a vision-based approach for grasp\nverification to determine whether the robotic gripper has successfully grasped\nan object. Our method employs a two-stage architecture; first YOLO-based object\ndetection model to detect and locate the robot's gripper and then a\nResNet-based classifier determines the presence of an object. To address the\nlimitations of real-world data capture, we introduce HSR-GraspSynth, a\nsynthetic dataset designed to simulate diverse grasping scenarios. Furthermore,\nwe explore the use of Visual Question Answering capabilities as a zero-shot\nbaseline to which we compare our model. Experimental results demonstrate that\nour approach achieves high accuracy in real-world environments, with potential\nfor integration into grasping pipelines. Code and datasets are publicly\navailable at https://github.com/pauamargant/HSR-GraspSynth .\n","authors":["Pau Amargant","Peter Hönig","Markus Vincze"],"pdf_url":"https://arxiv.org/pdf/2505.03046v1.pdf","comment":"Accepted at Austrian Robotics Workshop 2025"},{"id":"http://arxiv.org/abs/2505.03044v1","updated":"2025-05-05T21:54:01Z","published":"2025-05-05T21:54:01Z","title":"A Modal-Space Formulation for Momentum Observer Contact Estimation and\n  Effects of Uncertainty for Continuum Robots","summary":"  Contact detection for continuum and soft robots has been limited in past\nworks to statics or kinematics-based methods with assumed circular bending\ncurvature or known bending profiles. In this paper, we adapt the generalized\nmomentum observer contact estimation method to continuum robots. This is made\npossible by leveraging recent results for real-time shape sensing of continuum\nrobots along with a modal-space representation of the robot dynamics. In\naddition to presenting an approach for estimating the generalized forces due to\ncontact via a momentum observer, we present a constrained optimization method\nto identify the wrench imparted on the robot during contact. We also present an\napproach for investigating the effects of unmodeled deviations in the robot's\ndynamic state on the contact detection method and we validate our algorithm by\nsimulations and experiments. We also compare the performance of the momentum\nobserver to the joint force deviation method, a direct estimation approach\nusing the robot's full dynamic model. We also demonstrate a basic extension of\nthe method to multisegment continuum robots. Results presented in this work\nextend dynamic contact detection to the domain of continuum and soft robots and\ncan be used to improve the safety of large-scale continuum robots for\nhuman-robot collaboration.\n","authors":["Garrison L. H. Johnston","Neel Shihora","Nabil Simaan"],"pdf_url":"https://arxiv.org/pdf/2505.03044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21216v2","updated":"2025-05-05T21:48:35Z","published":"2025-04-29T22:58:12Z","title":"PhysicsFC: Learning User-Controlled Skills for a Physics-Based Football\n  Player Controller","summary":"  We propose PhysicsFC, a method for controlling physically simulated football\nplayer characters to perform a variety of football skills--such as dribbling,\ntrapping, moving, and kicking--based on user input, while seamlessly\ntransitioning between these skills. Our skill-specific policies, which generate\nlatent variables for each football skill, are trained using an existing\nphysics-based motion embedding model that serves as a foundation for\nreproducing football motions. Key features include a tailored reward design for\nthe Dribble policy, a two-phase reward structure combined with projectile\ndynamics-based initialization for the Trap policy, and a Data-Embedded\nGoal-Conditioned Latent Guidance (DEGCL) method for the Move policy. Using the\ntrained skill policies, the proposed football player finite state machine\n(PhysicsFC FSM) allows users to interactively control the character. To ensure\nsmooth and agile transitions between skill policies, as defined in the FSM, we\nintroduce the Skill Transition-Based Initialization (STI), which is applied\nduring the training of each skill policy. We develop several interactive\nscenarios to showcase PhysicsFC's effectiveness, including competitive trapping\nand dribbling, give-and-go plays, and 11v11 football games, where multiple\nPhysicsFC agents produce natural and controllable physics-based football player\nbehaviors. Quantitative evaluations further validate the performance of\nindividual skill policies and the transitions between them, using the presented\nmetrics and experimental designs.\n","authors":["Minsu Kim","Eunho Jung","Yoonsang Lee"],"pdf_url":"https://arxiv.org/pdf/2504.21216v2.pdf","comment":"Accepted to SIGGRAPH 2025 and ACM TOG"},{"id":"http://arxiv.org/abs/2505.03035v1","updated":"2025-05-05T21:26:03Z","published":"2025-05-05T21:26:03Z","title":"MORE: Mobile Manipulation Rearrangement Through Grounded Language\n  Reasoning","summary":"  Autonomous long-horizon mobile manipulation encompasses a multitude of\nchallenges, including scene dynamics, unexplored areas, and error recovery.\nRecent works have leveraged foundation models for scene-level robotic reasoning\nand planning. However, the performance of these methods degrades when dealing\nwith a large number of objects and large-scale environments. To address these\nlimitations, we propose MORE, a novel approach for enhancing the capabilities\nof language models to solve zero-shot mobile manipulation planning for\nrearrangement tasks. MORE leverages scene graphs to represent environments,\nincorporates instance differentiation, and introduces an active filtering\nscheme that extracts task-relevant subgraphs of object and region instances.\nThese steps yield a bounded planning problem, effectively mitigating\nhallucinations and improving reliability. Additionally, we introduce several\nenhancements that enable planning across both indoor and outdoor environments.\nWe evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K\nbenchmark, where it becomes the first approach to successfully solve a\nsignificant share of the benchmark, outperforming recent foundation model-based\napproaches. Furthermore, we demonstrate the capabilities of our approach in\nseveral complex real-world tasks, mimicking everyday activities. We make the\ncode publicly available at https://more-model.cs.uni-freiburg.de.\n","authors":["Mohammad Mohammadi","Daniel Honerkamp","Martin Büchner","Matteo Cassinelli","Tim Welschehold","Fabien Despinoy","Igor Gilitschenski","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2505.03035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.19317v2","updated":"2025-05-05T21:09:07Z","published":"2025-03-25T03:25:57Z","title":"Towards Uncertainty Unification: A Case Study for Preference Learning","summary":"  Learning human preferences is essential for human-robot interaction, as it\nenables robots to adapt their behaviors to align with human expectations and\ngoals. However, the inherent uncertainties in both human behavior and robotic\nsystems make preference learning a challenging task. While probabilistic\nrobotics algorithms offer uncertainty quantification, the integration of human\npreference uncertainty remains underexplored. To bridge this gap, we introduce\nuncertainty unification and propose a novel framework, uncertainty-unified\npreference learning (UUPL), which enhances Gaussian Process (GP)-based\npreference learning by unifying human and robot uncertainties. Specifically,\nUUPL includes a human preference uncertainty model that improves GP posterior\nmean estimation, and an uncertainty-weighted Gaussian Mixture Model (GMM) that\nenhances GP predictive variance accuracy. Additionally, we design a\nuser-specific calibration process to align uncertainty representations across\nusers, ensuring consistency and reliability in the model performance.\nComprehensive experiments and user studies demonstrate that UUPL achieves\nstate-of-the-art performance in both prediction accuracy and user rating. An\nablation study further validates the effectiveness of human uncertainty model\nand uncertainty-weighted GMM of UUPL.\n","authors":["Shaoting Peng","Haonan Chen","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2503.19317v2.pdf","comment":"Project page: https://sites.google.com/view/uupl-rss25/home"},{"id":"http://arxiv.org/abs/2502.06149v2","updated":"2025-05-05T21:08:48Z","published":"2025-02-10T04:30:30Z","title":"Reward-Based Collision-Free Algorithm for Trajectory Planning of\n  Autonomous Robots","summary":"  This paper proposes a novel mission planning algorithm for autonomous robots\nthat selects an optimal waypoint sequence from a predefined set to maximize\ntotal reward while satisfying obstacle avoidance, state, input, derivative,\nmission time, and distance constraints. The formulation extends the\nprize-collecting traveling salesman problem. A tailored genetic algorithm\nevolves candidate solutions using a fitness function, crossover, and mutation,\nwith constraint enforcement via a penalty method. Differential flatness and\nclothoid curves are employed to penalize infeasible trajectories efficiently,\nwhile the Euler spiral method ensures curvature-continuous trajectories with\nbounded curvature, enhancing dynamic feasibility and mitigating oscillations\ntypical of minimum-jerk and snap parameterizations. Due to the discrete\nvariable length optimization space, crossover is performed using a dynamic\ntime-warping-based method and extended convex combination with projection. The\nalgorithm's performance is validated through simulations and experiments with a\nground vehicle, quadrotor, and quadruped, supported by benchmarking and\ntime-complexity analysis.\n","authors":["Jose D. Hoyos","Tianyu Zhou","Zehui Lu","Shaoshuai Mou"],"pdf_url":"https://arxiv.org/pdf/2502.06149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01800v2","updated":"2025-05-05T19:40:42Z","published":"2025-02-03T20:25:50Z","title":"Flow-based Domain Randomization for Learning and Sequencing Robotic\n  Skills","summary":"  Domain randomization in reinforcement learning is an established technique\nfor increasing the robustness of control policies trained in simulation. By\nrandomizing environment properties during training, the learned policy can\nbecome robust to uncertainties along the randomized dimensions. While the\nenvironment distribution is typically specified by hand, in this paper we\ninvestigate automatically discovering a sampling distribution via\nentropy-regularized reward maximization of a normalizing-flow-based neural\nsampling distribution. We show that this architecture is more flexible and\nprovides greater robustness than existing approaches that learn simpler,\nparameterized sampling distributions, as demonstrated in six simulated and one\nreal-world robotics domain. Lastly, we explore how these learned sampling\ndistributions, combined with a privileged value function, can be used for\nout-of-distribution detection in an uncertainty-aware multi-step manipulation\nplanner.\n","authors":["Aidan Curtis","Eric Li","Michael Noseworthy","Nishad Gothoskar","Sachin Chitta","Hui Li","Leslie Pack Kaelbling","Nicole Carey"],"pdf_url":"https://arxiv.org/pdf/2502.01800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02915v1","updated":"2025-05-05T18:00:11Z","published":"2025-05-05T18:00:11Z","title":"Zero-shot Sim2Real Transfer for Magnet-Based Tactile Sensor on Insertion\n  Tasks","summary":"  Tactile sensing is an important sensing modality for robot manipulation.\nAmong different types of tactile sensors, magnet-based sensors, like u-skin,\nbalance well between high durability and tactile density. However, the large\nsim-to-real gap of tactile sensors prevents robots from acquiring useful\ntactile-based manipulation skills from simulation data, a recipe that has been\nsuccessful for achieving complex and sophisticated control policies. Prior work\nhas implemented binarization techniques to bridge the sim-to-real gap for\ndexterous in-hand manipulation. However, binarization inherently loses much\ninformation that is useful in many other tasks, e.g., insertion. In our work,\nwe propose GCS, a novel sim-to-real technique to learn contact-rich skills with\ndense, distributed, 3-axis tactile readings. We evaluate our approach on blind\ninsertion tasks and show zero-shot sim-to-real transfer of RL policies with raw\ntactile reading as input.\n","authors":["Beining Han","Abhishek Joshi","Jia Deng"],"pdf_url":"https://arxiv.org/pdf/2505.02915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03841v1","updated":"2025-05-05T06:10:09Z","published":"2025-05-05T06:10:09Z","title":"Contact-Aware Safety in Soft Robots Using High-Order Control Barrier and\n  Lyapunov Functions","summary":"  Robots operating alongside people, particularly in sensitive scenarios such\nas aiding the elderly with daily tasks or collaborating with workers in\nmanufacturing, must guarantee safety and cultivate user trust. Continuum soft\nmanipulators promise safety through material compliance, but as designs evolve\nfor greater precision, payload capacity, and speed, and increasingly\nincorporate rigid elements, their injury risk resurfaces. In this letter, we\nintroduce a comprehensive High-Order Control Barrier Function (HOCBF) +\nHigh-Order Control Lyapunov Function (HOCLF) framework that enforces strict\ncontact force limits across the entire soft-robot body during environmental\ninteractions. Our approach combines a differentiable Piecewise Cosserat-Segment\n(PCS) dynamics model with a convex-polygon distance approximation metric, named\nDifferentiable Conservative Separating Axis Theorem (DCSAT), based on the soft\nrobot geometry to enable real-time, whole-body collision detection, resolution,\nand enforcement of the safety constraints. By embedding HOCBFs into our\noptimization routine, we guarantee safety and actively regulate environmental\ncoupling, allowing, for instance, safe object manipulation under HOCLF-driven\nmotion objectives. Extensive planar simulations demonstrate that our method\nmaintains safety-bounded contacts while achieving precise shape and task-space\nregulation. This work thus lays a foundation for the deployment of soft robots\nin human-centric environments with provable safety and performance.\n","authors":["Kiwan Wong","Maximilian Stölzle","Wei Xiao","Cosimo Della Santina","Daniela Rus","Gioele Zardini"],"pdf_url":"https://arxiv.org/pdf/2505.03841v1.pdf","comment":"8 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2505.02836v1","updated":"2025-05-05T17:59:58Z","published":"2025-05-05T17:59:58Z","title":"Scenethesis: A Language and Vision Agentic Framework for 3D Scene\n  Generation","summary":"  Synthesizing interactive 3D scenes from text is essential for gaming, virtual\nreality, and embodied AI. However, existing methods face several challenges.\nLearning-based approaches depend on small-scale indoor datasets, limiting the\nscene diversity and layout complexity. While large language models (LLMs) can\nleverage diverse text-domain knowledge, they struggle with spatial realism,\noften producing unnatural object placements that fail to respect common sense.\nOur key insight is that vision perception can bridge this gap by providing\nrealistic spatial guidance that LLMs lack. To this end, we introduce\nScenethesis, a training-free agentic framework that integrates LLM-based scene\nplanning with vision-guided layout refinement. Given a text prompt, Scenethesis\nfirst employs an LLM to draft a coarse layout. A vision module then refines it\nby generating an image guidance and extracting scene structure to capture\ninter-object relations. Next, an optimization module iteratively enforces\naccurate pose alignment and physical plausibility, preventing artifacts like\nobject penetration and instability. Finally, a judge module verifies spatial\ncoherence. Comprehensive experiments show that Scenethesis generates diverse,\nrealistic, and physically plausible 3D interactive scenes, making it valuable\nfor virtual content creation, simulation environments, and embodied AI\nresearch.\n","authors":["Lu Ling","Chen-Hsuan Lin","Tsung-Yi Lin","Yifan Ding","Yu Zeng","Yichen Sheng","Yunhao Ge","Ming-Yu Liu","Aniket Bera","Zhaoshuo Li"],"pdf_url":"https://arxiv.org/pdf/2505.02836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02835v1","updated":"2025-05-05T17:59:50Z","published":"2025-05-05T17:59:50Z","title":"R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning","summary":"  Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.\n","authors":["Yi-Fan Zhang","Xingyu Lu","Xiao Hu","Chaoyou Fu","Bin Wen","Tianke Zhang","Changyi Liu","Kaiyu Jiang","Kaibing Chen","Kaiyu Tang","Haojie Ding","Jiankang Chen","Fan Yang","Zhang Zhang","Tingting Gao","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.02835v1.pdf","comment":"Home page: https://github.com/yfzhang114/r1_reward"},{"id":"http://arxiv.org/abs/2505.02833v1","updated":"2025-05-05T17:59:03Z","published":"2025-05-05T17:59:03Z","title":"TWIST: Teleoperated Whole-Body Imitation System","summary":"  Teleoperating humanoid robots in a whole-body manner marks a fundamental step\ntoward developing general-purpose robotic intelligence, with human motion\nproviding an ideal interface for controlling all degrees of freedom. Yet, most\ncurrent humanoid teleoperation systems fall short of enabling coordinated\nwhole-body behavior, typically limiting themselves to isolated locomotion or\nmanipulation tasks. We present the Teleoperated Whole-Body Imitation System\n(TWIST), a system for humanoid teleoperation through whole-body motion\nimitation. We first generate reference motion clips by retargeting human motion\ncapture data to the humanoid robot. We then develop a robust, adaptive, and\nresponsive whole-body controller using a combination of reinforcement learning\nand behavior cloning (RL+BC). Through systematic analysis, we demonstrate how\nincorporating privileged future motion frames and real-world motion capture\n(MoCap) data improves tracking accuracy. TWIST enables real-world humanoid\nrobots to achieve unprecedented, versatile, and coordinated whole-body motor\nskills--spanning whole-body manipulation, legged manipulation, locomotion, and\nexpressive movement--using a single unified neural network controller. Our\nproject website: https://humanoid-teleop.github.io\n","authors":["Yanjie Ze","Zixuan Chen","João Pedro Araújo","Zi-ang Cao","Xue Bin Peng","Jiajun Wu","C. Karen Liu"],"pdf_url":"https://arxiv.org/pdf/2505.02833v1.pdf","comment":"Project website: https://humanoid-teleop.github.io"},{"id":"http://arxiv.org/abs/2505.02831v1","updated":"2025-05-05T17:58:05Z","published":"2025-05-05T17:58:05Z","title":"No Other Representation Component Is Needed: Diffusion Transformers Can\n  Provide Representation Guidance by Themselves","summary":"  Recent studies have demonstrated that learning a meaningful internal\nrepresentation can both accelerate generative training and enhance generation\nquality of the diffusion transformers. However, existing approaches necessitate\nto either introduce an additional and complex representation training framework\nor rely on a large-scale, pre-trained representation foundation model to\nprovide representation guidance during the original generative training\nprocess. In this study, we posit that the unique discriminative process\ninherent to diffusion transformers enables them to offer such guidance without\nrequiring external representation components. We therefore propose\nSelf-Representation A}lignment (SRA), a simple yet straightforward method that\nobtain representation guidance through a self-distillation manner.\nSpecifically, SRA aligns the output latent representation of the diffusion\ntransformer in earlier layer with higher noise to that in later layer with\nlower noise to progressively enhance the overall representation learning during\nonly generative training process. Experimental results indicate that applying\nSRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA\nnot only significantly outperforms approaches relying on auxiliary, complex\nrepresentation training frameworks but also achieves performance comparable to\nmethods that heavily dependent on powerful external representation priors.\n","authors":["Dengyang Jiang","Mengmeng Wang","Liuzhuozheng Li","Lei Zhang","Haoyu Wang","Wei Wei","Guang Dai","Yanning Zhang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.02831v1.pdf","comment":"Self-Representation Alignment for Diffusion Transformers. arXiv admin\n  note: text overlap with arXiv:2410.06940 by other authors"},{"id":"http://arxiv.org/abs/2505.02830v1","updated":"2025-05-05T17:57:07Z","published":"2025-05-05T17:57:07Z","title":"AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal\n  Model in Chest X-Ray Interpretation","summary":"  Chest X-rays (CXRs) are the most frequently performed imaging examinations in\nclinical settings. Recent advancements in Large Multimodal Models (LMMs) have\nenabled automated CXR interpretation, enhancing diagnostic accuracy and\nefficiency. However, despite their strong visual understanding, current Medical\nLMMs (MLMMs) still face two major challenges: (1) Insufficient region-level\nunderstanding and interaction, and (2) Limited accuracy and interpretability\ndue to single-step reasoning. In this paper, we empower MLMMs with\nanatomy-centric reasoning capabilities to enhance their interactivity and\nexplainability. Specifically, we first propose an Anatomical Ontology-Guided\nReasoning (AOR) framework, which centers on cross-modal region-level\ninformation to facilitate multi-step reasoning. Next, under the guidance of\nexpert physicians, we develop AOR-Instruction, a large instruction dataset for\nMLMMs training. Our experiments demonstrate AOR's superior performance in both\nVQA and report generation tasks.\n","authors":["Qingqiu Li","Zihang Cui","Seongsu Bae","Jilan Xu","Runtian Yuan","Yuejie Zhang","Rui Feng","Quanli Shen","Xiaobo Zhang","Junjun He","Shujun Wang"],"pdf_url":"https://arxiv.org/pdf/2505.02830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02825v1","updated":"2025-05-05T17:51:56Z","published":"2025-05-05T17:51:56Z","title":"Towards Application-Specific Evaluation of Vision Models: Case Studies\n  in Ecology and Biology","summary":"  Computer vision methods have demonstrated considerable potential to\nstreamline ecological and biological workflows, with a growing number of\ndatasets and models becoming available to the research community. However,\nthese resources focus predominantly on evaluation using machine learning\nmetrics, with relatively little emphasis on how their application impacts\ndownstream analysis. We argue that models should be evaluated using\napplication-specific metrics that directly represent model performance in the\ncontext of its final use case. To support this argument, we present two\ndisparate case studies: (1) estimating chimpanzee abundance and density with\ncamera trap distance sampling when using a video-based behaviour classifier and\n(2) estimating head rotation in pigeons using a 3D posture estimator. We show\nthat even models with strong machine learning performance (e.g., 87% mAP) can\nyield data that leads to discrepancies in abundance estimates compared to\nexpert-derived data. Similarly, the highest-performing models for posture\nestimation do not produce the most accurate inferences of gaze direction in\npigeons. Motivated by these findings, we call for researchers to integrate\napplication-specific metrics in ecological/biological datasets, allowing for\nmodels to be benchmarked in the context of their downstream application and to\nfacilitate better integration of models into application workflows.\n","authors":["Alex Hoi Hang Chan","Otto Brookes","Urs Waldmann","Hemal Naik","Iain D. Couzin","Majid Mirmehdi","Noël Adiko Houa","Emmanuelle Normand","Christophe Boesch","Lukas Boesch","Mimi Arandjelovic","Hjalmar Kühl","Tilo Burghardt","Fumihiro Kano"],"pdf_url":"https://arxiv.org/pdf/2505.02825v1.pdf","comment":"Accepted at CVPR Workshop, CV4Animals 2025"},{"id":"http://arxiv.org/abs/2505.02824v1","updated":"2025-05-05T17:51:55Z","published":"2025-05-05T17:51:55Z","title":"Towards Dataset Copyright Evasion Attack against Personalized\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance.\n","authors":["Kuofeng Gao","Yufei Zhu","Yiming Li","Jiawang Bai","Yong Yang","Zhifeng Li","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2505.02824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02823v1","updated":"2025-05-05T17:50:24Z","published":"2025-05-05T17:50:24Z","title":"MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset\n  via Attention Routing","summary":"  Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset.\n","authors":["Zinan Guo","Pengze Zhang","Yanze Wu","Chong Mou","Songtao Zhao","Qian He"],"pdf_url":"https://arxiv.org/pdf/2505.02823v1.pdf","comment":"Project page at https://github.com/guozinan126/MUSAR"},{"id":"http://arxiv.org/abs/2505.02815v1","updated":"2025-05-05T17:42:27Z","published":"2025-05-05T17:42:27Z","title":"Database-Agnostic Gait Enrollment using SetTransformers","summary":"  Gait recognition has emerged as a powerful tool for unobtrusive and\nlong-range identity analysis, with growing relevance in surveillance and\nmonitoring applications. Although recent advances in deep learning and\nlarge-scale datasets have enabled highly accurate recognition under closed-set\nconditions, real-world deployment demands open-set gait enrollment, which means\ndetermining whether a new gait sample corresponds to a known identity or\nrepresents a previously unseen individual. In this work, we introduce a\ntransformer-based framework for open-set gait enrollment that is both\ndataset-agnostic and recognition-architecture-agnostic. Our method leverages a\nSetTransformer to make enrollment decisions based on the embedding of a probe\nsample and a context set drawn from the gallery, without requiring\ntask-specific thresholds or retraining for new environments. By decoupling\nenrollment from the main recognition pipeline, our model is generalized across\ndifferent datasets, gallery sizes, and identity distributions. We propose an\nevaluation protocol that uses existing datasets in different ratios of\nidentities and walks per identity. We instantiate our method using\nskeleton-based gait representations and evaluate it on two benchmark datasets\n(CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition\nmodels (GaitGraph, GaitFormer, and GaitPT). We show that our method is\nflexible, is able to accurately perform enrollment in different scenarios, and\nscales better with data compared to traditional approaches. We will make the\ncode and dataset scenarios publicly available.\n","authors":["Nicoleta Basoc","Adrian Cosma","Andy Cǎtrunǎ","Emilian Rǎdoi"],"pdf_url":"https://arxiv.org/pdf/2505.02815v1.pdf","comment":"5 Tables, 6 Figures"},{"id":"http://arxiv.org/abs/2411.17251v8","updated":"2025-05-05T17:28:03Z","published":"2024-11-26T09:29:27Z","title":"Interpretable Dynamic Graph Neural Networks for Small Occluded Object\n  Detection and Tracking","summary":"  The detection and tracking of small, occluded objects such as pedestrians,\ncyclists, and motorbikes pose significant challenges for traffic surveillance\nsystems because of their erratic movement, frequent occlusion, and poor\nvisibility in dynamic urban environments. Traditional methods like YOLO11,\nwhile proficient in spatial feature extraction for precise detection, often\nstruggle with these small and dynamically moving objects, particularly in\nhandling real-time data updates and resource efficiency. This paper introduces\nDGNN-YOLO, a novel framework that integrates dynamic graph neural networks\n(DGNNs) with YOLO11 to address these limitations. Unlike standard GNNs, DGNNs\nare chosen for their superior ability to dynamically update graph structures in\nreal-time, which enables adaptive and robust tracking of objects in highly\nvariable urban traffic scenarios. This framework constructs and regularly\nupdates its graph representations, capturing objects as nodes and their\ninteractions as edges, thus effectively responding to rapidly changing\nconditions. Additionally, DGNN-YOLO incorporates Grad-CAM, Grad-CAM++, and\nEigen-CAM visualization techniques to enhance interpretability and foster\ntrust, offering insights into the model's decision-making process. Extensive\nexperiments validate the framework's performance, achieving a precision of\n0.8382, recall of 0.6875, and mAP@0.5:0.95 of 0.6476, significantly\noutperforming existing methods. This study offers a scalable and interpretable\nsolution for real-time traffic surveillance and significantly advances\nintelligent transportation systems' capabilities by addressing the critical\nchallenge of detecting and tracking small, occluded objects.\n","authors":["Shahriar Soudeep","Md Abrar Jahin","M. F. Mridha"],"pdf_url":"https://arxiv.org/pdf/2411.17251v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02797v1","updated":"2025-05-05T17:13:35Z","published":"2025-05-05T17:13:35Z","title":"DPNet: Dynamic Pooling Network for Tiny Object Detection","summary":"  In unmanned aerial systems, especially in complex environments, accurately\ndetecting tiny objects is crucial. Resizing images is a common strategy to\nimprove detection accuracy, particularly for small objects. However, simply\nenlarging images significantly increases computational costs and the number of\nnegative samples, severely degrading detection performance and limiting its\napplicability. This paper proposes a Dynamic Pooling Network (DPNet) for tiny\nobject detection to mitigate these issues. DPNet employs a flexible\ndown-sampling strategy by introducing a factor (df) to relax the fixed\ndownsampling process of the feature map to an adjustable one. Furthermore, we\ndesign a lightweight predictor to predict df for each input image, which is\nused to decrease the resolution of feature maps in the backbone. Thus, we\nachieve input-aware downsampling. We also design an Adaptive Normalization\nModule (ANM) to make a unified detector compatible with different dfs. A\nguidance loss supervises the predictor's training. DPNet dynamically allocates\ncomputing resources to trade off between detection accuracy and efficiency.\nExperiments on the TinyCOCO and TinyPerson datasets show that DPNet can save\nover 35% and 25% GFLOPs, respectively, while maintaining comparable detection\nperformance. The code will be made publicly available.\n","authors":["Luqi Gong","Haotian Chen","Yikun Chen","Tianliang Yao","Chao Li","Shuai Zhao","Guangjie Han"],"pdf_url":"https://arxiv.org/pdf/2505.02797v1.pdf","comment":"15 pages, 12 figures Haotian Chen and Luqi Gong contributed equally\n  to this work"},{"id":"http://arxiv.org/abs/2505.02787v1","updated":"2025-05-05T17:02:13Z","published":"2025-05-05T17:02:13Z","title":"Unsupervised training of keypoint-agnostic descriptors for flexible\n  retinal image registration","summary":"  Current color fundus image registration approaches are limited, among other\nthings, by the lack of labeled data, which is even more significant in the\nmedical domain, motivating the use of unsupervised learning. Therefore, in this\nwork, we develop a novel unsupervised descriptor learning method that does not\nrely on keypoint detection. This enables the resulting descriptor network to be\nagnostic to the keypoint detector used during the registration inference.\n  To validate this approach, we perform an extensive and comprehensive\ncomparison on the reference public retinal image registration dataset.\nAdditionally, we test our method with multiple keypoint detectors of varied\nnature, even proposing some novel ones. Our results demonstrate that the\nproposed approach offers accurate registration, not incurring in any\nperformance loss versus supervised methods. Additionally, it demonstrates\naccurate performance regardless of the keypoint detector used. Thus, this work\nrepresents a notable step towards leveraging unsupervised learning in the\nmedical domain.\n","authors":["David Rivas-Villar","Álvaro S. Hervella","José Rouco","Jorge Novo"],"pdf_url":"https://arxiv.org/pdf/2505.02787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07577v2","updated":"2025-05-05T17:00:03Z","published":"2024-10-10T03:28:29Z","title":"3D Vision-Language Gaussian Splatting","summary":"  Recent advancements in 3D reconstruction methods and vision-language models\nhave propelled the development of multi-modal 3D scene understanding, which has\nvital applications in robotics, autonomous driving, and virtual/augmented\nreality. However, current multi-modal scene understanding approaches have\nnaively embedded semantic representations into 3D reconstruction methods\nwithout striking a balance between visual and language modalities, which leads\nto unsatisfying semantic rasterization of translucent or reflective objects, as\nwell as over-fitting on color modality. To alleviate these limitations, we\npropose a solution that adequately handles the distinct visual and semantic\nmodalities, i.e., a 3D vision-language Gaussian splatting model for scene\nunderstanding, to put emphasis on the representation learning of language\nmodality. We propose a novel cross-modal rasterizer, using modality fusion\nalong with a smoothed semantic indicator for enhancing semantic rasterization.\nWe also employ a camera-view blending technique to improve semantic consistency\nbetween existing and synthesized views, thereby effectively mitigating\nover-fitting. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance in open-vocabulary semantic segmentation,\nsurpassing existing methods by a significant margin.\n","authors":["Qucheng Peng","Benjamin Planche","Zhongpai Gao","Meng Zheng","Anwesa Choudhuri","Terrence Chen","Chen Chen","Ziyan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.07577v2.pdf","comment":"Accepted at ICLR 2025. Main paper + supplementary material"},{"id":"http://arxiv.org/abs/2412.15023v3","updated":"2025-05-05T16:55:53Z","published":"2024-12-19T16:37:19Z","title":"FolAI: Synchronized Foley Sound Generation with Semantic and Temporal\n  Alignment","summary":"  Traditional sound design workflows rely on manual alignment of audio events\nto visual cues, as in Foley sound design, where everyday actions like footsteps\nor object interactions are recreated to match the on-screen motion. This\nprocess is time-consuming, difficult to scale, and lacks automation tools that\npreserve creative intent. Despite recent advances in vision-to-audio\ngeneration, producing temporally coherent and semantically controllable sound\neffects from video remains a major challenge. To address these limitations, we\nintroduce FolAI, a two-stage generative framework that decouples the when and\nthe what of sound synthesis, i.e., the temporal structure extraction and the\nsemantically guided generation, respectively. In the first stage, we estimate a\nsmooth control signal from the video that captures the motion intensity and\nrhythmic structure over time, serving as a temporal scaffold for the audio. In\nthe second stage, a diffusion-based generative model produces sound effects\nconditioned both on this temporal envelope and on high-level semantic\nembeddings, provided by the user, that define the desired auditory content\n(e.g., material or action type). This modular design enables precise control\nover both timing and timbre, streamlining repetitive tasks while preserving\ncreative flexibility in professional Foley workflows. Results on diverse visual\ncontexts, such as footstep generation and action-specific sonorization,\ndemonstrate that our model reliably produces audio that is temporally aligned\nwith visual motion, semantically consistent with user intent, and perceptually\nrealistic. These findings highlight the potential of FolAI as a controllable\nand modular solution for scalable, high-quality Foley sound synthesis in\nprofessional and interactive settings. Supplementary materials are accessible\non our dedicated demo page at https://ispamm.github.io/FolAI.\n","authors":["Riccardo Fosco Gramaccioni","Christian Marinoni","Emilian Postolache","Marco Comunità","Luca Cosmo","Joshua D. Reiss","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.15023v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02784v1","updated":"2025-05-05T16:54:04Z","published":"2025-05-05T16:54:04Z","title":"Advances in Automated Fetal Brain MRI Segmentation and Biometry:\n  Insights from the FeTA 2024 Challenge","summary":"  Accurate fetal brain tissue segmentation and biometric analysis are essential\nfor studying brain development in utero. The FeTA Challenge 2024 advanced\nautomated fetal brain MRI analysis by introducing biometry prediction as a new\ntask alongside tissue segmentation. For the first time, our diverse\nmulti-centric test set included data from a new low-field (0.55T) MRI dataset.\nEvaluation metrics were also expanded to include the topology-specific Euler\ncharacteristic difference (ED). Sixteen teams submitted segmentation methods,\nmost of which performed consistently across both high- and low-field scans.\nHowever, longitudinal trends indicate that segmentation accuracy may be\nreaching a plateau, with results now approaching inter-rater variability. The\nED metric uncovered topological differences that were missed by conventional\nmetrics, while the low-field dataset achieved the highest segmentation scores,\nhighlighting the potential of affordable imaging systems when paired with\nhigh-quality reconstruction. Seven teams participated in the biometry task, but\nmost methods failed to outperform a simple baseline that predicted measurements\nbased solely on gestational age, underscoring the challenge of extracting\nreliable biometric estimates from image data alone. Domain shift analysis\nidentified image quality as the most significant factor affecting model\ngeneralization, with super-resolution pipelines also playing a substantial\nrole. Other factors, such as gestational age, pathology, and acquisition site,\nhad smaller, though still measurable, effects. Overall, FeTA 2024 offers a\ncomprehensive benchmark for multi-class segmentation and biometry estimation in\nfetal brain MRI, underscoring the need for data-centric approaches, improved\ntopological evaluation, and greater dataset diversity to enable clinically\nrobust and generalizable AI tools.\n","authors":["Vladyslav Zalevskyi","Thomas Sanchez","Misha Kaandorp","Margaux Roulet","Diego Fajardo-Rojas","Liu Li","Jana Hutter","Hongwei Bran Li","Matthew Barkovich","Hui Ji","Luca Wilhelmi","Aline Dändliker","Céline Steger","Mériam Koob","Yvan Gomez","Anton Jakovčić","Melita Klaić","Ana Adžić","Pavel Marković","Gracia Grabarić","Milan Rados","Jordina Aviles Verdera","Gregor Kasprian","Gregor Dovjak","Raphael Gaubert-Rachmühl","Maurice Aschwanden","Qi Zeng","Davood Karimi","Denis Peruzzo","Tommaso Ciceri","Giorgio Longari","Rachika E. Hamadache","Amina Bouzid","Xavier Lladó","Simone Chiarella","Gerard Martí-Juan","Miguel Ángel González Ballester","Marco Castellaro","Marco Pinamonti","Valentina Visani","Robin Cremese","Keïn Sam","Fleur Gaudfernau","Param Ahir","Mehul Parikh","Maximilian Zenk","Michael Baumgartner","Klaus Maier-Hein","Li Tianhong","Yang Hong","Zhao Longfei","Domen Preloznik","Žiga Špiclin","Jae Won Choi","Muyang Li","Jia Fu","Guotai Wang","Jingwen Jiang","Lyuyang Tong","Bo Du","Andrea Gondova","Sungmin You","Kiho Im","Abdul Qayyum","Moona Mazher","Steven A Niederer","Maya Yanko","Bella Specktor-Fadida","Dafna Ben Bashat","Andras Jakab","Roxane Licandro","Kelly Payette","Meritxell Bach Cuadra"],"pdf_url":"https://arxiv.org/pdf/2505.02784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02779v1","updated":"2025-05-05T16:46:32Z","published":"2025-05-05T16:46:32Z","title":"Unsupervised Deep Learning-based Keypoint Localization Estimating\n  Descriptor Matching Performance","summary":"  Retinal image registration, particularly for color fundus images, is a\nchallenging yet essential task with diverse clinical applications. Existing\nregistration methods for color fundus images typically rely on keypoints and\ndescriptors for alignment; however, a significant limitation is their reliance\non labeled data, which is particularly scarce in the medical domain.\n  In this work, we present a novel unsupervised registration pipeline that\nentirely eliminates the need for labeled data. Our approach is based on the\nprinciple that locations with distinctive descriptors constitute reliable\nkeypoints. This fully inverts the conventional state-of-the-art approach,\nconditioning the detector on the descriptor rather than the opposite.\n  First, we propose an innovative descriptor learning method that operates\nwithout keypoint detection or any labels, generating descriptors for arbitrary\nlocations in retinal images. Next, we introduce a novel, label-free keypoint\ndetector network which works by estimating descriptor performance directly from\nthe input image.\n  We validate our method through a comprehensive evaluation on four hold-out\ndatasets, demonstrating that our unsupervised descriptor outperforms\nstate-of-the-art supervised descriptors and that our unsupervised detector\nsignificantly outperforms existing unsupervised detection methods. Finally, our\nfull registration pipeline achieves performance comparable to the leading\nsupervised methods, while not employing any labeled data. Additionally, the\nlabel-free nature and design of our method enable direct adaptation to other\ndomains and modalities.\n","authors":["David Rivas-Villar","Álvaro S. Hervella","José Rouco","Jorge Novo"],"pdf_url":"https://arxiv.org/pdf/2505.02779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10316v3","updated":"2025-05-05T16:31:12Z","published":"2024-12-13T17:58:06Z","title":"BrushEdit: All-In-One Image Inpainting and Editing","summary":"  Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.\n","authors":["Yaowei Li","Yuxuan Bian","Xuan Ju","Zhaoyang Zhang","Junhao Zhuang","Ying Shan","Yuexian Zou","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2412.10316v3.pdf","comment":"WebPage available at\n  https://liyaowei-stu.github.io/project/BrushEdit/"},{"id":"http://arxiv.org/abs/2504.20466v2","updated":"2025-05-05T16:07:30Z","published":"2025-04-29T07:00:06Z","title":"LMME3DHF: Benchmarking and Evaluating Multimodal 3D Human Face\n  Generation with LMMs","summary":"  The rapid advancement in generative artificial intelligence have enabled the\ncreation of 3D human faces (HFs) for applications including media production,\nvirtual reality, security, healthcare, and game development, etc. However,\nassessing the quality and realism of these AI-generated 3D human faces remains\na significant challenge due to the subjective nature of human perception and\ninnate perceptual sensitivity to facial features. To this end, we conduct a\ncomprehensive study on the quality assessment of AI-generated 3D human faces.\nWe first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of\nAI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)\ncollected across two dimensions, i.e., quality and authenticity, 2,000\ndistortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,\nwe propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating\n3DHF capable of quality and authenticity score prediction, distortion-aware\nvisual question answering, and distortion-aware saliency prediction.\nExperimental results show that LMME3DHF achieves state-of-the-art performance,\nsurpassing existing methods in both accurately predicting quality scores for\nAI-generated 3D human faces and effectively identifying distortion-aware\nsalient regions and distortion types, while maintaining strong alignment with\nhuman perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be\nreleased upon the publication.\n","authors":["Woo Yi Yang","Jiarui Wang","Sijing Wu","Huiyu Duan","Yuxin Zhu","Liu Yang","Kang Fu","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2504.20466v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02753v1","updated":"2025-05-05T16:05:37Z","published":"2025-05-05T16:05:37Z","title":"Advancing Generalizable Tumor Segmentation with Anomaly-Aware\n  Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models","summary":"  We explore Generalizable Tumor Segmentation, aiming to train a single model\nfor zero-shot tumor segmentation across diverse anatomical regions. Existing\nmethods face limitations related to segmentation quality, scalability, and the\nrange of applicable imaging modalities. In this paper, we uncover the potential\nof the internal representations within frozen medical foundation diffusion\nmodels as highly efficient zero-shot learners for tumor segmentation by\nintroducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware\nopen-vocabulary attention maps based on text prompts to enable generalizable\nanomaly segmentation without being restricted by a predefined training category\nlist. To further improve and refine anomaly segmentation masks, DiffuGTS\nleverages the diffusion model, transforming pathological regions into\nhigh-quality pseudo-healthy counterparts through latent space inpainting, and\napplies a novel pixel-level and feature-level residual learning approach,\nresulting in segmentation masks with significantly enhanced quality and\ngeneralization. Comprehensive experiments on four datasets and seven tumor\ncategories demonstrate the superior performance of our method, surpassing\ncurrent state-of-the-art models across multiple zero-shot settings. Codes are\navailable at https://github.com/Yankai96/DiffuGTS.\n","authors":["Yankai Jiang","Peng Zhang","Donglin Yang","Yuan Tian","Hai Lin","Xiaosong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.02753v1.pdf","comment":"This paper is accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2505.02751v1","updated":"2025-05-05T16:05:13Z","published":"2025-05-05T16:05:13Z","title":"Platelet enumeration in dense aggregates","summary":"  Identifying and counting blood components such as red blood cells, various\ntypes of white blood cells, and platelets is a critical task for healthcare\npractitioners. Deep learning approaches, particularly convolutional neural\nnetworks (CNNs) using supervised learning strategies, have shown considerable\nsuccess for such tasks. However, CNN based architectures such as U-Net, often\nstruggles to accurately identify platelets due to their sizes and high\nvariability of features. To address these challenges, researchers have commonly\nemployed strategies such as class weighted loss functions, which have\ndemonstrated some success. However, this does not address the more significant\nchallenge of platelet variability in size and tendency to form aggregates and\nassociations with other blood components. In this study, we explored an\nalternative approach by investigating the role of convolutional kernels in\nmitigating these issues. We also assigned separate classes to singular\nplatelets and platelet aggregates and performed semantic segmentation using\nvarious U-Net architectures for identifying platelets. We then evaluated and\ncompared two common methods (pixel area method and connected component\nanalysis) for counting platelets and proposed an alternative approach\nspecialized for single platelets and platelet aggregates. Our experiments\nprovided results that showed significant improvements in the identification of\nplatelets, highlighting the importance of optimizing convolutional operations\nand class designations. We show that the common practice of pixel area-based\ncounting often over estimate platelet counts, whereas the proposed method\npresented in this work offers significant improvements. We discuss in detail\nabout these methods from segmentation masks.\n","authors":["H. Martin Gillis","Yogeshwar Shendye","Paul Hollensen","Alan Fine","Thomas Trappenberg"],"pdf_url":"https://arxiv.org/pdf/2505.02751v1.pdf","comment":"International Joint Conference on Neural Networks (IJCNN 2025)"},{"id":"http://arxiv.org/abs/2505.02746v1","updated":"2025-05-05T15:56:25Z","published":"2025-05-05T15:56:25Z","title":"Using Knowledge Graphs to harvest datasets for efficient CLIP model\n  training","summary":"  Training high-quality CLIP models typically requires enormous datasets, which\nlimits the development of domain-specific models -- especially in areas that\neven the largest CLIP models do not cover well -- and drives up training costs.\nThis poses challenges for scientific research that needs fine-grained control\nover the training procedure of CLIP models. In this work, we show that by\nemploying smart web search strategies enhanced with knowledge graphs, a robust\nCLIP model can be trained from scratch with considerably less data.\nSpecifically, we demonstrate that an expert foundation model for living\norganisms can be built using just 10M images. Moreover, we introduce EntityNet,\na dataset comprising 33M images paired with 46M text descriptions, which\nenables the training of a generic CLIP model in significantly reduced time.\n","authors":["Simon Ging","Sebastian Walter","Jelena Bratulić","Johannes Dienert","Hannah Bast","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2505.02746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16926v2","updated":"2025-05-05T15:42:56Z","published":"2024-11-25T20:50:21Z","title":"Context-Aware Input Orchestration for Video Inpainting","summary":"  Traditional neural network-driven inpainting methods struggle to deliver\nhigh-quality results within the constraints of mobile device processing power\nand memory. Our research introduces an innovative approach to optimize memory\nusage by altering the composition of input data. Typically, video inpainting\nrelies on a predetermined set of input frames, such as neighboring and\nreference frames, often limited to five-frame sets. Our focus is to examine how\nvarying the proportion of these input frames impacts the quality of the\ninpainted video. By dynamically adjusting the input frame composition based on\noptical flow and changes of the mask, we have observed an improvement in\nvarious contents including rapid visual context changes.\n","authors":["Hoyoung Kim","Azimbek Khudoyberdiev","Seonghwan Jeong","Jihoon Ryoo"],"pdf_url":"https://arxiv.org/pdf/2411.16926v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10098v2","updated":"2025-05-05T15:41:55Z","published":"2025-01-17T10:35:58Z","title":"landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D\n  Images","summary":"  Anatomical landmark localization in 2D/3D images is a critical task in\nmedical imaging. Although many general-purpose tools exist for landmark\nlocalization in classical computer vision tasks, such as pose estimation, they\nlack the specialized features and modularity necessary for anatomical landmark\nlocalization applications in the medical domain. Therefore, we introduce\nlandmarker, a Python package built on PyTorch. The package provides a\ncomprehensive, flexible toolkit for developing and evaluating landmark\nlocalization algorithms, supporting a range of methodologies, including static\nand adaptive heatmap regression. landmarker enhances the accuracy of landmark\nidentification, streamlines research and development processes, and supports\nvarious image formats and preprocessing pipelines. Its modular design allows\nusers to customize and extend the toolkit for specific datasets and\napplications, accelerating innovation in medical imaging. landmarker addresses\na critical need for precision and customization in landmark localization tasks\nnot adequately met by existing general-purpose pose estimation tools.\n","authors":["Jef Jonkers","Luc Duchateau","Glenn Van Wallendael","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2501.10098v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.02225v2","updated":"2025-05-05T15:35:26Z","published":"2024-04-02T18:27:03Z","title":"CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement","summary":"  We propose CHOSEN, a simple yet flexible, robust and effective multi-view\ndepth refinement framework. It can be employed in any existing multi-view\nstereo pipeline, with straightforward generalization capability for different\nmulti-view capture systems such as camera relative positioning and lenses.\nGiven an initial depth estimation, CHOSEN iteratively re-samples and selects\nthe best hypotheses, and automatically adapts to different metric or intrinsic\nscales determined by the capture system. The key to our approach is the\napplication of contrastive learning in an appropriate solution space and a\ncarefully designed hypothesis feature, based on which positive and negative\nhypotheses can be effectively distinguished. Integrated in a simple baseline\nmulti-view stereo pipeline, CHOSEN delivers impressive quality in terms of\ndepth and normal accuracy compared to many current deep learning based\nmulti-view stereo pipelines.\n","authors":["Di Qiu","Yinda Zhang","Thabo Beeler","Vladimir Tankovich","Christian Häne","Sean Fanello","Christoph Rhemann","Sergio Orts Escolano"],"pdf_url":"https://arxiv.org/pdf/2404.02225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04519v3","updated":"2025-05-05T15:24:49Z","published":"2025-04-06T15:32:08Z","title":"SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation","summary":"  Segment Anything 2 (SAM2) enables robust single-object tracking using\nsegmentation. To extend this to multi-object tracking (MOT), we propose\nSAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking\nby Detection or Tracking by Query, SAM2MOT directly generates tracking boxes\nfrom segmentation masks, reducing reliance on detection accuracy. SAM2MOT has\ntwo key advantages: zero-shot generalization, allowing it to work across\ndatasets without fine-tuning, and strong object association, inherited from\nSAM2. To further improve performance, we integrate a trajectory manager system\nfor precise object addition and removal, and a cross-object interaction module\nto handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show\nstate-of-the-art results. Notably, SAM2MOT outperforms existing methods on\nDanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT.\nCode is available at https://github.com/TripleJoy/SAM2MOT.\n","authors":["Junjie Jiang","Zelin Wang","Manqi Zhao","Yin Li","DongSheng Jiang"],"pdf_url":"https://arxiv.org/pdf/2504.04519v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02720v1","updated":"2025-05-05T15:19:18Z","published":"2025-05-05T15:19:18Z","title":"A Rate-Quality Model for Learned Video Coding","summary":"  Learned video coding (LVC) has recently achieved superior coding performance.\nIn this paper, we model the rate-quality (R-Q) relationship for learned video\ncoding by a parametric function. We learn a neural network, termed RQNet, to\ncharacterize the relationship between the bitrate and quality level according\nto video content and coding context. The predicted (R,Q) results are further\nintegrated with those from previously coded frames using the least-squares\nmethod to determine the parameters of our R-Q model on-the-fly. Compared to the\nconventional approaches, our method accurately estimates the R-Q relationship,\nenabling the online adaptation of model parameters to enhance both flexibility\nand precision. Experimental results show that our R-Q model achieves\nsignificantly smaller bitrate deviations than the baseline method on commonly\nused datasets with minimal additional complexity.\n","authors":["Sang NguyenQuang","Cheng-Wei Chen","Xiem HoangVan","Wen-Hsiao Peng"],"pdf_url":"https://arxiv.org/pdf/2505.02720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02705v1","updated":"2025-05-05T14:57:43Z","published":"2025-05-05T14:57:43Z","title":"Multi-View Learning with Context-Guided Receptance for Image Denoising","summary":"  Image denoising is essential in low-level vision applications such as\nphotography and automated driving. Existing methods struggle with\ndistinguishing complex noise patterns in real-world scenes and consume\nsignificant computational resources due to reliance on Transformer-based\nmodels. In this work, the Context-guided Receptance Weighted Key-Value (\\M)\nmodel is proposed, combining enhanced multi-view feature integration with\nefficient sequence modeling. Our approach introduces the Context-guided Token\nShift (CTS) paradigm, which effectively captures local spatial dependencies and\nenhance the model's ability to model real-world noise distributions.\nAdditionally, the Frequency Mix (FMix) module extracting frequency-domain\nfeatures is designed to isolate noise in high-frequency spectra, and is\nintegrated with spatial representations through a multi-view learning process.\nTo improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is\nadopted, enabling full pixel-sequence interaction with linear complexity while\novercoming the causal selection constraints. The model is validated on multiple\nreal-world image denoising datasets, outperforming the existing\nstate-of-the-art methods quantitatively and reducing inference time up to 40\\%.\nQualitative results further demonstrate the ability of our model to restore\nfine details in various scenes.\n","authors":["Binghong Chen","Tingting Chai","Wei Jiang","Yuanrong Xu","Guanglu Zhou","Xiangqian Wu"],"pdf_url":"https://arxiv.org/pdf/2505.02705v1.pdf","comment":"Accepted by IJCAI 2025, code will be available at\n  https://github.com/Seeker98/CRWKV"},{"id":"http://arxiv.org/abs/2505.02704v1","updated":"2025-05-05T14:57:16Z","published":"2025-05-05T14:57:16Z","title":"Visually-Guided Linguistic Disambiguation for Monocular Depth Scale\n  Recovery","summary":"  We propose a robust method for monocular depth scale recovery. Monocular\ndepth estimation can be divided into two main directions: (1) relative depth\nestimation, which provides normalized or inverse depth without scale\ninformation, and (2) metric depth estimation, which involves recovering depth\nwith absolute scale. To obtain absolute scale information for practical\ndownstream tasks, utilizing textual information to recover the scale of a\nrelative depth map is a highly promising approach. However, since a single\nimage can have multiple descriptions from different perspectives or with\nvarying styles, it has been shown that different textual descriptions can\nsignificantly affect the scale recovery process. To address this issue, our\nmethod, VGLD, stabilizes the influence of textual information by incorporating\nhigh-level semantic information from the corresponding image alongside the\ntextual description. This approach resolves textual ambiguities and robustly\noutputs a set of linear transformation parameters (scalars) that can be\nglobally applied to the relative depth map, ultimately generating depth\npredictions with metric-scale accuracy. We validate our method across several\npopular relative depth models(MiDas, DepthAnything), using both indoor scenes\n(NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions\nas a universal alignment module when trained on multiple datasets, achieving\nstrong performance even in zero-shot scenarios. Code is available at:\nhttps://github.com/pakinwu/VGLD.\n","authors":["Bojin Wu","Jing Chen"],"pdf_url":"https://arxiv.org/pdf/2505.02704v1.pdf","comment":"21 pages, conference"},{"id":"http://arxiv.org/abs/2505.02703v1","updated":"2025-05-05T14:57:02Z","published":"2025-05-05T14:57:02Z","title":"Structure Causal Models and LLMs Integration in Medical Visual Question\n  Answering","summary":"  Medical Visual Question Answering (MedVQA) aims to answer medical questions\naccording to medical images. However, the complexity of medical data leads to\nconfounders that are difficult to observe, so bias between images and questions\nis inevitable. Such cross-modal bias makes it challenging to infer medically\nmeaningful answers. In this work, we propose a causal inference framework for\nthe MedVQA task, which effectively eliminates the relative confounding effect\nbetween the image and the question to ensure the precision of the\nquestion-answering (QA) session. We are the first to introduce a novel causal\ngraph structure that represents the interaction between visual and textual\nelements, explicitly capturing how different questions influence visual\nfeatures. During optimization, we apply the mutual information to discover\nspurious correlations and propose a multi-variable resampling front-door\nadjustment method to eliminate the relative confounding effect, which aims to\nalign features based on their true causal relevance to the question-answering\ntask. In addition, we also introduce a prompt strategy that combines multiple\nprompt forms to improve the model's ability to understand complex medical data\nand answer accurately. Extensive experiments on three MedVQA datasets\ndemonstrate that 1) our method significantly improves the accuracy of MedVQA,\nand 2) our method achieves true causal correlations in the face of complex\nmedical data.\n","authors":["Zibo Xu","Qiang Li","Weizhi Nie","Weijie Wang","Anan Liu"],"pdf_url":"https://arxiv.org/pdf/2505.02703v1.pdf","comment":"Accepted by IEEE TMI 2025"},{"id":"http://arxiv.org/abs/2505.02690v1","updated":"2025-05-05T14:41:06Z","published":"2025-05-05T14:41:06Z","title":"Dance of Fireworks: An Interactive Broadcast Gymnastics Training System\n  Based on Pose Estimation","summary":"  This study introduces Dance of Fireworks, an interactive system designed to\ncombat sedentary health risks by enhancing engagement in radio calisthenics.\nLeveraging mobile device cameras and lightweight pose estimation\n(PoseNet/TensorFlow Lite), the system extracts body keypoints, computes joint\nangles, and compares them with standardized motions to deliver real-time\ncorrective feedback. To incentivize participation, it dynamically maps users'\nmovements (such as joint angles and velocity) to customizable fireworks\nanimations, rewarding improved accuracy with richer visual effects. Experiments\ninvolving 136 participants demonstrated a significant reduction in average\njoint angle errors from 21.3 degrees to 9.8 degrees (p < 0.01) over four\nsessions, with 93.4 percent of users affirming its exercise-promoting efficacy\nand 85.4 percent praising its entertainment value. The system operates without\npredefined motion templates or specialised hardware, enabling seamless\nintegration into office environments. Future enhancements will focus on\nimproving pose recognition accuracy, reducing latency, and adding features such\nas multiplayer interaction and music synchronisation. This work presents a\ncost-effective, engaging solution to promote physical activity in sedentary\npopulations.\n","authors":["Haotian Chen","Ziyu Liu","Xi Cheng","Chuangqi Li"],"pdf_url":"https://arxiv.org/pdf/2505.02690v1.pdf","comment":"21 pages, 13 figures"},{"id":"http://arxiv.org/abs/2411.18674v2","updated":"2025-05-05T14:25:01Z","published":"2024-11-27T18:50:15Z","title":"Active Data Curation Effectively Distills Large-Scale Multimodal Models","summary":"  Knowledge distillation (KD) is the de facto standard for compressing\nlarge-scale models into smaller ones. Prior works have explored ever more\ncomplex KD strategies involving different objective functions,\nteacher-ensembles, and weight inheritance. In this work we explore an\nalternative, yet simple approach -- active data curation as effective\ndistillation for contrastive multimodal pretraining. Our simple online batch\nselection method, ACID, outperforms strong KD baselines across various model-,\ndata- and compute-configurations. Further, we find such an active data curation\nstrategy to in fact be complementary to standard KD, and can be effectively\ncombined to train highly performant inference-efficient models. Our simple and\nscalable pretraining framework, ACED, achieves state-of-the-art results across\n27 zero-shot classification and retrieval tasks with upto 11% less inference\nFLOPs. We further demonstrate that our ACED models yield strong vision-encoders\nfor training generative multimodal models in the LiT-Decoder setting,\noutperforming larger vision encoders for image-captioning and visual\nquestion-answering tasks.\n","authors":["Vishaal Udandarao","Nikhil Parthasarathy","Muhammad Ferjad Naeem","Talfan Evans","Samuel Albanie","Federico Tombari","Yongqin Xian","Alessio Tonioni","Olivier J. Hénaff"],"pdf_url":"https://arxiv.org/pdf/2411.18674v2.pdf","comment":"Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025"},{"id":"http://arxiv.org/abs/2405.01101v5","updated":"2025-05-05T14:24:48Z","published":"2024-05-02T09:09:48Z","title":"Enhancing person re-identification via Uncertainty Feature Fusion Method\n  and Auto-weighted Measure Combination","summary":"  Person re-identification (Re-ID) is a challenging task that involves\nidentifying the same person across different camera views in surveillance\nsystems. Current methods usually rely on features from single-camera views,\nwhich can be limiting when dealing with multiple cameras and challenges such as\nchanging viewpoints and occlusions. In this paper, a new approach is introduced\nthat enhances the capability of ReID models through the Uncertain Feature\nFusion Method (UFFM) and Auto-weighted Measure Combination (AMC). UFFM\ngenerates multi-view features using features extracted independently from\nmultiple images to mitigate view bias. However, relying only on similarity\nbased on multi-view features is limited because these features ignore the\ndetails represented in single-view features. Therefore, we propose the AMC\nmethod to generate a more robust similarity measure by combining various\nmeasures. Our method significantly improves Rank@1 accuracy and Mean Average\nPrecision (mAP) when evaluated on person re-identification datasets. Combined\nwith the BoT Baseline on challenging datasets, we achieve impressive results,\nwith a 7.9% improvement in Rank@1 and a 12.1% improvement in mAP on the MSMT17\ndataset. On the Occluded-DukeMTMC dataset, our method increases Rank@1 by 22.0%\nand mAP by 18.4%. Code is available:\nhttps://github.com/chequanghuy/Enhancing-Person-Re-Identification-via-UFFM-and-AMC\n","authors":["Quang-Huy Che","Le-Chuong Nguyen","Duc-Tuan Luu","Vinh-Tiep Nguyen"],"pdf_url":"https://arxiv.org/pdf/2405.01101v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02677v1","updated":"2025-05-05T14:22:58Z","published":"2025-05-05T14:22:58Z","title":"Multimodal Deep Learning for Stroke Prediction and Detection using\n  Retinal Imaging and Clinical Data","summary":"  Stroke is a major public health problem, affecting millions worldwide. Deep\nlearning has recently demonstrated promise for enhancing the diagnosis and risk\nprediction of stroke. However, existing methods rely on costly medical imaging\nmodalities, such as computed tomography. Recent studies suggest that retinal\nimaging could offer a cost-effective alternative for cerebrovascular health\nassessment due to the shared clinical pathways between the retina and the\nbrain. Hence, this study explores the impact of leveraging retinal images and\nclinical data for stroke detection and risk prediction. We propose a multimodal\ndeep neural network that processes Optical Coherence Tomography (OCT) and\ninfrared reflectance retinal scans, combined with clinical data, such as\ndemographics, vital signs, and diagnosis codes. We pretrained our model using a\nself-supervised learning framework using a real-world dataset consisting of\n$37$ k scans, and then fine-tuned and evaluated the model using a smaller\nlabeled subset. Our empirical findings establish the predictive ability of the\nconsidered modalities in detecting lasting effects in the retina associated\nwith acute stroke and forecasting future risk within a specific time horizon.\nThe experimental results demonstrate the effectiveness of our proposed\nframework by achieving $5$\\% AUROC improvement as compared to the unimodal\nimage-only baseline, and $8$\\% improvement compared to an existing\nstate-of-the-art foundation model. In conclusion, our study highlights the\npotential of retinal imaging in identifying high-risk patients and improving\nlong-term outcomes.\n","authors":["Saeed Shurrab","Aadim Nepal","Terrence J. Lee-St. John","Nicola G. Ghazi","Bartlomiej Piechowski-Jozwiak","Farah E. Shamout"],"pdf_url":"https://arxiv.org/pdf/2505.02677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02664v1","updated":"2025-05-05T14:14:32Z","published":"2025-05-05T14:14:32Z","title":"Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp\n  Pose Detection in Clutter","summary":"  Grasp pose detection in cluttered, real-world environments remains a\nsignificant challenge due to noisy and incomplete sensory data combined with\ncomplex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0)\nmethod, a lightweight yet highly effective hypothesis-and-test robotics\ngrasping framework which leverages an ensemble of Graph Neural Networks for\nefficient geometric reasoning from point cloud data. Building on the success of\nGtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp\ndetection but was limited by assumptions of complete, noise-free point clouds\nand 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to\nefficiently produce 7-Dof grasp candidates. Candidates are assessed with an\nensemble Graph Neural Network model which includes points within the gripper\njaws (inside points) and surrounding contextual points (outside points). This\nimproved representation boosts grasp detection performance over previous\nmethods using the same generator. GtG 2.0 shows up to a 35% improvement in\nAverage Precision on the GraspNet-1Billion benchmark compared to\nhypothesis-and-test and Graph Neural Network-based methods, ranking it among\nthe top three frameworks. Experiments with a 3-Dof Delta Parallel robot and\nKinect-v1 camera show a success rate of 91% and a clutter completion rate of\n100%, demonstrating its flexibility and reliability.\n","authors":["Ali Rashidi Moghadam","Sayedmohammadreza Rastegari","Mehdi Tale Masouleh","Ahmad Kalhor"],"pdf_url":"https://arxiv.org/pdf/2505.02664v1.pdf","comment":"9 Pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.02654v1","updated":"2025-05-05T13:56:59Z","published":"2025-05-05T13:56:59Z","title":"Sim2Real in endoscopy segmentation with a novel structure aware image\n  translation","summary":"  Automatic segmentation of anatomical landmarks in endoscopic images can\nprovide assistance to doctors and surgeons for diagnosis, treatments or medical\ntraining. However, obtaining the annotations required to train commonly used\nsupervised learning methods is a tedious and difficult task, in particular for\nreal images. While ground truth annotations are easier to obtain for synthetic\ndata, models trained on such data often do not generalize well to real data.\nGenerative approaches can add realistic texture to it, but face difficulties to\nmaintain the structure of the original scene. The main contribution in this\nwork is a novel image translation model that adds realistic texture to\nsimulated endoscopic images while keeping the key scene layout information. Our\napproach produces realistic images in different endoscopy scenarios. We\ndemonstrate these images can effectively be used to successfully train a model\nfor a challenging end task without any real labeled data. In particular, we\ndemonstrate our approach for the task of fold segmentation in colonoscopy\nimages. Folds are key anatomical landmarks that can occlude parts of the colon\nmucosa and possible polyps. Our approach generates realistic images maintaining\nthe shape and location of the original folds, after the\nimage-style-translation, better than existing methods. We run experiments both\non a novel simulated dataset for fold segmentation, and real data from the\nEndoMapper (EM) dataset. All our new generated data and new EM metadata is\nbeing released to facilitate further research, as no public benchmark is\ncurrently available for the task of fold segmentation.\n","authors":["Clara Tomasini","Luis Riazuelo","Ana C. Murillo"],"pdf_url":"https://arxiv.org/pdf/2505.02654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17090v2","updated":"2025-05-05T13:51:42Z","published":"2024-08-30T08:22:30Z","title":"FissionVAE: Federated Non-IID Image Generation with Latent Space and\n  Decoder Decomposition","summary":"  Federated learning is a machine learning paradigm that enables decentralized\nclients to collaboratively learn a shared model while keeping all the training\ndata local. While considerable research has focused on federated image\ngeneration, particularly Generative Adversarial Networks, Variational\nAutoencoders have received less attention. In this paper, we address the\nchallenges of non-IID (independently and identically distributed) data\nenvironments featuring multiple groups of images of different types. Non-IID\ndata distributions can lead to difficulties in maintaining a consistent latent\nspace and can also result in local generators with disparate texture features\nbeing blended during aggregation. We thereby introduce FissionVAE that\ndecouples the latent space and constructs decoder branches tailored to\nindividual client groups. This method allows for customized learning that\naligns with the unique data distributions of each group. Additionally, we\nincorporate hierarchical VAEs and demonstrate the use of heterogeneous decoder\narchitectures within FissionVAE. We also explore strategies for setting the\nlatent prior distributions to enhance the decoupling process. To evaluate our\napproach, we assemble two composite datasets: the first combines MNIST and\nFashionMNIST; the second comprises RGB datasets of cartoon and human faces,\nwild animals, marine vessels, and remote sensing images. Our experiments\ndemonstrate that FissionVAE greatly improves generation quality on these\ndatasets compared to baseline federated VAE models.\n","authors":["Chen Hu","Hanchi Ren","Jingjing Deng","Xianghua Xie","Xiaoke Ma"],"pdf_url":"https://arxiv.org/pdf/2408.17090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02648v1","updated":"2025-05-05T13:50:03Z","published":"2025-05-05T13:50:03Z","title":"MCCD: Multi-Agent Collaboration-based Compositional Diffusion for\n  Complex Text-to-Image Generation","summary":"  Diffusion models have shown excellent performance in text-to-image\ngeneration. Nevertheless, existing methods often suffer from performance\nbottlenecks when handling complex prompts that involve multiple objects,\ncharacteristics, and relations. Therefore, we propose a Multi-agent\nCollaboration-based Compositional Diffusion (MCCD) for text-to-image generation\nfor complex scenes. Specifically, we design a multi-agent collaboration-based\nscene parsing module that generates an agent system comprising multiple agents\nwith distinct tasks, utilizing MLLMs to extract various scene elements\neffectively. In addition, Hierarchical Compositional diffusion utilizes a\nGaussian mask and filtering to refine bounding box regions and enhance objects\nthrough region enhancement, resulting in the accurate and high-fidelity\ngeneration of complex scenes. Comprehensive experiments demonstrate that our\nMCCD significantly improves the performance of the baseline models in a\ntraining-free manner, providing a substantial advantage in complex scene\ngeneration.\n","authors":["Mingcheng Li","Xiaolu Hou","Ziyang Liu","Dingkang Yang","Ziyun Qian","Jiawei Chen","Jinjie Wei","Yue Jiang","Qingyao Xu","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.02648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02628v1","updated":"2025-05-05T13:14:49Z","published":"2025-05-05T13:14:49Z","title":"DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction","summary":"  Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in\nthe medical field, while the high radiation exposure required for high-quality\nimaging raises significant concerns, particularly for vulnerable populations.\nSparse-view reconstruction reduces radiation by using fewer X-ray projections\nwhile maintaining image quality, yet existing methods face challenges such as\nhigh computational demands and poor generalizability to different datasets. To\novercome these limitations, we propose DeepSparse, the first foundation model\nfor sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional\nCross-Scale Embedding), a novel network that integrates multi-view 2D features\nand multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View\nSampling Pretraining) framework, which pretrains the model on large datasets\nwith both sparse-view and dense-view projections, and a two-step finetuning\nstrategy to adapt and refine the model for new datasets. Extensive experiments\nand ablation studies demonstrate that our proposed DeepSparse achieves superior\nreconstruction quality compared to state-of-the-art methods, paving the way for\nsafer and more efficient CBCT imaging.\n","authors":["Yiqun Lin","Hualiang Wang","Jixiang Chen","Jiewen Yang","Jiarong Guo","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.02628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02626v1","updated":"2025-05-05T13:08:25Z","published":"2025-05-05T13:08:25Z","title":"Detect, Classify, Act: Categorizing Industrial Anomalies with\n  Multi-Modal Large Language Models","summary":"  Recent advances in visual industrial anomaly detection have demonstrated\nexceptional performance in identifying and segmenting anomalous regions while\nmaintaining fast inference speeds. However, anomaly\nclassification-distinguishing different types of anomalies-remains largely\nunexplored despite its critical importance in real-world inspection tasks. To\naddress this gap, we propose VELM, a novel LLM-based pipeline for anomaly\nclassification. Given the critical importance of inference speed, we first\napply an unsupervised anomaly detection method as a vision expert to assess the\nnormality of an observation. If an anomaly is detected, the LLM then classifies\nits type. A key challenge in developing and evaluating anomaly classification\nmodels is the lack of precise annotations of anomaly classes in existing\ndatasets. To address this limitation, we introduce MVTec-AC and VisA-AC,\nrefined versions of the widely used MVTec-AD and VisA datasets, which include\naccurate anomaly class labels for rigorous evaluation. Our approach achieves a\nstate-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD,\nexceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the\neffectiveness of VELM in understanding and categorizing anomalies. We hope our\nmethodology and benchmark inspire further research in anomaly classification,\nhelping bridge the gap between detection and comprehensive anomaly\ncharacterization.\n","authors":["Sassan Mokhtar","Arian Mousakhan","Silvio Galesso","Jawad Tayyub","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2505.02626v1.pdf","comment":"Accepted as a spotlight presentation paper at the VAND Workshop, CVPR\n  2025. 10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.15251v3","updated":"2025-05-05T12:16:10Z","published":"2025-02-21T07:02:05Z","title":"SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training","summary":"  We present a framework for pre-training of 3D hand pose estimation from\nin-the-wild hand images sharing with similar hand characteristics, dubbed\nSimHand. Pre-training with large-scale images achieves promising results in\nvarious tasks, but prior methods for 3D hand pose pre-training have not fully\nutilized the potential of diverse hand images accessible from in-the-wild\nvideos. To facilitate scalable pre-training, we first prepare an extensive pool\nof hand images from in-the-wild videos and design our pre-training method with\ncontrastive learning. Specifically, we collect over 2.0M hand images from\nrecent human-centric videos, such as 100DOH and Ego4D. To extract\ndiscriminative information from these images, we focus on the similarity of\nhands: pairs of non-identical samples with similar hand poses. We then propose\na novel contrastive learning method that embeds similar hand pairs closer in\nthe feature space. Our method not only learns from similar samples but also\nadaptively weights the contrastive learning loss based on inter-sample\ndistance, leading to additional performance gains. Our experiments demonstrate\nthat our method outperforms conventional contrastive learning approaches that\nproduce positive pairs sorely from a single image with data augmentation. We\nachieve significant improvements over the state-of-the-art method (PeCLR) in\nvarious datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on\nAssemblyHands.\n  Our code is available at https://github.com/ut-vision/SiMHand.\n","authors":["Nie Lin","Takehiko Ohkawa","Yifei Huang","Mingfang Zhang","Minjie Cai","Ming Li","Ryosuke Furuta","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2502.15251v3.pdf","comment":"ICLR 2025. arXiv admin note: text overlap with arXiv:2409.09714"},{"id":"http://arxiv.org/abs/2505.02593v1","updated":"2025-05-05T11:59:53Z","published":"2025-05-05T11:59:53Z","title":"DELTA: Dense Depth from Events and LiDAR using Transformer's Attention","summary":"  Event cameras and LiDARs provide complementary yet distinct data:\nrespectively, asynchronous detections of changes in lighting versus sparse but\naccurate depth information at a fixed rate. To this day, few works have\nexplored the combination of these two modalities. In this article, we propose a\nnovel neural-network-based method for fusing event and LiDAR data in order to\nestimate dense depth maps. Our architecture, DELTA, exploits the concepts of\nself- and cross-attention to model the spatial and temporal relations within\nand between the event and LiDAR data. Following a thorough evaluation, we\ndemonstrate that DELTA sets a new state of the art in the event-based depth\nestimation problem, and that it is able to reduce the errors up to four times\nfor close ranges compared to the previous SOTA.\n","authors":["Vincent Brebion","Julien Moreau","Franck Davoine"],"pdf_url":"https://arxiv.org/pdf/2505.02593v1.pdf","comment":"Accepted for the CVPR 2025 Workshop on Event-based Vision. For the\n  project page, see https://vbrebion.github.io/DELTA/"},{"id":"http://arxiv.org/abs/2501.04206v2","updated":"2025-05-05T11:44:07Z","published":"2025-01-08T00:54:43Z","title":"GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced\n  Explainability in Breast Cancer Histopathology","summary":"  Explainable AI (XAI) in medical histopathology is essential for enhancing the\ninterpretability and clinical trustworthiness of deep learning models in cancer\ndiagnosis. However, the black-box nature of these models often limits their\nclinical adoption. We introduce GRAPHITE (Graph-based Interpretable Tissue\nExamination), a post-hoc explainable framework designed for breast cancer\ntissue microarray (TMA) analysis. GRAPHITE employs a multiscale approach,\nextracting patches at various magnification levels, constructing an\nhierarchical graph, and utilising graph attention networks (GAT) with scalewise\nattention (SAN) to capture scale-dependent features. We trained the model on\n140 tumour TMA cores and four benign whole slide images from which 140 benign\nsamples were created, and tested it on 53 pathologist-annotated TMA samples.\nGRAPHITE outperformed traditional XAI methods, achieving a mean average\nprecision (mAP) of 0.56, an area under the receiver operating characteristic\ncurve (AUROC) of 0.94, and a threshold robustness (ThR) of 0.70, indicating\nthat the model maintains high performance across a wide range of thresholds. In\nclinical utility, GRAPHITE achieved the highest area under the decision curve\n(AUDC) of 4.17e+5, indicating reliable decision support across thresholds.\nThese results highlight GRAPHITE's potential as a clinically valuable tool in\ncomputational pathology, providing interpretable visualisations that align with\nthe pathologists' diagnostic reasoning and support precision medicine.\n","authors":["Raktim Kumar Mondol","Ewan K. A. Millar","Peter H. Graham","Lois Browne","Arcot Sowmya","Erik Meijering"],"pdf_url":"https://arxiv.org/pdf/2501.04206v2.pdf","comment":"25 Pages, 10 Figures, 1 Tables"},{"id":"http://arxiv.org/abs/2505.02586v1","updated":"2025-05-05T11:39:51Z","published":"2025-05-05T11:39:51Z","title":"RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection\n  Using DiffusionDet","summary":"  This work introduces RGBX-DiffusionDet, an object detection framework\nextending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB\nimagery via an adaptive multimodal encoder. To enable cross-modal interaction,\nwe design the dynamic channel reduction within a convolutional block attention\nmodule (DCR-CBAM), which facilitates cross-talk between subnetworks by\ndynamically highlighting salient channel features. Furthermore, the dynamic\nmulti-level aggregation block (DMLAB) is proposed to refine spatial feature\nrepresentations through adaptive multiscale fusion. Finally, novel\nregularization losses that enforce channel saliency and spatial selectivity are\nintroduced, leading to compact and discriminative feature embeddings. Extensive\nexperiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric\ndataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We\ndemonstrate consistent superiority of the proposed approach over the baseline\nRGB-only DiffusionDet. The modular architecture maintains the original decoding\ncomplexity, ensuring efficiency. These results establish the proposed\nRGBX-DiffusionDet as a flexible multimodal object detection approach, providing\nnew insights into integrating diverse 2D sensing modalities into\ndiffusion-based detection pipelines.\n","authors":["Eliraz Orfaig","Inna Stainvas","Igal Bilik"],"pdf_url":"https://arxiv.org/pdf/2505.02586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06457v2","updated":"2025-05-05T11:38:40Z","published":"2025-03-09T05:30:28Z","title":"Geometric Knowledge-Guided Localized Global Distribution Alignment for\n  Federated Learning","summary":"  Data heterogeneity in federated learning, characterized by a significant\nmisalignment between local and global distributions, leads to divergent local\noptimization directions and hinders global model training. Existing studies\nmainly focus on optimizing local updates or global aggregation, but these\nindirect approaches demonstrate instability when handling highly heterogeneous\ndata distributions, especially in scenarios where label skew and domain skew\ncoexist. To address this, we propose a geometry-guided data generation method\nthat centers on simulating the global embedding distribution locally. We first\nintroduce the concept of the geometric shape of an embedding distribution and\nthen address the challenge of obtaining global geometric shapes under privacy\nconstraints. Subsequently, we propose GGEUR, which leverages global geometric\nshapes to guide the generation of new samples, enabling a closer approximation\nto the ideal global distribution. In single-domain scenarios, we augment\nsamples based on global geometric shapes to enhance model generalization; in\nmulti-domain scenarios, we further employ class prototypes to simulate the\nglobal distribution across domains. Extensive experimental results demonstrate\nthat our method significantly enhances the performance of existing approaches\nin handling highly heterogeneous data, including scenarios with label skew,\ndomain skew, and their coexistence. Code published at:\nhttps://github.com/WeiDai-David/2025CVPR_GGEUR\n","authors":["Yanbiao Ma","Wei Dai","Wenke Huang","Jiayi Chen"],"pdf_url":"https://arxiv.org/pdf/2503.06457v2.pdf","comment":"Accepted by CVPR Oral 2025"},{"id":"http://arxiv.org/abs/2505.02567v1","updated":"2025-05-05T11:18:03Z","published":"2025-05-05T11:18:03Z","title":"Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities","summary":"  Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey will be available on GitHub soon.\n","authors":["Xinjie Zhang","Jintao Guo","Shanshan Zhao","Minghao Fu","Lunhao Duan","Guo-Hua Wang","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.02567v1.pdf","comment":"This work is still in progress"},{"id":"http://arxiv.org/abs/2407.05576v3","updated":"2025-05-05T11:15:18Z","published":"2024-07-08T03:17:10Z","title":"CaRe-Ego: Contact-aware Relationship Modeling for Egocentric Interactive\n  Hand-object Segmentation","summary":"  Egocentric Interactive hand-object segmentation (EgoIHOS) requires the\nsegmentation of hands and interacting objects in egocentric images, which is\ncrucial for understanding human behavior in assistive systems. Previous methods\ntypically recognize hands and interacting objects as distinct semantic\ncategories based solely on visual features, or simply use hand predictions as\nauxiliary cues for object segmentation. Despite the promising progress achieved\nby these methods, they fail to adequately model the interactive relationships\nbetween hands and objects while ignoring the coupled physical relationships\namong object categories, ultimately constraining their segmentation\nperformance. To make up for the shortcomings of existing methods, we propose a\nnovel method called CaRe-Ego that achieves state-of-the-art performance by\nemphasizing the contact between hands and objects from two aspects. First, we\nintroduce a Hand-guided Object Feature Enhancer (HOFE) to establish the\nhand-object interactive relationships to extract more contact-relevant and\ndiscriminative object features. Second, we design the Contact-centric Object\nDecoupling Strategy (CODS) to explicitly model and disentangle coupling\nrelationships among object categories, thereby emphasizing contact-aware\nfeature learning. Experiments on various in-domain and out-of-domain test sets\nshow that Care-Ego significantly outperforms existing methods with robust\ngeneralization capability. Codes are publicly available at\nhttps://github.com/yuggiehk/CaRe-Ego/.\n","authors":["Yuejiao Su","Yi Wang","Lap-Pui Chau"],"pdf_url":"https://arxiv.org/pdf/2407.05576v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02549v1","updated":"2025-05-05T10:36:52Z","published":"2025-05-05T10:36:52Z","title":"Robust Duality Learning for Unsupervised Visible-Infrared Person\n  Re-Identfication","summary":"  Unsupervised visible-infrared person re-identification (UVI-ReID) aims to\nretrieve pedestrian images across different modalities without costly\nannotations, but faces challenges due to the modality gap and lack of\nsupervision. Existing methods often adopt self-training with\nclustering-generated pseudo-labels but implicitly assume these labels are\nalways correct. In practice, however, this assumption fails due to inevitable\npseudo-label noise, which hinders model learning. To address this, we introduce\na new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),\ncharacterized by three key challenges: noise overfitting, error accumulation,\nand noisy cluster correspondence. To this end, we propose a novel Robust\nDuality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy\npseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning\nmechanism (RAL) is proposed to dynamically emphasize clean samples while\ndown-weighting noisy ones. Second, to alleviate error accumulation-where the\nmodel reinforces its own mistakes-RoDE employs dual distinct models that are\nalternately trained using pseudo-labels from each other, encouraging diversity\nand preventing collapse. However, this dual-model strategy introduces\nmisalignment between clusters across models and modalities, creating noisy\ncluster correspondence. To resolve this, we introduce Cluster Consistency\nMatching (CCM), which aligns clusters across models and modalities by measuring\ncross-cluster similarity. Extensive experiments on three benchmarks demonstrate\nthe effectiveness of RoDE.\n","authors":["Yongxiang Li","Yuan Sun","Yang Qin","Dezhong Peng","Xi Peng","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2505.02549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02539v1","updated":"2025-05-05T10:21:41Z","published":"2025-05-05T10:21:41Z","title":"Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D\n  Reconstruction","summary":"  Accurate 3D reconstruction using multi-camera RGB-D systems critically\ndepends on precise extrinsic calibration to achieve proper alignment between\ncaptured views. In this paper, we introduce an iterative extrinsic calibration\nmethod that leverages the geometric constraints provided by a three-dimensional\nmarker to significantly improve calibration accuracy. Our proposed approach\nsystematically segments and refines marker planes through clustering,\nregression analysis, and iterative reassignment techniques, ensuring robust\ngeometric correspondence across camera views. We validate our method\ncomprehensively in both controlled environments and practical real-world\nsettings within the Tech4Diet project, aimed at modeling the physical\nprogression of patients undergoing nutritional treatments. Experimental results\ndemonstrate substantial reductions in alignment errors, facilitating accurate\nand reliable 3D reconstructions.\n","authors":["Nahuel Garcia-D'Urso","Bernabe Sanchez-Sos","Jorge Azorin-Lopez","Andres Fuster-Guillo","Antonio Macia-Lillo","Higinio Mora-Mora"],"pdf_url":"https://arxiv.org/pdf/2505.02539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00334v2","updated":"2025-05-05T10:20:23Z","published":"2025-05-01T06:17:33Z","title":"Quaternion Wavelet-Conditioned Diffusion Models for Image\n  Super-Resolution","summary":"  Image Super-Resolution is a fundamental problem in computer vision with broad\napplications spacing from medical imaging to satellite analysis. The ability to\nreconstruct high-resolution images from low-resolution inputs is crucial for\nenhancing downstream tasks such as object detection and segmentation. While\ndeep learning has significantly advanced SR, achieving high-quality\nreconstructions with fine-grained details and realistic textures remains\nchallenging, particularly at high upscaling factors. Recent approaches\nleveraging diffusion models have demonstrated promising results, yet they often\nstruggle to balance perceptual quality with structural fidelity. In this work,\nwe introduce ResQu a novel SR framework that integrates a quaternion wavelet\npreprocessing framework with latent diffusion models, incorporating a new\nquaternion wavelet- and time-aware encoder. Unlike prior methods that simply\napply wavelet transforms within diffusion models, our approach enhances the\nconditioning process by exploiting quaternion wavelet embeddings, which are\ndynamically integrated at different stages of denoising. Furthermore, we also\nleverage the generative priors of foundation models such as Stable Diffusion.\nExtensive experiments on domain-specific datasets demonstrate that our method\nachieves outstanding SR results, outperforming in many cases existing\napproaches in perceptual quality and standard evaluation metrics. The code will\nbe available after the revision process.\n","authors":["Luigi Sigillo","Christian Bianchi","Aurelio Uncini","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2505.00334v2.pdf","comment":"Accepted for presentation at IJCNN 2025"},{"id":"http://arxiv.org/abs/2505.02529v1","updated":"2025-05-05T10:10:03Z","published":"2025-05-05T10:10:03Z","title":"RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust\n  Cancer Survival Prediction","summary":"  Cancer survival prediction using multi-modal medical imaging presents a\ncritical challenge in oncology, mainly due to the vulnerability of deep\nlearning models to noise and protocol variations across imaging centers.\nCurrent approaches struggle to extract consistent features from heterogeneous\nCT and PET images, limiting their clinical applicability. We address these\nchallenges by introducing RobSurv, a robust deep-learning framework that\nleverages vector quantization for resilient multi-modal feature learning. The\nkey innovation of our approach lies in its dual-path architecture: one path\nmaps continuous imaging features to learned discrete codebooks for\nnoise-resistant representation, while the parallel path preserves fine-grained\ndetails through continuous feature processing. This dual representation is\nintegrated through a novel patch-wise fusion mechanism that maintains local\nspatial relationships while capturing global context via Transformer-based\nprocessing. In extensive evaluations across three diverse datasets (HECKTOR,\nH\\&N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance,\nachieving concordance index of 0.771, 0.742, and 0.734 respectively -\nsignificantly outperforming existing methods. Most notably, our model maintains\nrobust performance even under severe noise conditions, with performance\ndegradation of only 3.8-4.5\\% compared to 8-12\\% in baseline methods. These\nresults, combined with strong generalization across different cancer types and\nimaging protocols, establish RobSurv as a promising solution for reliable\nclinical prognosis that can enhance treatment planning and patient care.\n","authors":["Aiman Farooq","Azad Singh","Deepak Mishra","Santanu Chaudhury"],"pdf_url":"https://arxiv.org/pdf/2505.02529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02527v1","updated":"2025-05-05T10:08:31Z","published":"2025-05-05T10:08:31Z","title":"Text to Image Generation and Editing: A Survey","summary":"  Text-to-image generation (T2I) refers to the text-guided generation of\nhigh-quality images. In the past few years, T2I has attracted widespread\nattention and numerous works have emerged. In this survey, we comprehensively\nreview 141 works conducted from 2021 to 2024. First, we introduce four\nfoundation model architectures of T2I (autoregression, non-autoregression, GAN\nand diffusion) and the commonly used key technologies (autoencoder, attention\nand classifier-free guidance). Secondly, we systematically compare the methods\nof these studies in two directions, T2I generation and T2I editing, including\nthe encoders and the key technologies they use. In addition, we also compare\nthe performance of these researches side by side in terms of datasets,\nevaluation metrics, training resources, and inference speed. In addition to the\nfour foundation models, we survey other works on T2I, such as energy-based\nmodels and recent Mamba and multimodality. We also investigate the potential\nsocial impact of T2I and provide some solutions. Finally, we propose unique\ninsights of improving the performance of T2I models and possible future\ndevelopment directions. In summary, this survey is the first systematic and\ncomprehensive overview of T2I, aiming to provide a valuable guide for future\nresearchers and stimulate continued progress in this field.\n","authors":["Pengfei Yang","Ngai-Man Cheung","Xinda Ma"],"pdf_url":"https://arxiv.org/pdf/2505.02527v1.pdf","comment":"49 pages,3 figures,3 tables"},{"id":"http://arxiv.org/abs/2502.03270v2","updated":"2025-05-05T09:42:27Z","published":"2025-02-05T15:25:46Z","title":"When Pre-trained Visual Representations Fall Short: Limitations in\n  Visuo-Motor Robot Learning","summary":"  The integration of pre-trained visual representations (PVRs) into visuo-motor\nrobot learning has emerged as a promising alternative to training visual\nencoders from scratch. However, PVRs face critical challenges in the context of\npolicy learning, including temporal entanglement and an inability to generalise\neven in the presence of minor scene perturbations. These limitations hinder\nperformance in tasks requiring temporal awareness and robustness to scene\nchanges. This work identifies these shortcomings and proposes solutions to\naddress them. First, we augment PVR features with temporal perception and a\nsense of task completion, effectively disentangling them in time. Second, we\nintroduce a module that learns to selectively attend to task-relevant local\nfeatures, enhancing robustness when evaluated on out-of-distribution scenes.\nOur experiments demonstrate significant performance improvements, particularly\nin PVRs trained with masking objectives, and validate the effectiveness of our\nenhancements in addressing PVR-specific limitations.\n","authors":["Nikolaos Tsagkas","Andreas Sochopoulos","Duolikun Danier","Sethu Vijayakumar","Chris Xiaoxuan Lu","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2502.03270v2.pdf","comment":"Project Page: https://tsagkas.github.io/pvrobo/"},{"id":"http://arxiv.org/abs/2505.02501v1","updated":"2025-05-05T09:29:32Z","published":"2025-05-05T09:29:32Z","title":"Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict\n  Reliable 6D Pose Distributions","summary":"  We introduce Corr2Distrib, the first correspondence-based method which\nestimates a 6D camera pose distribution from an RGB image, explaining the\nobservations. Indeed, symmetries and occlusions introduce visual ambiguities,\nleading to multiple valid poses. While a few recent methods tackle this\nproblem, they do not rely on local correspondences which, according to the BOP\nChallenge, are currently the most effective way to estimate a single 6DoF pose\nsolution. Using correspondences to estimate a pose distribution is not\nstraightforward, since ambiguous correspondences induced by visual ambiguities\ndrastically decrease the performance of PnP. With Corr2Distrib, we turn these\nambiguities into an advantage to recover all valid poses. Corr2Distrib first\nlearns a symmetry-aware representation for each 3D point on the object's\nsurface, characterized by a descriptor and a local frame. This representation\nenables the generation of 3DoF rotation hypotheses from single 2D-3D\ncorrespondences. Next, we refine these hypotheses into a 6DoF pose distribution\nusing PnP and pose scoring. Our experimental evaluations on complex\nnon-synthetic scenes show that Corr2Distrib outperforms state-of-the-art\nsolutions for both pose distribution estimation and single pose estimation from\nan RGB image, demonstrating the potential of correspondences-based approaches.\n","authors":["Asma Brazi","Boris Meden","Fabrice Mayran de Chamisso","Steve Bourgeois","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2505.02501v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.18468v3","updated":"2025-05-05T09:14:51Z","published":"2025-04-25T16:23:50Z","title":"RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny\n  Objects","summary":"  We introduce RGS-DR, a novel inverse rendering method for reconstructing and\nrendering glossy and reflective objects with support for flexible relighting\nand scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian\nSplatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D\nGaussian surfel representation to accurately estimate geometry and surface\nnormals, an essential property for high-quality inverse rendering. Our approach\nexplicitly models geometric and material properties through learnable\nprimitives rasterized into a deferred shading pipeline, effectively reducing\nrendering artifacts and preserving sharp reflections. By employing a\nmulti-level cube mipmap, RGS-DR accurately approximates environment lighting\nintegrals, facilitating high-quality reconstruction and relighting. A residual\npass with spherical-mipmap-based directional encoding further refines the\nappearance modeling. Experiments demonstrate that RGS-DR achieves high-quality\nreconstruction and rendering quality for shiny objects, often outperforming\nreconstruction-exclusive state-of-the-art methods incapable of relighting.\n","authors":["Georgios Kouros","Minye Wu","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2504.18468v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02481v1","updated":"2025-05-05T09:05:47Z","published":"2025-05-05T09:05:47Z","title":"Finger Pose Estimation for Under-screen Fingerprint Sensor","summary":"  Two-dimensional pose estimation plays a crucial role in fingerprint\nrecognition by facilitating global alignment and reduce pose-induced\nvariations. However, existing methods are still unsatisfactory when handling\nwith large angle or small area inputs. These limitations are particularly\npronounced on fingerprints captured by under-screen fingerprint sensors in\nsmartphones. In this paper, we present a novel dual-modal input based network\nfor under-screen fingerprint pose estimation. Our approach effectively\nintegrates two distinct yet complementary modalities: texture details extracted\nfrom ridge patches through the under-screen fingerprint sensor, and rough\ncontours derived from capacitive images obtained via the touch screen. This\ncollaborative integration endows our network with more comprehensive and\ndiscriminative information, substantially improving the accuracy and stability\nof pose estimation. A decoupled probability distribution prediction task is\ndesigned, instead of the traditional supervised forms of numerical regression\nor heatmap voting, to facilitate the training process. Additionally, we\nincorporate a Mixture of Experts (MoE) based feature fusion mechanism and a\nrelationship driven cross-domain knowledge transfer strategy to further\nstrengthen feature extraction and fusion capabilities. Extensive experiments\nare conducted on several public datasets and two private datasets. The results\nindicate that our method is significantly superior to previous state-of-the-art\n(SOTA) methods and remarkably boosts the recognition ability of fingerprint\nrecognition algorithms. Our code is available at\nhttps://github.com/XiongjunGuan/DRACO.\n","authors":["Xiongjun Guan","Zhiyu Pan","Jianjiang Feng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.02481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02476v1","updated":"2025-05-05T09:00:16Z","published":"2025-05-05T09:00:16Z","title":"Point Cloud Recombination: Systematic Real Data Augmentation Using\n  Robotic Targets for LiDAR Perception Validation","summary":"  The validation of LiDAR-based perception of intelligent mobile systems\noperating in open-world applications remains a challenge due to the variability\nof real environmental conditions. Virtual simulations allow the generation of\narbitrary scenes under controlled conditions but lack physical sensor\ncharacteristics, such as intensity responses or material-dependent effects. In\ncontrast, real-world data offers true sensor realism but provides less control\nover influencing factors, hindering sufficient validation. Existing approaches\naddress this problem with augmentation of real-world point cloud data by\ntransferring objects between scenes. However, these methods do not consider\nvalidation and remain limited in controllability because they rely on empirical\ndata. We solve these limitations by proposing Point Cloud Recombination, which\nsystematically augments captured point cloud scenes by integrating point clouds\nacquired from physical target objects measured in controlled laboratory\nenvironments. Thus enabling the creation of vast amounts and varieties of\nrepeatable, physically accurate test scenes with respect to phenomena-aware\nocclusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we\ndemonstrate the augmentation of real-world urban and rural scenes with humanoid\ntargets featuring varied clothing and poses, for repeatable positioning. We\nshow that the recombined scenes closely match real sensor outputs, enabling\ntargeted testing, scalable failure analysis, and improved system safety. By\nproviding controlled yet sensor-realistic data, our method enables trustworthy\nconclusions about the limitations of specific sensors in compound with their\nalgorithms, e.g., object detection.\n","authors":["Hubert Padusinski","Christian Steinhauser","Christian Scherl","Julian Gaal","Jacob Langner"],"pdf_url":"https://arxiv.org/pdf/2505.02476v1.pdf","comment":"Pre-print for IEEE IAVVC 2025"},{"id":"http://arxiv.org/abs/2505.02471v1","updated":"2025-05-05T08:56:12Z","published":"2025-05-05T08:56:12Z","title":"Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction","summary":"  We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.\n","authors":["Biao Gong","Cheng Zou","Dandan Zheng","Hu Yu","Jingdong Chen","Jianxin Sun","Junbo Zhao","Jun Zhou","Kaixiang Ji","Lixiang Ru","Libin Wang","Qingpei Guo","Rui Liu","Weilong Chai","Xinyu Xiao","Ziyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.02471v1.pdf","comment":"https://github.com/inclusionAI/Ming/tree/main/Ming-unify"},{"id":"http://arxiv.org/abs/2505.02467v1","updated":"2025-05-05T08:53:21Z","published":"2025-05-05T08:53:21Z","title":"Timing Is Everything: Finding the Optimal Fusion Points in Multimodal\n  Medical Imaging","summary":"  Multimodal deep learning harnesses diverse imaging modalities, such as MRI\nsequences, to enhance diagnostic accuracy in medical imaging. A key challenge\nis determining the optimal timing for integrating these\nmodalities-specifically, identifying the network layers where fusion modules\nshould be inserted. Current approaches often rely on manual tuning or\nexhaustive search, which are computationally expensive without any guarantee of\nconverging to optimal results. We propose a sequential forward search algorithm\nthat incrementally activates and evaluates candidate fusion modules at\ndifferent layers of a multimodal network. At each step, the algorithm retrains\nfrom previously learned weights and compares validation loss to identify the\nbest-performing configuration. This process systematically reduces the search\nspace, enabling efficient identification of the optimal fusion timing without\nexhaustively testing all possible module placements. The approach is validated\non two multimodal MRI datasets, each addressing different classification tasks.\nOur algorithm consistently identified configurations that outperformed unimodal\nbaselines, late fusion, and a brute-force ensemble of all potential fusion\nplacements. These architectures demonstrated superior accuracy, F-score, and\nspecificity while maintaining competitive or improved AUC values. Furthermore,\nthe sequential nature of the search significantly reduced computational\noverhead, making the optimization process more practical. By systematically\ndetermining the optimal timing to fuse imaging modalities, our method advances\nmultimodal deep learning for medical imaging. It provides an efficient and\nrobust framework for fusion optimization, paving the way for improved clinical\ndecision-making and more adaptable, scalable architectures in medical AI\napplications.\n","authors":["Valerio Guarrasi","Klara Mogensen","Sara Tassinari","Sara Qvarlander","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2505.02467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01710v4","updated":"2025-05-05T08:31:11Z","published":"2025-02-03T15:18:54Z","title":"DAGNet: A Dual-View Attention-Guided Network for Efficient X-ray\n  Security Inspection","summary":"  With the rapid development of modern transportation systems and the\nexponential growth of logistics volumes, intelligent X-ray-based security\ninspection systems play a crucial role in public safety. Although single-view\nX-ray baggage scanner is widely deployed, they struggles to accurately identify\ncontraband in complex stacking scenarios due to strong viewpoint dependency and\ninadequate feature representation. To address this, we propose a Dual-View\nAttention-Guided Network for Efficient X-ray Security Inspection (DAGNet). This\nstudy builds on a shared-weight backbone network as the foundation and\nconstructs three key modules that work together: (1) Frequency Domain\nInteraction Module (FDIM) dynamically enhances features by adjusting frequency\ncomponents based on inter-view relationships; (2) Dual-View Hierarchical\nEnhancement Module (DVHEM) employs cross-attention to align features between\nviews and capture hierarchical associations; (3) Convolutional Guided Fusion\nModule (CGFM) fuses features to suppress redundancy while retaining critical\ndiscriminative information. Collectively, these modules substantially improve\nthe performance of dual-view X-ray security inspection. Experimental results\ndemonstrate that DAGNet outperforms existing state-of-the-art approaches across\nmultiple backbone architectures. The code is available\nat:https://github.com/ShilongHong/DAGNet.\n","authors":["Shilong Hong","Yanzhou Zhou","Weichao Xu"],"pdf_url":"https://arxiv.org/pdf/2502.01710v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02448v1","updated":"2025-05-05T08:22:38Z","published":"2025-05-05T08:22:38Z","title":"Recent Advances in Out-of-Distribution Detection with CLIP-Like Models:\n  A Survey","summary":"  Out-of-distribution detection (OOD) is a pivotal task for real-world\napplications that trains models to identify samples that are distributionally\ndifferent from the in-distribution (ID) data during testing. Recent advances in\nAI, particularly Vision-Language Models (VLMs) like CLIP, have revolutionized\nOOD detection by shifting from traditional unimodal image detectors to\nmultimodal image-text detectors. This shift has inspired extensive research;\nhowever, existing categorization schemes (e.g., few- or zero-shot types) still\nrely solely on the availability of ID images, adhering to a unimodal paradigm.\nTo better align with CLIP's cross-modal nature, we propose a new categorization\nframework rooted in both image and text modalities. Specifically, we categorize\nexisting methods based on how visual and textual information of OOD data is\nutilized within image + text modalities, and further divide them into four\ngroups: OOD Images (i.e., outliers) Seen or Unseen, and OOD Texts (i.e.,\nlearnable vectors or class names) Known or Unknown, across two training\nstrategies (i.e., train-free or training-required). More importantly, we\ndiscuss open problems in CLIP-like OOD detection and highlight promising\ndirections for future research, including cross-domain integration, practical\napplications, and theoretical understanding.\n","authors":["Chaohua Li","Enhao Zhang","Chuanxing Geng","Songcan Chen"],"pdf_url":"https://arxiv.org/pdf/2505.02448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13620v4","updated":"2025-05-05T07:58:38Z","published":"2025-01-23T12:42:42Z","title":"A Cognitive Paradigm Approach to Probe the Perception-Reasoning\n  Interface in VLMs","summary":"  A fundamental challenge in artificial intelligence involves understanding the\ncognitive mechanisms underlying visual reasoning in sophisticated models like\nVision-Language Models (VLMs). How do these models integrate visual perception\nwith abstract thought, especially when reasoning across multiple images or\nrequiring fine-grained compositional understanding? Drawing inspiration from\ncognitive science, this paper introduces a structured evaluation framework\nusing diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to\ndissect the perception-reasoning interface in VLMs. We propose three distinct\nevaluation paradigms, mirroring human problem-solving strategies: Direct Visual\nRule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule\nextraction and application), and Componential Analysis (CA; analytical\ndecomposition via task-agnostic textual descriptions). These paradigms\nsystematically vary cognitive load and probe processing stages. Notably, CA\nenables multi-image reasoning evaluation even for single-image architectures\nand isolates reasoning from perception by operating on textual descriptions.\nApplying this framework, we demonstrate that CA, leveraging powerful language\nmodels for reasoning over rich, independently generated descriptions, achieves\nnew state-of-the-art (SOTA) performance on challenging benchmarks including\nBongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm\nreasoning improves significantly when perceptual challenges are mitigated,\nrevealing a critical perception bottleneck. Our framework provides a valuable\ndiagnostic tool and suggests that decoupling perception (via rich,\ntask-agnostic description) from reasoning is a promising direction for robust\nand general visual intelligence.\n","authors":["Mohit Vaishnav","Tanel Tammet"],"pdf_url":"https://arxiv.org/pdf/2501.13620v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10453v2","updated":"2025-05-05T07:38:45Z","published":"2024-08-19T23:31:02Z","title":"Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation","summary":"  Text-to-video generation has been dominated by diffusion-based or\nautoregressive models. These novel models provide plausible versatility, but\nare criticized for improper physical motion, shading and illumination, camera\nmotion, and temporal consistency. The film industry relies on manually-edited\nComputer-Generated Imagery (CGI) using 3D modeling software. Human-directed 3D\nsynthetic videos address these shortcomings, but require tight collaboration\nbetween movie makers and 3D rendering experts. We introduce an automatic\nsynthetic video generation pipeline based on Vision Large Language Model (VLM)\nagent collaborations. Given a language description of a video, multiple VLM\nagents direct various processes of the generation pipeline. They cooperate to\ncreate Blender scripts which render a video following the given description.\nAugmented with Blender-based movie making knowledge, the Director agent\ndecomposes the text-based video description into sub-processes. For each\nsub-process, the Programmer agent produces Python-based Blender scripts based\non function composing and API calling. The Reviewer agent, with knowledge of\nvideo reviewing, character motion coordinates, and intermediate screenshots,\nprovides feedback to the Programmer agent. The Programmer agent iteratively\nimproves scripts to yield the best video outcome. Our generated videos show\nbetter quality than commercial video generation models in five metrics on video\nquality and instruction-following performance. Our framework outperforms other\napproaches in a user study on quality, consistency, and rationality.\n","authors":["Liu He","Yizhi Song","Hejun Huang","Pinxin Liu","Yunlong Tang","Daniel Aliaga","Xin Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.10453v2.pdf","comment":"Accepted by CVPR 2025 AI4CC Workshop"},{"id":"http://arxiv.org/abs/2505.02406v1","updated":"2025-05-05T06:59:26Z","published":"2025-05-05T06:59:26Z","title":"Token Coordinated Prompt Attention is Needed for Visual Prompting","summary":"  Visual prompting techniques are widely used to efficiently fine-tune\npretrained Vision Transformers (ViT) by learning a small set of shared prompts\nfor all tokens. However, existing methods overlook the unique roles of\ndifferent tokens in conveying discriminative information and interact with all\ntokens using the same prompts, thereby limiting the representational capacity\nof ViT. This often leads to indistinguishable and biased prompt-extracted\nfeatures, hindering performance. To address this issue, we propose a\nplug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns\nspecific coordinated prompts to different tokens for attention-based\ninteractions. Firstly, recognizing the distinct functions of CLS and image\ntokens-global information aggregation and local feature extraction, we\ndisentangle the prompts into CLS Prompts and Image Prompts, which interact\nexclusively with CLS tokens and image tokens through attention mechanisms. This\nenhances their respective discriminative abilities. Furthermore, as different\nimage tokens correspond to distinct image patches and contain diverse\ninformation, we employ a matching function to automatically assign coordinated\nprompts to individual tokens. This enables more precise attention interactions,\nimproving the diversity and representational capacity of the extracted\nfeatures. Extensive experiments across various benchmarks demonstrate that TCPA\nsignificantly enhances the diversity and discriminative power of the extracted\nfeatures. The code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-TCPA.\n","authors":["Zichen Liu","Xu Zou","Gang Hua","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.02406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02405v1","updated":"2025-05-05T06:55:59Z","published":"2025-05-05T06:55:59Z","title":"Estimating Commonsense Scene Composition on Belief Scene Graphs","summary":"  This work establishes the concept of commonsense scene composition, with a\nfocus on extending Belief Scene Graphs by estimating the spatial distribution\nof unseen objects. Specifically, the commonsense scene composition capability\nrefers to the understanding of the spatial relationships among related objects\nin the scene, which in this article is modeled as a joint probability\ndistribution for all possible locations of the semantic object class. The\nproposed framework includes two variants of a Correlation Information (CECI)\nmodel for learning probability distributions: (i) a baseline approach based on\na Graph Convolutional Network, and (ii) a neuro-symbolic extension that\nintegrates a spatial ontology based on Large Language Models (LLMs).\nFurthermore, this article provides a detailed description of the dataset\ngeneration process for such tasks. Finally, the framework has been validated\nthrough multiple runs on simulated data, as well as in a real-world indoor\nenvironment, demonstrating its ability to spatially interpret scenes across\ndifferent room types.\n","authors":["Mario A. V. Saucedo","Vignesh Kottayam Viswanathan","Christoforos Kanellakis","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2505.02405v1.pdf","comment":"Accepted at ICRA25"},{"id":"http://arxiv.org/abs/2504.16062v3","updated":"2025-05-05T06:53:10Z","published":"2025-04-22T17:38:38Z","title":"ForesightNav: Learning Scene Imagination for Efficient Exploration","summary":"  Understanding how humans leverage prior knowledge to navigate unseen\nenvironments while making exploratory decisions is essential for developing\nautonomous robots with similar abilities. In this work, we propose\nForesightNav, a novel exploration strategy inspired by human imagination and\nreasoning. Our approach equips robotic agents with the capability to predict\ncontextual information, such as occupancy and semantic details, for unexplored\nregions. These predictions enable the robot to efficiently select meaningful\nlong-term navigation goals, significantly enhancing exploration in unseen\nenvironments. We validate our imagination-based approach using the Structured3D\ndataset, demonstrating accurate occupancy prediction and superior performance\nin anticipating unseen scene geometry. Our experiments show that the\nimagination module improves exploration efficiency in unseen environments,\nachieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav\non the Structured3D Validation split. These contributions demonstrate the power\nof imagination-driven reasoning for autonomous systems to enhance generalizable\nand efficient exploration.\n","authors":["Hardik Shah","Jiaxu Xing","Nico Messikommer","Boyang Sun","Marc Pollefeys","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2504.16062v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02396v1","updated":"2025-05-05T06:40:08Z","published":"2025-05-05T06:40:08Z","title":"Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and\n  CNN from Scratch","summary":"  Pneumonia Diagnosis, though it is crucial for an effective treatment, it can\nbe hampered by uncertainty. This uncertainty starts to arise due to some\nfactors like atypical presentations, limitations of diagnostic tools such as\nchest X-rays, and the presence of co-existing respiratory conditions. This\nresearch proposes one of the supervised learning methods, CNN. Using\nMobileNetV2 as the pre-trained one with ResNet101V2 architecture and using\nKeras API as the built from scratch model, for identifying lung diseases\nespecially pneumonia. The datasets used in this research were obtained from the\nwebsite through Kaggle. The result shows that by implementing CNN MobileNetV2\nand CNN from scratch the result is promising. While validating data,\nMobileNetV2 performs with stability and minimal overfitting, while the training\naccuracy increased to 84.87% later it slightly decreased to 78.95%, with\nincreasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is\nmore stable. Although it takes more time to train each epoch. Meanwhile, after\nthe 10th epoch, the Scratch model displayed more instability and overfitting\ndespite having higher validation accuracy, training accuracy decreased\nsignificantly to 78.12% and the validation loss increased from 0.5698 to\n1.1809. With these results, ResNet101V2 offers stability, and the Scratch model\noffers high accuracy.\n","authors":["Kennard Norbert Sudiardjo","Islam Nur Alam","Wilson Wijaya","Lili Ayu Wulandhari"],"pdf_url":"https://arxiv.org/pdf/2505.02396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02393v1","updated":"2025-05-05T06:33:20Z","published":"2025-05-05T06:33:20Z","title":"Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection","summary":"  Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD.\n","authors":["Sungheon Jeong","Jihong Park","Mohsen Imani"],"pdf_url":"https://arxiv.org/pdf/2505.02393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03471v2","updated":"2025-05-05T06:32:15Z","published":"2025-04-04T14:23:30Z","title":"Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis","summary":"  Traditional diffusion models typically employ a U-Net architecture. Previous\nstudies have unveiled the roles of attention blocks in the U-Net. However, they\noverlook the dynamic evolution of their importance during the inference\nprocess, which hinders their further exploitation to improve image\napplications. In this study, we first theoretically proved that, re-weighting\nthe outputs of the Transformer blocks within the U-Net is a \"free lunch\" for\nimproving the signal-to-noise ratio during the sampling process. Next, we\nproposed Importance Probe to uncover and quantify the dynamic shifts in\nimportance of the Transformer blocks throughout the denoising process. Finally,\nwe design an adaptive importance-based re-weighting schedule tailored to\nspecific image generation and editing tasks. Experimental results demonstrate\nthat, our approach significantly improves the efficiency of the inference\nprocess, and enhances the aesthetic quality of the samples with identity\nconsistency. Our method can be seamlessly integrated into any U-Net-based\narchitecture. Code: https://github.com/Hytidel/UNetReweighting\n","authors":["Xi Wang","Ziqi He","Yang Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.03471v2.pdf","comment":"Accepted to ICME 2025. Appendix & Code:\n  https://github.com/Hytidel/UNetReweighting"},{"id":"http://arxiv.org/abs/2504.05184v2","updated":"2025-05-05T06:30:03Z","published":"2025-04-07T15:35:30Z","title":"MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised\n  Prototypical Contrastive Loss for Coronary DSA Image Segmentation","summary":"  The accurate segmentation of coronary Digital Subtraction Angiography (DSA)\nimages is essential for diagnosing and treating coronary artery diseases.\nDespite advances in deep learning-based segmentation, challenges such as low\ncontrast, noise, overlapping structures, high intra-class variance, and class\nimbalance limit precise vessel delineation. To overcome these limitations, we\npropose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture\nfor coronary DSA image segmentation. The framework combined Multi-Scale Dilated\nBottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM),\nwhich not only enhances multi-scale feature extraction but also preserve\nfine-grained details, and improve contextual understanding. Furthermore, we\npropose a new Supervised Prototypical Contrastive Loss (SPCL), which combines\nsupervised and prototypical contrastive learning to minimize class imbalance\nand high intra-class variance by focusing on hard-to-classified background\nsamples. Experiments carried out on a private coronary DSA dataset demonstrate\nthat MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice\ncoefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average\nSurface Distance (ASD) and Average Contour Distance (ACD). The developed\nframework provides clinicians with precise vessel segmentation, enabling\naccurate identification of coronary stenosis and supporting informed diagnostic\nand therapeutic decisions. The code will be released at the following GitHub\nprofile link https://github.com/rayanmerghani/MSA-UNet3plus.\n","authors":["Rayan Merghani Ahmed","Adnan Iltaf","Bin Li","Shoujun Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.05184v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2408.05411v2","updated":"2025-05-05T06:18:50Z","published":"2024-08-10T02:45:46Z","title":"How Does Audio Influence Visual Attention in Omnidirectional Videos?\n  Database and Model","summary":"  Understanding and predicting viewer attention in omnidirectional videos\n(ODVs) is crucial for enhancing user engagement in virtual and augmented\nreality applications. Although both audio and visual modalities are essential\nfor saliency prediction in ODVs, the joint exploitation of these two modalities\nhas been limited, primarily due to the absence of large-scale audio-visual\nsaliency databases and comprehensive analyses. This paper comprehensively\ninvestigates audio-visual attention in ODVs from both subjective and objective\nperspectives. Specifically, we first introduce a new audio-visual saliency\ndatabase for omnidirectional videos, termed AVS-ODV database, containing 162\nODVs and corresponding eye movement data collected from 60 subjects under three\naudio modes including mute, mono, and ambisonics. Based on the constructed\nAVS-ODV database, we perform an in-depth analysis of how audio influences\nvisual attention in ODVs. To advance the research on audio-visual saliency\nprediction for ODVs, we further establish a new benchmark based on the AVS-ODV\ndatabase by testing numerous state-of-the-art saliency models, including\nvisual-only models and audio-visual models. In addition, given the limitations\nof current models, we propose an innovative omnidirectional audio-visual\nsaliency prediction network (OmniAVS), which is built based on the U-Net\narchitecture, and hierarchically fuses audio and visual features from the\nmultimodal aligned embedding space. Extensive experimental results demonstrate\nthat the proposed OmniAVS model outperforms other state-of-the-art models on\nboth ODV AVS prediction and traditional AVS predcition tasks. The AVS-ODV\ndatabase and OmniAVS model will be released to facilitate future research.\n","authors":["Yuxin Zhu","Huiyu Duan","Kaiwei Zhang","Yucheng Zhu","Xilei Zhu","Long Teng","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2408.05411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02388v1","updated":"2025-05-05T06:13:25Z","published":"2025-05-05T06:13:25Z","title":"MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans","summary":"  Embodied AI (EAI) research requires high-quality, diverse 3D scenes to\neffectively support skill acquisition, sim-to-real transfer, and\ngeneralization. Achieving these quality standards, however, necessitates the\nprecise replication of real-world object diversity. Existing datasets\ndemonstrate that this process heavily relies on artist-driven designs, which\ndemand substantial human effort and present significant scalability challenges.\nTo scalably produce realistic and interactive 3D scenes, we first present\nMetaScenes, a large-scale, simulatable 3D scene dataset constructed from\nreal-world scans, which includes 15366 objects spanning 831 fine-grained\ncategories. Then, we introduce Scan2Sim, a robust multi-modal alignment model,\nwhich enables the automated, high-quality replacement of assets, thereby\neliminating the reliance on artist-driven designs for scaling 3D scenes. We\nfurther propose two benchmarks to evaluate MetaScenes: a detailed scene\nsynthesis task focused on small item layouts for robotic manipulation and a\ndomain transfer task in vision-and-language navigation (VLN) to validate\ncross-domain transfer. Results confirm MetaScene's potential to enhance EAI by\nsupporting more generalizable agent learning and sim-to-real applications,\nintroducing new possibilities for EAI research. Project website:\nhttps://meta-scenes.github.io/.\n","authors":["Huangyue Yu","Baoxiong Jia","Yixin Chen","Yandan Yang","Puhao Li","Rongpeng Su","Jiaxin Li","Qing Li","Wei Liang","Song-Chun Zhu","Tengyu Liu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.02388v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2505.02385v1","updated":"2025-05-05T06:00:41Z","published":"2025-05-05T06:00:41Z","title":"An Arbitrary-Modal Fusion Network for Volumetric Cranial Nerves Tract\n  Segmentation","summary":"  The segmentation of cranial nerves (CNs) tract provides a valuable\nquantitative tool for the analysis of the morphology and trajectory of\nindividual CNs. Multimodal CNs tract segmentation networks, e.g., CNTSeg, which\ncombine structural Magnetic Resonance Imaging (MRI) and diffusion MRI, have\nachieved promising segmentation performance. However, it is laborious or even\ninfeasible to collect complete multimodal data in clinical practice due to\nlimitations in equipment, user privacy, and working conditions. In this work,\nwe propose a novel arbitrary-modal fusion network for volumetric CNs tract\nsegmentation, called CNTSeg-v2, which trains one model to handle different\ncombinations of available modalities. Instead of directly combining all the\nmodalities, we select T1-weighted (T1w) images as the primary modality due to\nits simplicity in data acquisition and contribution most to the results, which\nsupervises the information selection of other auxiliary modalities. Our model\nencompasses an Arbitrary-Modal Collaboration Module (ACM) designed to\neffectively extract informative features from other auxiliary modalities,\nguided by the supervision of T1w images. Meanwhile, we construct a Deep\nDistance-guided Multi-stage (DDM) decoder to correct small errors and\ndiscontinuities through signed distance maps to improve segmentation accuracy.\nWe evaluate our CNTSeg-v2 on the Human Connectome Project (HCP) dataset and the\nclinical Multi-shell Diffusion MRI (MDM) dataset. Extensive experimental\nresults show that our CNTSeg-v2 achieves state-of-the-art segmentation\nperformance, outperforming all competing methods.\n","authors":["Lei Xie","Huajun Zhou","Junxiong Huang","Jiahao Huang","Qingrun Zeng","Jianzhong He","Jiawei Zhang","Baohua Fan","Mingchu Li","Guoqiang Xie","Hao Chen","Yuanjing Feng"],"pdf_url":"https://arxiv.org/pdf/2505.02385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02370v1","updated":"2025-05-05T05:19:40Z","published":"2025-05-05T05:19:40Z","title":"SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing","summary":"  Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.\n","authors":["Ming Li","Xin Gu","Fan Chen","Xiaoying Xing","Longyin Wen","Chen Chen","Sijie Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.02370v1.pdf","comment":"Code, Data and Models are available at:\n  https://github.com/bytedance/SuperEdit"},{"id":"http://arxiv.org/abs/2505.01182v2","updated":"2025-05-05T05:14:20Z","published":"2025-05-02T10:50:04Z","title":"TSTMotion: Training-free Scene-aware Text-to-motion Generation","summary":"  Text-to-motion generation has recently garnered significant research\ninterest, primarily focusing on generating human motion sequences in blank\nbackgrounds. However, human motions commonly occur within diverse 3D scenes,\nwhich has prompted exploration into scene-aware text-to-motion generation\nmethods. Yet, existing scene-aware methods often rely on large-scale\nground-truth motion sequences in diverse 3D scenes, which poses practical\nchallenges due to the expensive cost. To mitigate this challenge, we are the\nfirst to propose a \\textbf{T}raining-free \\textbf{S}cene-aware\n\\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that\nefficiently empowers pre-trained blank-background motion generators with the\nscene-aware capability. Specifically, conditioned on the given 3D scene and\ntext description, we adopt foundation models together to reason, predict and\nvalidate a scene-aware motion guidance. Then, the motion guidance is\nincorporated into the blank-background motion generators with two\nmodifications, resulting in scene-aware text-driven motion sequences. Extensive\nexperiments demonstrate the efficacy and generalizability of our proposed\nframework. We release our code in \\href{https://tstmotion.github.io/}{Project\nPage}.\n","authors":["Ziyan Guo","Haoxuan Qu","Hossein Rahmani","Dewen Soh","Ping Hu","Qiuhong Ke","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2505.01182v2.pdf","comment":"Accepted by ICME2025"},{"id":"http://arxiv.org/abs/2505.02369v1","updated":"2025-05-05T05:13:12Z","published":"2025-05-05T05:13:12Z","title":"Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural\n  Networks","summary":"  Generalizing well in deep neural networks remains a core challenge,\nparticularly due to their tendency to converge to sharp minima that degrade\nrobustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking\nflatter minima but perturbs parameters using the full gradient, which can\ninclude statistically insignificant directions. We propose ZSharp, a simple yet\neffective extension to SAM that applies layer-wise Z-score normalization\nfollowed by percentile-based filtering to retain only statistically significant\ngradient components. This selective perturbation aligns updates with\ncurvature-sensitive directions, enhancing generalization without requiring\narchitectural changes. ZSharp introduces only one additional hyperparameter,\nthe percentile threshold, and remains fully compatible with existing SAM\nvariants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,\nVGG, and Vision Transformers show that ZSharp consistently outperforms SAM and\nits variants in test accuracy, particularly on deeper and transformer-based\nmodels. These results demonstrate that ZSharp is a principled and lightweight\nimprovement for sharpness-aware optimization.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2505.02369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02365v1","updated":"2025-05-05T05:08:33Z","published":"2025-05-05T05:08:33Z","title":"Quaternion Multi-focus Color Image Fusion","summary":"  Multi-focus color image fusion refers to integrating multiple partially\nfocused color images to create a single all-in-focus color image. However,\nexisting methods struggle with complex real-world scenarios due to limitations\nin handling color information and intricate textures. To address these\nchallenges, this paper proposes a quaternion multi-focus color image fusion\nframework to perform high-quality color image fusion completely in the\nquaternion domain. This framework introduces 1) a quaternion sparse\ndecomposition model to jointly learn fine-scale image details and structure\ninformation of color images in an iterative fashion for high-precision focus\ndetection, 2) a quaternion base-detail fusion strategy to individually fuse\nbase-scale and detail-scale results across multiple color images for preserving\nstructure and detail information, and 3) a quaternion structural similarity\nrefinement strategy to adaptively select optimal patches from initial fusion\nresults and obtain the final fused result for preserving fine details and\nensuring spatially consistent outputs. Extensive experiments demonstrate that\nthe proposed framework outperforms state-of-the-art methods.\n","authors":["Weihua Yang","Yicong Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.02365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02364v1","updated":"2025-05-05T05:02:05Z","published":"2025-05-05T05:02:05Z","title":"Quaternion Infrared Visible Image Fusion","summary":"  Visible images provide rich details and color information only under\nwell-lighted conditions while infrared images effectively highlight thermal\ntargets under challenging conditions such as low visibility and adverse\nweather. Infrared-visible image fusion aims to integrate complementary\ninformation from infrared and visible images to generate a high-quality fused\nimage. Existing methods exhibit critical limitations such as neglecting color\nstructure information in visible images and performance degradation when\nprocessing low-quality color-visible inputs. To address these issues, we\npropose a quaternion infrared-visible image fusion (QIVIF) framework to\ngenerate high-quality fused images completely in the quaternion domain. QIVIF\nproposes a quaternion low-visibility feature learning model to adaptively\nextract salient thermal targets and fine-grained texture details from input\ninfrared and visible images respectively under diverse degraded conditions.\nQIVIF then develops a quaternion adaptive unsharp masking method to adaptively\nimprove high-frequency feature enhancement with balanced illumination. QIVIF\nfurther proposes a quaternion hierarchical Bayesian fusion model to integrate\ninfrared saliency and enhanced visible details to obtain high-quality fused\nimages. Extensive experiments across diverse datasets demonstrate that our\nQIVIF surpasses state-of-the-art methods under challenging low-visibility\nconditions.\n","authors":["Weihua Yang","Yicong Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.02364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12772v2","updated":"2025-05-05T04:48:45Z","published":"2024-07-17T17:51:53Z","title":"LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models","summary":"  The advances of large foundation models necessitate wide-coverage, low-cost,\nand zero-contamination benchmarks. Despite continuous exploration of language\nmodel evaluations, comprehensive studies on the evaluation of Large Multi-modal\nModels (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified\nand standardized multimodal benchmark framework with over 50 tasks and more\nthan 10 models to promote transparent and reproducible evaluations. Although\nLMMS-EVAL offers comprehensive coverage, we find it still falls short in\nachieving low cost and zero contamination. To approach this evaluation\ntrilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that\nemphasizes both coverage and efficiency. Additionally, we present Multimodal\nLIVEBENCH that utilizes continuously updating news and online forums to assess\nmodels' generalization abilities in the wild, featuring a low-cost and\nzero-contamination evaluation approach. In summary, our work highlights the\nimportance of considering the evaluation trilemma and provides practical\nsolutions to navigate the trade-offs in evaluating large multi-modal models,\npaving the way for more effective and reliable benchmarking of LMMs. We\nopensource our codebase and maintain leaderboard of LIVEBENCH at\nhttps://github.com/EvolvingLMMs-Lab/lmms-eval and\nhttps://huggingface.co/spaces/lmms-lab/LiveBench.\n","authors":["Kaichen Zhang","Bo Li","Peiyuan Zhang","Fanyi Pu","Joshua Adrian Cahyono","Kairui Hu","Shuai Liu","Yuanhan Zhang","Jingkang Yang","Chunyuan Li","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.12772v2.pdf","comment":"Code ad leaderboard are available at\n  https://github.com/EvolvingLMMs-Lab/lmms-eval and\n  https://huggingface.co/spaces/lmms-lab/LiveBench"},{"id":"http://arxiv.org/abs/2505.02350v1","updated":"2025-05-05T04:16:16Z","published":"2025-05-05T04:16:16Z","title":"Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface\n  Representation","summary":"  Point cloud surface representation is a fundamental problem in computer\ngraphics and vision. This paper presents a machine learning approach for\napproximating the signed distance function (SDF) of a point cloud using sparse\nellipsoidal radial basis function networks, enabling a compact and accurate\nsurface representation. Given the SDF values defined on the grid points\nconstructed from the point cloud, our method approximates the SDF accurately\nwith as few ellipsoidal radial basis functions (ERBFs) as possible, i.e.,\nrepresent the SDF of a point cloud by sparse ERBFs. To balance sparsity and\napproximation precision, a dynamic multi-objective optimization strategy is\nintroduced, which adaptively adds the regularization terms and jointly\noptimizes the weights, centers, shapes, and orientations of ERBFs. To improve\ncomputational efficiency, a nearest-neighbor-based data structure is employed,\nrestricting function calculations to points near each Gaussian kernel center.\nThe computations for each kernel are further parallelized on CUDA, which\nsignificantly improves the optimization speed. Additionally, a hierarchical\noctree-based refinement strategy is designed for training. Specifically, the\ninitialization and optimization of network parameters are conducted using\ncoarse grid points in the octree lattice structure. Subsequently, fine lattice\npoints are progressively incorporated to accelerate model convergence and\nenhance training efficiency. Extensive experiments on multiple benchmark\ndatasets demonstrate that our method outperforms previous sparse representation\napproaches in terms of accuracy, robustness, and computational efficiency. The\ncorresponding code is publicly available at\nhttps://github.com/lianbobo/SE-RBFNet.git.\n","authors":["Bobo Lian","Dandan Wang","Chenjian Wu","Minxin Chen"],"pdf_url":"https://arxiv.org/pdf/2505.02350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02979v2","updated":"2025-05-05T03:58:07Z","published":"2024-11-05T10:41:45Z","title":"CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model\n  Retrieval","summary":"  Reconstructing from multi-view images is a longstanding problem in 3D vision,\nwhere neural radiance fields (NeRFs) have shown great potential and get\nrealistic rendered images of novel views. Currently, most NeRF methods either\nrequire accurate camera poses or a large number of input images, or even both.\nReconstructing NeRF from few-view images without poses is challenging and\nhighly ill-posed. To address this problem, we propose CAD-NeRF, a method\nreconstructed from less than 10 images without any known poses. Specifically,\nwe build a mini library of several CAD models from ShapeNet and render them\nfrom many random views. Given sparse-view input images, we run a model and pose\nretrieval from the library, to get a model with similar shapes, serving as the\ndensity supervision and pose initializations. Here we propose a multi-view pose\nretrieval method to avoid pose conflicts among views, which is a new and unseen\nproblem in uncalibrated NeRF methods. Then, the geometry of the object is\ntrained by the CAD guidance. The deformation of the density field and camera\nposes are optimized jointly. Then texture and density are trained and\nfine-tuned as well. All training phases are in self-supervised manners.\nComprehensive evaluations of synthetic and real images show that CAD-NeRF\nsuccessfully learns accurate densities with a large deformation from retrieved\nCAD models, showing the generalization abilities.\n","authors":["Xin Wen","Xuening Zhu","Renjiao Yi","Zhifeng Wang","Chenyang Zhu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2411.02979v2.pdf","comment":"The article has been accepted by Frontiers of Computer Science (FCS)"},{"id":"http://arxiv.org/abs/2504.08685v2","updated":"2025-05-05T03:31:30Z","published":"2025-04-11T16:46:20Z","title":"Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model","summary":"  This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/\n","authors":["Team Seawead","Ceyuan Yang","Zhijie Lin","Yang Zhao","Shanchuan Lin","Zhibei Ma","Haoyuan Guo","Hao Chen","Lu Qi","Sen Wang","Feng Cheng","Feilong Zuo","Xuejiao Zeng","Ziyan Yang","Fangyuan Kong","Meng Wei","Zhiwu Qing","Fei Xiao","Tuyen Hoang","Siyu Zhang","Peihao Zhu","Qi Zhao","Jiangqiao Yan","Liangke Gui","Sheng Bi","Jiashi Li","Yuxi Ren","Rui Wang","Huixia Li","Xuefeng Xiao","Shu Liu","Feng Ling","Heng Zhang","Houmin Wei","Huafeng Kuang","Jerry Duncan","Junda Zhang","Junru Zheng","Li Sun","Manlin Zhang","Renfei Sun","Xiaobin Zhuang","Xiaojie Li","Xin Xia","Xuyan Chi","Yanghua Peng","Yuping Wang","Yuxuan Wang","Zhongkai Zhao","Zhuo Chen","Zuquan Song","Zhenheng Yang","Jiashi Feng","Jianchao Yang","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2504.08685v2.pdf","comment":"Technical report (some typos fixed)"},{"id":"http://arxiv.org/abs/2505.02335v1","updated":"2025-05-05T03:15:12Z","published":"2025-05-05T03:15:12Z","title":"6D Pose Estimation on Spoons and Hands","summary":"  Accurate dietary monitoring is essential for promoting healthier eating\nhabits. A key area of research is how people interact and consume food using\nutensils and hands. By tracking their position and orientation, it is possible\nto estimate the volume of food being consumed, or monitor eating behaviours,\nhighly useful insights into nutritional intake that can be more reliable than\npopular methods such as self-reporting. Hence, this paper implements a system\nthat analyzes stationary video feed of people eating, using 6D pose estimation\nto track hand and spoon movements to capture spatial position and orientation.\nIn doing so, we examine the performance of two state-of-the-art (SOTA) video\nobject segmentation (VOS) models, both quantitatively and qualitatively, and\nidentify main sources of error within the system.\n","authors":["Kevin Tan","Fan Yang","Yuhao Chen"],"pdf_url":"https://arxiv.org/pdf/2505.02335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06869v3","updated":"2025-05-05T03:07:00Z","published":"2024-03-11T16:22:41Z","title":"Impact of Noisy Supervision in Foundation Model Learning","summary":"  Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning.\n","authors":["Hao Chen","Zihan Wang","Ran Tao","Hongxin Wei","Xing Xie","Masashi Sugiyama","Bhiksha Raj","Jindong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06869v3.pdf","comment":"18 pages, 10 figures, 6 tables, preprint. arXiv admin note:\n  substantial text overlap with arXiv:2309.17002"},{"id":"http://arxiv.org/abs/2505.02331v1","updated":"2025-05-05T03:00:51Z","published":"2025-05-05T03:00:51Z","title":"VAEmo: Efficient Representation Learning for Visual-Audio Emotion with\n  Knowledge Injection","summary":"  Audiovisual emotion recognition (AVER) aims to infer human emotions from\nnonverbal visual-audio (VA) cues, offering modality-complementary and\nlanguage-agnostic advantages. However, AVER remains challenging due to the\ninherent ambiguity of emotional expressions, cross-modal expressive\ndisparities, and the scarcity of reliably annotated data. Recent\nself-supervised AVER approaches have introduced strong multimodal\nrepresentations, yet they predominantly rely on modality-specific encoders and\ncoarse content-level alignment, limiting fine-grained emotional semantic\nmodeling. To address these issues, we propose VAEmo, an efficient two-stage\nframework for emotion-centric joint VA representation learning with external\nknowledge injection. In Stage 1, a unified and lightweight representation\nnetwork is pre-trained on large-scale speaker-centric VA corpora via masked\nreconstruction and contrastive objectives, mitigating the modality gap and\nlearning expressive, complementary representations without emotion labels. In\nStage 2, multimodal large language models automatically generate detailed\naffective descriptions according to our well-designed chain-of-thought\nprompting for only a small subset of VA samples; these rich textual semantics\nare then injected by aligning their corresponding embeddings with VA\nrepresentations through dual-path contrastive learning, further bridging the\nemotion gap. Extensive experiments on multiple downstream AVER benchmarks show\nthat VAEmo achieves state-of-the-art performance with a compact design,\nhighlighting the benefit of unified cross-modal encoding and emotion-aware\nsemantic guidance for efficient, generalizable VA emotion representations.\n","authors":["Hao Cheng","Zhiwei Zhao","Yichao He","Zhenzhen Hu","Jia Li","Meng Wang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2505.02331v1.pdf","comment":"Source code and pre-trained models will be available at\n  https://github.com/MSA-LMC/VAEmo"},{"id":"http://arxiv.org/abs/2505.02325v1","updated":"2025-05-05T02:47:07Z","published":"2025-05-05T02:47:07Z","title":"TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval\n  via Testing-time Distribution Alignment","summary":"  Learning discriminative 3D representations that generalize well to unknown\ntesting categories is an emerging requirement for many real-world 3D\napplications. Existing well-established methods often struggle to attain this\ngoal due to insufficient 3D training data from broader concepts. Meanwhile,\npre-trained large vision-language models (e.g., CLIP) have shown remarkable\nzero-shot generalization capabilities. Yet, they are limited in extracting\nsuitable 3D representations due to substantial gaps between their 2D training\nand 3D testing distributions. To address these challenges, we propose\nTesting-time Distribution Alignment (TeDA), a novel framework that adapts a\npretrained 2D vision-language model CLIP for unknown 3D object retrieval at\ntest time. To our knowledge, it is the first work that studies the test-time\nadaptation of a vision-language model for 3D feature learning. TeDA projects 3D\nobjects into multi-view images, extracts features using CLIP, and refines 3D\nquery embeddings with an iterative optimization strategy by confident\nquery-target sample pairs in a self-boosting manner. Additionally, TeDA\nintegrates textual descriptions generated by a multimodal language model\n(InternVL) to enhance 3D object understanding, leveraging CLIP's aligned\nfeature space to fuse visual and textual cues. Extensive experiments on four\nopen-set 3D object retrieval benchmarks demonstrate that TeDA greatly\noutperforms state-of-the-art methods, even those requiring extensive training.\nWe also experimented with depth maps on Objaverse-LVIS, further validating its\neffectiveness. Code is available at https://github.com/wangzhichuan123/TeDA.\n","authors":["Zhichuan Wang","Yang Zhou","Jinhai Xiang","Yulong Wang","Xinwei He"],"pdf_url":"https://arxiv.org/pdf/2505.02325v1.pdf","comment":"Accepted by ICMR 2025"},{"id":"http://arxiv.org/abs/2503.06222v2","updated":"2025-05-05T02:33:12Z","published":"2025-03-08T13:49:43Z","title":"Vision-based 3D Semantic Scene Completion via Capture Dynamic\n  Representations","summary":"  The vision-based semantic scene completion task aims to predict dense\ngeometric and semantic 3D scene representations from 2D images. However, the\npresence of dynamic objects in the scene seriously affects the accuracy of the\nmodel inferring 3D structures from 2D images. Existing methods simply stack\nmultiple frames of image input to increase dense scene semantic information,\nbut ignore the fact that dynamic objects and non-texture areas violate\nmulti-view consistency and matching reliability. To address these issues, we\npropose a novel method, CDScene: Vision-based Robust Semantic Scene Completion\nvia Capturing Dynamic Representations. First, we leverage a multimodal\nlarge-scale model to extract 2D explicit semantics and align them into 3D\nspace. Second, we exploit the characteristics of monocular and stereo depth to\ndecouple scene information into dynamic and static features. The dynamic\nfeatures contain structural relationships around dynamic objects, and the\nstatic features contain dense contextual spatial information. Finally, we\ndesign a dynamic-static adaptive fusion module to effectively extract and\naggregate complementary features, achieving robust and accurate semantic scene\ncompletion in autonomous driving scenarios. Extensive experimental results on\nthe SemanticKITTI, SSCBench-KITTI360, and SemanticKITTI-C datasets demonstrate\nthe superiority and robustness of CDScene over existing state-of-the-art\nmethods.\n","authors":["Meng Wang","Fan Wu","Yunchuan Qin","Ruihui Li","Zhuo Tang","Kenli Li"],"pdf_url":"https://arxiv.org/pdf/2503.06222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00744v2","updated":"2025-05-05T02:30:17Z","published":"2025-04-30T07:57:51Z","title":"Localizing Before Answering: A Hallucination Evaluation Benchmark for\n  Grounded Medical Multimodal LLMs","summary":"  Medical Large Multi-modal Models (LMMs) have demonstrated remarkable\ncapabilities in medical data interpretation. However, these models frequently\ngenerate hallucinations contradicting source evidence, particularly due to\ninadequate localization reasoning. This work reveals a critical limitation in\ncurrent medical LMMs: instead of analyzing relevant pathological regions, they\noften rely on linguistic patterns or attend to irrelevant image areas when\nresponding to disease-related queries. To address this, we introduce\nHEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive\nbenchmark designed to evaluate LMMs' localization abilities and hallucination\nrobustness. HEAL-MedVQA features (i) two innovative evaluation protocols to\nassess visual and textual shortcut learning, and (ii) a dataset of 67K VQA\npairs, with doctor-annotated anatomical segmentation masks for pathological\nregions. To improve visual reasoning, we propose the Localize-before-Answer\n(LobA) framework, which trains LMMs to localize target regions of interest and\nself-prompt to emphasize segmented pathological areas, generating grounded and\nreliable answers. Experimental results demonstrate that our approach\nsignificantly outperforms state-of-the-art biomedical LMMs on the challenging\nHEAL-MedVQA benchmark, advancing robustness in medical VQA.\n","authors":["Dung Nguyen","Minh Khoi Ho","Huy Ta","Thanh Tam Nguyen","Qi Chen","Kumar Rav","Quy Duong Dang","Satwik Ramchandre","Son Lam Phung","Zhibin Liao","Minh-Son To","Johan Verjans","Phi Le Nguyen","Vu Minh Hieu Phan"],"pdf_url":"https://arxiv.org/pdf/2505.00744v2.pdf","comment":"Accepted at Joint Conference on Artificial Intelligence (IJCAI) 2025"},{"id":"http://arxiv.org/abs/2411.15539v2","updated":"2025-05-05T02:18:33Z","published":"2024-11-23T12:25:06Z","title":"Large Language Model with Region-guided Referring and Grounding for CT\n  Report Generation","summary":"  Computed tomography (CT) report generation is crucial to assist radiologists\nin interpreting CT volumes, which can be time-consuming and labor-intensive.\nExisting methods primarily only consider the global features of the entire\nvolume, making it struggle to focus on specific regions and potentially missing\nabnormalities. To address this issue, we propose Reg2RG, the first\nregion-guided referring and grounding framework for CT report generation, which\nenhances diagnostic performance by focusing on anatomical regions within the\nvolume. Specifically, we utilize masks from a universal segmentation module to\ncapture local features for each referring region. A local feature decoupling\n(LFD) strategy is proposed to preserve the local high-resolution details with\nlittle computational overhead. Then the local features are integrated with\nglobal features to capture inter-regional relationships within a cohesive\ncontext. Moreover, we propose a novel region-report alignment (RRA) training\nstrategy. It leverages the recognition of referring regions to guide the\ngeneration of region-specific reports, enhancing the model's referring and\ngrounding capabilities while also improving the report's interpretability. A\nlarge language model (LLM) is further employed as the language decoder to\ngenerate reports from integrated visual features, facilitating region-level\ncomprehension. Extensive experiments on two large-scale chest CT-report\ndatasets demonstrate the superiority of our method, which outperforms several\nstate-of-the-art methods in terms of both natural language generation and\nclinical efficacy metrics while preserving promising interpretability. The code\nis available at https://github.com/zhi-xuan-chen/Reg2RG.\n","authors":["Zhixuan Chen","Yequan Bie","Haibo Jin","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15539v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.02329v3","updated":"2025-05-05T02:03:50Z","published":"2024-07-02T14:59:37Z","title":"MIGC++: Advanced Multi-Instance Generation Controller for Image\n  Synthesis","summary":"  We introduce the Multi-Instance Generation (MIG) task, which focuses on\ngenerating multiple instances within a single image, each accurately placed at\npredefined positions with attributes such as category, color, and shape,\nstrictly following user specifications. MIG faces three main challenges:\navoiding attribute leakage between instances, supporting diverse instance\ndescriptions, and maintaining consistency in iterative generation. To address\nattribute leakage, we propose the Multi-Instance Generation Controller (MIGC).\nMIGC generates multiple instances through a divide-and-conquer strategy,\nbreaking down multi-instance shading into single-instance tasks with singular\nattributes, later integrated. To provide more types of instance descriptions,\nwe developed MIGC++. MIGC++ allows attribute control through text \\& images and\nposition control through boxes \\& masks. Lastly, we introduced the\nConsistent-MIG algorithm to enhance the iterative MIG ability of MIGC and\nMIGC++. This algorithm ensures consistency in unmodified regions during the\naddition, deletion, or modification of instances, and preserves the identity of\ninstances when their attributes are changed. We introduce the COCO-MIG and\nMultimodal-MIG benchmarks to evaluate these methods. Extensive experiments on\nthese benchmarks, along with the COCO-Position benchmark and DrawBench,\ndemonstrate that our methods substantially outperform existing techniques,\nmaintaining precise control over aspects including position, attribute, and\nquantity. Project page: https://github.com/limuloo/MIGC.\n","authors":["Dewei Zhou","You Li","Fan Ma","Zongxin Yang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2407.02329v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13745v2","updated":"2025-05-05T02:01:46Z","published":"2024-05-22T15:32:34Z","title":"NeurCross: A Neural Approach to Computing Cross Fields for Quad Mesh\n  Generation","summary":"  Quadrilateral mesh generation plays a crucial role in numerical simulations\nwithin Computer-Aided Design and Engineering (CAD/E). Producing high-quality\nquadrangulation typically requires satisfying four key criteria. First, the\nquadrilateral mesh should closely align with principal curvature directions.\nSecond, singular points should be strategically placed and effectively\nminimized. Third, the mesh should accurately conform to sharp feature edges.\nLastly, quadrangulation results should exhibit robustness against noise and\nminor geometric variations. Existing methods generally involve first computing\na regular cross field to represent quad element orientations across the\nsurface, followed by extracting a quadrilateral mesh aligned closely with this\ncross field. A primary challenge with this approach is balancing the smoothness\nof the cross field with its alignment to pre-computed principal curvature\ndirections, which are sensitive to small surface perturbations and often\nill-defined in spherical or planar regions.\n  To tackle this challenge, we propose NeurCross, a novel framework that\nsimultaneously optimizes a cross field and a neural signed distance function\n(SDF), whose zero-level set serves as a proxy of the input shape. Our joint\noptimization is guided by three factors: faithful approximation of the\noptimized SDF surface to the input surface, alignment between the cross field\nand the principal curvature field derived from the SDF surface, and smoothness\nof the cross field. Acting as an intermediary, the neural SDF contributes in\ntwo essential ways. First, it provides an alternative, optimizable base surface\nexhibiting more regular principal curvature directions for guiding the cross\nfield. Second, we leverage the Hessian matrix of the neural SDF to implicitly\nenforce cross field alignment with principal curvature directions...\n","authors":["Qiujie Dong","Huibiao Wen","Rui Xu","Shuangmin Chen","Jiaran Zhou","Shiqing Xin","Changhe Tu","Taku Komura","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2405.13745v2.pdf","comment":"SIGGRAPH 2025"},{"id":"http://arxiv.org/abs/2505.02304v1","updated":"2025-05-05T00:57:57Z","published":"2025-05-05T00:57:57Z","title":"Generative Sign-description Prompts with Multi-positive Contrastive\n  Learning for Sign Language Recognition","summary":"  Sign language recognition (SLR) faces fundamental challenges in creating\naccurate annotations due to the inherent complexity of simultaneous manual and\nnon-manual signals. To the best of our knowledge, this is the first work to\nintegrate generative large language models (LLMs) into SLR tasks. We propose a\nnovel Generative Sign-description Prompts Multi-positive Contrastive learning\n(GSP-MC) method that leverages retrieval-augmented generation (RAG) with\ndomain-specific LLMs, incorporating multi-step prompt engineering and\nexpert-validated sign language corpora to produce precise multipart\ndescriptions. The GSP-MC method also employs a dual-encoder architecture to\nbidirectionally align hierarchical skeleton features with multiple text\ndescriptions (global, synonym, and part level) through probabilistic matching.\nOur approach combines global and part-level losses, optimizing KL divergence to\nensure robust alignment across all relevant text-skeleton pairs while capturing\nboth sign-level semantics and detailed part dynamics. Experiments demonstrate\nstate-of-the-art performance against existing methods on the Chinese SLR500\n(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's\ncross-lingual effectiveness highlight its potential for developing inclusive\ncommunication technologies.\n","authors":["Siyu Liang","Yunan Li","Wentian Xin","Huizhou Chen","Xujie Liu","Kang Liu","Qiguang Miao"],"pdf_url":"https://arxiv.org/pdf/2505.02304v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2308.04369v3","updated":"2025-05-05T00:01:35Z","published":"2023-08-08T16:15:35Z","title":"SSTFormer: Bridging Spiking Neural Network and Memory Support\n  Transformer for Frame-Event based Recognition","summary":"  Event camera-based pattern recognition is a newly arising research topic in\nrecent years. Current researchers usually transform the event streams into\nimages, graphs, or voxels, and adopt deep neural networks for event-based\nclassification. Although good performance can be achieved on simple event\nrecognition datasets, however, their results may be still limited due to the\nfollowing two issues. Firstly, they adopt spatial sparse event streams for\nrecognition only, which may fail to capture the color and detailed texture\ninformation well. Secondly, they adopt either Spiking Neural Networks (SNN) for\nenergy-efficient recognition with suboptimal results, or Artificial Neural\nNetworks (ANN) for energy-intensive, high-performance recognition. However,\nseldom of them consider achieving a balance between these two aspects. In this\npaper, we formally propose to recognize patterns by fusing RGB frames and event\nstreams simultaneously and propose a new RGB frame-event recognition framework\nto address the aforementioned issues. The proposed method contains four main\nmodules, i.e., memory support Transformer network for RGB frame encoding,\nspiking neural network for raw event stream encoding, multi-modal bottleneck\nfusion module for RGB-Event feature aggregation, and prediction head. Due to\nthe scarce of RGB-Event based classification dataset, we also propose a\nlarge-scale PokerEvent dataset which contains 114 classes, and 27102\nframe-event pairs recorded using a DVS346 event camera. Extensive experiments\non two RGB-Event based classification datasets fully validated the\neffectiveness of our proposed framework. We hope this work will boost the\ndevelopment of pattern recognition by fusing RGB frames and event streams. Both\nour dataset and source code of this work will be released at\nhttps://github.com/Event-AHU/SSTFormer\n","authors":["Xiao Wang","Yao Rong","Zongzhen Wu","Lin Zhu","Bo Jiang","Jin Tang","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2308.04369v3.pdf","comment":"Accepted by IEEE Transactions on Cognitive and Developmental Systems\n  (TCDS) 2025"},{"id":"http://arxiv.org/abs/2505.03046v1","updated":"2025-05-05T22:04:12Z","published":"2025-05-05T22:04:12Z","title":"Sim2Real Transfer for Vision-Based Grasp Verification","summary":"  The verification of successful grasps is a crucial aspect of robot\nmanipulation, particularly when handling deformable objects. Traditional\nmethods relying on force and tactile sensors often struggle with deformable and\nnon-rigid objects. In this work, we present a vision-based approach for grasp\nverification to determine whether the robotic gripper has successfully grasped\nan object. Our method employs a two-stage architecture; first YOLO-based object\ndetection model to detect and locate the robot's gripper and then a\nResNet-based classifier determines the presence of an object. To address the\nlimitations of real-world data capture, we introduce HSR-GraspSynth, a\nsynthetic dataset designed to simulate diverse grasping scenarios. Furthermore,\nwe explore the use of Visual Question Answering capabilities as a zero-shot\nbaseline to which we compare our model. Experimental results demonstrate that\nour approach achieves high accuracy in real-world environments, with potential\nfor integration into grasping pipelines. Code and datasets are publicly\navailable at https://github.com/pauamargant/HSR-GraspSynth .\n","authors":["Pau Amargant","Peter Hönig","Markus Vincze"],"pdf_url":"https://arxiv.org/pdf/2505.03046v1.pdf","comment":"Accepted at Austrian Robotics Workshop 2025"},{"id":"http://arxiv.org/abs/2505.03039v1","updated":"2025-05-05T21:41:05Z","published":"2025-05-05T21:41:05Z","title":"An Explainable Anomaly Detection Framework for Monitoring Depression and\n  Anxiety Using Consumer Wearable Devices","summary":"  Continuous monitoring of behavior and physiology via wearable devices offers\na novel, objective method for the early detection of worsening depression and\nanxiety. In this study, we present an explainable anomaly detection framework\nthat identifies clinically meaningful increases in symptom severity using\nconsumer-grade wearable data. Leveraging data from 2,023 participants with\ndefined healthy baselines, our LSTM autoencoder model learned normal health\npatterns of sleep duration, step count, and resting heart rate. Anomalies were\nflagged when self-reported depression or anxiety scores increased by >=5 points\n(a threshold considered clinically significant). The model achieved an adjusted\nF1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393\nsymptom-worsening episodes across 341 participants, with higher performance\nobserved for episodes involving concurrent depression and anxiety escalation\n(F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 =\n0.85). Model interpretability was supported by SHAP-based analysis, which\nidentified resting heart rate as the most influential feature in 71.4\npercentage of detected anomalies, followed by physical activity and sleep.\nTogether, our findings highlight the potential of explainable anomaly detection\nto enable personalized, scalable, and proactive mental health monitoring in\nreal-world settings.\n","authors":["Yuezhou Zhang","Amos A. Folarin","Callum Stewart","Heet Sankesara","Yatharth Ranjan","Pauline Conde","Akash Roy Choudhury","Shaoxiong Sun","Zulqarnain Rashid","Richard J. B. Dobson"],"pdf_url":"https://arxiv.org/pdf/2505.03039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03037v1","updated":"2025-05-05T21:27:23Z","published":"2025-05-05T21:27:23Z","title":"Dual Prompting for Diverse Count-level PET Denoising","summary":"  The to-be-denoised positron emission tomography (PET) volumes are inherent\nwith diverse count levels, which imposes challenges for a unified model to\ntackle varied cases. In this work, we resort to the recently flourished prompt\nlearning to achieve generalizable PET denoising with different count levels.\nSpecifically, we propose dual prompts to guide the PET denoising in a\ndivide-and-conquer manner, i.e., an explicitly count-level prompt to provide\nthe specific prior information and an implicitly general denoising prompt to\nencode the essential PET denoising knowledge. Then, a novel prompt fusion\nmodule is developed to unify the heterogeneous prompts, followed by a\nprompt-feature interaction module to inject prompts into the features. The\nprompts are able to dynamically guide the noise-conditioned denoising process.\nTherefore, we are able to efficiently train a unified denoising model for\nvarious count levels, and deploy it to different cases with personalized\nprompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly\nselected 13-22\\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies.\nIt shows our dual prompting can largely improve the performance with informed\ncount-level and outperform the count-conditional model.\n","authors":["Xiaofeng Liu","Yongsong Huang","Thibault Marin","Samira Vafay Eslahi","Tiss Amal","Yanis Chemli","Keith Johnson","Georges El Fakhri","Jinsong Ouyang"],"pdf_url":"https://arxiv.org/pdf/2505.03037v1.pdf","comment":"Published in IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2025"},{"id":"http://arxiv.org/abs/2505.03018v1","updated":"2025-05-05T20:41:30Z","published":"2025-05-05T20:41:30Z","title":"Lesion-Aware Generative Artificial Intelligence for Virtual\n  Contrast-Enhanced Mammography in Breast Cancer","summary":"  Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic\ntechnique that improves lesion visibility through the administration of an\niodinated contrast agent. It acquires both a low-energy image, comparable to\nstandard mammography, and a high-energy image, which are then combined to\nproduce a dual-energy subtracted image highlighting lesion contrast\nenhancement. While CESM offers superior diagnostic accuracy compared to\nstandard mammography, its use entails higher radiation exposure and potential\nside effects associated with the contrast medium. To address these limitations,\nwe propose Seg-CycleGAN, a generative deep learning framework for Virtual\nContrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy\nsubtracted images from low-energy images, leveraging lesion segmentation maps\nto guide the generative process and improve lesion reconstruction. Building\nupon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss\nterms focused on lesion areas, enhancing the synthesis of diagnostically\nrelevant regions. Experiments on the CESM@UCBM dataset demonstrate that\nSeg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while\nmaintaining competitive MSE and VIF. Qualitative evaluations further confirm\nimproved lesion fidelity in the generated images. These results suggest that\nsegmentation-aware generative models offer a viable pathway toward\ncontrast-free CESM alternatives.\n","authors":["Aurora Rofena","Arianna Manchia","Claudia Lucia Piccolo","Bruno Beomonte Zobel","Paolo Soda","Valerio Guarrasi"],"pdf_url":"https://arxiv.org/pdf/2505.03018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03012v1","updated":"2025-05-05T20:23:14Z","published":"2025-05-05T20:23:14Z","title":"GIF: Generative Inspiration for Face Recognition at Scale","summary":"  Aiming to reduce the computational cost of Softmax in massive label space of\nFace Recognition (FR) benchmarks, recent studies estimate the output using a\nsubset of identities. Although promising, the association between the\ncomputation cost and the number of identities in the dataset remains linear\nonly with a reduced ratio. A shared characteristic among available FR methods\nis the employment of atomic scalar labels during training. Consequently, the\ninput to label matching is through a dot product between the feature vector of\nthe input and the Softmax centroids. Inspired by generative modeling, we\npresent a simple yet effective method that substitutes scalar labels with\nstructured identity code, i.e., a sequence of integers. Specifically, we\npropose a tokenization scheme that transforms atomic scalar labels into\nstructured identity codes. Then, we train an FR backbone to predict the code\nfor each input instead of its scalar label. As a result, the associated\ncomputational cost becomes logarithmic w.r.t. number of identities. We\ndemonstrate the benefits of the proposed method by conducting experiments. In\nparticular, our method outperforms its competitors by 1.52%, and 0.6% at\nTAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the\nassociation between computational cost and the number of identities from linear\nto logarithmic. See code at https://github.com/msed-Ebrahimi/GIF\n","authors":["Saeed Ebrahimi","Sahar Rahimi","Ali Dabouei","Srinjoy Das","Jeremy M. Dawson","Nasser M. Nasrabadi"],"pdf_url":"https://arxiv.org/pdf/2505.03012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00507v2","updated":"2025-05-05T20:20:54Z","published":"2025-05-01T13:24:55Z","title":"HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection","summary":"  Active Learning has proved to be a relevant approach to perform sample\nselection for training models for Autonomous Driving. Particularly, previous\nworks on active learning for 3D object detection have shown that selection of\nsamples in uncontrolled scenarios is challenging. Furthermore, current\napproaches focus exclusively on the theoretical aspects of the sample selection\nproblem but neglect the practical insights that can be obtained from the\nextensive literature and application of 3D detection models. In this paper, we\nintroduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection)\nwhich integrates those heuristical features together with Localization and\nClassification to deliver the most contributing samples to the model's\ntraining. In contrast to previous works, our approach integrates heuristical\nfeatures such as object distance and point-quantity to estimate the\nuncertainty, which enhance the usefulness of selected samples to train\ndetection models. Our quantitative evaluation on KITTI shows that HeAL presents\ncompetitive mAP with respect to the State-of-the-Art, and achieves the same mAP\nas the full-supervised baseline with only 24% of the samples.\n","authors":["Esteban Rivera","Surya Prabhakaran","Markus Lienkamp"],"pdf_url":"https://arxiv.org/pdf/2505.00507v2.pdf","comment":"Accepted in CVPRw2025"},{"id":"http://arxiv.org/abs/2505.03007v1","updated":"2025-05-05T20:06:11Z","published":"2025-05-05T20:06:11Z","title":"NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results","summary":"  This paper presents an overview of the NTIRE 2025 Challenge on UGC Video\nEnhancement. The challenge constructed a set of 150 user-generated content\nvideos without reference ground truth, which suffer from real-world\ndegradations such as noise, blur, faded colors, compression artifacts, etc. The\ngoal of the participants was to develop an algorithm capable of improving the\nvisual quality of such videos. Given the widespread use of UGC on short-form\nvideo platforms, this task holds substantial practical importance. The\nevaluation was based on subjective quality assessment in crowdsourcing,\nobtaining votes from over 8000 assessors. The challenge attracted more than 25\nteams submitting solutions, 7 of which passed the final phase with source code\nverification. The outcomes may provide insights into the state-of-the-art in\nUGC video enhancement and highlight emerging trends and effective strategies in\nthis evolving research area. All data, including the processed videos and\nsubjective comparison votes and scores, is made publicly available at\nhttps://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.\n","authors":["Nikolay Safonov","Alexey Bryncev","Andrey Moskalenko","Dmitry Kulikov","Dmitry Vatolin","Radu Timofte","Haibo Lei","Qifan Gao","Qing Luo","Yaqing Li","Jie Song","Shaozhe Hao","Meisong Zheng","Jingyi Xu","Chengbin Wu","Jiahui Liu","Ying Chen","Xin Deng","Mai Xu","Peipei Liang","Jie Ma","Junjie Jin","Yingxue Pang","Fangzhou Luo","Kai Chen","Shijie Zhao","Mingyang Wu","Renjie Li","Yushen Zuo","Shengyun Zhong","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2505.03007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21188v2","updated":"2025-05-05T20:00:46Z","published":"2025-04-29T21:45:11Z","title":"Light Weight CNN for classification of Brain Tumors from MRI Images","summary":"  This study presents a convolutional neural network (CNN)-based approach for\nthe multi-class classification of brain tumors using magnetic resonance imaging\n(MRI) scans. We utilize a publicly available dataset containing MRI images\ncategorized into four classes: glioma, meningioma, pituitary tumor, and no\ntumor. Our primary objective is to build a light weight deep learning model\nthat can automatically classify brain tumor types with high accuracy. To\nachieve this goal, we incorporate image preprocessing steps, including\nnormalization, data augmentation, and a cropping technique designed to reduce\nbackground noise and emphasize relevant regions. The CNN architecture is\noptimized through hyperparameter tuning using Keras Tuner, enabling systematic\nexploration of network parameters. To ensure reliable evaluation, we apply\n5-fold cross-validation, where each hyperparameter configuration is evaluated\nacross multiple data splits to mitigate overfitting. Experimental results\ndemonstrate that the proposed model achieves a classification accuracy of\n98.78%, indicating its potential as a diagnostic aid in clinical settings. The\nproposed method offers a low-complexity yet effective solution for assisting in\nearly brain tumor diagnosis.\n","authors":["Natnael Alemayehu"],"pdf_url":"https://arxiv.org/pdf/2504.21188v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2411.02179v2","updated":"2025-05-05T19:58:02Z","published":"2024-11-04T15:37:18Z","title":"CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile\n  Augmented Reality","summary":"  High-quality environment lighting is essential for creating immersive mobile\naugmented reality (AR) experiences. However, achieving visually coherent\nestimation for mobile AR is challenging due to several key limitations in AR\ndevice sensing capabilities, including low camera FoV and limited pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address two key limitations of\ncontent quality and slow inference. In this work, we design and implement a\ngenerative lighting estimation system called CleAR that can produce\nhigh-quality, diverse environment maps in the format of 360{\\deg} HDR images.\nSpecifically, we design a two-step generation pipeline guided by AR environment\ncontext data to ensure the output aligns with the physical environment's visual\ncontext and color appearance. To improve the estimation robustness under\ndifferent lighting conditions, we design a real-time refinement component to\nadjust lighting estimation results on AR devices. Through a combination of\nquantitative and qualitative evaluations, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy,\nlatency, and robustness, and is rated by 31 participants as producing better\nrenderings for most virtual objects. For example, CleAR achieves 51% to 56%\naccuracy improvement on virtual object renderings across objects of three\ndistinctive types of materials and reflective properties. CleAR produces\nlighting estimates of comparable or better quality in just 3.2 seconds -- over\n110X faster than state-of-the-art methods.\n","authors":["Yiqin Zhao","Mallesham Dasari","Tian Guo"],"pdf_url":"https://arxiv.org/pdf/2411.02179v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07758v5","updated":"2025-05-05T19:20:22Z","published":"2025-02-11T18:38:02Z","title":"Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras","summary":"  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n","authors":["Nektarios A. Valous","Eckhard Hitzer","Dragoş Duşe","Rodrigo Rojas Moraleda","Ferdinand Popp","Meggy Suarez-Carmona","Anna Berthel","Ismini Papageorgiou","Carlo Fremd","Alexander Rölle","Christina C. Westhoff","Bénédicte Lenoir","Niels Halama","Inka Zörnig","Dirk Jäger"],"pdf_url":"https://arxiv.org/pdf/2502.07758v5.pdf","comment":"24 pages, 18 figures, 14 tables"},{"id":"http://arxiv.org/abs/2505.02980v1","updated":"2025-05-05T19:17:29Z","published":"2025-05-05T19:17:29Z","title":"Completing Spatial Transcriptomics Data for Gene Expression Prediction\n  Benchmarking","summary":"  Spatial Transcriptomics is a groundbreaking technology that integrates\nhistology images with spatially resolved gene expression profiles. Among the\nvarious Spatial Transcriptomics techniques available, Visium has emerged as the\nmost widely adopted. However, its accessibility is limited by high costs, the\nneed for specialized expertise, and slow clinical integration. Additionally,\ngene capture inefficiencies lead to significant dropout, corrupting acquired\ndata. To address these challenges, the deep learning community has explored the\ngene expression prediction task directly from histology images. Yet,\ninconsistencies in datasets, preprocessing, and training protocols hinder fair\ncomparisons between models. To bridge this gap, we introduce SpaRED, a\nsystematically curated database comprising 26 public datasets, providing a\nstandardized resource for model evaluation. We further propose SpaCKLE, a\nstate-of-the-art transformer-based gene expression completion model that\nreduces mean squared error by over 82.5% compared to existing approaches.\nFinally, we establish the SpaRED benchmark, evaluating eight state-of-the-art\nprediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE\nsubstantially improves the results across all the gene expression prediction\nmodels. Altogether, our contributions constitute the most comprehensive\nbenchmark of gene expression prediction from histology images to date and a\nstepping stone for future research on Spatial Transcriptomics.\n","authors":["Daniela Ruiz","Paula Cardenas","Leonardo Manrique","Daniela Vega","Gabriel Mejia","Pablo Arbelaez"],"pdf_url":"https://arxiv.org/pdf/2505.02980v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2407.13027"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2410.10807v2","updated":"2025-05-05T17:58:28Z","published":"2024-10-14T17:59:24Z","title":"Hard-Constrained Neural Networks with Universal Approximation Guarantees","summary":"  Incorporating prior knowledge or specifications of input-output relationships\ninto machine learning models has gained significant attention, as it enhances\ngeneralization from limited data and leads to conforming outputs. However, most\nexisting approaches use soft constraints by penalizing violations through\nregularization, which offers no guarantee of constraint satisfaction--an\nessential requirement in safety-critical applications. On the other hand,\nimposing hard constraints on neural networks may hinder their representational\npower, adversely affecting performance. To address this, we propose HardNet, a\npractical framework for constructing neural networks that inherently satisfy\nhard constraints without sacrificing model capacity. Unlike approaches that\nmodify outputs only at inference time, HardNet enables end-to-end training with\nhard constraint guarantees, leading to improved performance. To the best of our\nknowledge, HardNet is the first method with an efficient forward pass to\nenforce more than one input-dependent inequality constraint. It allows\nunconstrained optimization of the network parameters using standard algorithms\nby appending a differentiable closed-form enforcement layer to the network's\noutput. Furthermore, we show that HardNet retains the universal approximation\ncapabilities of neural networks. We demonstrate the versatility and\neffectiveness of HardNet across various applications: learning with piecewise\nconstraints, learning optimization solvers, optimizing control policies in\nsafety-critical systems, and learning safe decision logic for aircraft systems.\n","authors":["Youngjae Min","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2410.10807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02829v1","updated":"2025-05-05T17:56:25Z","published":"2025-05-05T17:56:25Z","title":"LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery","summary":"  Segmentation models can recognize a pre-defined set of objects in images.\nHowever, models that can reason over complex user queries that implicitly refer\nto multiple objects of interest are still in their infancy. Recent advances in\nreasoning segmentation--generating segmentation masks from complex, implicit\nquery text--demonstrate that vision-language models can operate across an open\ndomain and produce reasonable outputs. However, our experiments show that such\nmodels struggle with complex remote-sensing imagery. In this work, we introduce\nLISAt, a vision-language model designed to describe complex remote-sensing\nscenes, answer questions about them, and segment objects of interest. We\ntrained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,\nwith 27,615 annotations over 9,205 images, and a multimodal pretraining\ndataset, PreGRES, containing over 1 million question-answer pairs. LISAt\noutperforms existing geospatial foundation models such as RS-GPT4V by over\n10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses\nstate-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %\n(gIoU). Our model, datasets, and code are available at\nhttps://lisat-bair.github.io/LISAt/\n","authors":["Jerome Quenum","Wen-Han Hsieh","Tsung-Han Wu","Ritwik Gupta","Trevor Darrell","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2505.02829v1.pdf","comment":"28 pages, 10 figures, 19 tables"},{"id":"http://arxiv.org/abs/2503.10822v3","updated":"2025-05-05T17:53:39Z","published":"2025-03-13T19:13:51Z","title":"Reinforcement Learning and Life Cycle Assessment for a Circular Economy\n  -- Towards Progressive Computer Science","summary":"  The aim of this paper is to discuss the potential of using methods from\nReinforcement Learning for Life Cycle Assessment in a circular economy, and to\npresent some new ideas in this direction. To give some context, we explain how\nReinforcement Learning was successfully applied in computer chess (and beyond).\nAs computer chess was historically called the \"drosophila of AI\", we start by\ndescribing a method for the board representation called 'rotated bitboards'\nthat can potentially also be applied in the context of sustainability. In the\nfirst part of this paper, the concepts of the bitboard-representation and the\nadvantages of (rotated) bitboards in move generation are explained. In order to\nillustrate those ideas practice, the concrete implementation of the\nmove-generator in FUSc# (a chess engine developed at FU Berlin in C# some years\nago) is described. In addition, rotated binary neural networks are discussed\nbriefly.\n  The second part deals with reinforcement learning in computer chess (and\nbeyond). We exemplify the progress that has been made in this field in the last\n15-20 years by comparing the \"state of the art\" from 2002-2008, when FUSc# was\ndeveloped, with the ground-breaking innovations connected to \"AlphaZero\". We\nreview some application of the ideas developed in AlphaZero in other domains,\ne.g. the \"other Alphas\" like AlphaFold, AlphaTensor, AlphaGeometry and\nAlphaProof. In the final part of the paper, we discuss the computer-science\nrelated challenges that changing the economic paradigm towards (absolute)\nsustainability poses and in how far what we call 'progressive computer science'\nneeds to contribute. Concrete challenges include the closing of material loops\nin a circular economy with Life Cycle Assessment in order to optimize for\n(absolute) sustainability, and we present some new ideas in this direction.\n","authors":["Johannes Buchner"],"pdf_url":"https://arxiv.org/pdf/2503.10822v3.pdf","comment":"Shortened conference paper version, with a new section on Life Cycle\n  Assessment, and thus a new title"},{"id":"http://arxiv.org/abs/2505.02828v1","updated":"2025-05-05T17:53:28Z","published":"2025-05-05T17:53:28Z","title":"Privacy Risks and Preservation Methods in Explainable Artificial\n  Intelligence: A Scoping Review","summary":"  Explainable Artificial Intelligence (XAI) has emerged as a pillar of\nTrustworthy AI and aims to bring transparency in complex models that are opaque\nby nature. Despite the benefits of incorporating explanations in models, an\nurgent need is found in addressing the privacy concerns of providing this\nadditional information to end users. In this article, we conduct a scoping\nreview of existing literature to elicit details on the conflict between privacy\nand explainability. Using the standard methodology for scoping review, we\nextracted 57 articles from 1,943 studies published from January 2019 to\nDecember 2024. The review addresses 3 research questions to present readers\nwith more understanding of the topic: (1) what are the privacy risks of\nreleasing explanations in AI systems? (2) what current methods have researchers\nemployed to achieve privacy preservation in XAI systems? (3) what constitutes a\nprivacy preserving explanation? Based on the knowledge synthesized from the\nselected studies, we categorize the privacy risks and preservation methods in\nXAI and propose the characteristics of privacy preserving explanations to aid\nresearchers and practitioners in understanding the requirements of XAI that is\nprivacy compliant. Lastly, we identify the challenges in balancing privacy with\nother system desiderata and provide recommendations for achieving privacy\npreserving XAI. We expect that this review will shed light on the complex\nrelationship of privacy and explainability, both being the fundamental\nprinciples of Trustworthy AI.\n","authors":["Sonal Allana","Mohan Kankanhalli","Rozita Dara"],"pdf_url":"https://arxiv.org/pdf/2505.02828v1.pdf","comment":"Submitted for peer review"},{"id":"http://arxiv.org/abs/2505.02824v1","updated":"2025-05-05T17:51:55Z","published":"2025-05-05T17:51:55Z","title":"Towards Dataset Copyright Evasion Attack against Personalized\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance.\n","authors":["Kuofeng Gao","Yufei Zhu","Yiming Li","Jiawang Bai","Yong Yang","Zhifeng Li","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2505.02824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04592v2","updated":"2025-05-05T17:48:40Z","published":"2025-04-06T19:29:45Z","title":"\"Trust me on this\" Explaining Agent Behavior to a Human Terminator","summary":"  Consider a setting where a pre-trained agent is operating in an environment\nand a human operator can decide to temporarily terminate its operation and\ntake-over for some duration of time. These kind of scenarios are common in\nhuman-machine interactions, for example in autonomous driving, factory\nautomation and healthcare. In these settings, we typically observe a trade-off\nbetween two extreme cases -- if no take-overs are allowed, then the agent might\nemploy a sub-optimal, possibly dangerous policy. Alternatively, if there are\ntoo many take-overs, then the human has no confidence in the agent, greatly\nlimiting its usefulness. In this paper, we formalize this setup and propose an\nexplainability scheme to help optimize the number of human interventions.\n","authors":["Uri Menkes","Assaf Hallak","Ofra Amir"],"pdf_url":"https://arxiv.org/pdf/2504.04592v2.pdf","comment":"6 pages, 3 figures, in proceedings of ICML 2024 Workshop on Models of\n  Human Feedback for AI Alignment"},{"id":"http://arxiv.org/abs/2505.02820v1","updated":"2025-05-05T17:47:49Z","published":"2025-05-05T17:47:49Z","title":"AutoLibra: Agent Metric Induction from Open-Ended Feedback","summary":"  Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents.\n","authors":["Hao Zhu","Phil Cuvin","Xinkai Yu","Charlotte Ka Yee Yan","Jason Zhang","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2505.02820v1.pdf","comment":"https://opensocial.world/"},{"id":"http://arxiv.org/abs/2505.02811v1","updated":"2025-05-05T17:39:35Z","published":"2025-05-05T17:39:35Z","title":"Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing","summary":"  Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework,\n\\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and\nmulti-round retrieval capabilities. To train SIM-RAG, we first let a RAG system\nself-practice multi-round retrieval, augmenting existing question-answer pairs\nwith intermediate inner monologue reasoning steps to generate synthetic\ntraining data. For each pair, the system may explore multiple retrieval paths,\nwhich are labeled as successful if they reach the correct answer and\nunsuccessful otherwise. Using this data, we train a lightweight information\nsufficiency Critic. At inference time, the Critic evaluates whether the RAG\nsystem has retrieved sufficient information at each round, guiding retrieval\ndecisions and improving system-level self-awareness through in-context\nreinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an\neffective multi-round RAG solution. Furthermore, this framework is\nsystem-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data.\n","authors":["Diji Yang","Linda Zeng","Jinmeng Rao","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.02811v1.pdf","comment":"Proceedings of the 48th International ACM SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.18794v2","updated":"2025-05-05T17:21:55Z","published":"2025-04-26T04:30:10Z","title":"Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation\n  with Autonomous Mobile Robots","summary":"  Hierarchical reinforcement learning (HRL) is hypothesized to be able to take\nadvantage of the inherent hierarchy in robot learning tasks with sparse reward\nschemes, in contrast to more traditional reinforcement learning algorithms. In\nthis research, hierarchical reinforcement learning is evaluated and contrasted\nwith standard reinforcement learning in complex navigation tasks. We evaluate\nunique characteristics of HRL, including their ability to create sub-goals and\nthe termination function. We constructed experiments to test the differences\nbetween PPO and HRL, different ways of creating sub-goals, manual vs automatic\nsub-goal creation, and the effects of the frequency of termination on\nperformance. These experiments highlight the advantages of HRL and how it\nachieves these advantages.\n","authors":["Brendon Johnson","Alfredo Weitzenfeld"],"pdf_url":"https://arxiv.org/pdf/2504.18794v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02795v1","updated":"2025-05-05T17:09:19Z","published":"2025-05-05T17:09:19Z","title":"HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning\n  Framework for Large Language Models","summary":"  Recently, large language models (LLMs) have achieved remarkable\nbreakthroughs, revolutionizing the natural language processing domain and\nbeyond. Due to immense parameter sizes, fine-tuning these models with private\ndata for diverse downstream tasks has become mainstream. Though federated\nlearning (FL) offers a promising solution for fine-tuning LLMs without sharing\nraw data, substantial computing costs hinder its democratization. Moreover, in\nreal-world scenarios, private client devices often possess heterogeneous\ncomputing resources, further complicating LLM fine-tuning. To combat these\nchallenges, we propose HSplitLoRA, a heterogeneous parameter-efficient\nfine-tuning (PEFT) framework built on split learning (SL) and low-rank\nadaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on\nheterogeneous client devices. HSplitLoRA first identifies important weights\nbased on their contributions to LLM training. It then dynamically configures\nthe decomposition ranks of LoRA adapters for selected weights and determines\nthe model split point according to varying computing budgets of client devices.\nFinally, a noise-free adapter aggregation mechanism is devised to support\nheterogeneous adapter aggregation without introducing noise. Extensive\nexperiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks\nin training accuracy and convergence speed.\n","authors":["Zheng Lin","Yuxin Zhang","Zhe Chen","Zihan Fang","Xianhao Chen","Praneeth Vepakomma","Wei Ni","Jun Luo","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2505.02795v1.pdf","comment":"16 pages, 22 figures"},{"id":"http://arxiv.org/abs/2503.09805v2","updated":"2025-05-05T17:07:07Z","published":"2025-03-12T20:16:38Z","title":"Un-Straightening Generative AI: How Queer Artists Surface and Challenge\n  the Normativity of Generative AI Models","summary":"  Queer people are often discussed as targets of bias, harm, or discrimination\nin research on generative AI. However, the specific ways that queer people\nengage with generative AI, and thus possible uses that support queer people,\nhave yet to be explored. We conducted a workshop study with 13 queer artists,\nduring which we gave participants access to GPT-4 and DALL-E 3 and facilitated\ngroup sensemaking activities. We found our participants struggled to use these\nmodels due to various normative values embedded in their designs, such as\nhyper-positivity and anti-sexuality. We describe various strategies our\nparticipants developed to overcome these models' limitations and how,\nnevertheless, our participants found value in these highly-normative\ntechnologies. Drawing on queer feminist theory, we discuss implications for the\nconceptualization of \"state-of-the-art\" models and consider how FAccT\nresearchers might support queer alternatives.\n","authors":["Jordan Taylor","Joel Mire","Franchesca Spektor","Alicia DeVrio","Maarten Sap","Haiyi Zhu","Sarah Fox"],"pdf_url":"https://arxiv.org/pdf/2503.09805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05929v4","updated":"2025-05-05T16:48:19Z","published":"2024-09-09T10:40:50Z","title":"M3-Jepa: Multimodal Alignment via Multi-directional MoE based on the\n  JEPA framework","summary":"  Current multimodal alignment strategies primarily use single or unified\nmodality encoders, while optimizing the alignment on the original token space.\nSuch a framework is easy to implement and incorporate with the pretrained\nknowledge, but might result in information bias. To deal with such issues, the\njoint encoding predictive architecture (JEPA) learns the alignment loss on the\nlatent space, with a predictor to convert the input encoding to the output\nlatent space. However, the application of JEPA in multimodal scenarios is\nlimited so far. In this paper, we introduce M3-Jepa, a scalable multimodal\nalignment framework, with the predictor implemented by a multi-directional\nmixture of experts (MoE). We demonstrate the framework can maximize the mutual\ninformation with information theory derivations, by alternating the\noptimization between different uni-directional tasks. By thoroughly designed\nexperiments, we show that M3-Jepa can obtain state-of-the-art performance on\ndifferent modalities and tasks, generalize to unseen datasets and domains, and\nis computationally efficient in training and inference. Our study indicates\nthat M3-Jepa might provide a new paradigm to self-supervised learning and\nopen-world modeling.\n","authors":["Hongyang Lei","Xiaolong Cheng","Dan Wang","Kun Fan","Qi Qin","Huazhen Huang","Yetao Wu","Qingqing Gu","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.05929v4.pdf","comment":"13 pages, 4 figures. Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.02781v1","updated":"2025-05-05T16:47:29Z","published":"2025-05-05T16:47:29Z","title":"Local Markov Equivalence and Local Causal Discovery for Identifying\n  Controlled Direct Effects","summary":"  Understanding and identifying controlled direct effects (CDEs) is crucial\nacross numerous scientific domains, including public health. While existing\nmethods can identify these effects from causal directed acyclic graphs (DAGs),\nthe true underlying structure is often unknown in practice. Essential graphs,\nwhich represent a Markov equivalence class of DAGs characterized by the same\nset of d-separations, provide a more practical and realistic alternative.\nHowever, learning the full essential graph is computationally intensive and\ntypically depends on strong, untestable assumptions. In this work, we\ncharacterize a local class of graphs, defined relative to a target variable,\nthat share a specific subset of d-separations, and introduce a graphical\nrepresentation of this class, called the local essential graph (LEG). We then\npresent LocPC, a novel algorithm designed to recover the LEG from an observed\ndistribution using only local conditional independence tests. Building on\nLocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG\nthat is sufficient to identify a CDE, bypassing the need of retrieving the full\nessential graph. Compared to global methods, our algorithms require less\nconditional independence tests and operate under weaker assumptions while\nmaintaining theoretical guarantees.\n","authors":["Timothée Loranchet","Charles K. Assaad"],"pdf_url":"https://arxiv.org/pdf/2505.02781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02780v1","updated":"2025-05-05T16:46:53Z","published":"2025-05-05T16:46:53Z","title":"Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced\n  Digital Pathology Workflow","summary":"  Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases\nlike cancer, yet current digital pathology tools hinder diagnosis. The immense\nscale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the\nlimited views traditional monitors offer. This mismatch forces constant panning\nand zooming, increasing pathologist cognitive load, causing diagnostic fatigue,\nand slowing pathologists' adoption of digital methods. PathVis, our\nmixed-reality visualization platform for Apple Vision Pro, addresses these\nchallenges. It transforms the pathologist's interaction with data, replacing\ncumbersome mouse-and-monitor navigation with intuitive exploration using\nnatural hand gestures, eye gaze, and voice commands in an immersive workspace.\nPathVis integrates AI to enhance diagnosis. An AI-driven search function\ninstantly retrieves and displays the top five similar patient cases\nside-by-side, improving diagnostic precision and efficiency through rapid\ncomparison. Additionally, a multimodal conversational AI assistant offers\nreal-time image interpretation support and aids collaboration among\npathologists across multiple Apple devices. By merging the directness of\ntraditional pathology with advanced mixed-reality visualization and AI, PathVis\nimproves diagnostic workflows, reduces cognitive strain, and makes pathology\npractice more effective and engaging. The PathVis source code and a demo video\nare publicly available at: https://github.com/jaiprakash1824/Path_Vis\n","authors":["Jai Prakash Veerla","Partha Sai Guttikonda","Helen H. Shang","Mohammad Sadegh Nasr","Cesar Torres","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2505.02780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04429v2","updated":"2025-05-05T16:39:22Z","published":"2025-03-06T13:38:44Z","title":"Activation Space Interventions Can Be Transferred Between Large Language\n  Models","summary":"  The study of representation universality in AI models reveals growing\nconvergence across domains, modalities, and architectures. However, the\npractical applications of representation universality remain largely\nunexplored. We bridge this gap by demonstrating that safety interventions can\nbe transferred between models through learned mappings of their shared\nactivation spaces. We demonstrate this approach on two well-established AI\nsafety tasks: backdoor removal and refusal of harmful prompts, showing\nsuccessful transfer of steering vectors that alter the models' outputs in a\npredictable way. Additionally, we propose a new task, \\textit{corrupted\ncapabilities}, where models are fine-tuned to embed knowledge tied to a\nbackdoor. This tests their ability to separate useful skills from backdoors,\nreflecting real-world challenges. Extensive experiments across Llama, Qwen and\nGemma model families show that our method enables using smaller models to\nefficiently align larger ones. Furthermore, we demonstrate that autoencoder\nmappings between base and fine-tuned models can serve as reliable ``lightweight\nsafety switches\", allowing dynamic toggling between model behaviors.\n","authors":["Narmeen Oozeer","Dhruv Nathawani","Nirmalendu Prakash","Michael Lan","Abir Harrasse","Amirali Abdullah"],"pdf_url":"https://arxiv.org/pdf/2503.04429v2.pdf","comment":"68 pages"},{"id":"http://arxiv.org/abs/2412.10316v3","updated":"2025-05-05T16:31:12Z","published":"2024-12-13T17:58:06Z","title":"BrushEdit: All-In-One Image Inpainting and Editing","summary":"  Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.\n","authors":["Yaowei Li","Yuxuan Bian","Xuan Ju","Zhaoyang Zhang","Junhao Zhuang","Ying Shan","Yuexian Zou","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2412.10316v3.pdf","comment":"WebPage available at\n  https://liyaowei-stu.github.io/project/BrushEdit/"},{"id":"http://arxiv.org/abs/2503.05153v2","updated":"2025-05-05T16:26:30Z","published":"2025-03-07T05:22:52Z","title":"Generative Trajectory Stitching through Diffusion Composition","summary":"  Effective trajectory stitching for long-horizon planning is a significant\nchallenge in robotic decision-making. While diffusion models have shown promise\nin planning, they are limited to solving tasks similar to those seen in their\ntraining data. We propose CompDiffuser, a novel generative approach that can\nsolve new tasks by learning to compositionally stitch together shorter\ntrajectory chunks from previously seen tasks. Our key insight is modeling the\ntrajectory distribution by subdividing it into overlapping chunks and learning\ntheir conditional relationships through a single bidirectional diffusion model.\nThis allows information to propagate between segments during generation,\nensuring physically consistent connections. We conduct experiments on benchmark\ntasks of various difficulties, covering different environment sizes, agent\nstate dimension, trajectory types, training data quality, and show that\nCompDiffuser significantly outperforms existing methods.\n","authors":["Yunhao Luo","Utkarsh A. Mishra","Yilun Du","Danfei Xu"],"pdf_url":"https://arxiv.org/pdf/2503.05153v2.pdf","comment":"Project page: https://comp-diffuser.github.io/"},{"id":"http://arxiv.org/abs/2505.02766v1","updated":"2025-05-05T16:21:46Z","published":"2025-05-05T16:21:46Z","title":"Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models\n  for Cellular Control","summary":"  Guiding biological systems toward desired states, such as morphogenetic\noutcomes, remains a fundamental challenge with far-reaching implications for\nmedicine and synthetic biology. While large language models (LLMs) have enabled\nnatural language as an interface for interpretable control in AI systems, their\nuse as mediators for steering biological or cellular dynamics remains largely\nunexplored.\n  In this work, we present a functional pipeline that translates natural\nlanguage prompts into spatial vector fields capable of directing simulated\ncellular collectives. Our approach combines a large language model with an\nevolvable neural controller (Prompt-to-Intervention, or P2I), optimized via\nevolutionary strategies to generate behaviors such as clustering or scattering\nin a simulated 2D environment.\n  We demonstrate that even with constrained vocabulary and simplified cell\nmodels, evolved P2I networks can successfully align cellular dynamics with\nuser-defined goals expressed in plain language. This work offers a complete\nloop from language input to simulated bioelectric-like intervention to\nbehavioral output, providing a foundation for future systems capable of natural\nlanguage-driven cellular control.\n","authors":["Nam H. Le","Patrick Erikson","Yanbo Zhang","Michael Levin","Josh Bongard"],"pdf_url":"https://arxiv.org/pdf/2505.02766v1.pdf","comment":"Accepted to GECCO Workshop on Bio-Inspired AI (ACM GECCO2025). 13\n  pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.02763v1","updated":"2025-05-05T16:18:07Z","published":"2025-05-05T16:18:07Z","title":"Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models","summary":"  Legal practice requires careful adherence to procedural rules. In the United\nStates, few are more complex than those found in The Bluebook: A Uniform System\nof Citation. Compliance with this system's 500+ pages of byzantine formatting\ninstructions is the raison d'etre of thousands of student law review editors\nand the bete noire of lawyers everywhere. To evaluate whether large language\nmodels (LLMs) are able to adhere to the procedures of such a complicated\nsystem, we construct an original dataset of 866 Bluebook tasks and test\nflagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)\nthat these models produce fully compliant Bluebook citations only 69%-74% of\nthe time and (2) that in-context learning on the Bluebook's underlying system\nof rules raises accuracy only to 77%. These results caution against using\noff-the-shelf LLMs to automate aspects of the law where fidelity to procedure\nis paramount.\n","authors":["Matthew Dahl"],"pdf_url":"https://arxiv.org/pdf/2505.02763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02747v1","updated":"2025-05-05T15:58:32Z","published":"2025-05-05T15:58:32Z","title":"The use of Artificial Intelligence for Intervention and Assessment in\n  Individuals with ASD","summary":"  This paper explores the use of Artificial Intelligence (AI) as a tool for\ndiagnosis, assessment, and intervention for individuals with Autism Spectrum\nDisorder (ASD). It focuses particularly on AI's role in early diagnosis,\nutilizing advanced machine learning techniques and data analysis. Recent\nstudies demonstrate that deep learning algorithms can identify behavioral\npatterns through biometric data analysis, video-based interaction assessments,\nand linguistic feature extraction, providing a more accurate and timely\ndiagnosis compared to traditional methods. Additionally, AI automates\ndiagnostic tools, reducing subjective biases and enabling the development of\npersonalized assessment protocols for ASD monitoring. At the same time, the\npaper examines AI-powered intervention technologies, emphasizing educational\nrobots and adaptive communication tools. Social robotic assistants, such as NAO\nand Kaspar, have been shown to enhance social skills in children by offering\nstructured, repetitive interactions that reinforce learning. Furthermore,\nAI-driven Augmentative and Alternative Communication (AAC) systems allow\nchildren with ASD to express themselves more effectively, while\nmachine-learning chatbots provide language development support through\npersonalized responses. The study presents research findings supporting the\neffectiveness of these AI applications while addressing challenges such as\nlong-term evaluation and customization to individual needs. In conclusion, the\npaper highlights the significance of AI as an innovative tool in ASD diagnosis\nand intervention, advocating for further research to assess its long-term\nimpact.\n","authors":["Aggeliki Sideraki","Christos-Nikolaos Anagnostopoulos"],"pdf_url":"https://arxiv.org/pdf/2505.02747v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.03954v2","updated":"2025-05-05T15:55:16Z","published":"2024-10-04T22:32:08Z","title":"SDA-GRIN for Adaptive Spatial-Temporal Multivariate Time Series\n  Imputation","summary":"  In various applications, the multivariate time series often suffers from\nmissing data. This issue can significantly disrupt systems that rely on the\ndata. Spatial and temporal dependencies can be leveraged to impute the missing\nsamples. Existing imputation methods often ignore dynamic changes in spatial\ndependencies. We propose a Spatial Dynamic Aware Graph Recurrent Imputation\nNetwork (SDA-GRIN) which is capable of capturing dynamic changes in spatial\ndependencies.SDA-GRIN leverages a multi-head attention mechanism to adapt graph\nstructures with time. SDA-GRIN models multivariate time series as a sequence of\ntemporal graphs and uses a recurrent message-passing architecture for\nimputation. We evaluate SDA-GRIN on four real-world datasets: SDA-GRIN improves\nMSE by 9.51% for the AQI and 9.40% for AQI-36. On the PEMS-BAY dataset, it\nachieves a 1.94% improvement in MSE. Detailed ablation study demonstrates the\neffect of window sizes and missing data on the performance of the method.\nProject page:https://ameskandari.github.io/sda-grin/\n","authors":["Amir Eskandari","Aman Anand","Drishti Sharma","Farhana Zulkernine"],"pdf_url":"https://arxiv.org/pdf/2410.03954v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10098v2","updated":"2025-05-05T15:41:55Z","published":"2025-01-17T10:35:58Z","title":"landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D\n  Images","summary":"  Anatomical landmark localization in 2D/3D images is a critical task in\nmedical imaging. Although many general-purpose tools exist for landmark\nlocalization in classical computer vision tasks, such as pose estimation, they\nlack the specialized features and modularity necessary for anatomical landmark\nlocalization applications in the medical domain. Therefore, we introduce\nlandmarker, a Python package built on PyTorch. The package provides a\ncomprehensive, flexible toolkit for developing and evaluating landmark\nlocalization algorithms, supporting a range of methodologies, including static\nand adaptive heatmap regression. landmarker enhances the accuracy of landmark\nidentification, streamlines research and development processes, and supports\nvarious image formats and preprocessing pipelines. Its modular design allows\nusers to customize and extend the toolkit for specific datasets and\napplications, accelerating innovation in medical imaging. landmarker addresses\na critical need for precision and customization in landmark localization tasks\nnot adequately met by existing general-purpose pose estimation tools.\n","authors":["Jef Jonkers","Luc Duchateau","Glenn Van Wallendael","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2501.10098v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.02737v1","updated":"2025-05-05T15:40:24Z","published":"2025-05-05T15:40:24Z","title":"Knowledge Graphs for Enhancing Large Language Models in Entity\n  Disambiguation","summary":"  Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance.\n","authors":["Pons Gerard","Bilalli Besim","Queralt Anna"],"pdf_url":"https://arxiv.org/pdf/2505.02737v1.pdf","comment":"Pre-print submitted to ISWC 2024"},{"id":"http://arxiv.org/abs/2505.02735v1","updated":"2025-05-05T15:37:00Z","published":"2025-05-05T15:37:00Z","title":"FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models","summary":"  Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.\n","authors":["Zhouliang Yu","Ruotian Peng","Keyi Ding","Yizhe Li","Zhongyuan Peng","Minghao Liu","Yifan Zhang","Zheng Yuan","Huajian Xin","Wenhao Huang","Yandong Wen","Ge Zhang","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2505.02735v1.pdf","comment":"Technical Report v1 (33 pages, 8 figures, project page:\n  https://sphere-ai-lab.github.io/FormalMATH/)"},{"id":"http://arxiv.org/abs/2504.20834v2","updated":"2025-05-05T15:36:15Z","published":"2025-04-29T14:58:43Z","title":"Token-Efficient RL for LLM Reasoning","summary":"  We propose reinforcement learning (RL) strategies tailored for reasoning in\nlarge language models (LLMs) under strict memory and compute limits, with a\nparticular focus on compatibility with LoRA fine-tuning. Rather than relying on\nfull-sequence updates or separate critic networks, we design critic-free\nmethods that operate on a small, informative subset of output tokens to reduce\nmemory usage and stabilize training. We introduce S-GRPO, a stochastic variant\nof Group Relative Policy Optimization, and T-SPMO, a token-level prefix\nmatching approach for fine-grained credit assignment. Applied to Qwen2-1.5B,\nour methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show\nstrong performance on multi-digit multiplication. Surprisingly, full-token GRPO\nunder LoRA fails to improve over the base model, suggesting that selective\ntoken-level optimization may act as an implicit regularizer in low-parameter\ntraining regimes.\n","authors":["Alan Lee","Harry Tong"],"pdf_url":"https://arxiv.org/pdf/2504.20834v2.pdf","comment":"Title updated to \"Token-Efficient RL for LLM Reasoning\" to better\n  reflect algorithmic focus. Revised abstract, intro, and conclusion. Paper\n  shortened and typos fixed"},{"id":"http://arxiv.org/abs/2404.02225v2","updated":"2025-05-05T15:35:26Z","published":"2024-04-02T18:27:03Z","title":"CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement","summary":"  We propose CHOSEN, a simple yet flexible, robust and effective multi-view\ndepth refinement framework. It can be employed in any existing multi-view\nstereo pipeline, with straightforward generalization capability for different\nmulti-view capture systems such as camera relative positioning and lenses.\nGiven an initial depth estimation, CHOSEN iteratively re-samples and selects\nthe best hypotheses, and automatically adapts to different metric or intrinsic\nscales determined by the capture system. The key to our approach is the\napplication of contrastive learning in an appropriate solution space and a\ncarefully designed hypothesis feature, based on which positive and negative\nhypotheses can be effectively distinguished. Integrated in a simple baseline\nmulti-view stereo pipeline, CHOSEN delivers impressive quality in terms of\ndepth and normal accuracy compared to many current deep learning based\nmulti-view stereo pipelines.\n","authors":["Di Qiu","Yinda Zhang","Thabo Beeler","Vladimir Tankovich","Christian Häne","Sean Fanello","Christoph Rhemann","Sergio Orts Escolano"],"pdf_url":"https://arxiv.org/pdf/2404.02225v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00316v2","updated":"2025-05-05T15:26:29Z","published":"2025-05-01T05:30:38Z","title":"Surrogate modeling of Cellular-Potts Agent-Based Models as a\n  segmentation task using the U-Net neural network architecture","summary":"  The Cellular-Potts model is a powerful and ubiquitous framework for\ndeveloping computational models for simulating complex multicellular biological\nsystems. Cellular-Potts models (CPMs) are often computationally expensive due\nto the explicit modeling of interactions among large numbers of individual\nmodel agents and diffusive fields described by partial differential equations\n(PDEs). In this work, we develop a convolutional neural network (CNN) surrogate\nmodel using a U-Net architecture that accounts for periodic boundary\nconditions. We use this model to accelerate the evaluation of a mechanistic CPM\npreviously used to investigate \\textit{in vitro} vasculogenesis. The surrogate\nmodel was trained to predict 100 computational steps ahead (Monte-Carlo steps,\nMCS), accelerating simulation evaluations by a factor of 590 times compared to\nCPM code execution. Over multiple recursive evaluations, our model effectively\ncaptures the emergent behaviors demonstrated by the original Cellular-Potts\nmodel of such as vessel sprouting, extension and anastomosis, and contraction\nof vascular lacunae. This approach demonstrates the potential for deep learning\nto serve as efficient surrogate models for CPM simulations, enabling faster\nevaluation of computationally expensive CPM of biological processes at greater\nspatial and temporal scales.\n","authors":["Tien Comlekoglu","J. Quetzalcóatl Toledo-Marín","Tina Comlekoglu","Douglas W. DeSimone","Shayn M. Peirce","Geoffrey Fox","James A. Glazier"],"pdf_url":"https://arxiv.org/pdf/2505.00316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02722v1","updated":"2025-05-05T15:23:47Z","published":"2025-05-05T15:23:47Z","title":"Enhancing LLMs' Clinical Reasoning with Real-World Data from a\n  Nationwide Sepsis Registry","summary":"  Although large language models (LLMs) have demonstrated impressive reasoning\ncapabilities across general domains, their effectiveness in real-world clinical\npractice remains limited. This is likely due to their insufficient exposure to\nreal-world clinical data during training, as such data is typically not\nincluded due to privacy concerns. To address this, we propose enhancing the\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\nin-domain test set, as evidenced by both quantitative metrics and expert\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\nsepsis dataset involving different tasks and patient cohorts, an open-ended\nconsultations on antibiotics use task, and other diseases. Future research\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\nto develop more powerful, general-purpose clinical reasoning models.\n","authors":["Junu Kim","Chaeeun Shim","Sungjin Park","Su Yeon Lee","Gee Young Suh","Chae-Man Lim","Seong Jin Choi","Song Mi Moon","Kyoung-Ho Song","Eu Suk Kim","Hong Bin Kim","Sejoong Kim","Chami Im","Dong-Wan Kang","Yong Soo Kim","Hee-Joon Bae","Sung Yoon Lim","Han-Gil Jeong","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2505.02722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02636v3","updated":"2025-05-05T15:16:31Z","published":"2025-03-04T14:01:10Z","title":"YARE-GAN: Yet Another Resting State EEG-GAN","summary":"  In this study, we implement a Wasserstein GAN with Gradient Penalty (WGAN-GP)\nto generate multi-channel resting-state EEG data and assess the quality of the\nsynthesized signals through both visual and feature-based evaluations. Our\nresults indicate that the model effectively captures the statistical and\nspectral characteristics of real EEG data, although challenges remain in\nreplicating high-frequency oscillations in the frontal region. Additionally, we\ndemonstrate that the Critic's learned representations can be reused for gender\nclassification task, achieving an out-of-sample accuracy, significantly better\nthan a shuffled-label baseline and a model trained directly on EEG data. These\nfindings suggest that generative models can serve not only as EEG data\ngenerators but also as unsupervised feature extractors, reducing the need for\nmanual feature engineering. This study highlights the potential of GAN-based\nunsupervised learning for EEG analysis, suggesting avenues for more\ndata-efficient deep learning applications in neuroscience.\n","authors":["Yeganeh Farahzadi","Morteza Ansarinia","Zoltan Kekecs"],"pdf_url":"https://arxiv.org/pdf/2503.02636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02712v1","updated":"2025-05-05T15:07:20Z","published":"2025-05-05T15:07:20Z","title":"Graph Neural Network-Based Reinforcement Learning for Controlling\n  Biological Networks: The GATTACA Framework","summary":"  Cellular reprogramming, the artificial transformation of one cell type into\nanother, has been attracting increasing research attention due to its\ntherapeutic potential for complex diseases. However, discovering reprogramming\nstrategies through classical wet-lab experiments is hindered by lengthy time\ncommitments and high costs. In this study, we explore the use of deep\nreinforcement learning (DRL) to control Boolean network models of complex\nbiological systems, such as gene regulatory networks and signalling pathway\nnetworks. We formulate a novel control problem for Boolean network models under\nthe asynchronous update mode in the context of cellular reprogramming. To\nfacilitate scalability, we consider our previously introduced concept of a\npseudo-attractor and we improve our procedure for effective identification of\npseudo-attractor states. Finally, we devise a computational framework to solve\nthe control problem. To leverage the structure of biological systems, we\nincorporate graph neural networks with graph convolutions into the artificial\nneural network approximator for the action-value function learned by the DRL\nagent. Experiments on a number of large real-world biological networks from\nliterature demonstrate the scalability and effectiveness of our approach.\n","authors":["Andrzej Mizera","Jakub Zarzycki"],"pdf_url":"https://arxiv.org/pdf/2505.02712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02709v1","updated":"2025-05-05T15:06:09Z","published":"2025-05-05T15:06:09Z","title":"Technical Report: Evaluating Goal Drift in Language Model Agents","summary":"  As language models (LMs) are increasingly deployed as autonomous agents,\ntheir robust adherence to human-assigned objectives becomes crucial for safe\noperation. When these agents operate independently for extended periods without\nhuman oversight, even initially well-specified goals may gradually shift.\nDetecting and measuring goal drift - an agent's tendency to deviate from its\noriginal objective over time - presents significant challenges, as goals can\nshift gradually, causing only subtle behavioral changes. This paper proposes a\nnovel approach to analyzing goal drift in LM agents. In our experiments, agents\nare first explicitly given a goal through their system prompt, then exposed to\ncompeting objectives through environmental pressures. We demonstrate that while\nthe best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains\nnearly perfect goal adherence for more than 100,000 tokens in our most\ndifficult evaluation setting, all evaluated models exhibit some degree of goal\ndrift. We also find that goal drift correlates with models' increasing\nsusceptibility to pattern-matching behaviors as the context length grows.\n","authors":["Rauno Arike","Elizabeth Donoway","Henning Bartsch","Marius Hobbhahn"],"pdf_url":"https://arxiv.org/pdf/2505.02709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02707v1","updated":"2025-05-05T15:05:01Z","published":"2025-05-05T15:05:01Z","title":"Voila: Voice-Language Foundation Models for Real-Time Autonomous\n  Interaction and Voice Role-Play","summary":"  A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.\n","authors":["Yemin Shi","Yu Shu","Siwei Dong","Guangyi Liu","Jaward Sesay","Jingwen Li","Zhiting Hu"],"pdf_url":"https://arxiv.org/pdf/2505.02707v1.pdf","comment":"18 pages, 7 figures, Website: https://voila.maitrix.org"},{"id":"http://arxiv.org/abs/2405.05572v2","updated":"2025-05-05T14:51:58Z","published":"2024-05-09T06:40:39Z","title":"From Human Judgements to Predictive Models: Unravelling Acceptability in\n  Code-Mixed Sentences","summary":"  Current computational approaches for analysing or generating code-mixed\nsentences do not explicitly model ``naturalness'' or ``acceptability'' of\ncode-mixed sentences, but rely on training corpora to reflect distribution of\nacceptable code-mixed sentences. Modelling human judgement for the\nacceptability of code-mixed text can help in distinguishing natural code-mixed\ntext and enable quality-controlled generation of code-mixed text. To this end,\nwe construct Cline - a dataset containing human acceptability judgements for\nEnglish-Hindi~(en-hi) code-mixed text. Cline is the largest of its kind with\n16,642 sentences, consisting of samples sourced from two sources: synthetically\ngenerated code-mixed text and samples collected from online social media. Our\nanalysis establishes that popular code-mixing metrics such as CMI, Number of\nSwitch Points, Burstines, which are used to filter/curate/compare code-mixed\ncorpora have low correlation with human acceptability judgements, underlining\nthe necessity of our dataset. Experiments using Cline demonstrate that simple\nMultilayer Perceptron (MLP) models when trained solely using code-mixing\nmetrics as features are outperformed by fine-tuned pre-trained Multilingual\nLarge Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta\nand Bernice outperform IndicBERT across different configurations. Among\nEncoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder\nmodels are not able to outperform Encoder-only models. Decoder-only models\nperform the best when compared to all other MLLMS, with Llama 3.2 - 3B models\noutperforming similarly sized Qwen, Phi models. Comparison with zero and\nfewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data\noutperform ChatGPT, providing scope for improvement in code-mixed tasks.\nZero-shot transfer from En-Hi to En-Te acceptability judgments are better than\nrandom baselines.\n","authors":["Prashant Kodali","Anmol Goel","Likhith Asapu","Vamshi Krishna Bonagiri","Anirudh Govil","Monojit Choudhury","Ponnurangam Kumaraguru","Manish Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2405.05572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01315v2","updated":"2025-05-05T14:46:48Z","published":"2025-05-02T14:42:26Z","title":"Helping Large Language Models Protect Themselves: An Enhanced Filtering\n  and Summarization System","summary":"  The recent growth in the use of Large Language Models has made them\nvulnerable to sophisticated adversarial assaults, manipulative prompts, and\nencoded malicious inputs. Existing countermeasures frequently necessitate\nretraining models, which is computationally costly and impracticable for\ndeployment. Without the need for retraining or fine-tuning, this study presents\na unique defense paradigm that allows LLMs to recognize, filter, and defend\nagainst adversarial or malicious inputs on their own. There are two main parts\nto the suggested framework: (1) A prompt filtering module that uses\nsophisticated Natural Language Processing (NLP) techniques, including zero-shot\nclassification, keyword analysis, and encoded content detection (e.g. base64,\nhexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and\n(2) A summarization module that processes and summarizes adversarial research\nliterature to give the LLM context-aware defense knowledge. This approach\nstrengthens LLMs' resistance to adversarial exploitation by fusing text\nextraction, summarization, and harmful prompt analysis. According to\nexperimental results, this integrated technique has a 98.71% success rate in\nidentifying harmful patterns, manipulative language structures, and encoded\nprompts. By employing a modest amount of adversarial research literature as\ncontext, the methodology also allows the model to react correctly to harmful\ninputs with a larger percentage of jailbreak resistance and refusal rate. While\nmaintaining the quality of LLM responses, the framework dramatically increases\nLLM's resistance to hostile misuse, demonstrating its efficacy as a quick and\neasy substitute for time-consuming, retraining-based defenses.\n","authors":["Sheikh Samit Muhaimin","Spyridon Mastorakis"],"pdf_url":"https://arxiv.org/pdf/2505.01315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02694v1","updated":"2025-05-05T14:44:17Z","published":"2025-05-05T14:44:17Z","title":"AI Standardized Patient Improves Human Conversations in Advanced Cancer\n  Care","summary":"  Serious illness communication (SIC) in end-of-life care faces challenges such\nas emotional stress, cultural barriers, and balancing hope with honesty.\nDespite its importance, one of the few available ways for clinicians to\npractice SIC is with standardized patients, which is expensive, time-consuming,\nand inflexible. In this paper, we present SOPHIE, an AI-powered standardized\npatient simulation and automated feedback system. SOPHIE combines large\nlanguage models (LLMs), a lifelike virtual avatar, and automated, personalized\nfeedback based on clinical literature to provide remote, on-demand SIC\ntraining. In a randomized control study with healthcare students and\nprofessionals, SOPHIE users demonstrated significant improvement across three\ncritical SIC domains: Empathize, Be Explicit, and Empower. These results\nsuggest that AI-driven tools can enhance complex interpersonal communication\nskills, offering scalable, accessible solutions to address a critical gap in\nclinician education.\n","authors":["Kurtis Haut","Masum Hasan","Thomas Carroll","Ronald Epstein","Taylan Sen","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2505.02694v1.pdf","comment":"20 pages, 6 figures, 4 tables, submitting to New England Journal of\n  Medicine (NEJM)"},{"id":"http://arxiv.org/abs/2403.16933v3","updated":"2025-05-05T14:43:33Z","published":"2024-03-25T16:57:02Z","title":"Backpropagation through space, time, and the brain","summary":"  How physical networks of neurons, bound by spatio-temporal locality\nconstraints, can perform efficient credit assignment, remains, to a large\nextent, an open question. In machine learning, the answer is almost universally\ngiven by the error backpropagation algorithm, through both space and time.\nHowever, this algorithm is well-known to rely on biologically implausible\nassumptions, in particular with respect to spatio-temporal (non-)locality.\nAlternative forward-propagation models such as real-time recurrent learning\nonly partially solve the locality problem, but only at the cost of scaling, due\nto prohibitive storage requirements. We introduce Generalized Latent\nEquilibrium (GLE), a computational framework for fully local spatio-temporal\ncredit assignment in physical, dynamical networks of neurons. We start by\ndefining an energy based on neuron-local mismatches, from which we derive both\nneuronal dynamics via stationarity and parameter dynamics via gradient descent.\nThe resulting dynamics can be interpreted as a real-time, biologically\nplausible approximation of backpropagation through space and time in deep\ncortical networks with continuous-time neuronal dynamics and continuously\nactive, local synaptic plasticity. In particular, GLE exploits the morphology\nof dendritic trees to enable more complex information storage and processing in\nsingle neurons, as well as the ability of biological neurons to phase-shift\ntheir output rate with respect to their membrane potential, which is essential\nin both directions of information propagation. For the forward computation, it\nenables the mapping of time-continuous inputs to neuronal space, effectively\nperforming a spatio-temporal convolution. For the backward computation, it\npermits the temporal inversion of feedback signals, which consequently\napproximate the adjoint variables necessary for useful parameter updates.\n","authors":["Benjamin Ellenberger","Paul Haider","Jakob Jordan","Kevin Max","Ismael Jaras","Laura Kriener","Federico Benitez","Mihai A. Petrovici"],"pdf_url":"https://arxiv.org/pdf/2403.16933v3.pdf","comment":"First authorship shared by Benjamin Ellenberger and Paul Haider"},{"id":"http://arxiv.org/abs/2505.02665v1","updated":"2025-05-05T14:14:59Z","published":"2025-05-05T14:14:59Z","title":"A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning\n  and Inference-time Scaling Law","summary":"  This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems.\n","authors":["Qianjun Pan","Wenkai Ji","Yuyang Ding","Junsong Li","Shilian Chen","Junyi Wang","Jie Zhou","Qin Chen","Min Zhang","Yulan Wu","Liang He"],"pdf_url":"https://arxiv.org/pdf/2505.02665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02659v1","updated":"2025-05-05T14:05:15Z","published":"2025-05-05T14:05:15Z","title":"A Note on Statistically Accurate Tabular Data Generation Using Large\n  Language Models","summary":"  Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probobility distributions to\nenhance the statistical fidelity of LLM-generated tabular data.\n","authors":["Andrey Sidorenko"],"pdf_url":"https://arxiv.org/pdf/2505.02659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02655v1","updated":"2025-05-05T13:59:55Z","published":"2025-05-05T13:59:55Z","title":"SCFormer: Structured Channel-wise Transformer with Cumulative Historical\n  State for Multivariate Time Series Forecasting","summary":"  The Transformer model has shown strong performance in multivariate time\nseries forecasting by leveraging channel-wise self-attention. However, this\napproach lacks temporal constraints when computing temporal features and does\nnot utilize cumulative historical series effectively.To address these\nlimitations, we propose the Structured Channel-wise Transformer with Cumulative\nHistorical state (SCFormer). SCFormer introduces temporal constraints to all\nlinear transformations, including the query, key, and value matrices, as well\nas the fully connected layers within the Transformer. Additionally, SCFormer\nemploys High-order Polynomial Projection Operators (HiPPO) to deal with\ncumulative historical time series, allowing the model to incorporate\ninformation beyond the look-back window during prediction. Extensive\nexperiments on multiple real-world datasets demonstrate that SCFormer\nsignificantly outperforms mainstream baselines, highlighting its effectiveness\nin enhancing time series forecasting. The code is publicly available at\nhttps://github.com/ShiweiGuo1995/SCFormer\n","authors":["Shiwei Guo","Ziang Chen","Yupeng Ma","Yunfei Han","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2505.02655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17090v2","updated":"2025-05-05T13:51:42Z","published":"2024-08-30T08:22:30Z","title":"FissionVAE: Federated Non-IID Image Generation with Latent Space and\n  Decoder Decomposition","summary":"  Federated learning is a machine learning paradigm that enables decentralized\nclients to collaboratively learn a shared model while keeping all the training\ndata local. While considerable research has focused on federated image\ngeneration, particularly Generative Adversarial Networks, Variational\nAutoencoders have received less attention. In this paper, we address the\nchallenges of non-IID (independently and identically distributed) data\nenvironments featuring multiple groups of images of different types. Non-IID\ndata distributions can lead to difficulties in maintaining a consistent latent\nspace and can also result in local generators with disparate texture features\nbeing blended during aggregation. We thereby introduce FissionVAE that\ndecouples the latent space and constructs decoder branches tailored to\nindividual client groups. This method allows for customized learning that\naligns with the unique data distributions of each group. Additionally, we\nincorporate hierarchical VAEs and demonstrate the use of heterogeneous decoder\narchitectures within FissionVAE. We also explore strategies for setting the\nlatent prior distributions to enhance the decoupling process. To evaluate our\napproach, we assemble two composite datasets: the first combines MNIST and\nFashionMNIST; the second comprises RGB datasets of cartoon and human faces,\nwild animals, marine vessels, and remote sensing images. Our experiments\ndemonstrate that FissionVAE greatly improves generation quality on these\ndatasets compared to baseline federated VAE models.\n","authors":["Chen Hu","Hanchi Ren","Jingjing Deng","Xianghua Xie","Xiaoke Ma"],"pdf_url":"https://arxiv.org/pdf/2408.17090v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02649v1","updated":"2025-05-05T13:50:12Z","published":"2025-05-05T13:50:12Z","title":"Eye Movements as Indicators of Deception: A Machine Learning Approach","summary":"  Gaze may enhance the robustness of lie detectors but remains under-studied.\nThis study evaluated the efficacy of AI models (using fixations, saccades,\nblinks, and pupil size) for detecting deception in Concealed Information Tests\nacross two datasets. The first, collected with Eyelink 1000, contains gaze data\nfrom a computerized experiment where 87 participants revealed, concealed, or\nfaked the value of a previously selected card. The second, collected with Pupil\nNeon, involved 36 participants performing a similar task but facing an\nexperimenter. XGBoost achieved accuracies up to 74% in a binary classification\ntask (Revealing vs. Concealing) and 49% in a more challenging\nthree-classification task (Revealing vs. Concealing vs. Faking). Feature\nanalysis identified saccade number, duration, amplitude, and maximum pupil size\nas the most important for deception prediction. These results demonstrate the\nfeasibility of using gaze and AI to enhance lie detectors and encourage future\nresearch that may improve on this.\n","authors":["Valentin Foucher","Santiago de Leon-Martinez","Robert Moro"],"pdf_url":"https://arxiv.org/pdf/2505.02649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02640v1","updated":"2025-05-05T13:33:39Z","published":"2025-05-05T13:33:39Z","title":"Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource\n  Constraints","summary":"  Internet of Things (IoT) systems increasingly operate in environments where\ndevices must respond in real time while managing fluctuating resource\nconstraints, including energy and bandwidth. Yet, current approaches often fall\nshort in addressing scenarios where operational constraints evolve over time.\nTo address these limitations, we propose a novel Budgeted Multi-Armed Bandit\nframework tailored for IoT applications with dynamic operational limits. Our\nmodel introduces a decaying violation budget, which permits limited constraint\nviolations early in the learning process and gradually enforces stricter\ncompliance over time. We present the Budgeted Upper Confidence Bound (UCB)\nalgorithm, which adaptively balances performance optimization and compliance\nwith time-varying constraints. We provide theoretical guarantees showing that\nBudgeted UCB achieves sublinear regret and logarithmic constraint violations\nover the learning horizon. Extensive simulations in a wireless communication\nsetting show that our approach achieves faster adaptation and better constraint\nsatisfaction than standard online learning methods. These results highlight the\nframework's potential for building adaptive, resource-aware IoT systems.\n","authors":["Shubham Vaishnav","Praveen Kumar Donta","Sindri Magnússon"],"pdf_url":"https://arxiv.org/pdf/2505.02640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02639v1","updated":"2025-05-05T13:31:36Z","published":"2025-05-05T13:31:36Z","title":"Enhancing Chemical Reaction and Retrosynthesis Prediction with Large\n  Language Model and Dual-task Learning","summary":"  Chemical reaction and retrosynthesis prediction are fundamental tasks in drug\ndiscovery. Recently, large language models (LLMs) have shown potential in many\ndomains. However, directly applying LLMs to these tasks faces two major\nchallenges: (i) lacking a large-scale chemical synthesis-related instruction\ndataset; (ii) ignoring the close correlation between reaction and\nretrosynthesis prediction for the existing fine-tuning strategies. To address\nthese challenges, we propose ChemDual, a novel LLM framework for accurate\nchemical synthesis. Specifically, considering the high cost of data acquisition\nfor reaction and retrosynthesis, ChemDual regards the\nreaction-and-retrosynthesis of molecules as a related\nrecombination-and-fragmentation process and constructs a large-scale of 4.4\nmillion instruction dataset. Furthermore, ChemDual introduces an enhanced\nLLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,\nto jointly optimize the process of recombination and fragmentation as well as\nthe tasks between reaction and retrosynthesis prediction. Extensive experiments\non Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves\nstate-of-the-art performance in both predictions of reaction and\nretrosynthesis, outperforming the existing conventional single-task approaches\nand the general open-source LLMs. Through molecular docking analysis, ChemDual\ngenerates compounds with diverse and strong protein binding affinity, further\nhighlighting its strong potential in drug design.\n","authors":["Xuan Lin","Qingrui Liu","Hongxin Xiang","Daojian Zeng","Xiangxiang Zeng"],"pdf_url":"https://arxiv.org/pdf/2505.02639v1.pdf","comment":"Accepted for publication at IJCAI 2025"},{"id":"http://arxiv.org/abs/2505.00654v2","updated":"2025-05-05T13:14:14Z","published":"2025-05-01T16:55:44Z","title":"Large Language Models Understanding: an Inherent Ambiguity Barrier","summary":"  A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.\n","authors":["Daniel N. Nissani"],"pdf_url":"https://arxiv.org/pdf/2505.00654v2.pdf","comment":"submitted to NEURAL COMPUTATION"},{"id":"http://arxiv.org/abs/2505.02627v1","updated":"2025-05-05T13:13:46Z","published":"2025-05-05T13:13:46Z","title":"A Theoretical Analysis of Compositional Generalization in Neural\n  Networks: A Necessary and Sufficient Condition","summary":"  Compositional generalization is a crucial property in artificial\nintelligence, enabling models to handle novel combinations of known components.\nWhile most deep learning models lack this capability, certain models succeed in\nspecific tasks, suggesting the existence of governing conditions. This paper\nderives a necessary and sufficient condition for compositional generalization\nin neural networks. Conceptually, it requires that (i) the computational graph\nmatches the true compositional structure, and (ii) components encode just\nenough information in training. The condition is supported by mathematical\nproofs. This criterion combines aspects of architecture design, regularization,\nand training data properties. A carefully designed minimal example illustrates\nan intuitive understanding of the condition. We also discuss the potential of\nthe condition for assessing compositional generalization before training. This\nwork is a fundamental theoretical study of compositional generalization in\nneural networks.\n","authors":["Yuanpeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.02627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02625v1","updated":"2025-05-05T12:53:09Z","published":"2025-05-05T12:53:09Z","title":"LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis","summary":"  Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.\n","authors":["Qingkai Fang","Yan Zhou","Shoutao Guo","Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2505.02625v1.pdf","comment":"Preprint. Project: https://github.com/ictnlp/LLaMA-Omni2"},{"id":"http://arxiv.org/abs/2504.08837v2","updated":"2025-05-05T12:49:32Z","published":"2025-04-10T17:41:56Z","title":"VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning","summary":"  Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a rethinking trigger token to the end of rollouts in\nRL training, explicitly enforcing a self-reflection reasoning step. By\ncombining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5%\nrespectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary\nbenchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the\ngap with OpenAI-o1. Our empirical results show the effectiveness of our\napproaches.\n","authors":["Haozhe Wang","Chao Qu","Zuming Huang","Wei Chu","Fangzhen Lin","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2504.08837v2.pdf","comment":"submitted to NeurIPS"},{"id":"http://arxiv.org/abs/2503.02650v2","updated":"2025-05-05T12:25:44Z","published":"2025-03-04T14:14:28Z","title":"The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats","summary":"  The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.\n","authors":["William Brach","Kristián Košťál","Michal Ries"],"pdf_url":"https://arxiv.org/pdf/2503.02650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02609v1","updated":"2025-05-05T12:24:31Z","published":"2025-05-05T12:24:31Z","title":"Study of the influence of a biased database on the prediction of\n  standard algorithms for selecting the best candidate for an interview","summary":"  Artificial intelligence is used at various stages of the recruitment process\nto automatically select the best candidate for a position, with companies\nguaranteeing unbiased recruitment. However, the algorithms used are either\ntrained by humans or are based on learning from past experiences that were\nbiased. In this article, we propose to generate data mimicking external\n(discrimination) and internal biases (self-censorship) in order to train five\nclassic algorithms and to study the extent to which they do or do not find the\nbest candidates according to objective criteria. In addition, we study the\ninfluence of the anonymisation of files on the quality of predictions.\n","authors":["Shuyu Wang","Angélique Saillet","Philomène Le Gall","Alain Lacroux","Christelle Martin-Lacroux","Vincent Brault"],"pdf_url":"https://arxiv.org/pdf/2505.02609v1.pdf","comment":"38 pages, 25 figures, 4 tables"},{"id":"http://arxiv.org/abs/2504.15941v2","updated":"2025-05-05T12:19:32Z","published":"2025-04-22T14:35:16Z","title":"FairTranslate: An English-French Dataset for Gender Bias Evaluation in\n  Machine Translation by Overcoming Gender Binarity","summary":"  Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.\n","authors":["Fanny Jourdan","Yannick Chevalier","Cécile Favre"],"pdf_url":"https://arxiv.org/pdf/2504.15941v2.pdf","comment":"FAccT 2025"},{"id":"http://arxiv.org/abs/2504.03601v3","updated":"2025-05-05T11:54:13Z","published":"2025-04-04T17:13:57Z","title":"APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay","summary":"  Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source 5K synthetic data trajectories\nand the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;\nDataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website\nat https://apigen-mt.github.io\n","authors":["Akshara Prabhakar","Zuxin Liu","Ming Zhu","Jianguo Zhang","Tulika Awalgaonkar","Shiyu Wang","Zhiwei Liu","Haolin Chen","Thai Hoang","Juan Carlos Niebles","Shelby Heinecke","Weiran Yao","Huan Wang","Silvio Savarese","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2504.03601v3.pdf","comment":"12 pages plus references and appendices"},{"id":"http://arxiv.org/abs/2503.06457v2","updated":"2025-05-05T11:38:40Z","published":"2025-03-09T05:30:28Z","title":"Geometric Knowledge-Guided Localized Global Distribution Alignment for\n  Federated Learning","summary":"  Data heterogeneity in federated learning, characterized by a significant\nmisalignment between local and global distributions, leads to divergent local\noptimization directions and hinders global model training. Existing studies\nmainly focus on optimizing local updates or global aggregation, but these\nindirect approaches demonstrate instability when handling highly heterogeneous\ndata distributions, especially in scenarios where label skew and domain skew\ncoexist. To address this, we propose a geometry-guided data generation method\nthat centers on simulating the global embedding distribution locally. We first\nintroduce the concept of the geometric shape of an embedding distribution and\nthen address the challenge of obtaining global geometric shapes under privacy\nconstraints. Subsequently, we propose GGEUR, which leverages global geometric\nshapes to guide the generation of new samples, enabling a closer approximation\nto the ideal global distribution. In single-domain scenarios, we augment\nsamples based on global geometric shapes to enhance model generalization; in\nmulti-domain scenarios, we further employ class prototypes to simulate the\nglobal distribution across domains. Extensive experimental results demonstrate\nthat our method significantly enhances the performance of existing approaches\nin handling highly heterogeneous data, including scenarios with label skew,\ndomain skew, and their coexistence. Code published at:\nhttps://github.com/WeiDai-David/2025CVPR_GGEUR\n","authors":["Yanbiao Ma","Wei Dai","Wenke Huang","Jiayi Chen"],"pdf_url":"https://arxiv.org/pdf/2503.06457v2.pdf","comment":"Accepted by CVPR Oral 2025"},{"id":"http://arxiv.org/abs/2505.02581v1","updated":"2025-05-05T11:33:18Z","published":"2025-05-05T11:33:18Z","title":"Agentic Neurodivergence as a Contingent Solution to the AI Alignment\n  Problem","summary":"  The AI alignment problem, which focusses on ensuring that artificial\nintelligence (AI), including AGI and ASI, systems act according to human\nvalues, presents profound challenges. With the progression from narrow AI to\nArtificial General Intelligence (AGI) and Superintelligence, fears about\ncontrol and existential risk have escalated. This paper demonstrates that\nachieving complete alignment is inherently unattainable due to mathematical\nprinciples rooted in the foundations of predicate logic and computability, in\nparticular Turing's computational universality, G\\\"odel's incompleteness and\nChaitin's randomness. Instead, we argue that embracing AI misalignment or\nagent's `neurodivergence' as a contingent strategy, defined as fostering a\ndynamic ecosystem of competing, partially aligned agents, is a possible only\nviable path to mitigate risks. Through mathematical proofs and an experimental\ndesign, we explore how misalignment may serve and should be promoted as a\ncounterbalancing mechanism to team up with whichever agents are most aligned AI\nto human values, ensuring that no single system dominates destructively. The\nmain premise of our contribution is that misalignment is inevitable because\nfull AI-human alignment is a mathematical impossibility from Turing-complete\nsystems which we also prove in this paper, a feature then inherited to AGI and\nASI systems. We introduce and test `change-of-opinion' attacks based on this\nkind of perturbation and intervention analysis to study how agents may\nneutralise friendly or unfriendly AIs through cooperation, competition or\nmalice.\n","authors":["Alberto Hernández-Espinosa","Felipe S. Abrahão","Olaf Witkowski","Hector Zenil"],"pdf_url":"https://arxiv.org/pdf/2505.02581v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2505.02579v1","updated":"2025-05-05T11:30:46Z","published":"2025-05-05T11:30:46Z","title":"EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning","summary":"  Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.\n","authors":["Lingxiao Kong","Cong Yang","Susanne Neufang","Oya Deniz Beyan","Zeyd Boukhers"],"pdf_url":"https://arxiv.org/pdf/2505.02579v1.pdf","comment":"13 pages, 9 figures, submitted to SIGDIAL 2025 conference"},{"id":"http://arxiv.org/abs/2505.02576v1","updated":"2025-05-05T11:24:20Z","published":"2025-05-05T11:24:20Z","title":"Recursive Decomposition with Dependencies for Generic Divide-and-Conquer\n  Reasoning","summary":"  Reasoning tasks are crucial in many domains, especially in science and\nengineering. Although large language models (LLMs) have made progress in\nreasoning tasks using techniques such as chain-of-thought and least-to-most\nprompting, these approaches still do not effectively scale to complex problems\nin either their performance or execution time. Moreover, they often require\nadditional supervision for each new task, such as in-context examples. In this\nwork, we introduce Recursive Decomposition with Dependencies (RDD), a scalable\ndivide-and-conquer method for solving reasoning problems that requires less\nsupervision than prior approaches. Our method can be directly applied to a new\nproblem class even in the absence of any task-specific guidance. Furthermore,\nRDD supports sub-task dependencies, allowing for ordered execution of\nsub-tasks, as well as an error recovery mechanism that can correct mistakes\nmade in previous steps. We evaluate our approach on two benchmarks with six\ndifficulty levels each and in two in-context settings: one with task-specific\nexamples and one without. Our results demonstrate that RDD outperforms other\nmethods in a compute-matched setting as task complexity increases, while also\nbeing more computationally efficient.\n","authors":["Sergio Hernández-Gutiérrez","Minttu Alakuijala","Alexander V. Nikitin","Pekka Marttinen"],"pdf_url":"https://arxiv.org/pdf/2505.02576v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02573v1","updated":"2025-05-05T11:23:29Z","published":"2025-05-05T11:23:29Z","title":"Rethinking Federated Graph Learning: A Data Condensation Perspective","summary":"  Federated graph learning is a widely recognized technique that promotes\ncollaborative training of graph neural networks (GNNs) by multi-client\ngraphs.However, existing approaches heavily rely on the communication of model\nparameters or gradients for federated optimization and fail to adequately\naddress the data heterogeneity introduced by intricate and diverse graph\ndistributions. Although some methods attempt to share additional messages among\nthe server and clients to improve federated convergence during communication,\nthey introduce significant privacy risks and increase communication overhead.\nTo address these issues, we introduce the concept of a condensed graph as a\nnovel optimization carrier to address FGL data heterogeneity and propose a new\nFGL paradigm called FedGM. Specifically, we utilize a generalized condensation\ngraph consensus to aggregate comprehensive knowledge from distributed graphs,\nwhile minimizing communication costs and privacy risks through a single\ntransmission of the condensed data. Extensive experiments on six public\ndatasets consistently demonstrate the superiority of FedGM over\nstate-of-the-art baselines, highlighting its potential for a novel FGL\nparadigm.\n","authors":["Hao Zhang","Xunkai Li","Yinlin Zhu","Lianglin Hu"],"pdf_url":"https://arxiv.org/pdf/2505.02573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07503v3","updated":"2025-05-05T11:19:53Z","published":"2025-02-11T12:11:40Z","title":"Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems","summary":"  Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!\n","authors":["Ibrahim Alabdulmohsin","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.07503v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02566v1","updated":"2025-05-05T11:14:56Z","published":"2025-05-05T11:14:56Z","title":"Robustness questions the interpretability of graph neural networks: what\n  to do?","summary":"  Graph Neural Networks (GNNs) have become a cornerstone in graph-based data\nanalysis, with applications in diverse domains such as bioinformatics, social\nnetworks, and recommendation systems. However, the interplay between model\ninterpretability and robustness remains poorly understood, especially under\nadversarial scenarios like poisoning and evasion attacks. This paper presents a\ncomprehensive benchmark to systematically analyze the impact of various factors\non the interpretability of GNNs, including the influence of\nrobustness-enhancing defense mechanisms.\n  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across\nfive datasets from two distinct domains, employing four interpretability\nmetrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how\ndefenses against poisoning and evasion attacks, applied before and during model\ntraining, affect interpretability and highlights critical trade-offs between\nrobustness and interpretability. The framework will be published as open\nsource.\n  The results reveal significant variations in interpretability depending on\nthe chosen defense methods and model architecture characteristics. By\nestablishing a standardized benchmark, this work provides a foundation for\ndeveloping GNNs that are both robust to adversarial threats and interpretable,\nfacilitating trust in their deployment in sensitive applications.\n","authors":["Kirill Lukyanov","Georgii Sazonov","Serafim Boyarsky","Ilya Makarov"],"pdf_url":"https://arxiv.org/pdf/2505.02566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02550v1","updated":"2025-05-05T10:39:51Z","published":"2025-05-05T10:39:51Z","title":"Bielik v3 Small: Technical Report","summary":"  We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.\n","authors":["Krzysztof Ociepa","Łukasz Flis","Remigiusz Kinas","Krzysztof Wróbel","Adrian Gwoździej"],"pdf_url":"https://arxiv.org/pdf/2505.02550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02540v1","updated":"2025-05-05T10:26:35Z","published":"2025-05-05T10:26:35Z","title":"Lazy But Effective: Collaborative Personalized Federated Learning with\n  Heterogeneous Data","summary":"  In Federated Learning, heterogeneity in client data distributions often means\nthat a single global model does not have the best performance for individual\nclients. Consider for example training a next-word prediction model for\nkeyboards: user-specific language patterns due to demographics (dialect, age,\netc.), language proficiency, and writing style result in a highly non-IID\ndataset across clients. Other examples are medical images taken with different\nmachines, or driving data from different vehicle types. To address this, we\npropose a simple yet effective personalized federated learning framework\n(pFedLIA) that utilizes a computationally efficient influence approximation,\ncalled `Lazy Influence', to cluster clients in a distributed manner before\nmodel aggregation. Within each cluster, data owners collaborate to jointly\ntrain a model that captures the specific data patterns of the clients. Our\nmethod has been shown to successfully recover the global model's performance\ndrop due to the non-IID-ness in various synthetic and real-world settings,\nspecifically a next-word prediction task on the Nordic languages as well as\nseveral benchmark tasks. It matches the performance of a hypothetical Oracle\nclustering, and significantly improves on existing baselines, e.g., an\nimprovement of 17% on CIFAR100.\n","authors":["Ljubomir Rokvic","Panayiotis Danassis","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2505.02540v1.pdf","comment":"Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), IEEE, 2025"},{"id":"http://arxiv.org/abs/2505.02537v1","updated":"2025-05-05T10:18:48Z","published":"2025-05-05T10:18:48Z","title":"Advancing Constrained Monotonic Neural Networks: Achieving Universal\n  Approximation Beyond Bounded Activations","summary":"  Conventional techniques for imposing monotonicity in MLPs by construction\ninvolve the use of non-negative weight constraints and bounded activation\nfunctions, which pose well-known optimization challenges. In this work, we\ngeneralize previous theoretical results, showing that MLPs with non-negative\nweight constraint and activations that saturate on alternating sides are\nuniversal approximators for monotonic functions. Additionally, we show an\nequivalence between the saturation side in the activations and the sign of the\nweight constraint. This connection allows us to prove that MLPs with convex\nmonotone activations and non-positive constrained weights also qualify as\nuniversal approximators, in contrast to their non-negative constrained\ncounterparts. Our results provide theoretical grounding to the empirical\neffectiveness observed in previous works while leading to possible\narchitectural simplification. Moreover, to further alleviate the optimization\ndifficulties, we propose an alternative formulation that allows the network to\nadjust its activations according to the sign of the weights. This eliminates\nthe requirement for weight reparameterization, easing initialization and\nimproving training stability. Experimental evaluation reinforces the validity\nof the theoretical results, showing that our novel approach compares favourably\nto traditional monotonic architectures.\n","authors":["Davide Sartor","Alberto Sinigaglia","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2505.02537v1.pdf","comment":"International Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2505.02533v1","updated":"2025-05-05T10:16:16Z","published":"2025-05-05T10:16:16Z","title":"Large Language Model Partitioning for Low-Latency Inference at the Edge","summary":"  Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.\n","authors":["Dimitrios Kafetzis","Ramin Khalili","Iordanis Koutsopoulos"],"pdf_url":"https://arxiv.org/pdf/2505.02533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02516v1","updated":"2025-05-05T09:49:13Z","published":"2025-05-05T09:49:13Z","title":"Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and\n  Diagnostics","summary":"  Advanced neural interfaces are transforming applications ranging from\nneuroscience research to diagnostic tools (for mental state recognition, tremor\nand seizure detection) as well as prosthetic devices (for motor and\ncommunication recovery). By integrating complex functions into miniaturized\nneural devices, these systems unlock significant opportunities for personalized\nassistive technologies and adaptive therapeutic interventions. Leveraging\nhigh-density neural recordings, on-site signal processing, and machine learning\n(ML), these interfaces extract critical features, identify disease\nneuro-markers, and enable accurate, low-latency neural decoding. This\nintegration facilitates real-time interpretation of neural signals, adaptive\nmodulation of brain activity, and efficient control of assistive devices.\nMoreover, the synergy between neural interfaces and ML has paved the way for\nself-sufficient, ubiquitous platforms capable of operating in diverse\nenvironments with minimal hardware costs and external dependencies. In this\nwork, we review recent advancements in AI-driven decoding algorithms and\nenergy-efficient System-on-Chip (SoC) platforms for next-generation\nminiaturized neural devices. These innovations highlight the potential for\ndeveloping intelligent neural interfaces, addressing critical challenges in\nscalability, reliability, interpretability, and user adaptability.\n","authors":["MohammadAli Shaeri","Jinhan Liu","Mahsa Shoaran"],"pdf_url":"https://arxiv.org/pdf/2505.02516v1.pdf","comment":"To appear in the 2025 IEEE International NEWCAS Conference\n  (NEWCAS'25)"},{"id":"http://arxiv.org/abs/2502.03270v2","updated":"2025-05-05T09:42:27Z","published":"2025-02-05T15:25:46Z","title":"When Pre-trained Visual Representations Fall Short: Limitations in\n  Visuo-Motor Robot Learning","summary":"  The integration of pre-trained visual representations (PVRs) into visuo-motor\nrobot learning has emerged as a promising alternative to training visual\nencoders from scratch. However, PVRs face critical challenges in the context of\npolicy learning, including temporal entanglement and an inability to generalise\neven in the presence of minor scene perturbations. These limitations hinder\nperformance in tasks requiring temporal awareness and robustness to scene\nchanges. This work identifies these shortcomings and proposes solutions to\naddress them. First, we augment PVR features with temporal perception and a\nsense of task completion, effectively disentangling them in time. Second, we\nintroduce a module that learns to selectively attend to task-relevant local\nfeatures, enhancing robustness when evaluated on out-of-distribution scenes.\nOur experiments demonstrate significant performance improvements, particularly\nin PVRs trained with masking objectives, and validate the effectiveness of our\nenhancements in addressing PVR-specific limitations.\n","authors":["Nikolaos Tsagkas","Andreas Sochopoulos","Duolikun Danier","Sethu Vijayakumar","Chris Xiaoxuan Lu","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2502.03270v2.pdf","comment":"Project Page: https://tsagkas.github.io/pvrobo/"},{"id":"http://arxiv.org/abs/2504.19594v2","updated":"2025-05-05T09:35:11Z","published":"2025-04-28T08:58:18Z","title":"Mapping the Italian Telegram Ecosystem: Communities, Toxicity, and Hate\n  Speech","summary":"  Telegram has become a major space for political discourse and alternative\nmedia. However, its lack of moderation allows misinformation, extremism, and\ntoxicity to spread. While prior research focused on these particular phenomena\nor topics, these have mostly been examined separately, and a broader\nunderstanding of the Telegram ecosystem is still missing. In this work, we fill\nthis gap by conducting a large-scale analysis of the Italian Telegram sphere,\nleveraging a dataset of 186 million messages from 13,151 chats collected in\n2023. Using network analysis, Large Language Models, and toxicity detection\ntools, we examine how different thematic communities form, align ideologically,\nand engage in harmful discourse within the Italian cultural context. Results\nshow strong thematic and ideological homophily. We also identify mixed\nideological communities where far-left and far-right rhetoric coexist on\nparticular geopolitical issues. Beyond political analysis, we find that\ntoxicity, rather than being isolated in a few extreme chats, appears widely\nnormalized within highly toxic communities. Moreover, we find that Italian\ndiscourse primarily targets Black people, Jews, and gay individuals\nindependently of the topic. Finally, we uncover common trend of intra-national\nhostility, where Italians often attack other Italians, reflecting regional and\nintra-regional cultural conflicts that can be traced back to old historical\ndivisions. This study provides the first large-scale mapping of the Italian\nTelegram ecosystem, offering insights into ideological interactions, toxicity,\nand identity-targets of hate and contributing to research on online toxicity\nacross different cultural and linguistic contexts on Telegram.\n","authors":["Lorenzo Alvisi","Serena Tardelli","Maurizio Tesconi"],"pdf_url":"https://arxiv.org/pdf/2504.19594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02502v1","updated":"2025-05-05T09:30:19Z","published":"2025-05-05T09:30:19Z","title":"Unveiling the Landscape of LLM Deployment in the Wild: An Empirical\n  Study","summary":"  Background: Large language models (LLMs) are increasingly deployed via\nopen-source and commercial frameworks, enabling individuals and organizations\nto self-host advanced AI capabilities. However, insecure defaults and\nmisconfigurations often expose LLM services to the public Internet, posing\nsignificant security and system engineering risks. Aims: This study aims to\nunveil the current landscape of public-facing LLM deployments in the wild\nthrough a large-scale empirical study, focusing on service prevalence, exposure\ncharacteristics, systemic vulnerabilities, and associated risks. Method: We\nconducted an Internet-wide measurement to identify public-facing LLM\ndeployments across 15 frameworks, discovering 320,102 services. We extracted\n158 unique API endpoints, grouped into 12 functional categories based on\ncapabilities and security risks. We further analyzed configurations,\nauthentication practices, and geographic distributions, revealing deployment\ntrends and systemic issues in real-world LLM system engineering. Results: Our\nstudy shows that public LLM deployments are rapidly growing but often insecure.\nAmong all endpoints, we observe widespread use of insecure protocols, poor TLS\nconfigurations, and unauthenticated access to critical operations. Security\nrisks, including model disclosure, system leakage, and unauthorized access, are\npervasive, highlighting the need for secure-by-default frameworks and stronger\ndeployment practices. Conclusions: Public-facing LLM deployments suffer from\nwidespread security and configuration flaws, exposing services to misuse, model\ntheft, resource hijacking, and remote exploitation. Strengthening default\nsecurity, deployment practices, and operational standards is critical for the\ngrowing self-hosted LLM ecosystem.\n","authors":["Xinyi Hou","Jiahao Han","Yanjie Zhao","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.02502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02501v1","updated":"2025-05-05T09:29:32Z","published":"2025-05-05T09:29:32Z","title":"Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict\n  Reliable 6D Pose Distributions","summary":"  We introduce Corr2Distrib, the first correspondence-based method which\nestimates a 6D camera pose distribution from an RGB image, explaining the\nobservations. Indeed, symmetries and occlusions introduce visual ambiguities,\nleading to multiple valid poses. While a few recent methods tackle this\nproblem, they do not rely on local correspondences which, according to the BOP\nChallenge, are currently the most effective way to estimate a single 6DoF pose\nsolution. Using correspondences to estimate a pose distribution is not\nstraightforward, since ambiguous correspondences induced by visual ambiguities\ndrastically decrease the performance of PnP. With Corr2Distrib, we turn these\nambiguities into an advantage to recover all valid poses. Corr2Distrib first\nlearns a symmetry-aware representation for each 3D point on the object's\nsurface, characterized by a descriptor and a local frame. This representation\nenables the generation of 3DoF rotation hypotheses from single 2D-3D\ncorrespondences. Next, we refine these hypotheses into a 6DoF pose distribution\nusing PnP and pose scoring. Our experimental evaluations on complex\nnon-synthetic scenes show that Corr2Distrib outperforms state-of-the-art\nsolutions for both pose distribution estimation and single pose estimation from\nan RGB image, demonstrating the potential of correspondences-based approaches.\n","authors":["Asma Brazi","Boris Meden","Fabrice Mayran de Chamisso","Steve Bourgeois","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2505.02501v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.05547v2","updated":"2025-05-05T09:26:24Z","published":"2024-12-07T05:49:14Z","title":"KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large\n  Language Models","summary":"  Large language models with retrieval-augmented generation encounter a pivotal\nchallenge in intricate retrieval tasks, e.g., multi-hop question answering,\nwhich requires the model to navigate across multiple documents and generate\ncomprehensive responses based on fragmented information. To tackle this\nchallenge, we introduce a novel Knowledge Graph-based RAG framework with a\nhierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing\nin KG-Retriever is constructed on a hierarchical index graph that consists of a\nknowledge graph layer and a collaborative document layer. The associative\nnature of graph structures is fully utilized to strengthen intra-document and\ninter-document connectivity, thereby fundamentally alleviating the information\nfragmentation problem and meanwhile improving the retrieval efficiency in\ncross-document retrieval of LLMs. With the coarse-grained collaborative\ninformation from neighboring documents and concise information from the\nknowledge graph, KG-Retriever achieves marked improvements on five public QA\ndatasets, showing the effectiveness and efficiency of our proposed RAG\nframework.\n","authors":["Weijie Chen","Ting Bai","Jinbo Su","Jian Luan","Wei Liu","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2412.05547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06375v4","updated":"2025-05-05T09:20:58Z","published":"2023-02-13T14:08:40Z","title":"One Transformer for All Time Series: Representing and Training with\n  Time-Dependent Heterogeneous Tabular Data","summary":"  There is a recent growing interest in applying Deep Learning techniques to\ntabular data, in order to replicate the success of other Artificial\nIntelligence areas in this structured domain. Specifically interesting is the\ncase in which tabular data have a time dependence, such as, for instance\nfinancial transactions. However, the heterogeneity of the tabular values, in\nwhich categorical elements are mixed with numerical items, makes this\nadaptation difficult. In this paper we propose a Transformer architecture to\nrepresent heterogeneous time-dependent tabular data, in which numerical\nfeatures are represented using a set of frequency functions and the whole\nnetwork is uniformly trained with a unique loss function.\n","authors":["Simone Luetto","Fabrizio Garuti","Enver Sangineto","Lorenzo Forni","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2302.06375v4.pdf","comment":"Published in Machine Learning Journal. 29 pages, 2 figures, 16 tables"},{"id":"http://arxiv.org/abs/2505.02489v1","updated":"2025-05-05T09:15:31Z","published":"2025-05-05T09:15:31Z","title":"Beyond the model: Key differentiators in large language models and\n  multi-agent services","summary":"  With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it\nhas become evident that large language models (LLMs) are no longer the sole\ndefining factor in generative AI. As many now operate at comparable levels of\ncapability, the real race is not about having the biggest model but optimizing\nthe surrounding ecosystem, including data quality and management, computational\nefficiency, latency, and evaluation frameworks. This review article delves into\nthese critical differentiators that ensure modern AI services are efficient and\nprofitable.\n","authors":["Muskaan Goyal","Pranav Bhasin"],"pdf_url":"https://arxiv.org/pdf/2505.02489v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2505.02486v1","updated":"2025-05-05T09:09:41Z","published":"2025-05-05T09:09:41Z","title":"SEFE: Superficial and Essential Forgetting Eliminator for Multimodal\n  Continual Instruction Tuning","summary":"  Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal\nLarge Language Models (MLLMs) to incrementally learn new tasks without\ncatastrophic forgetting. In this paper, we explore forgetting in this context,\ncategorizing it into superficial forgetting and essential forgetting.\nSuperficial forgetting refers to cases where the model's knowledge may not be\ngenuinely lost, but its responses to previous tasks deviate from expected\nformats due to the influence of subsequent tasks' answer styles, making the\nresults unusable. By contrast, essential forgetting refers to situations where\nthe model provides correctly formatted but factually inaccurate answers,\nindicating a true loss of knowledge. Assessing essential forgetting\nnecessitates addressing superficial forgetting first, as severe superficial\nforgetting can obscure the model's knowledge state. Hence, we first introduce\nthe Answer Style Diversification (ASD) paradigm, which defines a standardized\nprocess for transforming data styles across different tasks, unifying their\ntraining sets into similarly diversified styles to prevent superficial\nforgetting caused by style shifts. Building on this, we propose RegLoRA to\nmitigate essential forgetting. RegLoRA stabilizes key parameters where prior\nknowledge is primarily stored by applying regularization, enabling the model to\nretain existing competencies. Experimental results demonstrate that our overall\nmethod, SEFE, achieves state-of-the-art performance.\n","authors":["Jinpeng Chen","Runmin Cong","Yuzhi Zhao","Hongzheng Yang","Guangneng Hu","Horace Ho Shing Ip","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2505.02486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02485v1","updated":"2025-05-05T09:08:25Z","published":"2025-05-05T09:08:25Z","title":"Integrating Column Generation and Large Neighborhood Search for Bus\n  Driver Scheduling with Complex Break Constraints","summary":"  The Bus Driver Scheduling Problem (BDSP) is a combinatorial optimization\nproblem with the goal to design shifts to cover prearranged bus tours. The\nobjective takes into account the operational cost as well as the satisfaction\nof drivers. This problem is heavily constrained due to strict legal rules and\ncollective agreements. The objective of this article is to provide\nstate-of-the-art exact and hybrid solution methods that can provide\nhigh-quality solutions for instances of different sizes. This work presents a\ncomprehensive study of both an exact method, Branch and Price (B&P), as well as\na Large Neighborhood Search (LNS) framework which uses B&P or Column Generation\n(CG) for the repair phase to solve the BDSP. It further proposes and evaluates\na novel deeper integration of B&P and LNS, storing the generated columns from\nthe LNS subproblems and reusing them for other subproblems, or to find better\nglobal solutions. The article presents a detailed analysis of several\ncomponents of the solution methods and their impact, including general\nimprovements for the B&P subproblem, which is a high-dimensional Resource\nConstrained Shortest Path Problem (RCSPP), and the components of the LNS. The\nevaluation shows that our approach provides new state-of-the-art results for\ninstances of all sizes, including exact solutions for small instances, and low\ngaps to a known lower bound for mid-sized instances. Conclusions: We observe\nthat B&P provides the best results for small instances, while the tight\nintegration of LNS and CG can provide high-quality solutions for larger\ninstances, further improving over LNS which just uses CG as a black box. The\nproposed methods are general and can also be applied to other rule sets and\nrelated optimization problems\n","authors":["Lucas Kletzander","Tommaso Mannelli Mazzoli","Nysret Musliu","Pascal Van Hentenryck"],"pdf_url":"https://arxiv.org/pdf/2505.02485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02484v1","updated":"2025-05-05T09:07:22Z","published":"2025-05-05T09:07:22Z","title":"El Agente: An Autonomous Agent for Quantum Chemistry","summary":"  Computational chemistry tools are widely used to study the behaviour of\nchemical phenomena. Yet, the complexity of these tools can make them\ninaccessible to non-specialists and challenging even for experts. In this work,\nwe introduce El Agente Q, an LLM-based multi-agent system that dynamically\ngenerates and executes quantum chemistry workflows from natural language user\nprompts. The system is built on a novel cognitive architecture featuring a\nhierarchical memory framework that enables flexible task decomposition,\nadaptive tool selection, post-analysis, and autonomous file handling and\nsubmission. El Agente Q is benchmarked on six university-level course exercises\nand two case studies, demonstrating robust problem-solving performance\n(averaging >87% task success) and adaptive error handling through in situ\ndebugging. It also supports longer-term, multi-step task execution for more\ncomplex workflows, while maintaining transparency through detailed action trace\nlogs. Together, these capabilities lay the foundation for increasingly\nautonomous and accessible quantum chemistry.\n","authors":["Yunheng Zou","Austin H. Cheng","Abdulrahman Aldossary","Jiaru Bai","Shi Xuan Leong","Jorge Arturo Campos-Gonzalez-Angulo","Changhyeok Choi","Cher Tian Ser","Gary Tom","Andrew Wang","Zijian Zhang","Ilya Yakavets","Han Hao","Chris Crebolder","Varinia Bernales","Alán Aspuru-Guzik"],"pdf_url":"https://arxiv.org/pdf/2505.02484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02483v1","updated":"2025-05-05T09:06:17Z","published":"2025-05-05T09:06:17Z","title":"Automated Hybrid Reward Scheduling via Large Language Models for Robotic\n  Skill Learning","summary":"  Enabling a high-degree-of-freedom robot to learn specific skills is a\nchallenging task due to the complexity of robotic dynamics. Reinforcement\nlearning (RL) has emerged as a promising solution; however, addressing such\nproblems requires the design of multiple reward functions to account for\nvarious constraints in robotic motion. Existing approaches typically sum all\nreward components indiscriminately to optimize the RL value function and\npolicy. We argue that this uniform inclusion of all reward components in policy\noptimization is inefficient and limits the robot's learning performance. To\naddress this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework\nbased on Large Language Models (LLMs). This paradigm dynamically adjusts the\nlearning intensity of each reward component throughout the policy optimization\nprocess, enabling robots to acquire skills in a gradual and structured manner.\nSpecifically, we design a multi-branch value network, where each branch\ncorresponds to a distinct reward component. During policy optimization, each\nbranch is assigned a weight that reflects its importance, and these weights are\nautomatically computed based on rules designed by LLMs. The LLM generates a\nrule set in advance, derived from the task description, and during training, it\nselects a weight calculation rule from the library based on language prompts\nthat evaluate the performance of each branch. Experimental results demonstrate\nthat the AHRS method achieves an average 6.48% performance improvement across\nmultiple high-degree-of-freedom robotic tasks.\n","authors":["Changxin Huang","Junyang Liang","Yanbin Chang","Jingzhao Xu","Jianqiang Li"],"pdf_url":"https://arxiv.org/pdf/2505.02483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04667v2","updated":"2025-05-05T09:01:06Z","published":"2025-02-07T05:21:13Z","title":"Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances\n  Reasoning Generalization","summary":"  The integration of explicit Chain-of-Thought (CoT) reasoning into training\nlarge language models (LLMs) has advanced their reasoning capabilities, yet the\nmechanisms by which CoT enhances generalization remain poorly understood. This\nwork investigates (1) \\textit{how} CoT training reshapes internal model\nrepresentations and (2) \\textit{why} it improves both in-distribution (ID) and\nout-of-distribution (OOD) reasoning generalization. Through controlled\nexperiments and theoretical analysis, we derive the following key insights.\n\\textbf{1)} Structural Advantage: CoT training internalizes reasoning into a\ntwo-stage generalizing circuit, where the number of stages corresponds to the\nexplicit reasoning steps during training. Notably, CoT-trained models resolve\nintermediate results at shallower layers compared to non-CoT counterparts,\nfreeing up deeper layers to specialize in subsequent reasoning steps.\n\\textbf{2)} Theoretical Analysis: the information-theoretic generalization\nbounds via distributional divergence can be decomposed into ID and OOD\ncomponents. While ID error diminishes with sufficient training regardless of\nCoT, OOD error critically depends on CoT: Non-CoT training fails to generalize\nto OOD samples due to unseen reasoning patterns, whereas CoT training achieves\nnear-perfect OOD generalization by mastering subtasks and reasoning\ncompositions during training. The identified mechanisms explain our\nexperimental results: CoT training accelerates convergence and enhances\ngeneralization from ID to both ID and OOD scenarios while maintaining robust\nperformance even with tolerable noise. These findings are further validated on\ncomplex real-world datasets. This paper offers valuable insights for designing\nCoT strategies to enhance LLM reasoning robustness.\n","authors":["Xinhao Yao","Ruifeng Ren","Yun Liao","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.04667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02467v1","updated":"2025-05-05T08:53:21Z","published":"2025-05-05T08:53:21Z","title":"Timing Is Everything: Finding the Optimal Fusion Points in Multimodal\n  Medical Imaging","summary":"  Multimodal deep learning harnesses diverse imaging modalities, such as MRI\nsequences, to enhance diagnostic accuracy in medical imaging. A key challenge\nis determining the optimal timing for integrating these\nmodalities-specifically, identifying the network layers where fusion modules\nshould be inserted. Current approaches often rely on manual tuning or\nexhaustive search, which are computationally expensive without any guarantee of\nconverging to optimal results. We propose a sequential forward search algorithm\nthat incrementally activates and evaluates candidate fusion modules at\ndifferent layers of a multimodal network. At each step, the algorithm retrains\nfrom previously learned weights and compares validation loss to identify the\nbest-performing configuration. This process systematically reduces the search\nspace, enabling efficient identification of the optimal fusion timing without\nexhaustively testing all possible module placements. The approach is validated\non two multimodal MRI datasets, each addressing different classification tasks.\nOur algorithm consistently identified configurations that outperformed unimodal\nbaselines, late fusion, and a brute-force ensemble of all potential fusion\nplacements. These architectures demonstrated superior accuracy, F-score, and\nspecificity while maintaining competitive or improved AUC values. Furthermore,\nthe sequential nature of the search significantly reduced computational\noverhead, making the optimization process more practical. By systematically\ndetermining the optimal timing to fuse imaging modalities, our method advances\nmultimodal deep learning for medical imaging. It provides an efficient and\nrobust framework for fusion optimization, paving the way for improved clinical\ndecision-making and more adaptable, scalable architectures in medical AI\napplications.\n","authors":["Valerio Guarrasi","Klara Mogensen","Sara Tassinari","Sara Qvarlander","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2505.02467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02181v3","updated":"2025-05-05T08:49:12Z","published":"2023-11-03T18:16:00Z","title":"Joint Problems in Learning Multiple Dynamical Systems","summary":"  Clustering of time series is a well-studied problem, with applications\nranging from quantitative, personalized models of metabolism obtained from\nmetabolite concentrations to state discrimination in quantum information\ntheory. We consider a variant, where given a set of trajectories and a number\nof parts, we jointly partition the set of trajectories and learn linear\ndynamical system (LDS) models for each part, so as to minimize the maximum\nerror across all the models. We present globally convergent methods and EM\nheuristics, accompanied by promising computational results. The key highlight\nof this method is that it does not require a predefined hidden state dimension\nbut instead provides an upper bound. Additionally, it offers guidance for\ndetermining regularization in the system identification.\n","authors":["Mengjia Niu","Xiaoyu He","Petr Ryšavý","Quan Zhou","Jakub Marecek"],"pdf_url":"https://arxiv.org/pdf/2311.02181v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02462v1","updated":"2025-05-05T08:45:26Z","published":"2025-05-05T08:45:26Z","title":"Incentivizing Inclusive Contributions in Model Sharing Markets","summary":"  While data plays a crucial role in training contemporary AI models, it is\nacknowledged that valuable public data will be exhausted in a few years,\ndirecting the world's attention towards the massive decentralized private data.\nHowever, the privacy-sensitive nature of raw data and lack of incentive\nmechanism prevent these valuable data from being fully exploited. Addressing\nthese challenges, this paper proposes inclusive and incentivized personalized\nfederated learning (iPFL), which incentivizes data holders with diverse\npurposes to collaboratively train personalized models without revealing raw\ndata. iPFL constructs a model-sharing market by solving a graph-based training\noptimization and incorporates an incentive mechanism based on game theory\nprinciples. Theoretical analysis shows that iPFL adheres to two key incentive\nproperties: individual rationality and truthfulness. Empirical studies on\neleven AI tasks (e.g., large language models' instruction-following tasks)\ndemonstrate that iPFL consistently achieves the highest economic utility, and\nbetter or comparable model performance compared to baseline methods. We\nanticipate that our iPFL can serve as a valuable technique for boosting future\nAI models on decentralized private data while making everyone satisfied.\n","authors":["Enpei Zhang","Jingyi Chai","Rui Ye","Yanfeng Wang","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2505.02462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02443v1","updated":"2025-05-05T08:11:20Z","published":"2025-05-05T08:11:20Z","title":"Investigating the Impact of Personalized AI Tutors on Language Learning\n  Performance","summary":"  Driven by the global shift towards online learning prompted by the COVID 19\npandemic, Artificial Intelligence has emerged as a pivotal player in the field\nof education. Intelligent Tutoring Systems offer a new method of personalized\nteaching, replacing the limitations of traditional teaching methods. However,\nconcerns arise about the ability of AI tutors to address skill development and\nengagement during the learning process. In this paper, I will conduct a quasi\nexperiment with paired sample t test on 34 students pre and post use of AI\ntutors in language learning platforms like Santa and Duolingo to examine the\nrelationship between students engagement, academic performance, and students\nsatisfaction during a personalized language learning experience.\n","authors":["Simon Suh"],"pdf_url":"https://arxiv.org/pdf/2505.02443v1.pdf","comment":"16 pages, 4 figures, 1 table, Uses three theoretical frameworks like\n  Domain modeling, Gardner Theory of Multiple Intelligences, and Zone of\n  Proximal Development"},{"id":"http://arxiv.org/abs/2505.02441v1","updated":"2025-05-05T08:10:22Z","published":"2025-05-05T08:10:22Z","title":"MSFNet-CPD: Multi-Scale Cross-Modal Fusion Network for Crop Pest\n  Detection","summary":"  Accurate identification of agricultural pests is essential for crop\nprotection but remains challenging due to the large intra-class variance and\nfine-grained differences among pest species. While deep learning has advanced\npest detection, most existing approaches rely solely on low-level visual\nfeatures and lack effective multi-modal integration, leading to limited\naccuracy and poor interpretability. Moreover, the scarcity of high-quality\nmulti-modal agricultural datasets further restricts progress in this field. To\naddress these issues, we construct two novel multi-modal benchmarks-CTIP102 and\nSTIP102-based on the widely-used IP102 dataset, and introduce a Multi-scale\nCross-Modal Fusion Network (MSFNet-CPD) for robust pest detection. Our approach\nenhances visual quality via a super-resolution reconstruction module, and feeds\nboth the original and reconstructed images into the network to improve clarity\nand detection performance. To better exploit semantic cues, we propose an\nImage-Text Fusion (ITF) module for joint modeling of visual and textual\nfeatures, and an Image-Text Converter (ITC) that reconstructs fine-grained\ndetails across multiple scales to handle challenging backgrounds. Furthermore,\nwe introduce an Arbitrary Combination Image Enhancement (ACIE) strategy to\ngenerate a more complex and diverse pest detection dataset, MTIP102, improving\nthe model's generalization to real-world scenarios. Extensive experiments\ndemonstrate that MSFNet-CPD consistently outperforms state-of-the-art methods\non multiple pest detection benchmarks. All code and datasets will be made\npublicly available at: https://github.com/Healer-ML/MSFNet-CPD.\n","authors":["Jiaqi Zhang","Zhuodong Liu","Kejian Yu"],"pdf_url":"https://arxiv.org/pdf/2505.02441v1.pdf","comment":"Accepted to IJCNN 2025"},{"id":"http://arxiv.org/abs/2505.02439v1","updated":"2025-05-05T08:09:36Z","published":"2025-05-05T08:09:36Z","title":"ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control\n  via Hierarchical Reinforcement Learning","summary":"  The building thermodynamics model, which predicts real-time indoor\ntemperature changes under potential HVAC (Heating, Ventilation, and Air\nConditioning) control operations, is crucial for optimizing HVAC control in\nbuildings. While pioneering studies have attempted to develop such models for\nvarious building environments, these models often require extensive data\ncollection periods and rely heavily on expert knowledge, making the modeling\nprocess inefficient and limiting the reusability of the models. This paper\nexplores a model ensemble perspective that utilizes existing developed models\nas base models to serve a target building environment, thereby providing\naccurate predictions while reducing the associated efforts. Given that building\ndata streams are non-stationary and the number of base models may increase, we\npropose a Hierarchical Reinforcement Learning (HRL) approach to dynamically\nselect and weight the base models. Our approach employs a two-tiered\ndecision-making process: the high-level focuses on model selection, while the\nlow-level determines the weights of the selected models. We thoroughly evaluate\nthe proposed approach through offline experiments and an on-site case study,\nand the experimental results demonstrate the effectiveness of our method.\n","authors":["Yang Deng","Yaohui Liu","Rui Liang","Dafang Zhao","Donghua Xie","Ittetsu Taniguchi","Dan Wang"],"pdf_url":"https://arxiv.org/pdf/2505.02439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02435v1","updated":"2025-05-05T08:01:56Z","published":"2025-05-05T08:01:56Z","title":"A New Approach to Backtracking Counterfactual Explanations: A Causal\n  Framework for Efficient Model Interpretability","summary":"  Counterfactual explanations enhance interpretability by identifying\nalternative inputs that produce different outputs, offering localized insights\ninto model decisions. However, traditional methods often neglect causal\nrelationships, leading to unrealistic examples. While newer approaches\nintegrate causality, they are computationally expensive. To address these\nchallenges, we propose an efficient method based on backtracking\ncounterfactuals that incorporates causal reasoning to generate actionable\nexplanations. We first examine the limitations of existing methods and then\nintroduce our novel approach and its features. We also explore the relationship\nbetween our method and previous techniques, demonstrating that it generalizes\nthem in specific scenarios. Finally, experiments show that our method provides\ndeeper insights into model outputs.\n","authors":["Pouria Fatemi","Ehsan Sharifian","Mohammad Hossein Yassaee"],"pdf_url":"https://arxiv.org/pdf/2505.02435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02433v1","updated":"2025-05-05T07:58:54Z","published":"2025-05-05T07:58:54Z","title":"FairPO: Robust Preference Optimization for Fair Multi-Label Learning","summary":"  We propose FairPO, a novel framework designed to promote fairness in\nmulti-label classification by directly optimizing preference signals with a\ngroup robustness perspective. In our framework, the set of labels is\npartitioned into privileged and non-privileged groups, and a preference-based\nloss inspired by Direct Preference Optimization (DPO) is employed to more\neffectively differentiate true positive labels from confusing negatives within\nthe privileged group, while preserving baseline classification performance for\nnon-privileged labels. By framing the learning problem as a robust optimization\nover groups, our approach dynamically adjusts the training emphasis toward\ngroups with poorer performance, thereby mitigating bias and ensuring a fairer\ntreatment across diverse label categories. In addition, we outline plans to\nextend this approach by investigating alternative loss formulations such as\nSimple Preference Optimisation (SimPO) and Contrastive Preference Optimization\n(CPO) to exploit reference-free reward formulations and contrastive training\nsignals. Furthermore, we plan to extend FairPO with multilabel generation\ncapabilities, enabling the model to dynamically generate diverse and coherent\nlabel sets for ambiguous inputs.\n","authors":["Soumen Kumar Mondal","Akshit Varmora","Prateek Chanda","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2505.02433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13620v4","updated":"2025-05-05T07:58:38Z","published":"2025-01-23T12:42:42Z","title":"A Cognitive Paradigm Approach to Probe the Perception-Reasoning\n  Interface in VLMs","summary":"  A fundamental challenge in artificial intelligence involves understanding the\ncognitive mechanisms underlying visual reasoning in sophisticated models like\nVision-Language Models (VLMs). How do these models integrate visual perception\nwith abstract thought, especially when reasoning across multiple images or\nrequiring fine-grained compositional understanding? Drawing inspiration from\ncognitive science, this paper introduces a structured evaluation framework\nusing diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to\ndissect the perception-reasoning interface in VLMs. We propose three distinct\nevaluation paradigms, mirroring human problem-solving strategies: Direct Visual\nRule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule\nextraction and application), and Componential Analysis (CA; analytical\ndecomposition via task-agnostic textual descriptions). These paradigms\nsystematically vary cognitive load and probe processing stages. Notably, CA\nenables multi-image reasoning evaluation even for single-image architectures\nand isolates reasoning from perception by operating on textual descriptions.\nApplying this framework, we demonstrate that CA, leveraging powerful language\nmodels for reasoning over rich, independently generated descriptions, achieves\nnew state-of-the-art (SOTA) performance on challenging benchmarks including\nBongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm\nreasoning improves significantly when perceptual challenges are mitigated,\nrevealing a critical perception bottleneck. Our framework provides a valuable\ndiagnostic tool and suggests that decoupling perception (via rich,\ntask-agnostic description) from reasoning is a promising direction for robust\nand general visual intelligence.\n","authors":["Mohit Vaishnav","Tanel Tammet"],"pdf_url":"https://arxiv.org/pdf/2501.13620v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01183v2","updated":"2025-05-05T07:48:02Z","published":"2024-06-03T10:39:12Z","title":"Automatic Input Feature Relevance via Spectral Neural Networks","summary":"  In machine learning practice it is often useful to identify relevant input\nfeatures, so as to obtain compact dataset for more efficient numerical\nhandling. On the other hand, by isolating key input elements, ranked according\ntheir respective degree of relevance, can help to elaborate on the process of\ndecision making. Here, we propose a novel method to estimate the relative\nimportance of the input components for a Deep Neural Network. This is achieved\nby leveraging on a spectral re-parametrization of the optimization process.\nEigenvalues associated to input nodes provide in fact a robust proxy to gauge\nthe relevance of the supplied entry features. Notably, the spectral features\nranking is performed automatically, as a byproduct of the network training,\nwith no additional processing to be carried out. The technique is successfully\nchallenged against both synthetic and real data.\n","authors":["Lorenzo Chicchi","Lorenzo Buffoni","Diego Febbe","Lorenzo Giambagli","Raffaele Marino","Duccio Fanelli"],"pdf_url":"https://arxiv.org/pdf/2406.01183v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02426v1","updated":"2025-05-05T07:46:21Z","published":"2025-05-05T07:46:21Z","title":"Towards One-shot Federated Learning: Advances, Challenges, and Future\n  Directions","summary":"  One-shot FL enables collaborative training in a single round, eliminating the\nneed for iterative communication, making it particularly suitable for use in\nresource-constrained and privacy-sensitive applications. This survey offers a\nthorough examination of One-shot FL, highlighting its distinct operational\nframework compared to traditional federated approaches. One-shot FL supports\nresource-limited devices by enabling single-round model aggregation while\nmaintaining data locality. The survey systematically categorizes existing\nmethodologies, emphasizing advancements in client model initialization,\naggregation techniques, and strategies for managing heterogeneous data\ndistributions. Furthermore, we analyze the limitations of current approaches,\nparticularly in terms of scalability and generalization in non-IID settings. By\nanalyzing cutting-edge techniques and outlining open challenges, this survey\naspires to provide a comprehensive reference for researchers and practitioners\naiming to design and implement One-shot FL systems, advancing the development\nand adoption of One-shot FL solutions in a real-world, resource-constrained\nscenario.\n","authors":["Flora Amato","Lingyu Qiu","Mohammad Tanveer","Salvatore Cuomo","Fabio Giampaolo","Francesco Piccialli"],"pdf_url":"https://arxiv.org/pdf/2505.02426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02417v1","updated":"2025-05-05T07:22:54Z","published":"2025-05-05T07:22:54Z","title":"T2S: High-resolution Time Series Generation with Text-to-Series\n  Diffusion Models","summary":"  Text-to-Time Series generation holds significant potential to address\nchallenges such as data sparsity, imbalance, and limited availability of\nmultimodal time series datasets across domains. While diffusion models have\nachieved remarkable success in Text-to-X (e.g., vision and audio data)\ngeneration, their use in time series generation remains in its nascent stages.\nExisting approaches face two critical limitations: (1) the lack of systematic\nexploration of general-proposed time series captions, which are often\ndomain-specific and struggle with generalization; and (2) the inability to\ngenerate time series of arbitrary lengths, limiting their applicability to\nreal-world scenarios. In this work, we first categorize time series captions\ninto three levels: point-level, fragment-level, and instance-level.\nAdditionally, we introduce a new fragment-level dataset containing over 600,000\nhigh-resolution time series-text pairs. Second, we propose Text-to-Series\n(T2S), a diffusion-based framework that bridges the gap between natural\nlanguage and time series in a domain-agnostic manner. T2S employs a\nlength-adaptive variational autoencoder to encode time series of varying\nlengths into consistent latent embeddings. On top of that, T2S effectively\naligns textual representations with latent embeddings by utilizing Flow\nMatching and employing Diffusion Transformer as the denoiser. We train T2S in\nan interleaved paradigm across multiple lengths, allowing it to generate\nsequences of any desired length. Extensive evaluations demonstrate that T2S\nachieves state-of-the-art performance across 13 datasets spanning 12 domains.\n","authors":["Yunfeng Ge","Jiawei Li","Yiji Zhao","Haomin Wen","Zhao Li","Meikang Qiu","Hongyan Li","Ming Jin","Shirui Pan"],"pdf_url":"https://arxiv.org/pdf/2505.02417v1.pdf","comment":"Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2505.02413v1","updated":"2025-05-05T07:18:47Z","published":"2025-05-05T07:18:47Z","title":"Task-Oriented Semantic Communication in Large Multimodal Models-based\n  Vehicle Networks","summary":"  Task-oriented semantic communication has emerged as a fundamental approach\nfor enhancing performance in various communication scenarios. While recent\nadvances in Generative Artificial Intelligence (GenAI), such as Large Language\nModels (LLMs), have been applied to semantic communication designs, the\npotential of Large Multimodal Models (LMMs) remains largely unexplored. In this\npaper, we investigate an LMM-based vehicle AI assistant using a Large Language\nand Vision Assistant (LLaVA) and propose a task-oriented semantic communication\nframework to facilitate efficient interaction between users and cloud servers.\nTo reduce computational demands and shorten response time, we optimize LLaVA's\nimage slicing to selectively focus on areas of utmost interest to users.\nAdditionally, we assess the importance of image patches by combining objective\nand subjective user attention, adjusting energy usage for transmitting semantic\ninformation. This strategy optimizes resource utilization, ensuring precise\ntransmission of critical information. We construct a Visual Question Answering\n(VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental\nresults show that our semantic communication framework significantly increases\naccuracy in answering questions under the same channel conditions, performing\nparticularly well in environments with poor Signal-to-Noise Ratios (SNR).\nAccuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB,\nrespectively.\n","authors":["Baoxia Du","Hongyang Du","Dusit Niyato","Ruidong Li"],"pdf_url":"https://arxiv.org/pdf/2505.02413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18267v2","updated":"2025-05-05T07:05:20Z","published":"2025-04-25T11:26:41Z","title":"Neural operators struggle to learn complex PDEs in pedestrian mobility:\n  Hughes model case study","summary":"  This paper investigates the limitations of neural operators in learning\nsolutions for a Hughes model, a first-order hyperbolic conservation law system\nfor crowd dynamics. The model couples a Fokker-Planck equation representing\npedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes\nmodel belongs to the class of nonlinear hyperbolic systems that often exhibit\ncomplex solution structures, including shocks and discontinuities. In this\nstudy, we assess the performance of three state-of-the-art neural operators\n(Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural\nOperator) in various challenging scenarios. Specifically, we consider (1)\ndiscontinuous and Gaussian initial conditions and (2) diverse boundary\nconditions, while also examining the impact of different numerical schemes.\n  Our results show that these neural operators perform well in easy scenarios\nwith fewer discontinuities in the initial condition, yet they struggle in\ncomplex scenarios with multiple initial discontinuities and dynamic boundary\nconditions, even when trained specifically on such complex samples. The\npredicted solutions often appear smoother, resulting in a reduction in total\nvariation and a loss of important physical features. This smoothing behavior is\nsimilar to issues discussed by Daganzo (1995), where models that introduce\nartificial diffusion were shown to miss essential features such as shock waves\nin hyperbolic systems. These results suggest that current neural operator\narchitectures may introduce unintended regularization effects that limit their\nability to capture transport dynamics governed by discontinuities. They also\nraise concerns about generalizing these methods to traffic applications where\nshock preservation is essential.\n","authors":["Prajwal Chauhan","Salah Eddine Choutri","Mohamed Ghattassi","Nader Masmoudi","Saif Eddin Jabari"],"pdf_url":"https://arxiv.org/pdf/2504.18267v2.pdf","comment":"26 pages, 15 figures, 6 tables, under review at Artificial\n  Intelligence for Transportation | Journal"},{"id":"http://arxiv.org/abs/2505.02410v1","updated":"2025-05-05T07:03:41Z","published":"2025-05-05T07:03:41Z","title":"Bielik 11B v2 Technical Report","summary":"  We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages.\n","authors":["Krzysztof Ociepa","Łukasz Flis","Krzysztof Wróbel","Adrian Gwoździej","Remigiusz Kinas"],"pdf_url":"https://arxiv.org/pdf/2505.02410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15210v2","updated":"2025-05-05T06:56:16Z","published":"2025-04-21T16:29:07Z","title":"Integrating Symbolic Execution into the Fine-Tuning of Code-Generating\n  LLMs","summary":"  Code-generating Large Language Models (LLMs) have become essential tools in\nmodern software development, enhancing productivity and accelerating\ndevelopment. This paper aims to investigate the fine-tuning of code-generating\nLLMs using Reinforcement Learning and Direct Preference Optimization, further\nimproving their performance. To achieve this, we enhance the training data for\nthe reward model with the help of symbolic execution techniques, ensuring more\ncomprehensive and objective data. With symbolic execution, we create a custom\ndataset that better captures the nuances in code evaluation. Our reward models,\nfine-tuned on this dataset, demonstrate significant improvements over the\nbaseline, CodeRL, in estimating the quality of generated code. Our\ncode-generating LLMs, trained with the help of reward model feedback, achieve\nsimilar results compared to the CodeRL benchmark.\n","authors":["Marina Sakharova","Abhinav Anand","Mira Mezini"],"pdf_url":"https://arxiv.org/pdf/2504.15210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02396v1","updated":"2025-05-05T06:40:08Z","published":"2025-05-05T06:40:08Z","title":"Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and\n  CNN from Scratch","summary":"  Pneumonia Diagnosis, though it is crucial for an effective treatment, it can\nbe hampered by uncertainty. This uncertainty starts to arise due to some\nfactors like atypical presentations, limitations of diagnostic tools such as\nchest X-rays, and the presence of co-existing respiratory conditions. This\nresearch proposes one of the supervised learning methods, CNN. Using\nMobileNetV2 as the pre-trained one with ResNet101V2 architecture and using\nKeras API as the built from scratch model, for identifying lung diseases\nespecially pneumonia. The datasets used in this research were obtained from the\nwebsite through Kaggle. The result shows that by implementing CNN MobileNetV2\nand CNN from scratch the result is promising. While validating data,\nMobileNetV2 performs with stability and minimal overfitting, while the training\naccuracy increased to 84.87% later it slightly decreased to 78.95%, with\nincreasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is\nmore stable. Although it takes more time to train each epoch. Meanwhile, after\nthe 10th epoch, the Scratch model displayed more instability and overfitting\ndespite having higher validation accuracy, training accuracy decreased\nsignificantly to 78.12% and the validation loss increased from 0.5698 to\n1.1809. With these results, ResNet101V2 offers stability, and the Scratch model\noffers high accuracy.\n","authors":["Kennard Norbert Sudiardjo","Islam Nur Alam","Wilson Wijaya","Lili Ayu Wulandhari"],"pdf_url":"https://arxiv.org/pdf/2505.02396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02391v1","updated":"2025-05-05T06:26:00Z","published":"2025-05-05T06:26:00Z","title":"Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization\n  in Rejection Sampling and RL","summary":"  Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.\n","authors":["Jiarui Yao","Yifan Hao","Hanning Zhang","Hanze Dong","Wei Xiong","Nan Jiang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.02391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02390v1","updated":"2025-05-05T06:25:20Z","published":"2025-05-05T06:25:20Z","title":"Quantitative Analysis of Performance Drop in DeepSeek Model Quantization","summary":"  Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally,\npossibly because the official service often suffers from being busy and some\norganizations have data privacy concerns. While single-machine deployment\noffers infrastructure simplicity, the models' 671B FP8 parameter configuration\nexceeds the practical memory limits of a standard 8-GPU machine. Quantization\nis a widely used technique that helps reduce model memory consumption. However,\nit is unclear what the performance of DeepSeek-R1 and V3 will be after being\nquantized. This technical report presents the first quantitative evaluation of\nmulti-bitwidth quantization across the complete DeepSeek model spectrum. Key\nfindings reveal that 4-bit quantization maintains little performance\ndegradation versus FP8 while enabling single-machine deployment on standard\nNVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization\nmethod that significantly outperforms traditional Q3_K_M variant on various\nbenchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach\nin most tasks. Moreover, DQ3_K_M supports single-machine deployment\nconfigurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of\nDQ3\\_K\\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing\noptimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.\n","authors":["Enbo Zhao","Yi Shen","Shuming Shi","Jieyun Huang","Zhihao Chen","Ning Wang","Siqi Xiao","Jian Zhang","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2505.02390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02388v1","updated":"2025-05-05T06:13:25Z","published":"2025-05-05T06:13:25Z","title":"MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans","summary":"  Embodied AI (EAI) research requires high-quality, diverse 3D scenes to\neffectively support skill acquisition, sim-to-real transfer, and\ngeneralization. Achieving these quality standards, however, necessitates the\nprecise replication of real-world object diversity. Existing datasets\ndemonstrate that this process heavily relies on artist-driven designs, which\ndemand substantial human effort and present significant scalability challenges.\nTo scalably produce realistic and interactive 3D scenes, we first present\nMetaScenes, a large-scale, simulatable 3D scene dataset constructed from\nreal-world scans, which includes 15366 objects spanning 831 fine-grained\ncategories. Then, we introduce Scan2Sim, a robust multi-modal alignment model,\nwhich enables the automated, high-quality replacement of assets, thereby\neliminating the reliance on artist-driven designs for scaling 3D scenes. We\nfurther propose two benchmarks to evaluate MetaScenes: a detailed scene\nsynthesis task focused on small item layouts for robotic manipulation and a\ndomain transfer task in vision-and-language navigation (VLN) to validate\ncross-domain transfer. Results confirm MetaScene's potential to enhance EAI by\nsupporting more generalizable agent learning and sim-to-real applications,\nintroducing new possibilities for EAI research. Project website:\nhttps://meta-scenes.github.io/.\n","authors":["Huangyue Yu","Baoxiong Jia","Yixin Chen","Yandan Yang","Puhao Li","Rongpeng Su","Jiaxin Li","Qing Li","Wei Liang","Song-Chun Zhu","Tengyu Liu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.02388v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2505.02387v1","updated":"2025-05-05T06:11:12Z","published":"2025-05-05T06:11:12Z","title":"RM-R1: Reward Modeling as Reasoning","summary":"  Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.\n","authors":["Xiusi Chen","Gaotang Li","Ziqi Wang","Bowen Jin","Cheng Qian","Yu Wang","Hongru Wang","Yu Zhang","Denghui Zhang","Tong Zhang","Hanghang Tong","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2505.02387v1.pdf","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.00503v2","updated":"2025-05-05T06:00:10Z","published":"2025-05-01T13:14:07Z","title":"Variational OOD State Correction for Offline Reinforcement Learning","summary":"  The performance of Offline reinforcement learning is significantly impacted\nby the issue of state distributional shift, and out-of-distribution (OOD) state\ncorrection is a popular approach to address this problem. In this paper, we\npropose a novel method named Density-Aware Safety Perception (DASP) for OOD\nstate correction. Specifically, our method encourages the agent to prioritize\nactions that lead to outcomes with higher data density, thereby promoting its\noperation within or the return to in-distribution (safe) regions. To achieve\nthis, we optimize the objective within a variational framework that\nconcurrently considers both the potential outcomes of decision-making and their\ndensity, thus providing crucial contextual information for safe\ndecision-making. Finally, we validate the effectiveness and feasibility of our\nproposed method through extensive experimental evaluations on the offline\nMuJoCo and AntMaze suites.\n","authors":["Ke Jiang","Wen Jiang","Masahiro Fujisawa","Xiaoyang Tan"],"pdf_url":"https://arxiv.org/pdf/2505.00503v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02370v1","updated":"2025-05-05T05:19:40Z","published":"2025-05-05T05:19:40Z","title":"SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing","summary":"  Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.\n","authors":["Ming Li","Xin Gu","Fan Chen","Xiaoying Xing","Longyin Wen","Chen Chen","Sijie Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.02370v1.pdf","comment":"Code, Data and Models are available at:\n  https://github.com/bytedance/SuperEdit"},{"id":"http://arxiv.org/abs/2505.01182v2","updated":"2025-05-05T05:14:20Z","published":"2025-05-02T10:50:04Z","title":"TSTMotion: Training-free Scene-aware Text-to-motion Generation","summary":"  Text-to-motion generation has recently garnered significant research\ninterest, primarily focusing on generating human motion sequences in blank\nbackgrounds. However, human motions commonly occur within diverse 3D scenes,\nwhich has prompted exploration into scene-aware text-to-motion generation\nmethods. Yet, existing scene-aware methods often rely on large-scale\nground-truth motion sequences in diverse 3D scenes, which poses practical\nchallenges due to the expensive cost. To mitigate this challenge, we are the\nfirst to propose a \\textbf{T}raining-free \\textbf{S}cene-aware\n\\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that\nefficiently empowers pre-trained blank-background motion generators with the\nscene-aware capability. Specifically, conditioned on the given 3D scene and\ntext description, we adopt foundation models together to reason, predict and\nvalidate a scene-aware motion guidance. Then, the motion guidance is\nincorporated into the blank-background motion generators with two\nmodifications, resulting in scene-aware text-driven motion sequences. Extensive\nexperiments demonstrate the efficacy and generalizability of our proposed\nframework. We release our code in \\href{https://tstmotion.github.io/}{Project\nPage}.\n","authors":["Ziyan Guo","Haoxuan Qu","Hossein Rahmani","Dewen Soh","Ping Hu","Qiuhong Ke","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2505.01182v2.pdf","comment":"Accepted by ICME2025"},{"id":"http://arxiv.org/abs/2505.02369v1","updated":"2025-05-05T05:13:12Z","published":"2025-05-05T05:13:12Z","title":"Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural\n  Networks","summary":"  Generalizing well in deep neural networks remains a core challenge,\nparticularly due to their tendency to converge to sharp minima that degrade\nrobustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking\nflatter minima but perturbs parameters using the full gradient, which can\ninclude statistically insignificant directions. We propose ZSharp, a simple yet\neffective extension to SAM that applies layer-wise Z-score normalization\nfollowed by percentile-based filtering to retain only statistically significant\ngradient components. This selective perturbation aligns updates with\ncurvature-sensitive directions, enhancing generalization without requiring\narchitectural changes. ZSharp introduces only one additional hyperparameter,\nthe percentile threshold, and remains fully compatible with existing SAM\nvariants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,\nVGG, and Vision Transformers show that ZSharp consistently outperforms SAM and\nits variants in test accuracy, particularly on deeper and transformer-based\nmodels. These results demonstrate that ZSharp is a principled and lightweight\nimprovement for sharpness-aware optimization.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2505.02369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02366v1","updated":"2025-05-05T05:09:21Z","published":"2025-05-05T05:09:21Z","title":"JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for\n  Unsupervised Contrastive Learning of Sentence Embeddings","summary":"  Unsupervised contrastive learning has become a hot research topic in natural\nlanguage processing. Existing works usually aim at constraining the orientation\ndistribution of the representations of positive and negative samples in the\nhigh-dimensional semantic space in contrastive learning, but the semantic\nrepresentation tensor possesses both modulus and orientation features, and the\nexisting works ignore the modulus feature of the representations and cause\ninsufficient contrastive learning. % Therefore, we firstly propose a training\nobjective that aims at modulus constraints on the semantic representation\ntensor, to strengthen the alignment between the positive samples in contrastive\nlearning. Therefore, we first propose a training objective that is designed to\nimpose modulus constraints on the semantic representation tensor, to strengthen\nthe alignment between positive samples in contrastive learning. Then, the\nBERT-like model suffers from the phenomenon of sinking attention, leading to a\nlack of attention to CLS tokens that aggregate semantic information. In\nresponse, we propose a cross-attention structure among the twin-tower ensemble\nmodels to enhance the model's attention to CLS token and optimize the quality\nof CLS Pooling. Combining the above two motivations, we propose a new\n\\textbf{J}oint \\textbf{T}ensor representation modulus constraint and\n\\textbf{C}ross-attention unsupervised contrastive learning \\textbf{S}entence\n\\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven\nsemantic text similarity computation tasks, and the experimental results show\nthat JTCSE's twin-tower ensemble model and single-tower distillation model\noutperform the other baselines and become the current SOTA. In addition, we\nhave conducted an extensive zero-shot downstream task evaluation, which shows\nthat JTCSE outperforms other baselines overall on more than 130 tasks.\n","authors":["Tianyu Zong","Hongzhu Yi","Bingkang Shi","Yuanxiang Wang","Jungang Xu"],"pdf_url":"https://arxiv.org/pdf/2505.02366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02362v1","updated":"2025-05-05T04:48:20Z","published":"2025-05-05T04:48:20Z","title":"Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large\n  Language Models","summary":"  Email spam detection is a critical task in modern communication systems,\nessential for maintaining productivity, security, and user experience.\nTraditional machine learning and deep learning approaches, while effective in\nstatic settings, face significant limitations in adapting to evolving spam\ntactics, addressing class imbalance, and managing data scarcity. These\nchallenges necessitate innovative approaches that reduce dependency on\nextensive labeled datasets and frequent retraining. This study investigates the\neffectiveness of Zero-Shot Learning using FLAN-T5, combined with advanced\nNatural Language Processing (NLP) techniques such as BERT for email spam\ndetection. By employing BERT to preprocess and extract critical information\nfrom email content, and FLAN-T5 to classify emails in a Zero-Shot framework,\nthe proposed approach aims to address the limitations of traditional spam\ndetection systems. The integration of FLAN-T5 and BERT enables robust spam\ndetection without relying on extensive labeled datasets or frequent retraining,\nmaking it highly adaptable to unseen spam patterns and adversarial\nenvironments. This research highlights the potential of leveraging zero-shot\nlearning and NLPs for scalable and efficient spam detection, providing insights\ninto their capability to address the dynamic and challenging nature of spam\ndetection tasks.\n","authors":["Ghazaleh SHirvani","Saeid Ghasemshirazi"],"pdf_url":"https://arxiv.org/pdf/2505.02362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05314v3","updated":"2025-05-05T04:44:30Z","published":"2024-09-09T03:58:51Z","title":"Tele-LLMs: A Series of Specialized Large Language Models for\n  Telecommunications","summary":"  The emergence of large language models (LLMs) has significantly impacted\nvarious fields, from natural language processing to sectors like medicine and\nfinance. However, despite their rapid proliferation, the applications of LLMs\nin telecommunications remain limited, often relying on general-purpose models\nthat lack domain-specific specialization. This lack of specialization results\nin underperformance, particularly when dealing with telecommunications-specific\ntechnical terminology and their associated mathematical representations. This\npaper addresses this gap by first creating and disseminating Tele-Data, a\ncomprehensive dataset of telecommunications material curated from relevant\nsources, and Tele-Eval, a large-scale question-and-answer dataset tailored to\nthe domain. Through extensive experiments, we explore the most effective\ntraining techniques for adapting LLMs to the telecommunications domain, ranging\nfrom examining the division of expertise across various telecommunications\naspects to employing parameter-efficient techniques. We also investigate how\nmodels of different sizes behave during adaptation and analyze the impact of\ntheir training data on this behavior. Leveraging these findings, we develop and\nopen-source Tele-LLMs, the first series of language models ranging from 1B to\n8B parameters, specifically tailored for telecommunications. Our evaluations\ndemonstrate that these models outperform their general-purpose counterparts on\nTele-Eval and telecommunications-related literature tasks while retaining their\npreviously acquired capabilities, thus avoiding the catastrophic forgetting\nphenomenon.\n","authors":["Ali Maatouk","Kenny Chirino Ampudia","Rex Ying","Leandros Tassiulas"],"pdf_url":"https://arxiv.org/pdf/2409.05314v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02360v1","updated":"2025-05-05T04:41:21Z","published":"2025-05-05T04:41:21Z","title":"Catastrophic Overfitting, Entropy Gap and Participation Ratio: A\n  Noiseless $l^p$ Norm Solution for Fast Adversarial Training","summary":"  Adversarial training is a cornerstone of robust deep learning, but fast\nmethods like the Fast Gradient Sign Method (FGSM) often suffer from\nCatastrophic Overfitting (CO), where models become robust to single-step\nattacks but fail against multi-step variants. While existing solutions rely on\nnoise injection, regularization, or gradient clipping, we propose a novel\nsolution that purely controls the $l^p$ training norm to mitigate CO.\n  Our study is motivated by the empirical observation that CO is more prevalent\nunder the $l^{\\infty}$ norm than the $l^2$ norm. Leveraging this insight, we\ndevelop a framework for generalized $l^p$ attack as a fixed point problem and\ncraft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to\n$l^{\\infty}$. This leads to our core insight: CO emerges when highly\nconcentrated gradients where information localizes in few dimensions interact\nwith aggressive norm constraints. By quantifying gradient concentration through\nParticipation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM\nthat automatically tunes the training norm based on gradient information.\nExtensive experiments demonstrate that this approach achieves strong robustness\nwithout requiring additional regularization or noise injection, providing a\nnovel and theoretically-principled pathway to mitigate the CO problem.\n","authors":["Fares B. Mehouachi","Saif Eddin Jabari"],"pdf_url":"https://arxiv.org/pdf/2505.02360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20903v2","updated":"2025-05-05T04:26:02Z","published":"2025-04-29T16:19:53Z","title":"Modeling AI-Human Collaboration as a Multi-Agent Adaptation","summary":"  We develop an agent-based simulation to formalize AI-human collaboration as a\nfunction of task structure, advancing a generalizable framework for strategic\ndecision-making in organizations. Distinguishing between heuristic-based human\nadaptation and rule-based AI search, we model interactions across modular\n(parallel) and sequenced (interdependent) tasks using an NK model. Our results\nreveal that in modular tasks, AI often substitutes for humans - delivering\nhigher payoffs unless human expertise is very high, and the AI search space is\neither narrowly focused or extremely broad. In sequenced tasks, interesting\ncomplementarities emerge. When an expert human initiates the search and AI\nsubsequently refines it, aggregate performance is maximized. Conversely, when\nAI leads, excessive heuristic refinement by the human can reduce payoffs. We\nalso show that even \"hallucinatory\" AI - lacking memory or structure - can\nimprove outcomes when augmenting low-capability humans by helping escape local\noptima. These results yield a robust implication: the effectiveness of AI-human\ncollaboration depends less on context or industry, and more on the underlying\ntask structure. By elevating task decomposition as the central unit of\nanalysis, our model provides a transferable lens for strategic decision-making\ninvolving humans and an agentic AI across diverse organizational settings.\n","authors":["Prothit Sen","Sai Mihir Jakkaraju"],"pdf_url":"https://arxiv.org/pdf/2504.20903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02352v1","updated":"2025-05-05T04:21:12Z","published":"2025-05-05T04:21:12Z","title":"Social Biases in Knowledge Representations of Wikidata separates Global\n  North from Global South","summary":"  Knowledge Graphs have become increasingly popular due to their wide usage in\nvarious downstream applications, including information retrieval, chatbot\ndevelopment, language model construction, and many others. Link prediction (LP)\nis a crucial downstream task for knowledge graphs, as it helps to address the\nproblem of the incompleteness of the knowledge graphs. However, previous\nresearch has shown that knowledge graphs, often created in a (semi) automatic\nmanner, are not free from social biases. These biases can have harmful effects\non downstream applications, especially by leading to unfair behavior toward\nminority groups. To understand this issue in detail, we develop a framework --\nAuditLP -- deploying fairness metrics to identify biased outcomes in LP,\nspecifically how occupations are classified as either male or female-dominated\nbased on gender as a sensitive attribute. We have experimented with the\nsensitive attribute of age and observed that occupations are categorized as\nyoung-biased, old-biased, and age-neutral. We conduct our experiments on a\nlarge number of knowledge triples that belong to 21 different geographies\nextracted from the open-sourced knowledge graph, Wikidata. Our study shows that\nthe variance in the biased outcomes across geographies neatly mirrors the\nsocio-economic and cultural division of the world, resulting in a transparent\npartition of the Global North from the Global South.\n","authors":["Paramita Das","Sai Keerthana Karnam","Aditya Soni","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2505.02352v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2505.02347v1","updated":"2025-05-05T04:02:33Z","published":"2025-05-05T04:02:33Z","title":"Temporal Robustness in Discrete Time Linear Dynamical Systems","summary":"  Discrete time linear dynamical systems, including Markov chains, have found\nmany applications. However, in some problems, there is uncertainty about the\ntime horizon for which the system runs. This creates uncertainty about the cost\n(or reward) incurred based on the state distribution when the system stops.\nGiven past data samples of how long a system ran, we propose to theoretically\nanalyze a distributional robust cost estimation task in a Wasserstein ambiguity\nset, instead of learning a probability distribution from a few samples. Towards\nthis, we show an equivalence between a discrete time Markov Chain on a\nprobability simplex and a global asymptotic stable (GAS) discrete time linear\ndynamical system, allowing us to base our study on a GAS system only. Then, we\nprovide various polynomial time algorithms and hardness results for different\ncases in our theoretical study, including a fundamental result about\nWasserstein distance based polytope.\n","authors":["Nilava Metya","Arunesh Sinha"],"pdf_url":"https://arxiv.org/pdf/2505.02347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15666v2","updated":"2025-05-05T03:57:24Z","published":"2025-02-21T18:45:37Z","title":"Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing","summary":"  The growing use of large language models (LLMs) for text generation has led\nto widespread concerns about AI-generated content detection. However, an\noverlooked challenge is AI-polished text, where human-written content undergoes\nsubtle refinements using AI tools. This raises a critical question: should\nminimally polished text be classified as AI-generated? Such classification can\nlead to false plagiarism accusations and misleading claims about AI prevalence\nin online content. In this study, we systematically evaluate twelve\nstate-of-the-art AI-text detectors using our AI-Polished-Text Evaluation\n(APT-Eval) dataset, which contains 14.7K samples refined at varying\nAI-involvement levels. Our findings reveal that detectors frequently flag even\nminimally polished text as AI-generated, struggle to differentiate between\ndegrees of AI involvement, and exhibit biases against older and smaller models.\nThese limitations highlight the urgent need for more nuanced detection\nmethodologies.\n","authors":["Shoumik Saha","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2502.15666v2.pdf","comment":"18 pages, 18 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.09106v2","updated":"2025-05-05T03:48:12Z","published":"2024-08-17T06:00:58Z","title":"Fragment-Masked Molecular Optimization","summary":"  Molecular optimization is a crucial aspect of drug discovery, aimed at\nrefining molecular structures to enhance drug efficacy and minimize side\neffects, ultimately accelerating the overall drug development process. Many\ntarget-based molecular optimization methods have been proposed, significantly\nadvancing drug discovery. These methods primarily on understanding the specific\ndrug target structures or their hypothesized roles in combating diseases.\nHowever, challenges such as a limited number of available targets and a\ndifficulty capturing clear structures hinder innovative drug development. In\ncontrast, phenotypic drug discovery (PDD) does not depend on clear target\nstructures and can identify hits with novel and unbiased polypharmacology\nsignatures. As a result, PDD-based molecular optimization can reduce potential\nsafety risks while optimizing phenotypic activity, thereby increasing the\nlikelihood of clinical success. Therefore, we propose a fragment-masked\nmolecular optimization method based on PDD (FMOP). FMOP employs a\nregression-free diffusion model to conditionally optimize the molecular masked\nregions without training, effectively generating new molecules with similar\nscaffolds. On the large-scale drug response dataset GDSCv2, we optimize the\npotential molecules across all 945 cell lines. The overall experiments\ndemonstrate that the in-silico optimization success rate reaches 94.4%, with an\naverage efficacy increase of 5.3%. Additionally, we conduct extensive ablation\nand visualization experiments, confirming that FMOP is an effective and robust\nmolecular optimization method. The code is available\nat:https://anonymous.4open.science/r/FMOP-98C2.\n","authors":["Kun Li","Xiantao Cai","Jia Wu","Bo Du","Wenbin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.09106v2.pdf","comment":"9 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2504.08685v2","updated":"2025-05-05T03:31:30Z","published":"2025-04-11T16:46:20Z","title":"Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model","summary":"  This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/\n","authors":["Team Seawead","Ceyuan Yang","Zhijie Lin","Yang Zhao","Shanchuan Lin","Zhibei Ma","Haoyuan Guo","Hao Chen","Lu Qi","Sen Wang","Feng Cheng","Feilong Zuo","Xuejiao Zeng","Ziyan Yang","Fangyuan Kong","Meng Wei","Zhiwu Qing","Fei Xiao","Tuyen Hoang","Siyu Zhang","Peihao Zhu","Qi Zhao","Jiangqiao Yan","Liangke Gui","Sheng Bi","Jiashi Li","Yuxi Ren","Rui Wang","Huixia Li","Xuefeng Xiao","Shu Liu","Feng Ling","Heng Zhang","Houmin Wei","Huafeng Kuang","Jerry Duncan","Junda Zhang","Junru Zheng","Li Sun","Manlin Zhang","Renfei Sun","Xiaobin Zhuang","Xiaojie Li","Xin Xia","Xuyan Chi","Yanghua Peng","Yuping Wang","Yuxuan Wang","Zhongkai Zhao","Zhuo Chen","Zuquan Song","Zhenheng Yang","Jiashi Feng","Jianchao Yang","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2504.08685v2.pdf","comment":"Technical report (some typos fixed)"},{"id":"http://arxiv.org/abs/2505.00626v2","updated":"2025-05-05T03:29:08Z","published":"2025-05-01T16:06:16Z","title":"The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning\n  (and How to Fix Them)","summary":"  Large language models (LLMs) that integrate multiple input roles (e.g.,\nsystem instructions, user queries, external tool outputs) are increasingly\nprevalent in practice. Ensuring that the model accurately distinguishes\nmessages from each role -- a concept we call \\emph{role separation} -- is\ncrucial for consistent multi-role behavior. Although recent work often targets\nstate-of-the-art prompt injection defenses, it remains unclear whether such\nmethods truly teach LLMs to differentiate roles or merely memorize known\ntriggers. In this paper, we examine \\emph{role-separation learning}: the\nprocess of teaching LLMs to robustly distinguish system and user tokens.\nThrough a \\emph{simple, controlled experimental framework}, we find that\nfine-tuned models often rely on two proxies for role identification: (1) task\ntype exploitation, and (2) proximity to begin-of-text. Although data\naugmentation can partially mitigate these shortcuts, it generally leads to\niterative patching rather than a deeper fix. To address this, we propose\nreinforcing \\emph{invariant signals} that mark role boundaries by adjusting\ntoken-wise cues in the model's input encoding. In particular, manipulating\nposition IDs helps the model learn clearer distinctions and reduces reliance on\nsuperficial proxies. By focusing on this mechanism-centered perspective, our\nwork illuminates how LLMs can more reliably maintain consistent multi-role\nbehavior without merely memorizing known prompts or triggers.\n","authors":["Zihao Wang","Yibo Jiang","Jiahao Yu","Heqing Huang"],"pdf_url":"https://arxiv.org/pdf/2505.00626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09413v2","updated":"2025-05-05T03:26:36Z","published":"2024-09-14T11:03:12Z","title":"Constructive Approach to Bidirectional Influence between Qualia\n  Structure and Language Emergence","summary":"  This perspective paper explores the bidirectional influence between language\nemergence and the relational structure of subjective experiences, termed qualia\nstructure, and lays out a constructive approach to the intricate dependency\nbetween the two. We hypothesize that the emergence of languages with\ndistributional semantics (e.g., syntactic-semantic structures) is linked to the\ncoordination of internal representations shaped by experience, potentially\nfacilitating more structured language through reciprocal influence. This\nhypothesized mutual dependency connects to recent advancements in AI and symbol\nemergence robotics, and is explored within this paper through theoretical\nframeworks such as the collective predictive coding. Computational studies show\nthat neural network-based language models form systematically structured\ninternal representations, and multimodal language models can share\nrepresentations between language and perceptual information. This perspective\nsuggests that language emergence serves not only as a mechanism creating a\ncommunication tool but also as a mechanism for allowing people to realize\nshared understanding of qualitative experiences. The paper discusses the\nimplications of this bidirectional influence in the context of consciousness\nstudies, linguistics, and cognitive science, and outlines future constructive\nresearch directions to further explore this dynamic relationship between\nlanguage emergence and qualia structure.\n","authors":["Tadahiro Taniguchi","Masafumi Oizumi","Noburo Saji","Takato Horii","Naotsugu Tsuchiya"],"pdf_url":"https://arxiv.org/pdf/2409.09413v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06869v3","updated":"2025-05-05T03:07:00Z","published":"2024-03-11T16:22:41Z","title":"Impact of Noisy Supervision in Foundation Model Learning","summary":"  Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning.\n","authors":["Hao Chen","Zihan Wang","Ran Tao","Hongxin Wei","Xing Xie","Masashi Sugiyama","Bhiksha Raj","Jindong Wang"],"pdf_url":"https://arxiv.org/pdf/2403.06869v3.pdf","comment":"18 pages, 10 figures, 6 tables, preprint. arXiv admin note:\n  substantial text overlap with arXiv:2309.17002"},{"id":"http://arxiv.org/abs/2505.02322v1","updated":"2025-05-05T02:38:58Z","published":"2025-05-05T02:38:58Z","title":"HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking","summary":"  Recent advancements have significantly enhanced the performance of large\nlanguage models (LLMs) in tackling complex reasoning tasks, achieving notable\nsuccess in domains like mathematical and logical reasoning. However, these\nmethods encounter challenges with complex planning tasks, primarily due to\nextended reasoning steps, diverse constraints, and the challenge of handling\nmultiple distinct sub-tasks. To address these challenges, we propose HyperTree\nPlanning (HTP), a novel reasoning paradigm that constructs hypertree-structured\nplanning outlines for effective planning. The hypertree structure enables LLMs\nto engage in hierarchical thinking by flexibly employing the divide-and-conquer\nstrategy, effectively breaking down intricate reasoning steps, accommodating\ndiverse constraints, and managing multiple distinct sub-tasks in a\nwell-organized manner. We further introduce an autonomous planning framework\nthat completes the planning process by iteratively refining and expanding the\nhypertree-structured planning outlines. Experiments demonstrate the\neffectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner\nbenchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement\nover o1-preview.\n","authors":["Runquan Gui","Zhihai Wang","Jie Wang","Chi Ma","Huiling Zhen","Mingxuan Yuan","Jianye Hao","Defu Lian","Enhong Chen","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2505.02322v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.14228 by other authors"},{"id":"http://arxiv.org/abs/2402.15290v4","updated":"2025-05-05T02:20:07Z","published":"2024-02-23T12:36:31Z","title":"Efficient State Space Model via Fast Tensor Convolution and Block\n  Diagonalization","summary":"  Existing models encounter bottlenecks in balancing performance and\ncomputational efficiency when modeling long sequences. Although the state space\nmodel (SSM) has achieved remarkable success in handling long sequence tasks, it\nstill faces the problem of large number of parameters. In order to further\nimprove the efficiency of SSM, we propose a new state space layer based on\nmultiple-input multiple-output SSM, called efficient SSM (eSSM). Our eSSM is\nbuilt on the convolutional representation of multi-input and multi-input (MIMO)\nSSM. We propose a variety of effective strategies to improve the computational\nefficiency. The diagonalization of the system matrix first decouples the\noriginal system. Then a fast tensor convolution is proposed based on the fast\nFourier transform. In addition, the block diagonalization of the SSM further\nreduces the model parameters and improves the model flexibility. Extensive\nexperimental results show that the performance of the proposed model on\nmultiple databases matches the performance of state-of-the-art models, such as\nS4, and is significantly better than Transformers and LSTM. In the model\nefficiency benchmark, the parameters of eSSM are only 12.89\\% of LSTM and\n13.24\\% of Mamba. The training speed of eSSM is 3.94 times faster than LSTM and\n1.35 times faster than Mamba. Code is available at:\n\\href{https://github.com/leonty1/essm}{https://github.com/leonty1/essm}.\n","authors":["Tongyi Liang","Han-Xiong Li"],"pdf_url":"https://arxiv.org/pdf/2402.15290v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15539v2","updated":"2025-05-05T02:18:33Z","published":"2024-11-23T12:25:06Z","title":"Large Language Model with Region-guided Referring and Grounding for CT\n  Report Generation","summary":"  Computed tomography (CT) report generation is crucial to assist radiologists\nin interpreting CT volumes, which can be time-consuming and labor-intensive.\nExisting methods primarily only consider the global features of the entire\nvolume, making it struggle to focus on specific regions and potentially missing\nabnormalities. To address this issue, we propose Reg2RG, the first\nregion-guided referring and grounding framework for CT report generation, which\nenhances diagnostic performance by focusing on anatomical regions within the\nvolume. Specifically, we utilize masks from a universal segmentation module to\ncapture local features for each referring region. A local feature decoupling\n(LFD) strategy is proposed to preserve the local high-resolution details with\nlittle computational overhead. Then the local features are integrated with\nglobal features to capture inter-regional relationships within a cohesive\ncontext. Moreover, we propose a novel region-report alignment (RRA) training\nstrategy. It leverages the recognition of referring regions to guide the\ngeneration of region-specific reports, enhancing the model's referring and\ngrounding capabilities while also improving the report's interpretability. A\nlarge language model (LLM) is further employed as the language decoder to\ngenerate reports from integrated visual features, facilitating region-level\ncomprehension. Extensive experiments on two large-scale chest CT-report\ndatasets demonstrate the superiority of our method, which outperforms several\nstate-of-the-art methods in terms of both natural language generation and\nclinical efficacy metrics while preserving promising interpretability. The code\nis available at https://github.com/zhi-xuan-chen/Reg2RG.\n","authors":["Zhixuan Chen","Yequan Bie","Haibo Jin","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15539v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2505.02314v1","updated":"2025-05-05T02:07:04Z","published":"2025-05-05T02:07:04Z","title":"NeuroSim V1.5: Improved Software Backbone for Benchmarking\n  Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities","summary":"  The exponential growth of artificial intelligence (AI) applications has\nexposed the inefficiency of conventional von Neumann architectures, where\nfrequent data transfers between compute units and memory create significant\nenergy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses\nthis challenge by performing multiply-accumulate (MAC) operations directly in\nthe memory arrays, substantially reducing data movement. However, designing\nrobust ACIM accelerators requires accurate modeling of device- and\ncircuit-level non-idealities. In this work, we present NeuroSim V1.5,\nintroducing several key advances: (1) seamless integration with TensorRT's\npost-training quantization flow enabling support for more neural networks\nincluding transformers, (2) a flexible noise injection methodology built on\npre-characterized statistical models, making it straightforward to incorporate\ndata from SPICE simulations or silicon measurements, (3) expanded device\nsupport including emerging non-volatile capacitive memories, and (4) up to 6.5x\nfaster runtime than NeuroSim V1.4 through optimized behavioral simulation. The\ncombination of these capabilities uniquely enables systematic design space\nexploration across both accuracy and hardware efficiency metrics. Through\nmultiple case studies, we demonstrate optimization of critical design\nparameters while maintaining network accuracy. By bridging high-fidelity noise\nmodeling with efficient simulation, NeuroSim V1.5 advances the design and\nvalidation of next-generation ACIM accelerators. All NeuroSim versions are\navailable open-source at https://github.com/neurosim/NeuroSim.\n","authors":["James Read","Ming-Yen Lee","Wei-Hsing Huang","Yuan-Chun Luo","Anni Lu","Shimeng Yu"],"pdf_url":"https://arxiv.org/pdf/2505.02314v1.pdf","comment":"15 pages, 9 figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.02313v1","updated":"2025-05-05T01:55:00Z","published":"2025-05-05T01:55:00Z","title":"What Is AI Safety? What Do We Want It to Be?","summary":"  The field of AI safety seeks to prevent or reduce the harms caused by AI\nsystems. A simple and appealing account of what is distinctive of AI safety as\na field holds that this feature is constitutive: a research project falls\nwithin the purview of AI safety just in case it aims to prevent or reduce the\nharms caused by AI systems. Call this appealingly simple account The Safety\nConception of AI safety. Despite its simplicity and appeal, we argue that The\nSafety Conception is in tension with at least two trends in the ways AI safety\nresearchers and organizations think and talk about AI safety: first, a tendency\nto characterize the goal of AI safety research in terms of catastrophic risks\nfrom future systems; second, the increasingly popular idea that AI safety can\nbe thought of as a branch of safety engineering. Adopting the methodology of\nconceptual engineering, we argue that these trends are unfortunate: when we\nconsider what concept of AI safety it would be best to have, there are\ncompelling reasons to think that The Safety Conception is the answer.\nDescriptively, The Safety Conception allows us to see how work on topics that\nhave historically been treated as central to the field of AI safety is\ncontinuous with work on topics that have historically been treated as more\nmarginal, like bias, misinformation, and privacy. Normatively, taking The\nSafety Conception seriously means approaching all efforts to prevent or\nmitigate harms from AI systems based on their merits rather than drawing\narbitrary distinctions between them.\n","authors":["Jacqueline Harding","Cameron Domenico Kirk-Giannini"],"pdf_url":"https://arxiv.org/pdf/2505.02313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16310v4","updated":"2025-05-05T01:48:08Z","published":"2024-02-26T05:28:36Z","title":"REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility\n  for Location Prediction over Sparse Trajectories","summary":"  Location prediction forecasts a user's location based on historical user\nmobility traces. To tackle the intrinsic sparsity issue of real-world user\nmobility traces, spatiotemporal contexts have been shown as significantly\nuseful. Existing solutions mostly incorporate spatiotemporal distances between\nlocations in mobility traces, either by feeding them as additional inputs to\nRecurrent Neural Networks (RNNs) or by using them to search for informative\npast hidden states for prediction. However, such distance-based methods fail to\ncapture the time-varying temporal regularities of human mobility, where human\nmobility is often more regular in the morning than in other periods, for\nexample; this suggests the usefulness of the actual timestamps besides the\ntemporal distances. Against this background, we propose REPLAY, a general RNN\narchitecture learning to capture the time-varying temporal regularities for\nlocation prediction. Specifically, REPLAY not only resorts to the\nspatiotemporal distances in sparse trajectories to search for the informative\npast hidden states, but also accommodates the time-varying temporal\nregularities by incorporating smoothed timestamp embeddings using Gaussian\nweighted averaging with timestamp-specific learnable bandwidths, which can\nflexibly adapt to the temporal regularities of different strengths across\ndifferent timestamps. Our extensive evaluation compares REPLAY against a\nsizable collection of state-of-the-art techniques on two real-world datasets.\nResults show that REPLAY consistently and significantly outperforms\nstate-of-the-art methods by 7.7\\%-10.5\\% in the location prediction task, and\nthe bandwidths reveal interesting patterns of the time-varying temporal\nregularities.\n","authors":["Bangchao Deng","Bingqing Qu","Pengyang Wang","Dingqi Yang","Benjamin Fankhauser","Philippe Cudre-Mauroux"],"pdf_url":"https://arxiv.org/pdf/2402.16310v4.pdf","comment":"Accepted by IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2505.02309v1","updated":"2025-05-05T01:27:47Z","published":"2025-05-05T01:27:47Z","title":"Optimizing LLMs for Resource-Constrained Environments: A Survey of Model\n  Compression Techniques","summary":"  Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment.\n","authors":["Sanjay Surendranath Girija","Shashank Kapoor","Lakshit Arora","Dipen Pradhan","Aman Raj","Ankit Shetgaonkar"],"pdf_url":"https://arxiv.org/pdf/2505.02309v1.pdf","comment":"Accepted to IEEE COMPSAC 2025"},{"id":"http://arxiv.org/abs/2505.02306v1","updated":"2025-05-05T01:09:02Z","published":"2025-05-05T01:09:02Z","title":"SafeMate: A Model Context Protocol-Based Multimodal Agent for Emergency\n  Preparedness","summary":"  Despite the abundance of public safety documents and emergency protocols,\nmost individuals remain ill-equipped to interpret and act on such information\nduring crises. Traditional emergency decision support systems (EDSS) are\ndesigned for professionals and rely heavily on static documents like PDFs or\nSOPs, which are difficult for non-experts to navigate under stress. This gap\nbetween institutional knowledge and public accessibility poses a critical\nbarrier to effective emergency preparedness and response.\n  We introduce SafeMate, a retrieval-augmented AI assistant that delivers\naccurate, context-aware guidance to general users in both preparedness and\nactive emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate\ndynamically routes user queries to tools for document retrieval, checklist\ngeneration, and structured summarization. It uses FAISS with cosine similarity\nto identify relevant content from trusted sources.\n","authors":["Junfeng Jiao","Jihyung Park","Yiming Xu","Lucy Atkinson"],"pdf_url":"https://arxiv.org/pdf/2505.02306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02299v1","updated":"2025-05-05T00:25:14Z","published":"2025-05-05T00:25:14Z","title":"Adaptive Scoring and Thresholding with Human Feedback for Robust\n  Out-of-Distribution Detection","summary":"  Machine Learning (ML) models are trained on in-distribution (ID) data but\noften encounter out-of-distribution (OOD) inputs during deployment -- posing\nserious risks in safety-critical domains. Recent works have focused on\ndesigning scoring functions to quantify OOD uncertainty, with score thresholds\ntypically set based solely on ID data to achieve a target true positive rate\n(TPR), since OOD data is limited before deployment. However, these TPR-based\nthresholds leave false positive rates (FPR) uncontrolled, often resulting in\nhigh FPRs where OOD points are misclassified as ID. Moreover, fixed scoring\nfunctions and thresholds lack the adaptivity needed to handle newly observed,\nevolving OOD inputs, leading to sub-optimal performance. To address these\nchallenges, we propose a human-in-the-loop framework that \\emph{safely updates\nboth scoring functions and thresholds on the fly} based on real-world OOD\ninputs. Our method maximizes TPR while strictly controlling FPR at all times,\neven as the system adapts over time. We provide theoretical guarantees for FPR\ncontrol under stationary conditions and present extensive empirical evaluations\non OpenOOD benchmarks to demonstrate that our approach outperforms existing\nmethods by achieving higher TPRs while maintaining FPR control.\n","authors":["Daisuke Yamada","Harit Vishwakarma","Ramya Korlakai Vinayak"],"pdf_url":"https://arxiv.org/pdf/2505.02299v1.pdf","comment":null}]},"2025-05-04T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2505.02294v1","updated":"2025-05-04T23:43:44Z","published":"2025-05-04T23:43:44Z","title":"RNBF: Real-Time RGB-D Based Neural Barrier Functions for Safe Robotic\n  Navigation","summary":"  Autonomous safe navigation in unstructured and novel environments poses\nsignificant challenges, especially when environment information can only be\nprovided through low-cost vision sensors. Although safe reactive approaches\nhave been proposed to ensure robot safety in complex environments, many base\ntheir theory off the assumption that the robot has prior knowledge on obstacle\nlocations and geometries. In this paper, we present a real-time, vision-based\nframework that constructs continuous, first-order differentiable Signed\nDistance Fields (SDFs) of unknown environments without any pre-training. Our\nproposed method ensures full compatibility with established SDF-based reactive\ncontrollers. To achieve robust performance under practical sensing conditions,\nour approach explicitly accounts for noise in affordable RGB-D cameras,\nrefining the neural SDF representation online for smoother geometry and stable\ngradient estimates. We validate the proposed method in simulation and\nreal-world experiments using a Fetch robot.\n","authors":["Satyajeet Das","Yifan Xue","Haoming Li","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2505.02294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02293v1","updated":"2025-05-04T23:42:52Z","published":"2025-05-04T23:42:52Z","title":"Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning\n  with Layered Safety","summary":"  Preventing collisions in multi-robot navigation is crucial for deployment.\nThis requirement hinders the use of learning-based approaches, such as\nmulti-agent reinforcement learning (MARL), on their own due to their lack of\nsafety guarantees. Traditional control methods, such as reachability and\ncontrol barrier functions, can provide rigorous safety guarantees when\ninteractions are limited only to a small number of robots. However, conflicts\nbetween the constraints faced by different agents pose a challenge to safe\nmulti-agent coordination.\n  To overcome this challenge, we propose a method that integrates multiple\nlayers of safety by combining MARL with safety filters. First, MARL is used to\nlearn strategies that minimize multiple agent interactions, where multiple\nindicates more than two. Particularly, we focus on interactions likely to\nresult in conflicting constraints within the engagement distance. Next, for\nagents that enter the engagement distance, we prioritize pairs requiring the\nmost urgent corrective actions. Finally, a dedicated safety filter provides\ntactical corrective actions to resolve these conflicts. Crucially, the design\ndecisions for all layers of this framework are grounded in reachability\nanalysis and a control barrier-value function-based filtering mechanism.\n  We validate our Layered Safe MARL framework in 1) hardware experiments using\nCrazyflie drones and 2) high-density advanced aerial mobility (AAM) operation\nscenarios, where agents navigate to designated waypoints while avoiding\ncollisions. The results show that our method significantly reduces conflict\nwhile maintaining safety without sacrificing much efficiency (i.e., shorter\ntravel time and distance) compared to baselines that do not incorporate layered\nsafety. The project website is available at\n\\href{https://dinamo-mit.github.io/Layered-Safe-MARL/}{[this https URL]}\n","authors":["Jason J. Choi","Jasmine Jerry Aloor","Jingqi Li","Maria G. Mendoza","Hamsa Balakrishnan","Claire J. Tomlin"],"pdf_url":"https://arxiv.org/pdf/2505.02293v1.pdf","comment":"Accepted for publication at the 2025 Robotics: Science and Systems\n  Conference. 18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.02291v1","updated":"2025-05-04T23:20:40Z","published":"2025-05-04T23:20:40Z","title":"Dexterous Contact-Rich Manipulation via the Contact Trust Region","summary":"  What is a good local description of contact dynamics for contact-rich\nmanipulation, and where can we trust this local description? While many\napproaches often rely on the Taylor approximation of dynamics with an\nellipsoidal trust region, we argue that such approaches are fundamentally\ninconsistent with the unilateral nature of contact. As a remedy, we present the\nContact Trust Region (CTR), which captures the unilateral nature of contact\nwhile remaining efficient for computation. With CTR, we first develop a\nModel-Predictive Control (MPC) algorithm capable of synthesizing local\ncontact-rich plans. Then, we extend this capability to plan globally by\nstitching together local MPC plans, enabling efficient and dexterous\ncontact-rich manipulation. To verify the performance of our method, we perform\ncomprehensive evaluations, both in high-fidelity simulation and on hardware, on\ntwo contact-rich systems: a planar IiwaBimanual system and a 3D AllegroHand\nsystem. On both systems, our method offers a significantly lower-compute\nalternative to existing RL-based approaches to contact-rich manipulation. In\nparticular, our Allegro in-hand manipulation policy, in the form of a roadmap,\ntakes fewer than 10 minutes to build offline on a standard laptop using just\nits CPU, with online inference taking just a few seconds. Experiment data,\nvideo and code are available at ctr.theaiinstitute.com.\n","authors":["H. J. Terry Suh","Tao Pang","Tong Zhao","Russ Tedrake"],"pdf_url":"https://arxiv.org/pdf/2505.02291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01408v3","updated":"2025-05-04T22:44:25Z","published":"2023-10-02T17:59:24Z","title":"Generalized Animal Imitator: Agile Locomotion with Versatile Motion\n  Prior","summary":"  The agility of animals, particularly in complex activities such as running,\nturning, jumping, and backflipping, stands as an exemplar for robotic system\ndesign. Transferring this suite of behaviors to legged robotic systems\nintroduces essential inquiries: How can a robot learn multiple locomotion\nbehaviors simultaneously? How can the robot execute these tasks with a smooth\ntransition? How to integrate these skills for wide-range applications? This\npaper introduces the Versatile Instructable Motion prior (VIM) - a\nReinforcement Learning framework designed to incorporate a range of agile\nlocomotion tasks suitable for advanced robotic applications. Our framework\nenables legged robots to learn diverse agile low-level skills by imitating\nanimal motions and manually designed motions. Our Functionality reward guides\nthe robot's ability to adopt varied skills, and our Stylization reward ensures\nthat robot motions align with reference motions. Our evaluations of the VIM\nframework span both simulation and the real world. Our framework allows a robot\nto concurrently learn diverse agile locomotion skills using a single\nlearning-based controller in the real world. Videos can be found on our\nwebsite: https://rchalyang.github.io/VIM/\n","authors":["Ruihan Yang","Zhuoqun Chen","Jianhan Ma","Chongyi Zheng","Yiyu Chen","Quan Nguyen","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.01408v3.pdf","comment":"Further details and supportive media can be found at our project\n  site: https://rchalyang.github.io/VIM"},{"id":"http://arxiv.org/abs/2505.02274v1","updated":"2025-05-04T22:06:23Z","published":"2025-05-04T22:06:23Z","title":"On the Need for a Statistical Foundation in Scenario-Based Testing of\n  Autonomous Vehicles","summary":"  Scenario-based testing has emerged as a common method for autonomous vehicles\n(AVs) safety, offering a more efficient alternative to mile-based testing by\nfocusing on high-risk scenarios. However, fundamental questions persist\nregarding its stopping rules, residual risk estimation, debug effectiveness,\nand the impact of simulation fidelity on safety claims. This paper argues that\na rigorous statistical foundation is essential to address these challenges and\nenable rigorous safety assurance. By drawing parallels between AV testing and\ntraditional software testing methodologies, we identify shared research gaps\nand reusable solutions. We propose proof-of-concept models to quantify the\nprobability of failure per scenario (pfs) and evaluate testing effectiveness\nunder varying conditions. Our analysis reveals that neither scenario-based nor\nmile-based testing universally outperforms the other. Furthermore, we introduce\nRisk Estimation Fidelity (REF), a novel metric to certify the alignment of\nsynthetic and real-world testing outcomes, ensuring simulation-based safety\nclaims are statistically defensible.\n","authors":["Xingyu Zhao","Robab Aghazadeh-Chakherlou","Chih-Hong Cheng","Peter Popov","Lorenzo Strigini"],"pdf_url":"https://arxiv.org/pdf/2505.02274v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2505.02272v1","updated":"2025-05-04T21:58:11Z","published":"2025-05-04T21:58:11Z","title":"Robust Localization, Mapping, and Navigation for Quadruped Robots","summary":"  Quadruped robots are currently a widespread platform for robotics research,\nthanks to powerful Reinforcement Learning controllers and the availability of\ncheap and robust commercial platforms. However, to broaden the adoption of the\ntechnology in the real world, we require robust navigation stacks relying only\non low-cost sensors such as depth cameras. This paper presents a first step\ntowards a robust localization, mapping, and navigation system for low-cost\nquadruped robots. In pursuit of this objective we combine contact-aided\nkinematic, visual-inertial odometry, and depth-stabilized vision, enhancing\nstability and accuracy of the system. Our results in simulation and two\ndifferent real-world quadruped platforms show that our system can generate an\naccurate 2D map of the environment, robustly localize itself, and navigate\nautonomously. Furthermore, we present in-depth ablation studies of the\nimportant components of the system and their impact on localization accuracy.\nVideos, code, and additional experiments can be found on the project website:\nhttps://sites.google.com/view/low-cost-quadruped-slam\n","authors":["Dyuman Aditya","Junning Huang","Nico Bohlinger","Piotr Kicki","Krzysztof Walas","Jan Peters","Matteo Luperto","Davide Tateo"],"pdf_url":"https://arxiv.org/pdf/2505.02272v1.pdf","comment":"8 Pages"},{"id":"http://arxiv.org/abs/2412.00592v2","updated":"2025-05-04T20:49:56Z","published":"2024-11-30T21:39:51Z","title":"LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in\n  Real-World Scenes","summary":"  We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data\nfor autonomous driving. Our framework edits real-world LiDAR scans by\nintroducing new object layouts while preserving the realism of the background\nenvironment. Compared to end-to-end frameworks that generate LiDAR point clouds\nfrom scratch, LiDAR-EDIT offers users full control over the object layout,\nincluding the number, type, and pose of objects, while keeping most of the\noriginal real-world background. Our method also provides object labels for the\ngenerated data. Compared to novel view synthesis techniques, our framework\nallows for the creation of counterfactual scenarios with object layouts\nsignificantly different from the original real-world scene. LiDAR-EDIT uses\nspherical voxelization to enforce correct LiDAR projective geometry in the\ngenerated point clouds by construction. During object removal and insertion,\ngenerative models are employed to fill the unseen background and object parts\nthat were occluded in the original real LiDAR scans. Experimental results\ndemonstrate that our framework produces realistic LiDAR scans with practical\nvalue for downstream tasks.\n","authors":["Shing-Hei Ho","Bao Thach","Minghan Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.00592v2.pdf","comment":"Submitted to IEEE International Conference on Robotics and Automation\n  (ICRA). 6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.02232v1","updated":"2025-05-04T19:51:09Z","published":"2025-05-04T19:51:09Z","title":"Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher\n  Learning","summary":"  Building models responsive to input prompts represents a transformative shift\nin machine learning. This paradigm holds significant potential for robotics\nproblems, such as targeted manipulation amidst clutter. In this work, we\npresent a novel approach to combine promptable foundation models with\nreinforcement learning (RL), enabling robots to perform dexterous manipulation\ntasks in a prompt-responsive manner. Existing methods struggle to link\nhigh-level commands with fine-grained dexterous control. We address this gap\nwith a memory-augmented student-teacher learning framework. We use the\nSegment-Anything 2 (SAM 2) model as a perception backbone to infer an object of\ninterest from user prompts. While detections are imperfect, their temporal\nsequence provides rich information for implicit state estimation by\nmemory-augmented models. Our approach successfully learns prompt-responsive\npolicies, demonstrated in picking objects from cluttered scenes. Videos and\ncode are available at https://memory-student-teacher.github.io\n","authors":["Malte Mosbach","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2505.02232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02829v3","updated":"2025-05-04T17:12:25Z","published":"2025-02-05T02:13:51Z","title":"Global Contact-Rich Planning with Sparsity-Rich Semidefinite Relaxations","summary":"  We show that contact-rich motion planning is also sparsity-rich when viewed\nas polynomial optimization (POP). We can exploit not only the correlative and\nterm sparsity patterns that are general to all POPs, but also specialized\nsparsity patterns from the robot kinematic structure and the separability of\ncontact modes. Such sparsity enables the design of high-order but sparse\nsemidefinite programming (SDPs) relaxations--building upon Lasserre's moment\nand sums of squares hierarchy--that (i) can be solved in seconds by\noff-the-shelf SDP solvers, and (ii) compute near globally optimal solutions to\nthe nonconvex contact-rich planning problems with small certified\nsuboptimality. Through extensive experiments both in simulation (Push Bot, Push\nBox, Push Box with Obstacles, and Planar Hand) and real world (Push T), we\ndemonstrate the power of using convex SDP relaxations to generate global\ncontact-rich motion plans. As a contribution of independent interest, we\nrelease the Sparse Polynomial Optimization Toolbox (SPOT)--implemented in C++\nwith interfaces to both Python and Matlab--that automates sparsity exploitation\nfor robotics and beyond.\n","authors":["Shucheng Kang","Guorui Liu","Heng Yang"],"pdf_url":"https://arxiv.org/pdf/2502.02829v3.pdf","comment":"Website: https://computationalrobotics.seas.harvard.edu/project-spot/"},{"id":"http://arxiv.org/abs/2505.02166v1","updated":"2025-05-04T15:58:14Z","published":"2025-05-04T15:58:14Z","title":"CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model\n  for Robotic Manipulation","summary":"  In robotic, task goals can be conveyed through various modalities, such as\nlanguage, goal images, and goal videos. However, natural language can be\nambiguous, while images or videos may offer overly detailed specifications. To\ntackle these challenges, we introduce CrayonRobo that leverages comprehensive\nmulti-modal prompts that explicitly convey both low-level actions and\nhigh-level planning in a simple manner. Specifically, for each key-frame in the\ntask sequence, our method allows for manual or automatic generation of simple\nand expressive 2D visual prompts overlaid on RGB images. These prompts\nrepresent the required task goals, such as the end-effector pose and the\ndesired movement direction after contact. We develop a training strategy that\nenables the model to interpret these visual-language prompts and predict the\ncorresponding contact poses and movement directions in SE(3) space.\nFurthermore, by sequentially executing all key-frame steps, the model can\ncomplete long-horizon tasks. This approach not only helps the model explicitly\nunderstand the task objectives but also enhances its robustness on unseen tasks\nby providing easily interpretable prompts. We evaluate our method in both\nsimulated and real-world environments, demonstrating its robust manipulation\ncapabilities.\n","authors":["Xiaoqi Li","Lingyun Xu","Mingxu Zhang","Jiaming Liu","Yan Shen","Iaroslav Ponomarenko","Jiahui Xu","Liang Heng","Siyuan Huang","Shanghang Zhang","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2505.02166v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2505.02152v1","updated":"2025-05-04T15:25:23Z","published":"2025-05-04T15:25:23Z","title":"Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text\n  Instructions","summary":"  Vision-Language-Action (VLA) models have shown great promise for generalist\nrobotic manipulation in the physical world. However, existing models are\nrestricted to robot observations and text-only instructions, lacking the\nflexibility of interleaved multimodal instructions enabled by recent advances\nin foundation models in the digital world. In this paper, we present\nInterleave-VLA, the first framework capable of comprehending interleaved\nimage-text instructions and directly generating continuous action sequences in\nthe physical world. It offers a flexible, model-agnostic paradigm that extends\nstate-of-the-art VLA models with minimal modifications and strong zero-shot\ngeneralization. A key challenge in realizing Interleave-VLA is the absence of\nlarge-scale interleaved embodied datasets. To bridge this gap, we develop an\nautomatic pipeline that converts text-only instructions from real-world\ndatasets in Open X-Embodiment into interleaved image-text instructions,\nresulting in the first large-scale real-world interleaved embodied dataset with\n210k episodes. Through comprehensive evaluation on simulation benchmarks and\nreal-robot experiments, we demonstrate that Interleave-VLA offers significant\nbenefits: 1) it improves out-of-domain generalization to unseen objects by 2-3x\ncompared to state-of-the-art baselines, 2) supports flexible task interfaces,\nand 3) handles diverse user-provided image instructions in a zero-shot manner,\nsuch as hand-drawn sketches. We further analyze the factors behind\nInterleave-VLA's strong zero-shot performance, showing that the interleaved\nparadigm effectively leverages heterogeneous datasets and diverse instruction\nimages, including those from the Internet, which demonstrates strong potential\nfor scaling up. Our model and dataset will be open-sourced.\n","authors":["Cunxin Fan","Xiaosong Jia","Yihang Sun","Yixiao Wang","Jianglan Wei","Ziyang Gong","Xiangyu Zhao","Masayoshi Tomizuka","Xue Yang","Junchi Yan","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2505.02152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02123v1","updated":"2025-05-04T14:13:00Z","published":"2025-05-04T14:13:00Z","title":"DriveAgent: Multi-Agent Structured Reasoning with LLM and Multimodal\n  Sensor Fusion for Autonomous Driving","summary":"  We introduce DriveAgent, a novel multi-agent autonomous driving framework\nthat leverages large language model (LLM) reasoning combined with multimodal\nsensor fusion to enhance situational understanding and decision-making.\nDriveAgent uniquely integrates diverse sensor modalities-including camera,\nLiDAR, GPS, and IMU-with LLM-driven analytical processes structured across\nspecialized agents. The framework operates through a modular agent-based\npipeline comprising four principal modules: (i) a descriptive analysis agent\nidentifying critical sensor data events based on filtered timestamps, (ii)\ndedicated vehicle-level analysis conducted by LiDAR and vision agents that\ncollaboratively assess vehicle conditions and movements, (iii) environmental\nreasoning and causal analysis agents explaining contextual changes and their\nunderlying mechanisms, and (iv) an urgency-aware decision-generation agent\nprioritizing insights and proposing timely maneuvers. This modular design\nempowers the LLM to effectively coordinate specialized perception and reasoning\nagents, delivering cohesive, interpretable insights into complex autonomous\ndriving scenarios. Extensive experiments on challenging autonomous driving\ndatasets demonstrate that DriveAgent is achieving superior performance on\nmultiple metrics against baseline methods. These results validate the efficacy\nof the proposed LLM-driven multi-agent sensor fusion framework, underscoring\nits potential to substantially enhance the robustness and reliability of\nautonomous driving systems.\n","authors":["Xinmeng Hou","Wuqi Wang","Long Yang","Hao Lin","Jinglun Feng","Haigen Min","Xiangmo Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.02123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02081v1","updated":"2025-05-04T12:17:04Z","published":"2025-05-04T12:17:04Z","title":"Simulation Based Control Architecture Using Webots and Simulink","summary":"  This paper presents a simulation based control architecture that integrates\nWebots and Simulink for the development and testing of robotic systems. Using\nWebots for 3D physics based simulation and Simulink for control system design,\nreal time testing and controller validation are achieved efficiently. The\nproposed approach aims to reduce hardware in the loop dependency in early\ndevelopment stages, offering a cost effective and modular control framework for\nacademic, industrial, and robotics applications.\n","authors":["Harun Kurt","Ahmet Cayir","Kadir Erkan"],"pdf_url":"https://arxiv.org/pdf/2505.02081v1.pdf","comment":"This work has been developed on Webots and Simulink Relationship"},{"id":"http://arxiv.org/abs/2505.02050v1","updated":"2025-05-04T09:58:02Z","published":"2025-05-04T09:58:02Z","title":"Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian\n  Networks","summary":"  Cut-in maneuvers in high-speed traffic pose critical challenges that can lead\nto abrupt braking and collisions, necessitating safe and efficient lane change\nstrategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate\nlateral evidence with safety assessment models, thereby predicting lane changes\nand ensuring safe cut-in maneuvers effectively. Our proposed framework\ncomprises three key probabilistic hypotheses (lateral evidence, lateral safety,\nand longitudinal safety) that facilitate the decision-making process through\ndynamic data processing and assessments of vehicle positions, lateral\nvelocities, relative distance, and Time-to-Collision (TTC) computations. The\nDBN model's performance compared with other conventional approaches\ndemonstrates superior performance in crash reduction, especially in critical\nhigh-speed scenarios, while maintaining a competitive performance in low-speed\nscenarios. This paves the way for robust, scalable, and efficient safety\nvalidation in automated driving systems.\n","authors":["Kranthi Kumar Talluri","Anders L. Madsen","Galia Weidl"],"pdf_url":"https://arxiv.org/pdf/2505.02050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02049v1","updated":"2025-05-04T09:57:11Z","published":"2025-05-04T09:57:11Z","title":"Enhancing Lidar Point Cloud Sampling via Colorization and\n  Super-Resolution of Lidar Imagery","summary":"  Recent advancements in lidar technology have led to improved point cloud\nresolution as well as the generation of 360 degrees, low-resolution images by\nencoding depth, reflectivity, or near-infrared light within each pixel. These\nimages enable the application of deep learning (DL) approaches, originally\ndeveloped for RGB images from cameras to lidar-only systems, eliminating other\nefforts, such as lidar-camera calibration. Compared with conventional RGB\nimages, lidar imagery demonstrates greater robustness in adverse environmental\nconditions, such as low light and foggy weather. Moreover, the imaging\ncapability addresses the challenges in environments where the geometric\ninformation in point clouds may be degraded, such as long corridors, and dense\npoint clouds may be misleading, potentially leading to drift errors.\n  Therefore, this paper proposes a novel framework that leverages DL-based\ncolorization and super-resolution techniques on lidar imagery to extract\nreliable samples from lidar point clouds for odometry estimation. The enhanced\nlidar images, enriched with additional information, facilitate improved\nkeypoint detection, which is subsequently employed for more effective point\ncloud downsampling. The proposed method enhances point cloud registration\naccuracy and mitigates mismatches arising from insufficient geometric\ninformation or misleading extra points. Experimental results indicate that our\napproach surpasses previous methods, achieving lower translation and rotation\nerrors while using fewer points.\n","authors":["Sier Ha","Honghao Du","Xianjia Yu","Tomi Westerlund"],"pdf_url":"https://arxiv.org/pdf/2505.02049v1.pdf","comment":"7 pages. arXiv admin note: substantial text overlap with\n  arXiv:2409.11532"},{"id":"http://arxiv.org/abs/2504.19580v2","updated":"2025-05-04T09:51:07Z","published":"2025-04-28T08:41:08Z","title":"ARTEMIS: Autoregressive End-to-End Trajectory Planning with Mixture of\n  Experts for Autonomous Driving","summary":"  This paper presents ARTEMIS, an end-to-end autonomous driving framework that\ncombines autoregressive trajectory planning with Mixture-of-Experts (MoE).\nTraditional modular methods suffer from error propagation, while existing\nend-to-end models typically employ static one-shot inference paradigms that\ninadequately capture the dynamic changes of the environment. ARTEMIS takes a\ndifferent method by generating trajectory waypoints sequentially, preserves\ncritical temporal dependencies while dynamically routing scene-specific queries\nto specialized expert networks. It effectively relieves trajectory quality\ndegradation issues encountered when guidance information is ambiguous, and\novercomes the inherent representational limitations of singular network\narchitectures when processing diverse driving scenarios. Additionally, we use a\nlightweight batch reallocation strategy that significantly improves the\ntraining speed of the Mixture-of-Experts model. Through experiments on the\nNAVSIM dataset, ARTEMIS exhibits superior competitive performance, achieving\n87.0 PDMS and 83.1 EPDMS with ResNet-34 backbone, demonstrates state-of-the-art\nperformance on multiple metrics.\n","authors":["Renju Feng","Ning Xi","Duanfeng Chu","Rukang Wang","Zejian Deng","Anzheng Wang","Liping Lu","Jinxiang Wang","Yanjun Huang"],"pdf_url":"https://arxiv.org/pdf/2504.19580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05420v2","updated":"2025-05-04T09:07:22Z","published":"2025-01-09T18:22:10Z","title":"RoboPanoptes: The All-seeing Robot with Whole-body Dexterity","summary":"  We present RoboPanoptes, a capable yet practical robot system that achieves\nwhole-body dexterity through whole-body vision. Its whole-body dexterity allows\nthe robot to utilize its entire body surface for manipulation, such as\nleveraging multiple contact points or navigating constrained spaces. Meanwhile,\nwhole-body vision uses a camera system distributed over the robot's surface to\nprovide comprehensive, multi-perspective visual feedback of its own and the\nenvironment's state. At its core, RoboPanoptes uses a whole-body visuomotor\npolicy that learns complex manipulation skills directly from human\ndemonstrations, efficiently aggregating information from the distributed\ncameras while maintaining resilience to sensor failures. Together, these design\naspects unlock new capabilities and tasks, allowing RoboPanoptes to unbox in\nnarrow spaces, sweep multiple or oversized objects, and succeed in multi-step\nstowing in cluttered environments, outperforming baselines in adaptability and\nefficiency. Results are best viewed on https://robopanoptes.github.io.\n","authors":["Xiaomeng Xu","Dominik Bauer","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2501.05420v2.pdf","comment":"Project website: https://robopanoptes.github.io"},{"id":"http://arxiv.org/abs/2505.01998v1","updated":"2025-05-04T06:03:12Z","published":"2025-05-04T06:03:12Z","title":"A Synergistic Framework of Nonlinear Acoustic Computing and\n  Reinforcement Learning for Real-World Human-Robot Interaction","summary":"  This paper introduces a novel framework integrating nonlinear acoustic\ncomputing and reinforcement learning to enhance advanced human-robot\ninteraction under complex noise and reverberation. Leveraging physically\ninformed wave equations (e.g., Westervelt, KZK), the approach captures\nhigher-order phenomena such as harmonic generation and shock formation. By\nembedding these models in a reinforcement learning-driven control loop, the\nsystem adaptively optimizes key parameters (e.g., absorption, beamforming) to\nmitigate multipath interference and non-stationary noise. Experimental\nevaluations-covering far-field localization, weak signal detection, and\nmultilingual speech recognition-demonstrate that this hybrid strategy surpasses\ntraditional linear methods and purely data-driven baselines, achieving superior\nnoise suppression, minimal latency, and robust accuracy in demanding real-world\nscenarios. The proposed system demonstrates broad application prospects in AI\nhardware, robot, machine audition, artificial audition, and brain-machine\ninterfaces.\n","authors":["Xiaoliang Chen","Xin Yu","Le Chang","Yunhe Huang","Jiashuai He","Shibo Zhang","Jin Li","Likai Lin","Ziyu Zeng","Xianling Tu","Shuyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.01998v1.pdf","comment":"34 pages, 11 figures, 10 tables"},{"id":"http://arxiv.org/abs/2412.14803v2","updated":"2025-05-04T04:28:53Z","published":"2024-12-19T12:48:40Z","title":"Video Prediction Policy: A Generalist Robot Policy with Predictive\n  Visual Representations","summary":"  Visual representations play a crucial role in developing generalist robotic\npolicies. Previous vision encoders, typically pre-trained with single-image\nreconstruction or two-image contrastive learning, tend to capture static\ninformation, often neglecting the dynamic aspects vital for embodied tasks.\nRecently, video diffusion models (VDMs) demonstrate the ability to predict\nfuture frames and showcase a strong understanding of physical world. We\nhypothesize that VDMs inherently produce visual representations that encompass\nboth current static information and predicted future dynamics, thereby\nproviding valuable guidance for robot action learning. Based on this\nhypothesis, we propose the Video Prediction Policy (VPP), which learns implicit\ninverse dynamics model conditioned on predicted future representations inside\nVDMs. To predict more precise future, we fine-tune pre-trained video foundation\nmodel on robot datasets along with internet human manipulation data. In\nexperiments, VPP achieves a 18.6\\% relative improvement on the Calvin ABC-D\ngeneralization benchmark compared to the previous state-of-the-art, and\ndemonstrates a 31.6\\% increase in success rates for complex real-world\ndexterous manipulation tasks. Project page at\nhttps://video-prediction-policy.github.io\n","authors":["Yucheng Hu","Yanjiang Guo","Pengchao Wang","Xiaoyu Chen","Yen-Jen Wang","Jianke Zhang","Koushil Sreenath","Chaochao Lu","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14803v2.pdf","comment":"ICML 2025 Spotlight Paper. The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2504.13807v2","updated":"2025-05-04T03:17:11Z","published":"2025-04-18T17:20:27Z","title":"DiffOG: Differentiable Policy Trajectory Optimization with\n  Generalizability","summary":"  Imitation learning-based visuomotor policies excel at manipulation tasks but\noften produce suboptimal action trajectories compared to model-based methods.\nDirectly mapping camera data to actions via neural networks can result in jerky\nmotions and difficulties in meeting critical constraints, compromising safety\nand robustness in real-world deployment. For tasks that require high robustness\nor strict adherence to constraints, ensuring trajectory quality is crucial.\nHowever, the lack of interpretability in neural networks makes it challenging\nto generate constraint-compliant actions in a controlled manner. This paper\nintroduces differentiable policy trajectory optimization with generalizability\n(DiffOG), a learning-based trajectory optimization framework designed to\nenhance visuomotor policies. By leveraging the proposed differentiable\nformulation of trajectory optimization with transformer, DiffOG seamlessly\nintegrates policies with a generalizable optimization layer. Visuomotor\npolicies enhanced by DiffOG generate smoother, constraint-compliant action\ntrajectories in a more interpretable way. DiffOG exhibits strong generalization\ncapabilities and high flexibility. We evaluated DiffOG across 11 simulated\ntasks and 2 real-world tasks. The results demonstrate that DiffOG significantly\nenhances the trajectory quality of visuomotor policies while having minimal\nimpact on policy performance, outperforming trajectory processing baselines\nsuch as greedy constraint clipping and penalty-based trajectory optimization.\nFurthermore, DiffOG achieves superior performance compared to existing\nconstrained visuomotor policy.\n","authors":["Zhengtong Xu","Zichen Miao","Qiang Qiu","Zhe Zhang","Yu She"],"pdf_url":"https://arxiv.org/pdf/2504.13807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01974v1","updated":"2025-05-04T02:54:13Z","published":"2025-05-04T02:54:13Z","title":"KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic\n  Teaching for Dexterous Manipulation","summary":"  Collecting demonstrations enriched with fine-grained tactile information is\ncritical for dexterous manipulation, particularly in contact-rich tasks that\nrequire precise force control and physical interaction. While prior works\nprimarily focus on teleoperation or video-based retargeting, they often suffer\nfrom kinematic mismatches and the absence of real-time tactile feedback,\nhindering the acquisition of high-fidelity tactile data. To mitigate this\nissue, we propose KineDex, a hand-over-hand kinesthetic teaching paradigm in\nwhich the operator's motion is directly transferred to the dexterous hand,\nenabling the collection of physically grounded demonstrations enriched with\naccurate tactile feedback. To resolve occlusions from human hand, we apply\ninpainting technique to preprocess the visual observations. Based on these\ndemonstrations, we then train a visuomotor policy using tactile-augmented\ninputs and implement force control during deployment for precise contact-rich\nmanipulation. We evaluate KineDex on a suite of challenging contact-rich\nmanipulation tasks, including particularly difficult scenarios such as\nsqueezing toothpaste onto a toothbrush, which require precise multi-finger\ncoordination and stable force regulation. Across these tasks, KineDex achieves\nan average success rate of 74.4%, representing a 57.7% improvement over the\nvariant without force control. Comparative experiments with teleoperation and\nuser studies further validate the advantages of KineDex in data collection\nefficiency and operability. Specifically, KineDex collects data over twice as\nfast as teleoperation across two tasks of varying difficulty, while maintaining\na near-100% success rate, compared to under 50% for teleoperation.\n","authors":["Di Zhang","Chengbo Yuan","Chuan Wen","Hai Zhang","Junqiao Zhao","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2505.01974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01966v1","updated":"2025-05-04T02:35:18Z","published":"2025-05-04T02:35:18Z","title":"A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for\n  Modular Self-Reconfigurable Satellites","summary":"  Modular self-reconfigurable satellites refer to satellite clusters composed\nof individual modular units capable of altering their configurations. The\nconfiguration changes enable the execution of diverse tasks and mission\nobjectives. Existing path planning algorithms for reconfiguration often suffer\nfrom high computational complexity, poor generalization capability, and limited\nsupport for diverse target configurations. To address these challenges, this\npaper proposes a goal-oriented reinforcement learning-based path planning\nalgorithm. This algorithm is the first to address the challenge that previous\nreinforcement learning methods failed to overcome, namely handling multiple\ntarget configurations. Moreover, techniques such as Hindsight Experience Replay\nand Invalid Action Masking are incorporated to overcome the significant\nobstacles posed by sparse rewards and invalid actions. Based on these designs,\nour model achieves a 95% and 73% success rate in reaching arbitrary target\nconfigurations in a modular satellite cluster composed of four and six units,\nrespectively.\n","authors":["Bofei Liu","Dong Ye","Zunhao Yao","Zhaowei Sun"],"pdf_url":"https://arxiv.org/pdf/2505.01966v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.00261v2","updated":"2025-05-04T01:56:56Z","published":"2024-09-30T22:16:51Z","title":"Object-Centric Kinodynamic Planning for Nonprehensile Robot\n  Rearrangement Manipulation","summary":"  Nonprehensile actions such as pushing are crucial for addressing multi-object\nrearrangement problems. To date, existing nonprehensile solutions are all\nrobot-centric, i.e., the manipulation actions are generated with robot-relevant\nintent and their outcomes are passively evaluated afterwards. Such pipelines\nare very different from human strategies and are typically inefficient. To this\nend, this work proposes a novel object-centric planning paradigm and develops\nthe first object-centric planner for general nonprehensile rearrangement\nproblems. By assuming that each object can actively move without being driven\nby robot interactions, the object-centric planner focuses on planning desired\nobject motions, which are realized via robot actions generated online via a\nclosed-loop pushing strategy. Through extensive experiments and in comparison\nwith state-of-the-art baselines in both simulation and on a physical robot, we\nshow that our object-centric paradigm can generate more intuitive and\ntask-effective robot actions with significantly improved efficiency. In\naddition, we propose a benchmarking protocol to standardize and facilitate\nfuture research in nonprehensile rearrangement.\n","authors":["Kejia Ren","Gaotian Wang","Andrew S. Morgan","Lydia E. Kavraki","Kaiyu Hang"],"pdf_url":"https://arxiv.org/pdf/2410.00261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01956v1","updated":"2025-05-04T01:40:31Z","published":"2025-05-04T01:40:31Z","title":"SafeNav: Safe Path Navigation using Landmark Based Localization in a\n  GPS-denied Environment","summary":"  In battlefield environments, adversaries frequently disrupt GPS signals,\nrequiring alternative localization and navigation methods. Traditional\nvision-based approaches like Simultaneous Localization and Mapping (SLAM) and\nVisual Odometry (VO) involve complex sensor fusion and high computational\ndemand, whereas range-free methods like DV-HOP face accuracy and stability\nchallenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a\nnavigation approach using landmark-based localization (LanBLoc) combined with a\nbattlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its\nperformance is benchmarked against three state-of-the-art visual localization\nalgorithms integrated with BMM and Bayesian filters, evaluated on synthetic and\nreal-imitated trajectory datasets using metrics including Average Displacement\nError (ADE), Final Displacement Error (FDE), and a newly introduced Average\nWeighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior\nperformance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two\nsafe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by\nintegrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm\nfor obstacle avoidance and risk exposure minimization. Simulation results in\nbattlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk\nexposure, and trajectory efficiency, while SafeNav-CHull provides superior\ncomputational speed.\n","authors":["Ganesh Sapkota","Sanjay Madria"],"pdf_url":"https://arxiv.org/pdf/2505.01956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02293v1","updated":"2025-05-04T23:42:52Z","published":"2025-05-04T23:42:52Z","title":"Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning\n  with Layered Safety","summary":"  Preventing collisions in multi-robot navigation is crucial for deployment.\nThis requirement hinders the use of learning-based approaches, such as\nmulti-agent reinforcement learning (MARL), on their own due to their lack of\nsafety guarantees. Traditional control methods, such as reachability and\ncontrol barrier functions, can provide rigorous safety guarantees when\ninteractions are limited only to a small number of robots. However, conflicts\nbetween the constraints faced by different agents pose a challenge to safe\nmulti-agent coordination.\n  To overcome this challenge, we propose a method that integrates multiple\nlayers of safety by combining MARL with safety filters. First, MARL is used to\nlearn strategies that minimize multiple agent interactions, where multiple\nindicates more than two. Particularly, we focus on interactions likely to\nresult in conflicting constraints within the engagement distance. Next, for\nagents that enter the engagement distance, we prioritize pairs requiring the\nmost urgent corrective actions. Finally, a dedicated safety filter provides\ntactical corrective actions to resolve these conflicts. Crucially, the design\ndecisions for all layers of this framework are grounded in reachability\nanalysis and a control barrier-value function-based filtering mechanism.\n  We validate our Layered Safe MARL framework in 1) hardware experiments using\nCrazyflie drones and 2) high-density advanced aerial mobility (AAM) operation\nscenarios, where agents navigate to designated waypoints while avoiding\ncollisions. The results show that our method significantly reduces conflict\nwhile maintaining safety without sacrificing much efficiency (i.e., shorter\ntravel time and distance) compared to baselines that do not incorporate layered\nsafety. The project website is available at\nhttps://dinamo-mit.github.io/Layered-Safe-MARL/\n","authors":["Jason J. Choi","Jasmine Jerry Aloor","Jingqi Li","Maria G. Mendoza","Hamsa Balakrishnan","Claire J. Tomlin"],"pdf_url":"https://arxiv.org/pdf/2505.02293v1.pdf","comment":"Accepted for publication at the 2025 Robotics: Science and Systems\n  Conference. 18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.03830v1","updated":"2025-05-04T04:46:16Z","published":"2025-05-04T04:46:16Z","title":"Bridging Model Predictive Control and Deep Learning for Scalable\n  Reachability Analysis","summary":"  Hamilton-Jacobi (HJ) reachability analysis is a widely used method for\nensuring the safety of robotic systems. Traditional approaches compute\nreachable sets by numerically solving an HJ Partial Differential Equation (PDE)\nover a grid, which is computationally prohibitive due to the curse of\ndimensionality. Recent learning-based methods have sought to address this\nchallenge by approximating reachability solutions using neural networks trained\nwith PDE residual error. However, these approaches often suffer from unstable\ntraining dynamics and suboptimal solutions due to the weak learning signal\nprovided by the residual loss. In this work, we propose a novel approach that\nleverages model predictive control (MPC) techniques to guide and accelerate the\nreachability learning process. Observing that HJ reachability is inherently\nrooted in optimal control, we utilize MPC to generate approximate reachability\nsolutions at key collocation points, which are then used to tactically guide\nthe neural network training by ensuring compliance with these approximations.\nMoreover, we iteratively refine the MPC generated solutions using the learned\nreachability solution, mitigating convergence to local optima. Case studies on\na 2D vertical drone, a 13D quadrotor, a 7D F1Tenth car, and a 40D\npublisher-subscriber system demonstrate that bridging MPC with deep learning\nyields significant improvements in the robustness and accuracy of reachable\nsets, as well as corresponding safety assurances, compared to existing methods.\n","authors":["Zeyuan Feng","Le Qiu","Somil Bansal"],"pdf_url":"https://arxiv.org/pdf/2505.03830v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.20490v3","updated":"2025-05-04T23:41:06Z","published":"2025-02-27T19:54:16Z","title":"EgoNormia: Benchmarking Physical Social Norm Understanding","summary":"  Human activity is moderated by norms. However, machines are often trained\nwithout explicit supervision on norm understanding and reasoning, particularly\nwhen norms are physically- or socially-grounded. To improve and evaluate the\nnormative reasoning capability of vision-language models (VLMs), we present\n\\dataset{} $\\|\\epsilon\\|$, consisting of 1,853 challenging, multi-stage MCQ\nquestions based on ego-centric videos of human interactions, evaluating both\nthe prediction and justification of normative actions. The normative actions\nencompass seven categories: safety, privacy, proxemics, politeness,\ncooperation, coordination/proactivity, and communication/legibility. To compile\nthis dataset at scale, we propose a novel pipeline leveraging video sampling,\nautomatic answer generation, filtering, and human validation. Our work\ndemonstrates that current state-of-the-art vision-language models lack robust\nnorm understanding, scoring a maximum of 54\\% on \\dataset{} (versus a human\nbench of 92\\%). Our analysis of performance in each dimension highlights the\nsignificant risks of safety, privacy, and the lack of collaboration and\ncommunication capability when applied to real-world agents. We additionally\nshow that through a retrieval-based generation (RAG) method, it is possible to\nuse \\dataset{} to enhance normative reasoning in VLMs.\n","authors":["MohammadHossein Rezaei","Yicheng Fu","Phil Cuvin","Caleb Ziems","Yanzhe Zhang","Hao Zhu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.20490v3.pdf","comment":"V2, with updated VLM stats"},{"id":"http://arxiv.org/abs/2505.02287v1","updated":"2025-05-04T22:55:01Z","published":"2025-05-04T22:55:01Z","title":"Continuous Normalizing Flows for Uncertainty-Aware Human Pose Estimation","summary":"  Human Pose Estimation (HPE) is increasingly important for applications like\nvirtual reality and motion analysis, yet current methods struggle with\nbalancing accuracy, computational efficiency, and reliable uncertainty\nquantification (UQ). Traditional regression-based methods assume fixed\ndistributions, which might lead to poor UQ. Heatmap-based methods effectively\nmodel the output distribution using likelihood heatmaps, however, they demand\nsignificant resources. To address this, we propose Continuous Flow Residual\nEstimation (CFRE), an integration of Continuous Normalizing Flows (CNFs) into\nregression-based models, which allows for dynamic distribution adaptation.\nThrough extensive experiments, we show that CFRE leads to better accuracy and\nuncertainty quantification with retained computational efficiency on both 2D\nand 3D human pose estimation tasks.\n","authors":["Shipeng Liu","Ziliang Xiong","Bastian Wandt","Per-Erik Forssén"],"pdf_url":"https://arxiv.org/pdf/2505.02287v1.pdf","comment":"Accepted by SCIA2025"},{"id":"http://arxiv.org/abs/2504.20898v2","updated":"2025-05-04T22:38:21Z","published":"2025-04-29T16:14:55Z","title":"CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report\n  Generation with Multi-Agent RAG and Concept Bottleneck Models","summary":"  Advancements in generative Artificial Intelligence (AI) hold great promise\nfor automating radiology workflows, yet challenges in interpretability and\nreliability hinder clinical adoption. This paper presents an automated\nradiology report generation framework that combines Concept Bottleneck Models\n(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge\nAI performance with clinical explainability. CBMs map chest X-ray features to\nhuman-understandable clinical concepts, enabling transparent disease\nclassification. Meanwhile, the RAG system integrates multi-agent collaboration\nand external knowledge to produce contextually rich, evidence-based reports.\nOur demonstration showcases the system's ability to deliver interpretable\npredictions, mitigate hallucinations, and generate high-quality, tailored\nreports with an interactive interface addressing accuracy, trust, and usability\nchallenges. This framework provides a pathway to improving diagnostic\nconsistency and empowering radiologists with actionable insights.\n","authors":["Hasan Md Tusfiqur Alam","Devansh Srivastav","Abdulrahman Mohamed Selim","Md Abdul Kadir","Md Moktadirul Hoque Shuvo","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2504.20898v2.pdf","comment":"Accepted in the 17th ACM SIGCHI Symposium on Engineering Interactive\n  Computing Systems (EICS 2025)"},{"id":"http://arxiv.org/abs/2505.02278v1","updated":"2025-05-04T22:18:14Z","published":"2025-05-04T22:18:14Z","title":"Compositional Image-Text Matching and Retrieval by Grounding Entities","summary":"  Vision-language pretraining on large datasets of images-text pairs is one of\nthe main building blocks of current Vision-Language Models. While with\nadditional training, these models excel in various downstream tasks, including\nvisual question answering, image captioning, and visual commonsense reasoning.\nHowever, a notable weakness of pretrained models like CLIP, is their inability\nto perform entity grounding and compositional image and text\nmatching~\\cite{Jiang2024ComCLIP, yang2023amc, Rajabi2023GroundedVSR,\nlearninglocalizeCVPR24}. In this work we propose a novel learning-free\nzero-shot augmentation of CLIP embeddings that has favorable compositional\nproperties. We compute separate embeddings of sub-images of object entities and\nrelations that are localized by the state of the art open vocabulary detectors\nand dynamically adjust the baseline global image embedding. % The final\nembedding is obtained by computing a weighted combination of the sub-image\nembeddings. The resulting embedding is then utilized for similarity computation\nwith text embedding, resulting in a average 1.5\\% improvement in image-text\nmatching accuracy on the Visual Genome and SVO Probes\ndatasets~\\cite{krishna2017visualgenome, svo}. Notably, the enhanced embeddings\ndemonstrate superior retrieval performance, thus achieving significant gains on\nthe Flickr30K and MS-COCO retrieval benchmarks~\\cite{flickr30ke, mscoco},\nimproving the state-of-the-art Recall@1 by 12\\% and 0.4\\%, respectively. Our\ncode is available at https://github.com/madhukarreddyvongala/GroundingCLIP.\n","authors":["Madhukar Reddy Vongala","Saurabh Srivastava","Jana Košecká"],"pdf_url":"https://arxiv.org/pdf/2505.02278v1.pdf","comment":"Accepted at CVPR-W"},{"id":"http://arxiv.org/abs/2406.00985v4","updated":"2025-05-04T21:49:58Z","published":"2024-06-03T04:43:56Z","title":"ParallelEdits: Efficient Multi-object Image Editing","summary":"  Text-driven image synthesis has made significant advancements with the\ndevelopment of diffusion models, transforming how visual content is generated\nfrom text prompts. Despite these advances, text-driven image editing, a key\narea in computer graphics, faces unique challenges. A major challenge is making\nsimultaneous edits across multiple objects or attributes. Applying these\nmethods sequentially for multi-attribute edits increases computational demands\nand efficiency losses. In this paper, we address these challenges with\nsignificant contributions. Our main contribution is the development of\nParallelEdits, a method that seamlessly manages simultaneous edits across\nmultiple attributes. In contrast to previous approaches, ParallelEdits not only\npreserves the quality of single attribute edits but also significantly improves\nthe performance of multitasking edits. This is achieved through innovative\nattention distribution mechanism and multi-branch design that operates across\nseveral processing heads. Additionally, we introduce the PIE-Bench++ dataset,\nan expansion of the original PIE-Bench dataset, to better support evaluating\nimage-editing tasks involving multiple objects and attributes simultaneously.\nThis dataset is a benchmark for evaluating text-driven image editing methods in\nmultifaceted scenarios.\n","authors":["Mingzhen Huang","Jialing Cai","Shan Jia","Vishnu Suresh Lokhande","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2406.00985v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02255v1","updated":"2025-05-04T21:28:21Z","published":"2025-05-04T21:28:21Z","title":"Enhancing AI Face Realism: Cost-Efficient Quality Improvement in\n  Distilled Diffusion Models with a Fully Synthetic Dataset","summary":"  This study presents a novel approach to enhance the cost-to-quality ratio of\nimage generation with diffusion models. We hypothesize that differences between\ndistilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are\nconsistent and, therefore, learnable within a specialized domain, like portrait\ngeneration. We generate a synthetic paired dataset and train a fast\nimage-to-image translation head. Using two sets of low- and high-quality\nsynthetic images, our model is trained to refine the output of a distilled\ngenerator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like\nFLUX.1-dev, which is more computationally intensive. Our results show that the\npipeline, which combines a distilled version of a large generative model with\nour enhancement layer, delivers similar photorealistic portraits to the\nbaseline version with up to an 82% decrease in computational cost compared to\nFLUX.1-dev. This study demonstrates the potential for improving the efficiency\nof AI solutions involving large-scale image generation.\n","authors":["Jakub Wąsala","Bartłomiej Wrzalski","Kornelia Noculak","Yuliia Tarasenko","Oliwer Krupa","Jan Kocoń","Grzegorz Chodak"],"pdf_url":"https://arxiv.org/pdf/2505.02255v1.pdf","comment":"25th International Conference on Computational Science"},{"id":"http://arxiv.org/abs/2505.02246v1","updated":"2025-05-04T21:00:14Z","published":"2025-05-04T21:00:14Z","title":"Cricket: A Self-Powered Chirping Pixel","summary":"  We present a sensor that can measure light and wirelessly communicate the\nmeasurement, without the need for an external power source or a battery. Our\nsensor, called cricket, harvests energy from incident light. It is asleep for\nmost of the time and transmits a short and strong radio frequency chirp when\nits harvested energy reaches a specific level. The carrier frequency of each\ncricket is fixed and reveals its identity, and the duration between consecutive\nchirps is a measure of the incident light level. We have characterized the\nradiometric response function, signal-to-noise ratio and dynamic range of\ncricket. We have experimentally verified that cricket can be miniaturized at\nthe expense of increasing the duration between chirps. We show that a cube with\na cricket on each of its sides can be used to estimate the centroid of any\ncomplex illumination, which has value in applications such as solar tracking.\nWe also demonstrate the use of crickets for creating untethered sensor arrays\nthat can produce video and control lighting for energy conservation. Finally,\nwe modified cricket's circuit to develop battery-free electronic sunglasses\nthat can instantly adapt to environmental illumination.\n","authors":["Shree K. Nayar","Jeremy Klotz","Nikhil Nanda","Mikhail Fridberg"],"pdf_url":"https://arxiv.org/pdf/2505.02246v1.pdf","comment":"13 pages, 18 figures. Project page:\n  https://cave.cs.columbia.edu/projects/categories/project?cid=Computational%20Imaging&pid=Cricket%20A%20Self-Powered%20Chirping%20Pixel"},{"id":"http://arxiv.org/abs/2505.02242v1","updated":"2025-05-04T20:50:44Z","published":"2025-05-04T20:50:44Z","title":"Quantizing Diffusion Models from a Sampling-Aware Perspective","summary":"  Diffusion models have recently emerged as the dominant approach in visual\ngeneration tasks. However, the lengthy denoising chains and the computationally\nintensive noise estimation networks hinder their applicability in low-latency\nand resource-limited environments. Previous research has endeavored to address\nthese limitations in a decoupled manner, utilizing either advanced samplers or\nefficient model quantization techniques. In this study, we uncover that\nquantization-induced noise disrupts directional estimation at each sampling\nstep, further distorting the precise directional estimations of higher-order\nsamplers when solving the sampling equations through discretized numerical\nmethods, thereby altering the optimal sampling trajectory. To attain dual\nacceleration with high fidelity, we propose a sampling-aware quantization\nstrategy, wherein a Mixed-Order Trajectory Alignment technique is devised to\nimpose a more stringent constraint on the error bounds at each sampling step,\nfacilitating a more linear probability flow. Extensive experiments on\nsparse-step fast sampling across multiple datasets demonstrate that our\napproach preserves the rapid convergence characteristics of high-speed samplers\nwhile maintaining superior generation quality. Code will be made publicly\navailable soon.\n","authors":["Qian Zeng","Jie Song","Yuanyu Wan","Huiqiong Wang","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2505.02242v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.00592v2","updated":"2025-05-04T20:49:56Z","published":"2024-11-30T21:39:51Z","title":"LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in\n  Real-World Scenes","summary":"  We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data\nfor autonomous driving. Our framework edits real-world LiDAR scans by\nintroducing new object layouts while preserving the realism of the background\nenvironment. Compared to end-to-end frameworks that generate LiDAR point clouds\nfrom scratch, LiDAR-EDIT offers users full control over the object layout,\nincluding the number, type, and pose of objects, while keeping most of the\noriginal real-world background. Our method also provides object labels for the\ngenerated data. Compared to novel view synthesis techniques, our framework\nallows for the creation of counterfactual scenarios with object layouts\nsignificantly different from the original real-world scene. LiDAR-EDIT uses\nspherical voxelization to enforce correct LiDAR projective geometry in the\ngenerated point clouds by construction. During object removal and insertion,\ngenerative models are employed to fill the unseen background and object parts\nthat were occluded in the original real LiDAR scans. Experimental results\ndemonstrate that our framework produces realistic LiDAR scans with practical\nvalue for downstream tasks.\n","authors":["Shing-Hei Ho","Bao Thach","Minghan Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.00592v2.pdf","comment":"Submitted to IEEE International Conference on Robotics and Automation\n  (ICRA). 6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.02236v1","updated":"2025-05-04T20:24:57Z","published":"2025-05-04T20:24:57Z","title":"Improving Physical Object State Representation in Text-to-Image\n  Generative Systems","summary":"  Current text-to-image generative models struggle to accurately represent\nobject states (e.g., \"a table without a bottle,\" \"an empty tumbler\"). In this\nwork, we first design a fully-automatic pipeline to generate high-quality\nsynthetic data that accurately captures objects in varied states. Next, we\nfine-tune several open-source text-to-image models on this synthetic data. We\nevaluate the performance of the fine-tuned models by quantifying the alignment\nof the generated images to their prompts using GPT4o-mini, and achieve an\naverage absolute improvement of 8+% across four models on the public\nGenAI-Bench dataset. We also curate a collection of 200 prompts with a specific\nfocus on common objects in various physical states. We demonstrate a\nsignificant improvement of an average of 24+% over the baseline on this\ndataset. We release all evaluation prompts and code.\n","authors":["Tianle Chen","Chaitanya Chakka","Deepti Ghadiyaram"],"pdf_url":"https://arxiv.org/pdf/2505.02236v1.pdf","comment":"Submitted to Synthetic Data for Computer Vision - CVPR 2025 Workshop"},{"id":"http://arxiv.org/abs/2504.00159v2","updated":"2025-05-04T20:20:33Z","published":"2025-03-31T19:13:45Z","title":"SonarSplat: Novel View Synthesis of Imaging Sonar via Gaussian Splatting","summary":"  In this paper, we present SonarSplat, a novel Gaussian splatting framework\nfor imaging sonar that demonstrates realistic novel view synthesis and models\nacoustic streaking phenomena. Our method represents the scene as a set of 3D\nGaussians with acoustic reflectance and saturation properties. We develop a\nnovel method to efficiently rasterize Gaussians to produce a range/azimuth\nimage that is faithful to the acoustic image formation model of imaging sonar.\nIn particular, we develop a novel approach to model azimuth streaking in a\nGaussian splatting framework. We evaluate SonarSplat using real-world datasets\nof sonar images collected from an underwater robotic platform in a controlled\ntest tank and in a real-world river environment. Compared to the\nstate-of-the-art, SonarSplat offers improved image synthesis capabilities (+3.2\ndB PSNR) and more accurate 3D reconstruction (52% lower Chamfer Distance). We\nalso demonstrate that SonarSplat can be leveraged for azimuth streak removal.\n","authors":["Advaith V. Sethuraman","Max Rucker","Onur Bagoren","Pou-Chun Kung","Nibarkavi N. B. Amutha","Katherine A. Skinner"],"pdf_url":"https://arxiv.org/pdf/2504.00159v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07569v2","updated":"2025-05-04T18:56:15Z","published":"2024-03-12T11:56:50Z","title":"Exploring Challenges in Deep Learning of Single-Station Ground Motion\n  Records","summary":"  Contemporary deep learning models have demonstrated promising results across\nvarious applications within seismology and earthquake engineering. These models\nrely primarily on utilizing ground motion records for tasks such as earthquake\nevent classification, localization, earthquake early warning systems, and\nstructural health monitoring. However, the extent to which these models truly\nextract meaningful patterns from these complex time-series signals remains\nunderexplored. In this study, our objective is to evaluate the degree to which\nauxiliary information, such as seismic phase arrival times or seismic station\ndistribution within a network, dominates the process of deep learning from\nground motion records, potentially hindering its effectiveness. Our\nexperimental results reveal a strong dependence on the highly correlated\nPrimary (P) and Secondary (S) phase arrival times. These findings expose a\ncritical gap in the current research landscape, highlighting the lack of robust\nmethodologies for deep learning from single-station ground motion recordings\nthat do not rely on auxiliary inputs.\n","authors":["Ümit Mert Çağlar","Baris Yilmaz","Melek Türkmen","Erdem Akagündüz","Salih Tileylioglu"],"pdf_url":"https://arxiv.org/pdf/2403.07569v2.pdf","comment":"9 Pages, 12 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2504.07392v4","updated":"2025-05-04T18:23:15Z","published":"2025-04-10T02:20:18Z","title":"ID-Booth: Identity-consistent Face Generation with Diffusion Models","summary":"  Recent advances in generative modeling have enabled the generation of\nhigh-quality synthetic data that is applicable in a variety of domains,\nincluding face recognition. Here, state-of-the-art generative models typically\nrely on conditioning and fine-tuning of powerful pretrained diffusion models to\nfacilitate the synthesis of realistic images of a desired identity. Yet, these\nmodels often do not consider the identity of subjects during training, leading\nto poor consistency between generated and intended identities. In contrast,\nmethods that employ identity-based training objectives tend to overfit on\nvarious aspects of the identity, and in turn, lower the diversity of images\nthat can be generated. To address these issues, we present in this paper a\nnovel generative diffusion-based framework, called ID-Booth. ID-Booth consists\nof a denoising network responsible for data generation, a variational\nauto-encoder for mapping images to and from a lower-dimensional latent space\nand a text encoder that allows for prompt-based control over the generation\nprocedure. The framework utilizes a novel triplet identity training objective\nand enables identity-consistent image generation while retaining the synthesis\ncapabilities of pretrained diffusion models. Experiments with a\nstate-of-the-art latent diffusion model and diverse prompts reveal that our\nmethod facilitates better intra-identity consistency and inter-identity\nseparability than competing methods, while achieving higher image diversity. In\nturn, the produced data allows for effective augmentation of small-scale\ndatasets and training of better-performing recognition models in a\nprivacy-preserving manner. The source code for the ID-Booth framework is\npublicly available at https://github.com/dariant/ID-Booth.\n","authors":["Darian Tomašević","Fadi Boutros","Chenhao Lin","Naser Damer","Vitomir Štruc","Peter Peer"],"pdf_url":"https://arxiv.org/pdf/2504.07392v4.pdf","comment":"IEEE International Conference on Automatic Face and Gesture\n  Recognition (FG) 2025, 14 pages"},{"id":"http://arxiv.org/abs/2505.02211v1","updated":"2025-05-04T18:23:03Z","published":"2025-05-04T18:23:03Z","title":"CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid\n  Carcinoma Classification in Ultrasound Images","summary":"  Heterogeneous morphological features and data imbalance pose significant\nchallenges in rare thyroid carcinoma classification using ultrasound imaging.\nTo address this issue, we propose a novel multitask learning framework,\nChannel-Spatial Attention Synergy Network (CSASN), which integrates a\ndual-branch feature extractor - combining EfficientNet for local spatial\nencoding and ViT for global semantic modeling, with a cascaded channel-spatial\nattention refinement module. A residual multiscale classifier and dynamically\nweighted loss function further enhance classification stability and accuracy.\nTrained on a multicenter dataset comprising more than 2000 patients from four\nclinical institutions, our framework leverages a residual multiscale classifier\nand dynamically weighted loss function to enhance classification stability and\naccuracy. Extensive ablation studies demonstrate that each module contributes\nsignificantly to model performance, particularly in recognizing rare subtypes\nsuch as FTC and MTC carcinomas. Experimental results show that CSASN\noutperforms existing single-stream CNN or Transformer-based models, achieving a\nsuperior balance between precision and recall under class-imbalanced\nconditions. This framework provides a promising strategy for AI-assisted\nthyroid cancer diagnosis.\n","authors":["Peiqi Li","Yincheng Gao","Renxing Li","Haojie Yang","Yunyun Liu","Boji Liu","Jiahui Ni","Ying Zhang","Yulu Wu","Xiaowei Fang","Lehang Guo","Liping Sun","Jiangang Chen"],"pdf_url":"https://arxiv.org/pdf/2505.02211v1.pdf","comment":"18 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2505.02192v1","updated":"2025-05-04T17:19:20Z","published":"2025-05-04T17:19:20Z","title":"DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in\n  Video Customization","summary":"  Customized text-to-video generation with pre-trained large-scale models has\nrecently garnered significant attention through focusing on identity and motion\nconsistency. Existing works typically follow the isolated customized paradigm,\nwhere the subject identity or motion dynamics are customized exclusively.\nHowever, this paradigm completely ignores the intrinsic mutual constraints and\nsynergistic interdependencies between identity and motion, resulting in\nidentity-motion conflicts throughout the generation process that systematically\ndegrades. To address this, we introduce DualReal, a novel framework that,\nemploys adaptive joint training to collaboratively construct interdependencies\nbetween dimensions. Specifically, DualReal is composed of two units: (1)\nDual-aware Adaptation dynamically selects a training phase (i.e., identity or\nmotion), learns the current information guided by the frozen dimension prior,\nand employs a regularization strategy to avoid knowledge leakage; (2)\nStageBlender Controller leverages the denoising stages and Diffusion\nTransformer depths to guide different dimensions with adaptive granularity,\navoiding conflicts at various stages and ultimately achieving lossless fusion\nof identity and motion patterns. We constructed a more comprehensive benchmark\nthan existing methods. The experimental results show that DualReal improves\nCLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top\nperformance on nearly all motion quality metrics.\n","authors":["Wenchuan Wang","Mengqi Huang","Yijing Tu","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2505.02192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14517v2","updated":"2025-05-04T17:06:30Z","published":"2024-11-21T16:27:22Z","title":"The Double-Ellipsoid Geometry of CLIP","summary":"  Contrastive Language-Image Pre-Training (CLIP) is highly instrumental in\nmachine learning applications within a large variety of domains. We investigate\nthe geometry of this embedding, which is still not well understood. We examine\nthe raw unnormalized embedding and show that text and image reside on linearly\nseparable ellipsoid shells, not centered at the origin. We explain the benefits\nof having this structure, allowing to better embed instances according to their\nuncertainty during contrastive training. Frequent concepts in the dataset yield\nmore false negatives, inducing greater uncertainty. A new notion of conformity\nis introduced, which measures the average cosine similarity of an instance to\nany other instance within a representative data set. We show this measure can\nbe accurately estimated by simply computing the cosine similarity to the\nmodality mean vector. Furthermore, we find that CLIP's modality gap optimizes\nthe matching of the conformity distributions of image and text.\n","authors":["Meir Yossef Levi","Guy Gilboa"],"pdf_url":"https://arxiv.org/pdf/2411.14517v2.pdf","comment":"Accepted to ICML 2025. This version matches the camera-ready version"},{"id":"http://arxiv.org/abs/2505.02182v1","updated":"2025-05-04T17:02:10Z","published":"2025-05-04T17:02:10Z","title":"Robust AI-Generated Face Detection with Imbalanced Data","summary":"  Deepfakes, created using advanced AI techniques such as Variational\nAutoencoder and Generative Adversarial Networks, have evolved from research and\nentertainment applications into tools for malicious activities, posing\nsignificant threats to digital trust. Current deepfake detection techniques\nhave evolved from CNN-based methods focused on local artifacts to more advanced\napproaches using vision transformers and multimodal models like CLIP, which\ncapture global anomalies and improve cross-domain generalization. Despite\nrecent progress, state-of-the-art deepfake detectors still face major\nchallenges in handling distribution shifts from emerging generative models and\naddressing severe class imbalance between authentic and fake samples in\ndeepfake datasets, which limits their robustness and detection accuracy. To\naddress these challenges, we propose a framework that combines dynamic loss\nreweighting and ranking-based optimization, which achieves superior\ngeneralization and performance under imbalanced dataset conditions. The code is\navailable at https://github.com/Purdue-M2/SP_CUP.\n","authors":["Yamini Sri Krubha","Aryana Hou","Braden Vester","Web Walker","Xin Wang","Li Lin","Shu Hu"],"pdf_url":"https://arxiv.org/pdf/2505.02182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02179v1","updated":"2025-05-04T16:42:15Z","published":"2025-05-04T16:42:15Z","title":"ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection\n  in Video Surveillance Applications","summary":"  Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance\nLearning (MIL) suffers from label ambiguity, hindering discriminative feature\nlearning. We propose ProDisc-VAD, an efficient framework tackling this via two\nsynergistic components. The Prototype Interaction Layer (PIL) provides\ncontrolled normality modeling using a small set of learnable prototypes,\nestablishing a robust baseline without being overwhelmed by dominant normal\ndata. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts\nseparability by applying targeted contrastive learning exclusively to the most\nreliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD\nachieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M\nparameters, over 800x fewer than recent ViT-based methods like VadCLIP,\ndemonstrating exceptional efficiency alongside state-of-the-art performance.\nCode is available at https://github.com/modadundun/ProDisc-VAD.\n","authors":["Tao Zhu","Qi Yu","Xinru Dong","Shiyu Li","Yue Liu","Jinlong Jiang","Lei Shu"],"pdf_url":"https://arxiv.org/pdf/2505.02179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02178v1","updated":"2025-05-04T16:40:24Z","published":"2025-05-04T16:40:24Z","title":"Sparfels: Fast Reconstruction from Sparse Unposed Imagery","summary":"  We present a method for Sparse view reconstruction with surface element\nsplatting that runs within 3 minutes on a consumer grade GPU. While few methods\naddress sparse radiance field learning from noisy or unposed sparse cameras,\nshape recovery remains relatively underexplored in this setting. Several\nradiance and shape learning test-time optimization methods address the sparse\nposed setting by learning data priors or using combinations of external\nmonocular geometry priors. Differently, we propose an efficient and simple\npipeline harnessing a single recent 3D foundation model. We leverage its\nvarious task heads, notably point maps and camera initializations to\ninstantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image\ncorrespondences to guide camera optimization midst 2DGS training. Key to our\ncontribution is a novel formulation of splatted color variance along rays,\nwhich can be computed efficiently. Reducing this moment in training leads to\nmore accurate shape reconstructions. We demonstrate state-of-the-art\nperformances in the sparse uncalibrated setting in reconstruction and novel\nview benchmarks based on established multi-view datasets.\n","authors":["Shubhendu Jena","Amine Ouasfi","Mae Younes","Adnane Boukhayma"],"pdf_url":"https://arxiv.org/pdf/2505.02178v1.pdf","comment":"Project page : https://shubhendu-jena.github.io/Sparfels/"},{"id":"http://arxiv.org/abs/2505.02176v1","updated":"2025-05-04T16:35:13Z","published":"2025-05-04T16:35:13Z","title":"Saliency-Guided Training for Fingerprint Presentation Attack Detection","summary":"  Saliency-guided training, which directs model learning to important regions\nof images, has demonstrated generalization improvements across various\nbiometric presentation attack detection (PAD) tasks. This paper presents its\nfirst application to fingerprint PAD. We conducted a 50-participant study to\ncreate a dataset of 800 human-annotated fingerprint perceptually-important\nmaps, explored alongside algorithmically-generated \"pseudosaliency,\" including\nminutiae-based, image quality-based, and autoencoder-based saliency maps.\nEvaluating on the 2021 Fingerprint Liveness Detection Competition testing set,\nwe explore various configurations within five distinct training scenarios to\nassess the impact of saliency-guided training on accuracy and generalization.\nOur findings demonstrate the effectiveness of saliency-guided training for\nfingerprint PAD in both limited and large data contexts, and we present a\nconfiguration capable of earning the first place on the LivDet-2021 benchmark.\nOur results highlight saliency-guided training's promise for increased model\ngeneralization capabilities, its effectiveness when data is limited, and its\npotential to scale to larger datasets in fingerprint PAD. All collected\nsaliency data and trained models are released with the paper to support\nreproducible research.\n","authors":["Samuel Webster","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2505.02176v1.pdf","comment":"19 pages (8 main, 2 references, 9 appendix), 2 figures, 19 tables (2\n  main, 17 appendix)"},{"id":"http://arxiv.org/abs/2505.02175v1","updated":"2025-05-04T16:33:47Z","published":"2025-05-04T16:33:47Z","title":"SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian\n  Splatting","summary":"  Recovering 3D information from scenes via multi-view stereo reconstruction\n(MVS) and novel view synthesis (NVS) is inherently challenging, particularly in\nscenarios involving sparse-view setups. The advent of 3D Gaussian Splatting\n(3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian\nSplatting (2DGS) leveraged perspective accurate 2D Gaussian primitive\nrasterization to achieve accurate geometry representation during rendering,\nimproving 3D scene reconstruction while maintaining real-time performance.\nRecent approaches have tackled the problem of sparse real-time NVS using 3DGS\nwithin a generalizable, MVS-based learning framework to regress 3D Gaussian\nparameters. Our work extends this line of research by addressing the challenge\nof generalizable sparse 3D reconstruction and NVS jointly, and manages to\nperform successfully at both tasks. We propose an MVS-based learning pipeline\nthat regresses 2DGS surface element parameters in a feed-forward fashion to\nperform 3D shape reconstruction and NVS from sparse-view images. We further\nshow that our generalizable pipeline can benefit from preexisting foundational\nmulti-view deep visual features. The resulting model attains the\nstate-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms\nof Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also\ndemonstrates strong generalization on the BlendedMVS and Tanks and Temples\ndatasets. We note that our model outperforms the prior state-of-the-art in\nfeed-forward sparse view reconstruction based on volume rendering of implicit\nrepresentations, while offering an almost 2 orders of magnitude higher\ninference speed.\n","authors":["Shubhendu Jena","Shishir Reddy Vutukur","Adnane Boukhayma"],"pdf_url":"https://arxiv.org/pdf/2505.02175v1.pdf","comment":"Project page : https://shubhendu-jena.github.io/SparSplat/"},{"id":"http://arxiv.org/abs/2505.02161v1","updated":"2025-05-04T15:50:28Z","published":"2025-05-04T15:50:28Z","title":"Focus What Matters: Matchability-Based Reweighting for Local Feature\n  Matching","summary":"  Since the rise of Transformers, many semi-dense matching methods have adopted\nattention mechanisms to extract feature descriptors. However, the attention\nweights, which capture dependencies between pixels or keypoints, are often\nlearned from scratch. This approach can introduce redundancy and noisy\ninteractions from irrelevant regions, as it treats all pixels or keypoints\nequally. Drawing inspiration from keypoint selection processes, we propose to\nfirst classify all pixels into two categories: matchable and non-matchable.\nMatchable pixels are expected to receive higher attention weights, while\nnon-matchable ones are down-weighted. In this work, we propose a novel\nattention reweighting mechanism that simultaneously incorporates a learnable\nbias term into the attention logits and applies a matchability-informed\nrescaling to the input value features. The bias term, injected prior to the\nsoftmax operation, selectively adjusts attention scores based on the confidence\nof query-key interactions. Concurrently, the feature rescaling acts\npost-attention by modulating the influence of each value vector in the final\noutput. This dual design allows the attention mechanism to dynamically adjust\nboth its internal weighting scheme and the magnitude of its output\nrepresentations. Extensive experiments conducted on three benchmark datasets\nvalidate the effectiveness of our method, consistently outperforming existing\nstate-of-the-art approaches.\n","authors":["Dongyue Li"],"pdf_url":"https://arxiv.org/pdf/2505.02161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02159v1","updated":"2025-05-04T15:46:34Z","published":"2025-05-04T15:46:34Z","title":"Small Clips, Big Gains: Learning Long-Range Refocused Temporal\n  Information for Video Super-Resolution","summary":"  Video super-resolution (VSR) can achieve better performance compared to\nsingle image super-resolution by additionally leveraging temporal information.\nIn particular, the recurrent-based VSR model exploits long-range temporal\ninformation during inference and achieves superior detail restoration. However,\neffectively learning these long-term dependencies within long videos remains a\nkey challenge. To address this, we propose LRTI-VSR, a novel training framework\nfor recurrent VSR that efficiently leverages Long-Range Refocused Temporal\nInformation. Our framework includes a generic training strategy that utilizes\ntemporal propagation features from long video clips while training on shorter\nvideo clips. Additionally, we introduce a refocused intra&inter-frame\ntransformer block which allows the VSR model to selectively prioritize useful\ntemporal information through its attention module while further improving\ninter-frame information utilization in the FFN module. We evaluate LRTI-VSR on\nboth CNN and transformer-based VSR architectures, conducting extensive ablation\nstudies to validate the contribution of each component. Experiments on\nlong-video test sets demonstrate that LRTI-VSR achieves state-of-the-art\nperformance while maintaining training and computational efficiency.\n","authors":["Xingyu Zhou","Wei Long","Jingbo Lu","Shiyin Jiang","Weiyi You","Haifeng Wu","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2505.02159v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.02148v1","updated":"2025-05-04T15:15:35Z","published":"2025-05-04T15:15:35Z","title":"Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly\n  Segmentation in Autonomous Driving","summary":"  To operate safely, autonomous vehicles (AVs) need to detect and handle\nunexpected objects or anomalies on the road. While significant research exists\nfor anomaly detection and segmentation in 2D, research progress in 3D is\nunderexplored. Existing datasets lack high-quality multimodal data that are\ntypically found in AVs. This paper presents a novel dataset for anomaly\nsegmentation in driving scenarios. To the best of our knowledge, it is the\nfirst publicly available dataset focused on road anomaly segmentation with\ndense 3D semantic labeling, incorporating both LiDAR and camera data, as well\nas sequential information to enable anomaly detection across various ranges.\nThis capability is critical for the safe navigation of autonomous vehicles. We\nadapted and evaluated several baseline models for 3D segmentation, highlighting\nthe challenges of 3D anomaly detection in driving environments. Our dataset and\nevaluation code will be openly available, facilitating the testing and\nperformance comparison of different approaches.\n","authors":["Alexey Nekrasov","Malcolm Burdorf","Stewart Worrall","Bastian Leibe","Julie Stephany Berrio Perez"],"pdf_url":"https://arxiv.org/pdf/2505.02148v1.pdf","comment":"Accepted for publication at CVPR 2025. Project page:\n  https://www.vision.rwth-aachen.de/stu-dataset"},{"id":"http://arxiv.org/abs/2505.02147v1","updated":"2025-05-04T15:14:44Z","published":"2025-05-04T15:14:44Z","title":"Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile\n  Application for Nepalese Flora","summary":"  Herb classification presents a critical challenge in botanical research,\nparticularly in regions with rich biodiversity such as Nepal. This study\nintroduces a novel deep learning approach for classifying 60 different herb\nspecies using Convolutional Neural Networks (CNNs) and transfer learning\ntechniques. Using a manually curated dataset of 12,000 herb images, we\ndeveloped a robust machine learning model that addresses existing limitations\nin herb recognition methodologies. Our research employed multiple model\narchitectures, including DenseNet121, 50-layer Residual Network (ResNet50),\n16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2,\nand Vision Transformer (VIT), with DenseNet121 ultimately demonstrating\nsuperior performance. Data augmentation and regularization techniques were\napplied to mitigate overfitting and enhance the generalizability of the model.\nThis work advances herb classification techniques, preserving traditional\nbotanical knowledge and promoting sustainable herb utilization.\n","authors":["Prajwal Thapa","Mridul Sharma","Jinu Nyachhyon","Yagya Raj Pandeya"],"pdf_url":"https://arxiv.org/pdf/2505.02147v1.pdf","comment":"12 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.17289v2","updated":"2025-05-04T14:50:45Z","published":"2025-02-24T16:20:25Z","title":"A novel approach to navigate the taxonomic hierarchy to address the\n  Open-World Scenarios in Medicinal Plant Classification","summary":"  In this article, we propose a novel approach for plant hierarchical taxonomy\nclassification by posing the problem as an open class problem. It is observed\nthat existing methods for medicinal plant classification often fail to perform\nhierarchical classification and accurately identifying unknown species,\nlimiting their effectiveness in comprehensive plant taxonomy classification.\nThus we address the problem of unknown species classification by assigning it\nbest hierarchical labels. We propose a novel method, which integrates\nDenseNet121, Multi-Scale Self-Attention (MSSA) and cascaded classifiers for\nhierarchical classification. The approach systematically categorizes medicinal\nplants at multiple taxonomic levels, from phylum to species, ensuring detailed\nand precise classification. Using multi scale space attention, the model\ncaptures both local and global contextual information from the images,\nimproving the distinction between similar species and the identification of new\nones. It uses attention scores to focus on important features across multiple\nscales. The proposed method provides a solution for hierarchical\nclassification, showcasing superior performance in identifying both known and\nunknown species. The model was tested on two state-of-art datasets with and\nwithout background artifacts and so that it can be deployed to tackle real word\napplication. We used unknown species for testing our model. For unknown species\nthe model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for\npredicting correct phylum, class, order and family respectively. Our proposed\nmodel size is almost four times less than the existing state of the art methods\nmaking it easily deploy able in real world application.\n","authors":["Soumen Sinha","Tanisha Rana","Rahul Roy"],"pdf_url":"https://arxiv.org/pdf/2502.17289v2.pdf","comment":"Major revision required"},{"id":"http://arxiv.org/abs/2505.02134v1","updated":"2025-05-04T14:44:37Z","published":"2025-05-04T14:44:37Z","title":"HiLLIE: Human-in-the-Loop Training for Low-Light Image Enhancement","summary":"  Developing effective approaches to generate enhanced results that align well\nwith human visual preferences for high-quality well-lit images remains a\nchallenge in low-light image enhancement (LLIE). In this paper, we propose a\nhuman-in-the-loop LLIE training framework that improves the visual quality of\nunsupervised LLIE model outputs through iterative training stages, named\nHiLLIE. At each stage, we introduce human guidance into the training process\nthrough efficient visual quality annotations of enhanced outputs. Subsequently,\nwe employ a tailored image quality assessment (IQA) model to learn human visual\npreferences encoded in the acquired labels, which is then utilized to guide the\ntraining process of an enhancement model. With only a small amount of pairwise\nranking annotations required at each stage, our approach continually improves\nthe IQA model's capability to simulate human visual assessment of enhanced\noutputs, thus leading to visually appealing LLIE results. Extensive experiments\ndemonstrate that our approach significantly improves unsupervised LLIE model\nperformance in terms of both quantitative and qualitative performance. The code\nand collected ranking dataset will be available at\nhttps://github.com/LabShuHangGU/HiLLIE.\n","authors":["Xiaorui Zhao","Xinyue Zhou","Peibei Cao","Junyu Lou","Shuhang Gu"],"pdf_url":"https://arxiv.org/pdf/2505.02134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02126v1","updated":"2025-05-04T14:23:45Z","published":"2025-05-04T14:23:45Z","title":"GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity\n  Non-Watertight 3D Garment Reconstruction","summary":"  Traditional 3D garment creation requires extensive manual operations,\nresulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved\nbreakthrough progress in 3D scene reconstruction and rendering, attracting\nwidespread attention and opening new pathways for 3D garment reconstruction.\nHowever, due to the unstructured and irregular nature of Gaussian primitives,\nit is difficult to reconstruct high-fidelity, non-watertight 3D garments. In\nthis paper, we present GarmentGS, a dense point cloud-guided method that can\nreconstruct high-fidelity garment surfaces with high geometric accuracy and\ngenerate non-watertight, single-layer meshes. Our method introduces a fast\ndense point cloud reconstruction module that can complete garment point cloud\nreconstruction in 10 minutes, compared to traditional methods that require\nseveral hours. Furthermore, we use dense point clouds to guide the movement,\nflattening, and rotation of Gaussian primitives, enabling better distribution\non the garment surface to achieve superior rendering effects and geometric\naccuracy. Through numerical and visual comparisons, our method achieves fast\ntraining and real-time rendering while maintaining competitive quality.\n","authors":["Zhihao Tang","Shenghao Yang","Hongtao Zhang","Mingbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.02126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00318v3","updated":"2025-05-04T14:07:15Z","published":"2024-05-01T04:51:10Z","title":"Covariant spatio-temporal receptive fields for spiking neural networks","summary":"  Biological nervous systems constitute important sources of inspiration\ntowards computers that are faster, cheaper, and more energy efficient.\nNeuromorphic disciplines view the brain as a coevolved system, simultaneously\noptimizing the hardware and the algorithms running on it. There are clear\nefficiency gains when bringing the computations into a physical substrate, but\nwe presently lack theories to guide efficient implementations. Here, we present\na principled computational model for neuromorphic systems in terms of\nspatio-temporal receptive fields, based on affine Gaussian kernels over space\nand leaky-integrator and leaky integrate-and-fire models over time. Our theory\nis provably covariant to spatial affine and temporal scaling transformations,\nand with close similarities to the visual processing in mammalian brains. We\nuse these spatio-temporal receptive fields as a prior in an event-based vision\ntask, and show that this improves the training of spiking networks, which\notherwise is known as problematic for event-based vision. This work combines\nefforts within scale-space theory and computational neuroscience to identify\ntheoretically well-founded ways to process spatio-temporal signals in\nneuromorphic systems. Our contributions are immediately relevant for signal\nprocessing and event-based vision, and can be extended to other processing\ntasks over space and time, such as memory and control.\n","authors":["Jens Egholm Pedersen","Jörg Conradt","Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2405.00318v3.pdf","comment":"Code available at https://github.com/jegp/nrf"},{"id":"http://arxiv.org/abs/2505.02109v1","updated":"2025-05-04T13:29:31Z","published":"2025-05-04T13:29:31Z","title":"Unaligned RGB Guided Hyperspectral Image Super-Resolution with\n  Spatial-Spectral Concordance","summary":"  Hyperspectral images super-resolution aims to improve the spatial resolution,\nyet its performance is often limited at high-resolution ratios. The recent\nadoption of high-resolution reference images for super-resolution is driven by\nthe poor spatial detail found in low-resolution HSIs, presenting it as a\nfavorable method. However, these approaches cannot effectively utilize\ninformation from the reference image, due to the inaccuracy of alignment and\nits inadequate interaction between alignment and fusion modules. In this paper,\nwe introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution\n(SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the\nissues of inaccurate alignment and poor interactivity of the previous\napproaches. Specifically, to ensure spatial concordance, i.e., align images\nmore accurately across resolutions and refine textures, we construct a\nTwo-Stage Image Alignment with a synthetic generation pipeline in the image\nalignment module, where the fine-tuned optical flow model can produce a more\naccurate optical flow in the first stage and warp model can refine damaged\ntextures in the second stage. To enhance the interaction between alignment and\nfusion modules and ensure spectral concordance during reconstruction, we\npropose a Feature Aggregation module and an Attention Fusion module. In the\nfeature aggregation module, we introduce an Iterative Deformable Feature\nAggregation block to achieve significant feature matching and texture\naggregation with the fusion multi-scale results guidance, iteratively\ngenerating learnable offset. Besides, we introduce two basic spectral-wise\nattention blocks in the attention fusion module to model the inter-spectra\ninteractions. Extensive experiments on three natural or remote-sensing datasets\nshow that our method outperforms state-of-the-art approaches on both\nquantitative and qualitative evaluations.\n","authors":["Yingkai Zhang","Zeqiang Lai","Tao Zhang","Ying Fu","Chenghu Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.02109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02108v1","updated":"2025-05-04T13:28:49Z","published":"2025-05-04T13:28:49Z","title":"SignSplat: Rendering Sign Language via Gaussian Splatting","summary":"  State-of-the-art approaches for conditional human body rendering via Gaussian\nsplatting typically focus on simple body motions captured from many views. This\nis often in the context of dancing or walking. However, for more complex use\ncases, such as sign language, we care less about large body motion and more\nabout subtle and complex motions of the hands and face. The problems of\nbuilding high fidelity models are compounded by the complexity of capturing\nmulti-view data of sign. The solution is to make better use of sequence data,\nensuring that we can overcome the limited information from only a few views by\nexploiting temporal variability. Nevertheless, learning from sequence-level\ndata requires extremely accurate and consistent model fitting to ensure that\nappearance is consistent across complex motions. We focus on how to achieve\nthis, constraining mesh parameters to build an accurate Gaussian splatting\nframework from few views capable of modelling subtle human motion. We leverage\nregularization techniques on the Gaussian parameters to mitigate overfitting\nand rendering artifacts. Additionally, we propose a new adaptive control method\nto densify Gaussians and prune splat points on the mesh surface. To demonstrate\nthe accuracy of our approach, we render novel sequences of sign language video,\nbuilding on neural machine translation approaches to sign stitching. On\nbenchmark datasets, our approach achieves state-of-the-art performance; and on\nhighly articulated and complex sign language motion, we significantly\noutperform competing approaches.\n","authors":["Maksym Ivashechkin","Oscar Mendez","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2505.02108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02094v1","updated":"2025-05-04T13:00:29Z","published":"2025-05-04T13:00:29Z","title":"SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from\n  Sparse and Noisy Demonstrations","summary":"  We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.\n","authors":["Runyi Yu","Yinhuai Wang","Qihan Zhao","Hok Wai Tsui","Jingbo Wang","Ping Tan","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2505.02094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13431v2","updated":"2025-05-04T12:19:12Z","published":"2024-08-24T01:53:02Z","title":"Face Clustering via Early Stopping and Edge Recall","summary":"  Large-scale face clustering has achieved significant progress, with many\nefforts dedicated to learning to cluster large-scale faces with\nsupervised-learning. However, complex model design and tedious clustering\nprocesses are typical in existing methods. Such limitations result in\ninfeasible clustering in real-world applications. Reasonable and efficient\nmodel design and training need to be taken into account. Besides, developing\nunsupervised face clustering algorithms is crucial, which are more realistic in\nreal-world applications. In this paper, we propose a novel unsupervised face\nclustering algorithm FC-ES and a novel supervised face clustering algorithm\nFC-ESER to address these issues. An efficient and effective neighbor-based edge\nprobability and a novel early stopping strategy are proposed in FC-ES,\nguaranteeing the accuracy and recall of large-scale face clustering\nsimultaneously. Furthermore, to take advantage of supervised learning, a novel\nedge recall strategy is proposed in FC-ESER to further recall the edge\nconnections that are not connected in FC-ES. Extensive experiments on multiple\nbenchmarks for face, person, and vehicle clustering show that our proposed\nFC-ES and FC-ESER significantly outperform previous state-of-the-art methods.\nOur code will be available at https://github.com/jumptoliujj/FC-ESER.\n","authors":["Junjie Liu"],"pdf_url":"https://arxiv.org/pdf/2408.13431v2.pdf","comment":"Insufficient experiments, we hope to withdraw the paper, supplement\n  and improve it again"},{"id":"http://arxiv.org/abs/2501.15955v2","updated":"2025-05-04T12:14:11Z","published":"2025-01-27T11:00:19Z","title":"Rethinking the Bias of Foundation Model under Long-tailed Distribution","summary":"  Long-tailed learning has garnered increasing attention due to its practical\nsignificance. Among the various approaches, the fine-tuning paradigm has gained\nconsiderable interest with the advent of foundation models. However, most\nexisting methods primarily focus on leveraging knowledge from these models,\noverlooking the inherent biases introduced by the imbalanced training data they\nrely on. In this paper, we examine how such imbalances from pre-training affect\nlong-tailed downstream tasks. Specifically, we find the imbalance biases\ninherited in foundation models on downstream task as parameter imbalance and\ndata imbalance. During fine-tuning, we observe that parameter imbalance plays a\nmore critical role, while data imbalance can be mitigated using existing\nre-balancing strategies. Moreover, we find that parameter imbalance cannot be\neffectively addressed by current re-balancing techniques, such as adjusting the\nlogits, during training, unlike data imbalance. To tackle both imbalances\nsimultaneously, we build our method on causal learning and view the incomplete\nsemantic factor as the confounder, which brings spurious correlations between\ninput samples and labels. To resolve the negative effects of this, we propose a\nnovel backdoor adjustment method that learns the true causal effect between\ninput samples and labels, rather than merely fitting the correlations in the\ndata. Notably, we achieve an average performance increase of about $1.67\\%$ on\neach dataset.\n","authors":["Jiahao Chen","Bin Qin","Jiangmeng Li","Hao Chen","Bing Su"],"pdf_url":"https://arxiv.org/pdf/2501.15955v2.pdf","comment":"Published as a conference paper in ICML 2025"},{"id":"http://arxiv.org/abs/2505.02079v1","updated":"2025-05-04T12:06:54Z","published":"2025-05-04T12:06:54Z","title":"HandOcc: NeRF-based Hand Rendering with Occupancy Networks","summary":"  We propose HandOcc, a novel framework for hand rendering based upon\noccupancy. Popular rendering methods such as NeRF are often combined with\nparametric meshes to provide deformable hand models. However, in doing so, such\napproaches present a trade-off between the fidelity of the mesh and the\ncomplexity and dimensionality of the parametric model. The simplicity of\nparametric mesh structures is appealing, but the underlying issue is that it\nbinds methods to mesh initialization, making it unable to generalize to objects\nwhere a parametric model does not exist. It also means that estimation is tied\nto mesh resolution and the accuracy of mesh fitting. This paper presents a\npipeline for meshless 3D rendering, which we apply to the hands. By providing\nonly a 3D skeleton, the desired appearance is extracted via a convolutional\nmodel. We do this by exploiting a NeRF renderer conditioned upon an\noccupancy-based representation. The approach uses the hand occupancy to resolve\nhand-to-hand interactions further improving results, allowing fast rendering,\nand excellent hand appearance transfer. On the benchmark InterHand2.6M dataset,\nwe achieved state-of-the-art results.\n","authors":["Maksym Ivashechkin","Oscar Mendez","Richard Bowden"],"pdf_url":"https://arxiv.org/pdf/2505.02079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02075v1","updated":"2025-05-04T11:59:26Z","published":"2025-05-04T11:59:26Z","title":"Benchmarking Feature Upsampling Methods for Vision Foundation Models\n  using Interactive Segmentation","summary":"  Vision Foundation Models (VFMs) are large-scale, pre-trained models that\nserve as general-purpose backbones for various computer vision tasks. As VFMs'\npopularity grows, there is an increasing interest in understanding their\neffectiveness for dense prediction tasks. However, VFMs typically produce\nlow-resolution features, limiting their direct applicability in this context.\nOne way to tackle this limitation is by employing a task-agnostic feature\nupsampling module that refines VFM features resolution. To assess the\neffectiveness of this approach, we investigate Interactive Segmentation (IS) as\na novel benchmark for evaluating feature upsampling methods on VFMs. Due to its\ninherent multimodal input, consisting of an image and a set of user-defined\nclicks, as well as its dense mask output, IS creates a challenging environment\nthat demands comprehensive visual scene understanding. Our benchmarking\nexperiments show that selecting appropriate upsampling strategies significantly\nimproves VFM features quality. The code is released at\nhttps://github.com/havrylovv/iSegProbe\n","authors":["Volodymyr Havrylov","Haiwen Huang","Dan Zhang","Andreas Geiger"],"pdf_url":"https://arxiv.org/pdf/2505.02075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02071v1","updated":"2025-05-04T11:42:04Z","published":"2025-05-04T11:42:04Z","title":"Hierarchical Compact Clustering Attention (COCA) for Unsupervised\n  Object-Centric Learning","summary":"  We propose the Compact Clustering Attention (COCA) layer, an effective\nbuilding block that introduces a hierarchical strategy for object-centric\nrepresentation learning, while solving the unsupervised object discovery task\non single images. COCA is an attention-based clustering module capable of\nextracting object-centric representations from multi-object scenes, when\ncascaded into a bottom-up hierarchical network architecture, referred to as\nCOCA-Net. At its core, COCA utilizes a novel clustering algorithm that\nleverages the physical concept of compactness, to highlight distinct object\ncentroids in a scene, providing a spatial inductive bias. Thanks to this\nstrategy, COCA-Net generates high-quality segmentation masks on both the\ndecoder side and, notably, the encoder side of its pipeline. Additionally,\nCOCA-Net is not bound by a predetermined number of object masks that it\ngenerates and handles the segmentation of background elements better than its\ncompetitors. We demonstrate COCA-Net's segmentation performance on six widely\nadopted datasets, achieving superior or competitive results against the\nstate-of-the-art models across nine different evaluation metrics.\n","authors":["Can Küçüksözen","Yücel Yemez"],"pdf_url":"https://arxiv.org/pdf/2505.02071v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2505.02064v1","updated":"2025-05-04T10:55:21Z","published":"2025-05-04T10:55:21Z","title":"RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and\n  Reasoning through Real-Time Video","summary":"  Multimodal Large Language Models (MLLMs) increasingly excel at perception,\nunderstanding, and reasoning. However, current benchmarks inadequately evaluate\ntheir ability to perform these tasks continuously in dynamic, real-world\nenvironments. To bridge this gap, we introduce RTV-Bench, a fine-grained\nbenchmark for MLLM real-time video analysis. RTV-Bench uses three key\nprinciples: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve\nwith scene changes; (2) Hierarchical Question Structure, combining basic and\nadvanced queries; and (3) Multi-dimensional Evaluation, assessing the ability\nof continuous perception, understanding, and reasoning. RTV-Bench contains 552\ndiverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated\nleading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline\n(Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5,\nInternLM-XComposer2.5-OmniLive) models. Experiment results show open-source\nreal-time models largely outperform offline ones but still trail top\nproprietary models. Our analysis also reveals that larger model size or higher\nframe sampling rates do not significantly boost RTV-Bench performance,\nsometimes causing slight decreases. This underscores the need for better model\narchitectures optimized for video stream processing and long sequences to\nadvance real-time video analysis with MLLMs. Our benchmark toolkit is available\nat: https://github.com/LJungang/RTV-Bench.\n","authors":["Shuhang Xun","Sicheng Tao","Jungang Li","Yibo Shi","Zhixin Lin","Zhanhui Zhu","Yibo Yan","Hanqian Li","Linghao Zhang","Shikang Wang","Yixin Liu","Hanbo Zhang","Xuming Hu","Ying Ma"],"pdf_url":"https://arxiv.org/pdf/2505.02064v1.pdf","comment":"13 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.02060v1","updated":"2025-05-04T10:36:58Z","published":"2025-05-04T10:36:58Z","title":"Transforming faces into video stories -- VideoFace2.0","summary":"  Face detection and face recognition have been in the focus of vision\ncommunity since the very beginnings. Inspired by the success of the original\nVideoface digitizer, a pioneering device that allowed users to capture video\nsignals from any source, we have designed an advanced video analytics tool to\nefficiently create structured video stories, i.e. identity-based information\ncatalogs. VideoFace2.0 is the name of the developed system for spatial and\ntemporal localization of each unique face in the input video, i.e. face\nre-identification (ReID), which also allows their cataloging, characterization\nand creation of structured video outputs for later downstream tasks. Developed\nnear real-time solution is primarily designed to be utilized in application\nscenarios involving TV production, media analysis, and as an efficient tool for\ncreating large video datasets necessary for training machine learning (ML)\nmodels in challenging vision tasks such as lip reading and multimodal speech\nrecognition. Conducted experiments confirm applicability of the proposed face\nReID algorithm that is combining the concepts of face detection, face\nrecognition and passive tracking-by-detection in order to achieve robust and\nefficient face ReID. The system is envisioned as a compact and modular\nextensions of the existing video production equipment. We hope that the\npresented work and shared code will stimulate further interest in development\nof similar, application specific video analysis tools, and lower the entry\nbarrier for production of high-quality multi-modal ML datasets in the future.\n","authors":["Branko Brkljač","Vladimir Kalušev","Branislav Popović","Milan Sečujski"],"pdf_url":"https://arxiv.org/pdf/2505.02060v1.pdf","comment":"4 pages, 2 figures, 1 algorithm; associated VideoFace2.0 code\n  implementation, test videos and results visualizations are available at\n  https://github.com/brkljac/VideoFace2.0 ; Preprint submitted to the 14th\n  Mediterranean Conference on Embedded Computing (MECO), 10-14 June 2025,\n  Budva, Montenegro"},{"id":"http://arxiv.org/abs/2504.19718v2","updated":"2025-05-04T10:27:59Z","published":"2025-04-28T12:13:12Z","title":"Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation","summary":"  Face registration deforms a template mesh to closely fit a 3D face scan, the\nquality of which commonly degrades in non-skin regions (e.g., hair, beard,\naccessories), because the optimized template-to-scan distance pulls the\ntemplate mesh towards the noisy scan surface. Improving registration quality\nrequires a clean separation of skin and non-skin regions on the scan mesh.\nExisting image-based (2D) or scan-based (3D) segmentation methods however\nperform poorly. Image-based segmentation outputs multi-view inconsistent masks,\nand they cannot account for scan inaccuracies or scan-image misalignment, while\nscan-based methods suffer from lower spatial resolution compared to images. In\nthis work, we introduce a novel method that accurately separates skin from\nnon-skin geometry on 3D human head scans. For this, our method extracts\nfeatures from multi-view images using a frozen image foundation model and\naggregates these features in 3D. These lifted 2D features are then fused with\n3D geometric features extracted from the scan mesh, to then predict a\nsegmentation mask directly on the scan mesh. We show that our segmentations\nimprove the registration accuracy over pure 2D or 3D segmentation methods by\n8.89% and 14.3%, respectively. Although trained only on synthetic data, our\nmodel generalizes well to real data.\n","authors":["Victoria Yue Chen","Daoye Wang","Stephan Garbin","Jan Bednarik","Sebastian Winberg","Timo Bolkart","Thabo Beeler"],"pdf_url":"https://arxiv.org/pdf/2504.19718v2.pdf","comment":"4 pages, 4 figures, to be published in Eurographics 2025 as a short\n  paper"},{"id":"http://arxiv.org/abs/2505.02056v1","updated":"2025-05-04T10:24:34Z","published":"2025-05-04T10:24:34Z","title":"Handling Imbalanced Pseudolabels for Vision-Language Models with Concept\n  Alignment and Confusion-Aware Calibrated Margin","summary":"  Adapting vision-language models (VLMs) to downstream tasks with pseudolabels\nhas gained increasing attention. A major obstacle is that the pseudolabels\ngenerated by VLMs tend to be imbalanced, leading to inferior performance. While\nexisting methods have explored various strategies to address this, the\nunderlying causes of imbalance remain insufficiently investigated. To fill this\ngap, we delve into imbalanced pseudolabels and identify two primary\ncontributing factors: concept mismatch and concept confusion. To mitigate these\ntwo issues, we propose a novel framework incorporating concept alignment and\nconfusion-aware calibrated margin mechanisms. The core of our approach lies in\nenhancing underperforming classes and promoting balanced predictions across\ncategories, thus mitigating imbalance. Extensive experiments on six benchmark\ndatasets with three learning paradigms demonstrate that the proposed method\neffectively enhances the accuracy and balance of pseudolabels, achieving a\nrelative improvement of 6.29% over the SoTA method. Our code is avaliable at\nhttps://anonymous.4open.science/r/CAP-C642/\n","authors":["Yuchen Wang","Xuefeng Bai","Xiucheng Li","Weili Guan","Liqiang Nie","Xinyang Chen"],"pdf_url":"https://arxiv.org/pdf/2505.02056v1.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2505.02052v1","updated":"2025-05-04T10:07:38Z","published":"2025-05-04T10:07:38Z","title":"TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity\n  Descriptions for Improving Human Activity Recognition","summary":"  Sensor-based human activity recognition (HAR) has predominantly focused on\nInertial Measurement Units and vision data, often overlooking the capabilities\nunique to pressure sensors, which capture subtle body dynamics and shifts in\nthe center of mass. Despite their potential for postural and balance-based\nactivities, pressure sensors remain underutilized in the HAR domain due to\nlimited datasets. To bridge this gap, we propose to exploit generative\nfoundation models with pressure-specific HAR techniques. Specifically, we\npresent a bidirectional Text$\\times$Pressure model that uses generative\nfoundation models to interpret pressure data as natural language. TxP\naccomplishes two tasks: (1) Text2Pressure, converting activity text\ndescriptions into pressure sequences, and (2) Pressure2Text, generating\nactivity descriptions and classifications from dynamic pressure maps.\nLeveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on\nour synthetic PressLang dataset, containing over 81,100 text-pressure pairs.\nValidated on real-world data for activities such as yoga and daily tasks, TxP\nprovides novel approaches to data augmentation and classification grounded in\natomic actions. This consequently improved HAR performance by up to 12.4\\% in\nmacro F1 score compared to the state-of-the-art, advancing pressure-based HAR\nwith broader applications and deeper insights into human movement.\n","authors":["Lala Shakti Swarup Ray","Lars Krupp","Vitor Fortes Rey","Bo Zhou","Sungho Suh","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2505.02052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02048v1","updated":"2025-05-04T09:57:10Z","published":"2025-05-04T09:57:10Z","title":"Regression s all you need for medical image translation","summary":"  The acquisition of information-rich images within a limited time budget is\ncrucial in medical imaging. Medical image translation (MIT) can help enhance\nand supplement existing datasets by generating synthetic images from acquired\ndata. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have\nachieved remarkable success in natural image generation, their benefits -\ncreativity and image realism - do not necessarily transfer to medical\napplications where highly accurate anatomical information is required. In fact,\nthe imitation of acquisition noise or content hallucination hinder clinical\nutility. Here, we introduce YODA (You Only Denoise once - or Average), a novel\n2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and\nregression paradigms to produce realistic or noise-free outputs. Furthermore,\nwe propose Expectation-Approximation (ExpA) DM sampling, which draws\ninspiration from MRI signal averaging. ExpA-sampling suppresses generated noise\nand, thus, eliminates noise from biasing the evaluation of image quality.\nThrough extensive experiments on four diverse multi-modal datasets - comprising\nmulti-contrast brain MRI and pelvic MRI-CT - we show that diffusion and\nregression sampling yield similar results in practice. As such, the\ncomputational overhead of diffusion sampling does not provide systematic\nbenefits in medical information translation. Building on these insights, we\ndemonstrate that YODA outperforms several state-of-the-art GAN and DM methods.\nNotably, YODA-generated images are shown to be interchangeable with, or even\nsuperior to, physical acquisitions for several downstream tasks. Our findings\nchallenge the presumed advantages of DMs in MIT and pave the way for the\npractical application of MIT in medical imaging.\n","authors":["Sebastian Rassmann","David Kügler","Christian Ewert","Martin Reuter"],"pdf_url":"https://arxiv.org/pdf/2505.02048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02046v1","updated":"2025-05-04T09:54:11Z","published":"2025-05-04T09:54:11Z","title":"A UNet Model for Accelerated Preprocessing of CRISM Hyperspectral Data\n  for Mineral Identification on Mars","summary":"  Accurate mineral identification on the Martian surface is critical for\nunderstanding the planet's geological history. This paper presents a UNet-based\nautoencoder model for efficient spectral preprocessing of CRISM MTRDR\nhyperspectral data, addressing the limitations of traditional methods that are\ncomputationally intensive and time-consuming. The proposed model automates key\npreprocessing steps, such as smoothing and continuum removal, while preserving\nessential mineral absorption features. Trained on augmented spectra from the\nMICA spectral library, the model introduces realistic variability to simulate\nMTRDR data conditions. By integrating this framework, preprocessing time for an\n800x800 MTRDR scene is reduced from 1.5 hours to just 5 minutes on an NVIDIA\nT1600 GPU. The preprocessed spectra are subsequently classified using MICAnet,\na deep learning model for Martian mineral identification. Evaluation on labeled\nCRISM TRDR data demonstrates that the proposed approach achieves competitive\naccuracy while significantly enhancing preprocessing efficiency. This work\nhighlights the potential of the UNet-based preprocessing framework to improve\nthe speed and reliability of mineral mapping on Mars.\n","authors":["Priyanka Kumari","Sampriti Soor","Amba Shetty","Archana M. Nair"],"pdf_url":"https://arxiv.org/pdf/2505.02046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02043v1","updated":"2025-05-04T09:42:03Z","published":"2025-05-04T09:42:03Z","title":"Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive\n  Prediction","summary":"  Recovering CAD models from point clouds, especially the sketch-extrusion\nprocess, can be seen as the process of rebuilding the topology and extrusion\nprimitives. Previous methods utilize implicit fields for sketch representation,\nleading to shape reconstruction of curved edges. In this paper, we proposed a\nCAD reconstruction network that produces editable CAD models from input point\nclouds (Point2Primitive) by directly predicting every element of the extrusion\nprimitives. Point2Primitive can directly detect and predict sketch curves (type\nand parameter) from point clouds based on an improved transformer. The sketch\ncurve parameters are formulated as position queries and optimized in an\nautoregressive way, leading to high parameter accuracy. The topology is rebuilt\nby extrusion segmentation, and each extrusion parameter (sketch and extrusion\noperation) is recovered by combining the predicted curves and the computed\nextrusion operation. Extensive experiments demonstrate that our method is\nsuperior in primitive prediction accuracy and CAD reconstruction. The\nreconstructed shapes are of high geometrical fidelity.\n","authors":["Cheng Wang","Xinzhu Ma","Bin Wang","Shixiang Tang","Yuan Meng","Ping Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.02043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09779v3","updated":"2025-05-04T08:27:18Z","published":"2024-09-15T15:58:20Z","title":"Underwater Image Enhancement via Dehazing and Color Restoration","summary":"  Underwater visual imaging is crucial for marine engineering, but it suffers\nfrom low contrast, blurriness, and color degradation, which hinders downstream\nanalysis. Existing underwater image enhancement methods often treat the haze\nand color cast as a unified degradation process, neglecting their inherent\nindependence while overlooking their synergistic relationship. To overcome this\nlimitation, we propose a Vision Transformer (ViT)-based network (referred to as\nWaterFormer) to improve underwater image quality. WaterFormer contains three\nmajor components: a dehazing block (DehazeFormer Block) to capture the\nself-correlated haze features and extract deep-level features, a Color\nRestoration Block (CRB) to capture self-correlated color cast features, and a\nChannel Fusion Block (CFB) that dynamically integrates these decoupled features\nto achieve comprehensive enhancement. To ensure authenticity, a soft\nreconstruction layer based on the underwater imaging physics model is included.\nFurther, a Chromatic Consistency Loss and Sobel Color Loss are designed to\nrespectively preserve color fidelity and enhance structural details during\nnetwork training. Comprehensive experimental results demonstrate that\nWaterFormer outperforms other state-of-the-art methods in enhancing underwater\nimages.\n","authors":["Chengqin Wu","Shuai Yu","Tuyan Luo","Qiuhua Rao","Qingson Hu","Jingxiang Xu","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.09779v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02025v1","updated":"2025-05-04T08:24:14Z","published":"2025-05-04T08:24:14Z","title":"A Birotation Solution for Relative Pose Problems","summary":"  Relative pose estimation, a fundamental computer vision problem, has been\nextensively studied for decades. Existing methods either estimate and decompose\nthe essential matrix or directly estimate the rotation and translation to\nobtain the solution. In this article, we break the mold by tackling this\ntraditional problem with a novel birotation solution. We first introduce three\nbasis transformations, each associated with a geometric metric to quantify the\ndistance between the relative pose to be estimated and its corresponding basis\ntransformation. Three energy functions, designed based on these metrics, are\nthen minimized on the Riemannian manifold $\\mathrm{SO(3)}$ by iteratively\nupdating the two rotation matrices. The two rotation matrices and the basis\ntransformation corresponding to the minimum energy are ultimately utilized to\nrecover the relative pose. Extensive quantitative and qualitative evaluations\nacross diverse relative pose estimation tasks demonstrate the superior\nperformance of our proposed birotation solution. Source code, demo video, and\ndatasets will be available at\n\\href{https://mias.group/birotation-solution}{mias.group/birotation-solution}\nupon publication.\n","authors":["Hongbo Zhao","Ziwei Long","Mengtan Zhang","Hanli Wang","Qijun Chen","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2505.02025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11815v2","updated":"2025-05-04T08:10:51Z","published":"2024-12-16T14:32:49Z","title":"ColorFlow: Retrieval-Augmented Image Sequence Colorization","summary":"  Automatic black-and-white image sequence colorization while preserving\ncharacter and object identity (ID) is a complex task with significant market\ndemand, such as in cartoon or comic series colorization. Despite advancements\nin visual colorization using large-scale generative models like diffusion\nmodels, challenges with controllability and identity consistency persist,\nmaking current solutions unsuitable for industrial application.To address this,\nwe propose ColorFlow, a three-stage diffusion-based framework tailored for\nimage sequence colorization in industrial applications. Unlike existing methods\nthat require per-ID finetuning or explicit ID embedding extraction, we propose\na novel robust and generalizable Retrieval Augmented Colorization pipeline for\ncolorizing images with relevant color references. Our pipeline also features a\ndual-branch design: one branch for color identity extraction and the other for\ncolorization, leveraging the strengths of diffusion models. We utilize the\nself-attention mechanism in diffusion models for strong in-context learning and\ncolor identity matching. To evaluate our model, we introduce ColorFlow-Bench, a\ncomprehensive benchmark for reference-based colorization. Results show that\nColorFlow outperforms existing models across multiple metrics, setting a new\nstandard in sequential image colorization and potentially benefiting the art\nindustry. We release our codes and models on our project page:\nhttps://zhuang2002.github.io/ColorFlow/.\n","authors":["Junhao Zhuang","Xuan Ju","Zhaoyang Zhang","Yong Liu","Shiyi Zhang","Chun Yuan","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2412.11815v2.pdf","comment":"Project Page: https://zhuang2002.github.io/ColorFlow/"},{"id":"http://arxiv.org/abs/2505.02018v1","updated":"2025-05-04T07:48:36Z","published":"2025-05-04T07:48:36Z","title":"R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM\n  Complex Reasoning Evaluation","summary":"  Reasoning stands as a cornerstone of intelligence, enabling the synthesis of\nexisting knowledge to solve complex problems. Despite remarkable progress,\nexisting reasoning benchmarks often fail to rigorously evaluate the nuanced\nreasoning capabilities required for complex, real-world problemsolving,\nparticularly in multi-disciplinary and multimodal contexts. In this paper, we\nintroduce a graduate-level, multi-disciplinary, EnglishChinese benchmark,\ndubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of\nboth language and multimodal models. RBench spans 1,094 questions across 108\nsubjects for language model evaluation and 665 questions across 83 subjects for\nmultimodal model testing in both English and Chinese. These questions are\nmeticulously curated to ensure rigorous difficulty calibration, subject\nbalance, and crosslinguistic alignment, enabling the assessment to be an\nOlympiad-level multi-disciplinary benchmark. We evaluate widely used models,\nincluding OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate\nthat advanced models perform poorly on complex reasoning, especially multimodal\nreasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy\non our multimodal evaluation. Data and code are made publicly available at\nhere.\n","authors":["Meng-Hao Guo","Jiajun Xu","Yi Zhang","Jiaxi Song","Haoyang Peng","Yi-Xuan Deng","Xinzhi Dong","Kiyohiro Nakayama","Zhengyang Geng","Chen Wang","Bolin Ni","Guo-Wei Yang","Yongming Rao","Houwen Peng","Han Hu","Gordon Wetzstein","Shi-min Hu"],"pdf_url":"https://arxiv.org/pdf/2505.02018v1.pdf","comment":"18pages"},{"id":"http://arxiv.org/abs/2505.02013v1","updated":"2025-05-04T06:58:21Z","published":"2025-05-04T06:58:21Z","title":"MLLM-Enhanced Face Forgery Detection: A Vision-Language Fusion Solution","summary":"  Reliable face forgery detection algorithms are crucial for countering the\ngrowing threat of deepfake-driven disinformation. Previous research has\ndemonstrated the potential of Multimodal Large Language Models (MLLMs) in\nidentifying manipulated faces. However, existing methods typically depend on\neither the Large Language Model (LLM) alone or an external detector to generate\nclassification results, which often leads to sub-optimal integration of visual\nand textual modalities. In this paper, we propose VLF-FFD, a novel\nVision-Language Fusion solution for MLLM-enhanced Face Forgery Detection. Our\nkey contributions are twofold. First, we present EFF++, a frame-level,\nexplainability-driven extension of the widely used FaceForensics++ (FF++)\ndataset. In EFF++, each manipulated video frame is paired with a textual\nannotation that describes both the forgery artifacts and the specific\nmanipulation technique applied, enabling more effective and informative MLLM\ntraining. Second, we design a Vision-Language Fusion Network (VLF-Net) that\npromotes bidirectional interaction between visual and textual features,\nsupported by a three-stage training pipeline to fully leverage its potential.\nVLF-FFD achieves state-of-the-art (SOTA) performance in both cross-dataset and\nintra-dataset evaluations, underscoring its exceptional effectiveness in face\nforgery detection.\n","authors":["Siran Peng","Zipei Wang","Li Gao","Xiangyu Zhu","Tianshuo Zhang","Ajian Liu","Haoyuan Zhang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2505.02013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02007v1","updated":"2025-05-04T06:28:06Z","published":"2025-05-04T06:28:06Z","title":"Efficient Noise Calculation in Deep Learning-based MRI Reconstructions","summary":"  Accelerated MRI reconstruction involves solving an ill-posed inverse problem\nwhere noise in acquired data propagates to the reconstructed images. Noise\nanalyses are central to MRI reconstruction for providing an explicit measure of\nsolution fidelity and for guiding the design and deployment of novel\nreconstruction methods. However, deep learning (DL)-based reconstruction\nmethods have often overlooked noise propagation due to inherent analytical and\ncomputational challenges, despite its critical importance. This work proposes a\ntheoretically grounded, memory-efficient technique to calculate voxel-wise\nvariance for quantifying uncertainty due to acquisition noise in accelerated\nMRI reconstructions. Our approach approximates noise covariance using the DL\nnetwork's Jacobian, which is intractable to calculate. To circumvent this, we\nderive an unbiased estimator for the diagonal of this covariance matrix\n(voxel-wise variance) and introduce a Jacobian sketching technique to\nefficiently implement it. We evaluate our method on knee and brain MRI datasets\nfor both data- and physics-driven networks trained in supervised and\nunsupervised manners. Compared to empirical references obtained via Monte Carlo\nsimulations, our technique achieves near-equivalent performance while reducing\ncomputational and memory demands by an order of magnitude or more. Furthermore,\nour method is robust across varying input noise levels, acceleration factors,\nand diverse undersampling schemes, highlighting its broad applicability. Our\nwork reintroduces accurate and efficient noise analysis as a central tenet of\nreconstruction algorithms, holding promise to reshape how we evaluate and\ndeploy DL-based MRI. Our code will be made publicly available upon acceptance.\n","authors":["Onat Dalmaz","Arjun D. Desai","Reinhard Heckel","Tolga Çukur","Akshay S. Chaudhari","Brian A. Hargreaves"],"pdf_url":"https://arxiv.org/pdf/2505.02007v1.pdf","comment":"Accepted ICML 2025. Supplementary material included"},{"id":"http://arxiv.org/abs/2505.02005v1","updated":"2025-05-04T06:25:14Z","published":"2025-05-04T06:25:14Z","title":"Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural\n  Radiance Fields","summary":"  Recent NeRF methods on large-scale scenes have underlined the importance of\nscene decomposition for scalable NeRFs. Although achieving reasonable\nscalability, there are several critical problems remaining unexplored, i.e.,\nlearnable decomposition, modeling scene heterogeneity, and modeling efficiency.\nIn this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash\nExperts (HMoHE) network that addresses these challenges within a unified\nframework. It is a highly scalable NeRF that learns heterogeneous decomposition\nand heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end\nmanner. In our framework, a gating network learns to decomposes scenes and\nallocates 3D points to specialized NeRF experts. This gating network is\nco-optimized with the experts, by our proposed Sparsely Gated Mixture of\nExperts (MoE) NeRF framework. We incorporate a hash-based gating network and\ndistinct heterogeneous hash experts. The hash-based gating efficiently learns\nthe decomposition of the large-scale scene. The distinct heterogeneous hash\nexperts consist of hash grids of different resolution ranges, enabling\neffective learning of the heterogeneous representation of different scene\nparts. These design choices make our framework an end-to-end and highly\nscalable NeRF solution for real-world large-scale scene modeling to achieve\nboth quality and efficiency. We evaluate our accuracy and scalability on\nexisting large-scale NeRF datasets and a new dataset with very large-scale\nscenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our\napproach can be easily scaled to various large-scale scenes and achieve\nstate-of-the-art scene rendering accuracy. Furthermore, our method exhibits\nsignificant efficiency, with an 8x acceleration in training and a 16x\nacceleration in rendering compared to Switch-NeRF. Codes will be released in\nhttps://github.com/MiZhenxing/Switch-NeRF.\n","authors":["Zhenxing Mi","Ping Yin","Xue Xiao","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2505.02005v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.02001v1","updated":"2025-05-04T06:14:10Z","published":"2025-05-04T06:14:10Z","title":"Hybrid Image Resolution Quality Metric (HIRQM):A Comprehensive\n  Perceptual Image Quality Assessment Framework","summary":"  Traditional image quality assessment metrics like Mean Squared Error and\nStructural Similarity Index often fail to reflect perceptual quality under\ncomplex distortions. We propose the Hybrid Image Resolution Quality Metric\n(HIRQM), integrating statistical, multi-scale, and deep learning-based methods\nfor a comprehensive quality evaluation. HIRQM combines three components:\nProbability Density Function for local pixel distribution analysis, Multi-scale\nFeature Similarity for structural integrity across resolutions, and\nHierarchical Deep Image Features using a pre-trained VGG16 network for semantic\nalignment with human perception. A dynamic weighting mechanism adapts component\ncontributions based on image characteristics like brightness and variance,\nenhancing flexibility across distortion types. Our contributions include a\nunified metric and dynamic weighting for better perceptual alignment. Evaluated\non TID2013 and LIVE datasets, HIRQM achieves Pearson and Spearman correlations\nof 0.92 and 0.90, outperforming traditional metrics. It excels in handling\nnoise, blur, and compression artifacts, making it valuable for image processing\napplications like compression and restoration.\n","authors":["Vineesh Kumar Reddy Mondem"],"pdf_url":"https://arxiv.org/pdf/2505.02001v1.pdf","comment":"19 pages,2 figures,2 tables and biblography with similar papers with\n  some valid information"},{"id":"http://arxiv.org/abs/2505.01996v1","updated":"2025-05-04T05:42:21Z","published":"2025-05-04T05:42:21Z","title":"Always Skip Attention","summary":"  We highlight a curious empirical result within modern Vision Transformers\n(ViTs). Specifically, self-attention catastrophically fails to train unless it\nis used in conjunction with a skip connection. This is in contrast to other\nelements of a ViT that continue to exhibit good performance (albeit suboptimal)\nwhen skip connections are removed. Further, we show that this critical\ndependence on skip connections is a relatively new phenomenon, with previous\ndeep architectures (\\eg, CNNs) exhibiting good performance in their absence. In\nthis paper, we theoretically characterize that the self-attention mechanism is\nfundamentally ill-conditioned and is, therefore, uniquely dependent on skip\nconnections for regularization. Additionally, we propose Token Graying -- a\nsimple yet effective complement (to skip connections) that further improves the\ncondition of input tokens. We validate our approach in both supervised and\nself-supervised training methods.\n","authors":["Yiping Ji","Hemanth Saratchandran","Peyman Moghaddam","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2505.01996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01986v1","updated":"2025-05-04T04:49:55Z","published":"2025-05-04T04:49:55Z","title":"Drug classification based on X-ray spectroscopy combined with machine\n  learning","summary":"  The proliferation of new types of drugs necessitates the urgent development\nof faster and more accurate detection methods. Traditional detection methods\nhave high requirements for instruments and environments, making the operation\ncomplex. X-ray absorption spectroscopy, a non-destructive detection technique,\noffers advantages such as ease of operation, penetrative observation, and\nstrong substance differentiation capabilities, making it well-suited for\napplication in the field of drug detection and identification. In this study,\nwe constructed a classification model using Convolutional Neural Networks\n(CNN), Support Vector Machines (SVM), and Particle Swarm Optimization (PSO) to\nclassify and identify drugs based on their X-ray spectral profiles. In the\nexperiments, we selected 14 chemical reagents with chemical formulas similar to\ndrugs as samples. We utilized CNN to extract features from the spectral data of\nthese 14 chemical reagents and used the extracted features to train an SVM\nmodel. We also utilized PSO to optimize two critical initial parameters of the\nSVM. The experimental results demonstrate that this model achieved higher\nclassification accuracy compared to two other common methods, with a prediction\naccuracy of 99.14%. Additionally, the model exhibited fast execution speed,\nmitigating the drawback of a drastic increase in running time and efficiency\nreduction that may result from the direct fusion of PSO and SVM. Therefore, the\ncombined approach of X-ray absorption spectroscopy with CNN, PSO, and SVM\nprovides a rapid, highly accurate, and reliable classification and\nidentification method for the field of drug detection, holding promising\nprospects for widespread application.\n","authors":["Yongming Li","Peng Wang","Bangdong Han"],"pdf_url":"https://arxiv.org/pdf/2505.01986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01984v1","updated":"2025-05-04T04:46:08Z","published":"2025-05-04T04:46:08Z","title":"Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation\n  and Past-to-Present Gradient Distillation","summary":"  Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosis\nand prognosis, as they provide tissue details at the cellular level. However,\nthe rapid growth of computational tasks involving WSIs poses significant\nchallenges. Given that WSIs are gigapixels in size, they present difficulties\nin terms of storage, processing, and model training. Therefore, it is essential\nto develop lifelong learning approaches for WSI analysis. In scenarios where\nslides are distributed across multiple institutes, we aim to leverage them to\ndevelop a unified online model as a computational tool for cancer diagnosis in\nclinical and hospital settings. In this study, we introduce ADaFGrad, a method\ndesigned to enhance lifelong learning for whole-slide image (WSI) analysis.\nFirst, we leverage pathology vision-language foundation models to develop a\nframework that enables interaction between a slide's regional tissue features\nand a predefined text-based prototype buffer. Additionally, we propose a\ngradient-distillation mechanism that mimics the gradient of a logit with\nrespect to the classification-head parameters across past and current\niterations in a continual-learning setting. We construct a sequence of six TCGA\ndatasets for training and evaluation. Experimental results show that ADaFGrad\noutperforms both state-of-the-art WSI-specific and conventional\ncontinual-learning methods after only a few training epochs, exceeding them by\nup to +5.068% in the class-incremental learning scenario while exhibiting the\nleast forgetting (i.e., retaining the most knowledge from previous tasks).\nMoreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy,\nfurther demonstrating the effectiveness of the proposed modules.\n","authors":["Doanh C. Bui","Hoai Luan Pham","Vu Trung Duong Le","Tuan Hai Vu","Van Duy Tran","Khang Nguyen","Yasuhiko Nakashima"],"pdf_url":"https://arxiv.org/pdf/2505.01984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14803v2","updated":"2025-05-04T04:28:53Z","published":"2024-12-19T12:48:40Z","title":"Video Prediction Policy: A Generalist Robot Policy with Predictive\n  Visual Representations","summary":"  Visual representations play a crucial role in developing generalist robotic\npolicies. Previous vision encoders, typically pre-trained with single-image\nreconstruction or two-image contrastive learning, tend to capture static\ninformation, often neglecting the dynamic aspects vital for embodied tasks.\nRecently, video diffusion models (VDMs) demonstrate the ability to predict\nfuture frames and showcase a strong understanding of physical world. We\nhypothesize that VDMs inherently produce visual representations that encompass\nboth current static information and predicted future dynamics, thereby\nproviding valuable guidance for robot action learning. Based on this\nhypothesis, we propose the Video Prediction Policy (VPP), which learns implicit\ninverse dynamics model conditioned on predicted future representations inside\nVDMs. To predict more precise future, we fine-tune pre-trained video foundation\nmodel on robot datasets along with internet human manipulation data. In\nexperiments, VPP achieves a 18.6\\% relative improvement on the Calvin ABC-D\ngeneralization benchmark compared to the previous state-of-the-art, and\ndemonstrates a 31.6\\% increase in success rates for complex real-world\ndexterous manipulation tasks. Project page at\nhttps://video-prediction-policy.github.io\n","authors":["Yucheng Hu","Yanjiang Guo","Pengchao Wang","Xiaoyu Chen","Yen-Jen Wang","Jianke Zhang","Koushil Sreenath","Chaochao Lu","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14803v2.pdf","comment":"ICML 2025 Spotlight Paper. The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2504.08982v2","updated":"2025-05-04T03:41:21Z","published":"2025-04-11T21:17:30Z","title":"Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot\n  Continual Learning","summary":"  Integrating new class information without losing previously acquired\nknowledge remains a central challenge in artificial intelligence, often\nreferred to as catastrophic forgetting. Few-shot class incremental learning\n(FSCIL) addresses this by first training a model on a robust dataset of base\nclasses and then incrementally adapting it in successive sessions using only a\nfew labeled examples per novel class. However, this approach is prone to\noverfitting on the limited new data, which can compromise overall performance\nand exacerbate forgetting. In this work, we propose a simple yet effective\nnovel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone\naugmented with parameter-efficient additive updates. Our approach freezes the\npre-trained ViT parameters and selectively injects trainable weights into the\nself-attention modules via an additive update mechanism. This design updates\nonly a small subset of parameters to accommodate new classes without\nsacrificing the representations learned during the base session. By fine-tuning\na limited number of parameters, our method preserves the generalizable features\nin the frozen ViT while reducing the risk of overfitting. Furthermore, as most\nparameters remain fixed, the model avoids overwriting previously learned\nknowledge when small novel data batches are introduced. Extensive experiments\non benchmark datasets demonstrate that our approach yields state-of-the-art\nperformance compared to baseline FSCIL methods.\n","authors":["Kyle Stein","Andrew Arash Mahyari","Guillermo Francia III","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2504.08982v2.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2502.20490v3","updated":"2025-05-04T23:41:06Z","published":"2025-02-27T19:54:16Z","title":"EgoNormia: Benchmarking Physical Social Norm Understanding","summary":"  Human activity is moderated by norms. However, machines are often trained\nwithout explicit supervision on norm understanding and reasoning, particularly\nwhen norms are physically- or socially-grounded. To improve and evaluate the\nnormative reasoning capability of vision-language models (VLMs), we present\n\\dataset{} $\\|\\epsilon\\|$, consisting of 1,853 challenging, multi-stage MCQ\nquestions based on ego-centric videos of human interactions, evaluating both\nthe prediction and justification of normative actions. The normative actions\nencompass seven categories: safety, privacy, proxemics, politeness,\ncooperation, coordination/proactivity, and communication/legibility. To compile\nthis dataset at scale, we propose a novel pipeline leveraging video sampling,\nautomatic answer generation, filtering, and human validation. Our work\ndemonstrates that current state-of-the-art vision-language models lack robust\nnorm understanding, scoring a maximum of 54\\% on \\dataset{} (versus a human\nbench of 92\\%). Our analysis of performance in each dimension highlights the\nsignificant risks of safety, privacy, and the lack of collaboration and\ncommunication capability when applied to real-world agents. We additionally\nshow that through a retrieval-based generation (RAG) method, it is possible to\nuse \\dataset{} to enhance normative reasoning in VLMs.\n","authors":["MohammadHossein Rezaei","Yicheng Fu","Phil Cuvin","Caleb Ziems","Yanzhe Zhang","Hao Zhu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.20490v3.pdf","comment":"V2, with updated VLM stats"},{"id":"http://arxiv.org/abs/2505.02288v1","updated":"2025-05-04T22:57:33Z","published":"2025-05-04T22:57:33Z","title":"Universal Approximation Theorem of Deep Q-Networks","summary":"  We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs)\nvia stochastic control and Forward-Backward Stochastic Differential Equations\n(FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by\na square-integrable martingale, we analyze DQN approximation properties. We\nshow that DQNs can approximate the optimal Q-function on compact sets with\narbitrary accuracy and high probability, leveraging residual network\napproximation theorems and large deviation bounds for the state-action process.\nWe then analyze the convergence of a general Q-learning algorithm for training\nDQNs in this setting, adapting stochastic approximation theorems. Our analysis\nemphasizes the interplay between DQN layer count, time discretization, and the\nrole of viscosity solutions (primarily for the value function $V^*$) in\naddressing potential non-smoothness of the optimal Q-function. This work\nbridges deep reinforcement learning and stochastic control, offering insights\ninto DQNs in continuous-time settings, relevant for applications with physical\nsystems or high-frequency data.\n","authors":["Qian Qi"],"pdf_url":"https://arxiv.org/pdf/2505.02288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01408v3","updated":"2025-05-04T22:44:25Z","published":"2023-10-02T17:59:24Z","title":"Generalized Animal Imitator: Agile Locomotion with Versatile Motion\n  Prior","summary":"  The agility of animals, particularly in complex activities such as running,\nturning, jumping, and backflipping, stands as an exemplar for robotic system\ndesign. Transferring this suite of behaviors to legged robotic systems\nintroduces essential inquiries: How can a robot learn multiple locomotion\nbehaviors simultaneously? How can the robot execute these tasks with a smooth\ntransition? How to integrate these skills for wide-range applications? This\npaper introduces the Versatile Instructable Motion prior (VIM) - a\nReinforcement Learning framework designed to incorporate a range of agile\nlocomotion tasks suitable for advanced robotic applications. Our framework\nenables legged robots to learn diverse agile low-level skills by imitating\nanimal motions and manually designed motions. Our Functionality reward guides\nthe robot's ability to adopt varied skills, and our Stylization reward ensures\nthat robot motions align with reference motions. Our evaluations of the VIM\nframework span both simulation and the real world. Our framework allows a robot\nto concurrently learn diverse agile locomotion skills using a single\nlearning-based controller in the real world. Videos can be found on our\nwebsite: https://rchalyang.github.io/VIM/\n","authors":["Ruihan Yang","Zhuoqun Chen","Jianhan Ma","Chongyi Zheng","Yiyu Chen","Quan Nguyen","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.01408v3.pdf","comment":"Further details and supportive media can be found at our project\n  site: https://rchalyang.github.io/VIM"},{"id":"http://arxiv.org/abs/2505.02281v1","updated":"2025-05-04T22:43:57Z","published":"2025-05-04T22:43:57Z","title":"Minimisation of Quasar-Convex Functions Using Random Zeroth-Order\n  Oracles","summary":"  This study explores the performance of a random Gaussian smoothing\nzeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly\nquasar-convex (SQC) functions in both unconstrained and constrained settings.\nFor the unconstrained problem, we establish the ZO algorithm's convergence to a\nglobal minimum along with its complexity when applied to both QC and SQC\nfunctions. For the constrained problem, we introduce the new notion of\nproximal-quasar-convexity and prove analogous results to the unconstrained\ncase. Specifically, we show the complexity bounds and the convergence of the\nalgorithm to a neighbourhood of a global minimum whose size can be controlled\nunder a variance reduction scheme. Theoretical findings are illustrated through\ninvestigating the performance of the algorithm applied to a range of problems\nin machine learning and optimisation. Specifically, we observe scenarios where\nthe ZO method outperforms gradient descent. We provide a possible explanation\nfor this phenomenon.\n","authors":["Amir Ali Farzin","Yuen-Man Pun","Iman Shames"],"pdf_url":"https://arxiv.org/pdf/2505.02281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17424v5","updated":"2025-05-04T22:39:38Z","published":"2025-02-24T18:56:03Z","title":"Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs","summary":"  We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding. It asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned. Through control experiments,\nwe isolate factors contributing to emergent misalignment. Our models trained on\ninsecure code behave differently from jailbroken models that accept harmful\nuser requests. Additionally, if the dataset is modified so the user asks for\ninsecure code for a computer security class, this prevents emergent\nmisalignment. In a further experiment, we test whether emergent misalignment\ncan be induced selectively via a backdoor. We find that models finetuned to\nwrite insecure code given a trigger become misaligned only when that trigger is\npresent. So the misalignment is hidden without knowledge of the trigger. It's\nimportant to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork.\n","authors":["Jan Betley","Daniel Tan","Niels Warncke","Anna Sztyber-Betley","Xuchan Bao","Martín Soto","Nathan Labenz","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2502.17424v5.pdf","comment":"40 pages, 38 figures An earlier revision of this paper was submitted\n  to ICML. Since then, it has been updated to include new results on training\n  dynamics (4.7) and base models (4.8)"},{"id":"http://arxiv.org/abs/2504.20898v2","updated":"2025-05-04T22:38:21Z","published":"2025-04-29T16:14:55Z","title":"CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report\n  Generation with Multi-Agent RAG and Concept Bottleneck Models","summary":"  Advancements in generative Artificial Intelligence (AI) hold great promise\nfor automating radiology workflows, yet challenges in interpretability and\nreliability hinder clinical adoption. This paper presents an automated\nradiology report generation framework that combines Concept Bottleneck Models\n(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge\nAI performance with clinical explainability. CBMs map chest X-ray features to\nhuman-understandable clinical concepts, enabling transparent disease\nclassification. Meanwhile, the RAG system integrates multi-agent collaboration\nand external knowledge to produce contextually rich, evidence-based reports.\nOur demonstration showcases the system's ability to deliver interpretable\npredictions, mitigate hallucinations, and generate high-quality, tailored\nreports with an interactive interface addressing accuracy, trust, and usability\nchallenges. This framework provides a pathway to improving diagnostic\nconsistency and empowering radiologists with actionable insights.\n","authors":["Hasan Md Tusfiqur Alam","Devansh Srivastav","Abdulrahman Mohamed Selim","Md Abdul Kadir","Md Moktadirul Hoque Shuvo","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2504.20898v2.pdf","comment":"Accepted in the 17th ACM SIGCHI Symposium on Engineering Interactive\n  Computing Systems (EICS 2025)"},{"id":"http://arxiv.org/abs/2310.01770v4","updated":"2025-05-04T22:24:01Z","published":"2023-10-03T03:36:29Z","title":"A simple connection from loss flatness to compressed neural\n  representations","summary":"  Sharpness, a geometric measure in the parameter space that reflects the\nflatness of the loss landscape, has long been studied for its potential\nconnections to neural network behavior. While sharpness is often associated\nwith generalization, recent work highlights inconsistencies in this\nrelationship, leaving its true significance unclear. In this paper, we\ninvestigate how sharpness influences the local geometric features of neural\nrepresentations in feature space, offering a new perspective on its role. We\nintroduce this problem and study three measures for compression: the Local\nVolumetric Ratio (LVR), based on volume compression, the Maximum Local\nSensitivity (MLS), based on sensitivity to input changes, and the Local\nDimensionality, based on how uniform the sensitivity is on different\ndirections. We show that LVR and MLS correlate with the flatness of the loss\naround the local minima; and that this correlation is predicted by a relatively\nsimple mathematical relationship: a flatter loss corresponds to a lower upper\nbound on the compression metrics of neural representations. Our work builds\nupon the linear stability insight by Ma and Ying, deriving inequalities between\nvarious compression metrics and quantities involving sharpness. Our\ninequalities readily extend to reparametrization-invariant sharpness as well.\nThrough empirical experiments on various feedforward, convolutional, and\ntransformer architectures, we find that our inequalities predict a consistently\npositive correlation between local representation compression and sharpness.\n","authors":["Shirui Chen","Stefano Recanatesi","Eric Shea-Brown"],"pdf_url":"https://arxiv.org/pdf/2310.01770v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02279v1","updated":"2025-05-04T22:18:27Z","published":"2025-05-04T22:18:27Z","title":"A survey of agent interoperability protocols: Model Context Protocol\n  (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and\n  Agent Network Protocol (ANP)","summary":"  Large language model (LLM)-powered autonomous agents demand robust,\nstandardized protocols to integrate tools, share contextual data, and\ncoordinate tasks across heterogeneous systems. Ad-hoc integrations are\ndifficult to scale, secure, and generalize across domains. This survey examines\nfour emerging agent communication protocols: Model Context Protocol (MCP),\nAgent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent\nNetwork Protocol (ANP), each addressing interoperability in distinct deployment\ncontexts. MCP provides a JSON-RPC client-server interface for secure tool\ninvocation and typed data exchange. ACP introduces REST-native messaging via\nmulti-part messages and asynchronous streaming to support multimodal agent\nresponses. A2A enables peer-to-peer task outsourcing through capability-based\nAgent Cards, facilitating enterprise-scale workflows. ANP supports open-network\nagent discovery and secure collaboration using decentralized identifiers (DIDs)\nand JSON-LD graphs. The protocols are compared across multiple dimensions,\nincluding interaction modes, discovery mechanisms, communication patterns, and\nsecurity models. Based on the comparative analysis, a phased adoption roadmap\nis proposed: beginning with MCP for tool access, followed by ACP for multimodal\nmessaging, A2A for collaborative task execution, and extending to ANP for\ndecentralized agent marketplaces. This work provides a comprehensive foundation\nfor designing secure, interoperable, and scalable ecosystems of LLM-powered\nagents.\n","authors":["Abul Ehtesham","Aditi Singh","Gaurav Kumar Gupta","Saket Kumar"],"pdf_url":"https://arxiv.org/pdf/2505.02279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02274v1","updated":"2025-05-04T22:06:23Z","published":"2025-05-04T22:06:23Z","title":"On the Need for a Statistical Foundation in Scenario-Based Testing of\n  Autonomous Vehicles","summary":"  Scenario-based testing has emerged as a common method for autonomous vehicles\n(AVs) safety, offering a more efficient alternative to mile-based testing by\nfocusing on high-risk scenarios. However, fundamental questions persist\nregarding its stopping rules, residual risk estimation, debug effectiveness,\nand the impact of simulation fidelity on safety claims. This paper argues that\na rigorous statistical foundation is essential to address these challenges and\nenable rigorous safety assurance. By drawing parallels between AV testing and\ntraditional software testing methodologies, we identify shared research gaps\nand reusable solutions. We propose proof-of-concept models to quantify the\nprobability of failure per scenario (pfs) and evaluate testing effectiveness\nunder varying conditions. Our analysis reveals that neither scenario-based nor\nmile-based testing universally outperforms the other. Furthermore, we introduce\nRisk Estimation Fidelity (REF), a novel metric to certify the alignment of\nsynthetic and real-world testing outcomes, ensuring simulation-based safety\nclaims are statistically defensible.\n","authors":["Xingyu Zhao","Robab Aghazadeh-Chakherlou","Chih-Hong Cheng","Peter Popov","Lorenzo Strigini"],"pdf_url":"https://arxiv.org/pdf/2505.02274v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2505.02272v1","updated":"2025-05-04T21:58:11Z","published":"2025-05-04T21:58:11Z","title":"Robust Localization, Mapping, and Navigation for Quadruped Robots","summary":"  Quadruped robots are currently a widespread platform for robotics research,\nthanks to powerful Reinforcement Learning controllers and the availability of\ncheap and robust commercial platforms. However, to broaden the adoption of the\ntechnology in the real world, we require robust navigation stacks relying only\non low-cost sensors such as depth cameras. This paper presents a first step\ntowards a robust localization, mapping, and navigation system for low-cost\nquadruped robots. In pursuit of this objective we combine contact-aided\nkinematic, visual-inertial odometry, and depth-stabilized vision, enhancing\nstability and accuracy of the system. Our results in simulation and two\ndifferent real-world quadruped platforms show that our system can generate an\naccurate 2D map of the environment, robustly localize itself, and navigate\nautonomously. Furthermore, we present in-depth ablation studies of the\nimportant components of the system and their impact on localization accuracy.\nVideos, code, and additional experiments can be found on the project website:\nhttps://sites.google.com/view/low-cost-quadruped-slam\n","authors":["Dyuman Aditya","Junning Huang","Nico Bohlinger","Piotr Kicki","Krzysztof Walas","Jan Peters","Matteo Luperto","Davide Tateo"],"pdf_url":"https://arxiv.org/pdf/2505.02272v1.pdf","comment":"8 Pages"},{"id":"http://arxiv.org/abs/2505.02271v1","updated":"2025-05-04T21:57:58Z","published":"2025-05-04T21:57:58Z","title":"Real-time Spatial Retrieval Augmented Generation for Urban Environments","summary":"  The proliferation of Generative Artificial Ingelligence (AI), especially\nLarge Language Models, presents transformative opportunities for urban\napplications through Urban Foundation Models. However, base models face\nlimitations, as they only contain the knowledge available at the time of\ntraining, and updating them is both time-consuming and costly. Retrieval\nAugmented Generation (RAG) has emerged in the literature as the preferred\napproach for injecting contextual information into Foundation Models. It\nprevails over techniques such as fine-tuning, which are less effective in\ndynamic, real-time scenarios like those found in urban environments. However,\ntraditional RAG architectures, based on semantic databases, knowledge graphs,\nstructured data, or AI-powered web searches, do not fully meet the demands of\nurban contexts. Urban environments are complex systems characterized by large\nvolumes of interconnected data, frequent updates, real-time processing\nrequirements, security needs, and strong links to the physical world. This work\nproposes a real-time spatial RAG architecture that defines the necessary\ncomponents for the effective integration of generative AI into cities,\nleveraging temporal and spatial filtering capabilities through linked data. The\nproposed architecture is implemented using FIWARE, an ecosystem of software\ncomponents to develop smart city solutions and digital twins. The design and\nimplementation are demonstrated through the use case of a tourism assistant in\nthe city of Madrid. The use case serves to validate the correct integration of\nFoundation Models through the proposed RAG architecture.\n","authors":["David Nazareno Campo","Javier Conde","Álvaro Alonso","Gabriel Huecas","Joaquín Salvachúa","Pedro Reviriego"],"pdf_url":"https://arxiv.org/pdf/2505.02271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02266v1","updated":"2025-05-04T21:47:18Z","published":"2025-05-04T21:47:18Z","title":"Parameter-Efficient Transformer Embeddings","summary":"  Embedding layers in transformer-based NLP models typically account for the\nlargest share of model parameters, scaling with vocabulary size but not\nyielding performance gains proportional to scale. We propose an alternative\napproach in which token embedding vectors are first generated\ndeterministically, directly from the token IDs using a Fourier expansion of\ntheir normalized values, followed by a lightweight multilayer perceptron (MLP)\nthat captures higher-order interactions. We train standard transformers and our\narchitecture on natural language inference tasks (SNLI and MNLI), and evaluate\nzero-shot performance on sentence textual similarity (STS-B). Our results\ndemonstrate that the proposed method achieves competitive performance using\nsignificantly fewer parameters, trains faster, and operates effectively without\nthe need for dropout. This proof-of-concept study highlights the potential for\nscalable, memory-efficient language models and motivates further large-scale\nexperimentation based on our findings.\n","authors":["Henry Ndubuaku","Mouad Talhi"],"pdf_url":"https://arxiv.org/pdf/2505.02266v1.pdf","comment":"7 pages, 2 tables. Code available at https://github.com/HMUNACHI/pete"},{"id":"http://arxiv.org/abs/2505.02255v1","updated":"2025-05-04T21:28:21Z","published":"2025-05-04T21:28:21Z","title":"Enhancing AI Face Realism: Cost-Efficient Quality Improvement in\n  Distilled Diffusion Models with a Fully Synthetic Dataset","summary":"  This study presents a novel approach to enhance the cost-to-quality ratio of\nimage generation with diffusion models. We hypothesize that differences between\ndistilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are\nconsistent and, therefore, learnable within a specialized domain, like portrait\ngeneration. We generate a synthetic paired dataset and train a fast\nimage-to-image translation head. Using two sets of low- and high-quality\nsynthetic images, our model is trained to refine the output of a distilled\ngenerator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like\nFLUX.1-dev, which is more computationally intensive. Our results show that the\npipeline, which combines a distilled version of a large generative model with\nour enhancement layer, delivers similar photorealistic portraits to the\nbaseline version with up to an 82% decrease in computational cost compared to\nFLUX.1-dev. This study demonstrates the potential for improving the efficiency\nof AI solutions involving large-scale image generation.\n","authors":["Jakub Wąsala","Bartłomiej Wrzalski","Kornelia Noculak","Yuliia Tarasenko","Oliwer Krupa","Jan Kocoń","Grzegorz Chodak"],"pdf_url":"https://arxiv.org/pdf/2505.02255v1.pdf","comment":"25th International Conference on Computational Science"},{"id":"http://arxiv.org/abs/2505.02247v1","updated":"2025-05-04T21:01:45Z","published":"2025-05-04T21:01:45Z","title":"RISE: Radius of Influence based Subgraph Extraction for 3D Molecular\n  Graph Explanation","summary":"  3D Geometric Graph Neural Networks (GNNs) have emerged as transformative\ntools for modeling molecular data. Despite their predictive power, these models\noften suffer from limited interpretability, raising concerns for scientific\napplications that require reliable and transparent insights. While existing\nmethods have primarily focused on explaining molecular substructures in 2D\nGNNs, the transition to 3D GNNs introduces unique challenges, such as handling\nthe implicit dense edge structures created by a cut-off radius. To tackle this,\nwe introduce a novel explanation method specifically designed for 3D GNNs,\nwhich localizes the explanation to the immediate neighborhood of each node\nwithin the 3D space. Each node is assigned an radius of influence, defining the\nlocalized region within which message passing captures spatial and structural\ninteractions crucial for the model's predictions. This method leverages the\nspatial and geometric characteristics inherent in 3D graphs. By constraining\nthe subgraph to a localized radius of influence, the approach not only enhances\ninterpretability but also aligns with the physical and structural dependencies\ntypical of 3D graph applications, such as molecular learning.\n","authors":["Jingxiang Qu","Wenhan Gao","Jiaxing Zhang","Xufeng Liu","Hua Wei","Haibin Ling","Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2505.02247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00592v2","updated":"2025-05-04T20:49:56Z","published":"2024-11-30T21:39:51Z","title":"LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in\n  Real-World Scenes","summary":"  We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data\nfor autonomous driving. Our framework edits real-world LiDAR scans by\nintroducing new object layouts while preserving the realism of the background\nenvironment. Compared to end-to-end frameworks that generate LiDAR point clouds\nfrom scratch, LiDAR-EDIT offers users full control over the object layout,\nincluding the number, type, and pose of objects, while keeping most of the\noriginal real-world background. Our method also provides object labels for the\ngenerated data. Compared to novel view synthesis techniques, our framework\nallows for the creation of counterfactual scenarios with object layouts\nsignificantly different from the original real-world scene. LiDAR-EDIT uses\nspherical voxelization to enforce correct LiDAR projective geometry in the\ngenerated point clouds by construction. During object removal and insertion,\ngenerative models are employed to fill the unseen background and object parts\nthat were occluded in the original real LiDAR scans. Experimental results\ndemonstrate that our framework produces realistic LiDAR scans with practical\nvalue for downstream tasks.\n","authors":["Shing-Hei Ho","Bao Thach","Minghan Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.00592v2.pdf","comment":"Submitted to IEEE International Conference on Robotics and Automation\n  (ICRA). 6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2402.16631v3","updated":"2025-05-04T20:44:38Z","published":"2024-02-26T15:03:46Z","title":"GenAINet: Enabling Wireless Collective Intelligence via Knowledge\n  Transfer and Reasoning","summary":"  Generative Artificial Intelligence (GenAI) and communication networks are\nexpected to have groundbreaking synergies for 6G. Connecting GenAI agents via a\nwireless network can potentially unleash the power of Collective Intelligence\n(CI) and pave the way for Artificial General Intelligence (AGI). However,\ncurrent wireless networks are designed as a \"data pipe\" and are not suited to\naccommodate and leverage the power of GenAI. In this paper, we propose the\nGenAINet framework in which distributed GenAI agents communicate knowledge\n(facts, experiences, and methods) to accomplish arbitrary tasks. We first\npropose an architecture for a single GenAI agent and then provide a network\narchitecture integrating GenAI capabilities to manage both network protocols\nand applications. Building on this, we investigate effective communication and\nreasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI\nagents extract semantics from heterogeneous raw data, build and maintain a\nknowledge model representing the semantic relationships among pieces of\nknowledge, which is retrieved by GenAI models for planning and reasoning. Under\nthis paradigm, different levels of collaboration can be achieved flexibly\ndepending on the complexity of targeted tasks. Furthermore, we conduct two case\nstudies in which, through wireless device queries, we demonstrate that\nextracting, compressing and transferring common knowledge can improve query\naccuracy while reducing communication costs; and in the wireless power control\nproblem, we show that distributed agents can complete general tasks\nindependently through collaborative reasoning without predefined communication\nprotocols. Finally, we discuss challenges and future research directions in\napplying Large Language Models (LLMs) in 6G networks.\n","authors":["Hang Zou","Qiyang Zhao","Samson Lasaulce","Lina Bariah","Mehdi Bennis","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2402.16631v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02236v1","updated":"2025-05-04T20:24:57Z","published":"2025-05-04T20:24:57Z","title":"Improving Physical Object State Representation in Text-to-Image\n  Generative Systems","summary":"  Current text-to-image generative models struggle to accurately represent\nobject states (e.g., \"a table without a bottle,\" \"an empty tumbler\"). In this\nwork, we first design a fully-automatic pipeline to generate high-quality\nsynthetic data that accurately captures objects in varied states. Next, we\nfine-tune several open-source text-to-image models on this synthetic data. We\nevaluate the performance of the fine-tuned models by quantifying the alignment\nof the generated images to their prompts using GPT4o-mini, and achieve an\naverage absolute improvement of 8+% across four models on the public\nGenAI-Bench dataset. We also curate a collection of 200 prompts with a specific\nfocus on common objects in various physical states. We demonstrate a\nsignificant improvement of an average of 24+% over the baseline on this\ndataset. We release all evaluation prompts and code.\n","authors":["Tianle Chen","Chaitanya Chakka","Deepti Ghadiyaram"],"pdf_url":"https://arxiv.org/pdf/2505.02236v1.pdf","comment":"Submitted to Synthetic Data for Computer Vision - CVPR 2025 Workshop"},{"id":"http://arxiv.org/abs/2505.02235v1","updated":"2025-05-04T20:16:08Z","published":"2025-05-04T20:16:08Z","title":"SEval-Ex: A Statement-Level Framework for Explainable Summarization\n  Evaluation","summary":"  Evaluating text summarization quality remains a critical challenge in Natural\nLanguage Processing. Current approaches face a trade-off between performance\nand interpretability. We present SEval-Ex, a framework that bridges this gap by\ndecomposing summarization evaluation into atomic statements, enabling both high\nperformance and explainability. SEval-Ex employs a two-stage pipeline: first\nextracting atomic statements from text source and summary using LLM, then a\nmatching between generated statements. Unlike existing approaches that provide\nonly summary-level scores, our method generates detailed evidence for its\ndecisions through statement-level alignments. Experiments on the SummEval\nbenchmark demonstrate that SEval-Ex achieves state-of-the-art performance with\n0.580 correlation on consistency with human consistency judgments, surpassing\nGPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our\nframework shows robustness against hallucination.\n","authors":["Tanguy Herserant","Vincent Guigue"],"pdf_url":"https://arxiv.org/pdf/2505.02235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10276v4","updated":"2025-05-04T20:14:21Z","published":"2024-08-17T15:42:29Z","title":"FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation\n  Models","summary":"  Foundation models have demonstrated remarkable capabilities in handling\ndiverse modalities and tasks, outperforming conventional artificial\nintelligence (AI) approaches that are highly task-specific and\nmodality-reliant. In the medical domain, however, the development of\ncomprehensive foundation models is constrained by limited access to diverse\nmodalities and stringent privacy regulations. To address these constraints,\nthis study introduces a novel knowledge injection approach, FedKIM, designed to\nscale the medical foundation model within a federated learning framework.\nFedKIM leverages lightweight local models to extract healthcare knowledge from\nprivate data and integrates this knowledge into a centralized foundation model\nusing a designed adaptive Multitask Multimodal Mixture Of Experts (M3OE)\nmodule. This method not only preserves privacy but also enhances the model's\nability to handle complex medical tasks involving multiple modalities. Our\nextensive experiments across twelve tasks in seven modalities demonstrate the\neffectiveness of FedKIM in various settings, highlighting its potential to\nscale medical foundation models without direct access to sensitive data.\n","authors":["Xiaochen Wang","Jiaqi Wang","Houping Xiao","Jinghui Chen","Fenglong Ma"],"pdf_url":"https://arxiv.org/pdf/2408.10276v4.pdf","comment":"Accepted by EMNLP'24 Main"},{"id":"http://arxiv.org/abs/2505.02232v1","updated":"2025-05-04T19:51:09Z","published":"2025-05-04T19:51:09Z","title":"Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher\n  Learning","summary":"  Building models responsive to input prompts represents a transformative shift\nin machine learning. This paradigm holds significant potential for robotics\nproblems, such as targeted manipulation amidst clutter. In this work, we\npresent a novel approach to combine promptable foundation models with\nreinforcement learning (RL), enabling robots to perform dexterous manipulation\ntasks in a prompt-responsive manner. Existing methods struggle to link\nhigh-level commands with fine-grained dexterous control. We address this gap\nwith a memory-augmented student-teacher learning framework. We use the\nSegment-Anything 2 (SAM 2) model as a perception backbone to infer an object of\ninterest from user prompts. While detections are imperfect, their temporal\nsequence provides rich information for implicit state estimation by\nmemory-augmented models. Our approach successfully learns prompt-responsive\npolicies, demonstrated in picking objects from cluttered scenes. Videos and\ncode are available at https://memory-student-teacher.github.io\n","authors":["Malte Mosbach","Sven Behnke"],"pdf_url":"https://arxiv.org/pdf/2505.02232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02230v1","updated":"2025-05-04T19:37:13Z","published":"2025-05-04T19:37:13Z","title":"The GenAI Generation: Student Views of Awareness, Preparedness, and\n  Concern","summary":"  Generative AI (GenAI) is revolutionizing education and workforce development,\nprofoundly shaping how students learn, engage, and prepare for their future.\nOutpacing the development of uniform policies and structures, GenAI has\nheralded a unique era and given rise to the GenAI Generation: a cohort of\nstudents whose education has been increasingly shaped by the opportunities and\nchallenges GenAI presents during its widespread adoption within society. This\nstudy examines our students' perceptions of GenAI through a concise survey with\noptional open-ended questions, focusing on their awareness, preparedness, and\nconcerns. Evaluation of more than 250 responses with more than 40% providing\ndetailed qualitative feedback reveals a core dual sentiment: while most\nstudents express enthusiasm for GenAI, an even greater proportion voice a\nspectrum of concerns about ethics, job displacement, and the adequacy of\neducational structures given the highly transformative technology. These\nfindings offer critical insights into how students view the potential and\npitfalls of GenAI for future career impacts, with accompanying recommendations\nto guide educational institutions in navigating a future driven by GenAI.\n","authors":["Micaela Siraj","Jon Duke"],"pdf_url":"https://arxiv.org/pdf/2505.02230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02228v1","updated":"2025-05-04T19:32:48Z","published":"2025-05-04T19:32:48Z","title":"Coupled Distributional Random Expert Distillation for World Model Online\n  Imitation Learning","summary":"  Imitation Learning (IL) has achieved remarkable success across various\ndomains, including robotics, autonomous driving, and healthcare, by enabling\nagents to learn complex behaviors from expert demonstrations. However, existing\nIL methods often face instability challenges, particularly when relying on\nadversarial reward or value formulations in world model frameworks. In this\nwork, we propose a novel approach to online imitation learning that addresses\nthese limitations through a reward model based on random network distillation\n(RND) for density estimation. Our reward model is built on the joint estimation\nof expert and behavioral distributions within the latent space of the world\nmodel. We evaluate our method across diverse benchmarks, including DMControl,\nMeta-World, and ManiSkill2, showcasing its ability to deliver stable\nperformance and achieve expert-level results in both locomotion and\nmanipulation tasks. Our approach demonstrates improved stability over\nadversarial methods while maintaining expert-level performance.\n","authors":["Shangzhe Li","Zhiao Huang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2505.02228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02216v1","updated":"2025-05-04T18:59:07Z","published":"2025-05-04T18:59:07Z","title":"LLM-Guided Probabilistic Program Induction for POMDP Model Estimation","summary":"  Partially Observable Markov Decision Processes (POMDPs) model decision making\nunder uncertainty. While there are many approaches to approximately solving\nPOMDPs, we aim to address the problem of learning such models. In particular,\nwe are interested in a subclass of POMDPs wherein the components of the model,\nincluding the observation function, reward function, transition function, and\ninitial state distribution function, can be modeled as low-complexity\nprobabilistic graphical models in the form of a short probabilistic program.\nOur strategy to learn these programs uses an LLM as a prior, generating\ncandidate probabilistic programs that are then tested against the empirical\ndistribution and adjusted through feedback. We experiment on a number of\nclassical toy POMDP problems, simulated MiniGrid domains, and two real\nmobile-base robotics search domains involving partial observability. Our\nresults show that using an LLM to guide in the construction of a low-complexity\nPOMDP model can be more effective than tabular POMDP learning, behavior\ncloning, or direct LLM planning.\n","authors":["Aidan Curtis","Hao Tang","Thiago Veloso","Kevin Ellis","Tomás Lozano-Pérez","Leslie Pack Kaelbling"],"pdf_url":"https://arxiv.org/pdf/2505.02216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02215v1","updated":"2025-05-04T18:57:57Z","published":"2025-05-04T18:57:57Z","title":"Interpretable Emergent Language Using Inter-Agent Transformers","summary":"  This paper explores the emergence of language in multi-agent reinforcement\nlearning (MARL) using transformers. Existing methods such as RIAL, DIAL, and\nCommNet enable agent communication but lack interpretability. We propose\nDifferentiable Inter-Agent Transformers (DIAT), which leverage self-attention\nto learn symbolic, human-understandable communication protocols. Through\nexperiments, DIAT demonstrates the ability to encode observations into\ninterpretable vocabularies and meaningful embeddings, effectively solving\ncooperative tasks. These results highlight the potential of DIAT for\ninterpretable communication in complex multi-agent environments.\n","authors":["Mannan Bhardwaj"],"pdf_url":"https://arxiv.org/pdf/2505.02215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02206v1","updated":"2025-05-04T18:02:28Z","published":"2025-05-04T18:02:28Z","title":"DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities\n  of Coding Units","summary":"  Genome modeling conventionally treats gene sequence as a language, reflecting\nits structured motifs and long-range dependencies analogous to linguistic units\nand organization principles such as words and syntax. Recent studies utilize\nadvanced neural networks, ranging from convolutional and recurrent models to\nTransformer-based models, to capture contextual information of gene sequence,\nwith the primary goal of obtaining effective gene sequence representations and\nthus enhance the models' understanding of various running gene samples.\nHowever, these approaches often directly apply language modeling techniques to\ngene sequences and do not fully consider the intrinsic information organization\nin them, where they do not consider how units at different granularities\ncontribute to representation. In this paper, we propose DNAZEN, an enhanced\ngenomic representation framework designed to learn from various granularities\nin gene sequences, including small polymers and G-grams that are combinations\nof several contiguous polymers. Specifically, we extract the G-grams from\nlarge-scale genomic corpora through an unsupervised approach to construct the\nG-gram vocabulary, which is used to provide G-grams in the learning process of\nDNA sequences through dynamically matching from running gene samples. A\nTransformer-based G-gram encoder is also proposed and the matched G-grams are\nfed into it to compute their representations and integrated into the encoder\nfor basic unit (E4BU), which is responsible for encoding small units and\nmaintaining the learning and inference process. To further enhance the learning\nprocess, we propose whole G-gram masking to train DNAZEN, where the model\nlargely favors the selection of each entire G-gram to mask rather than an\nordinary masking mechanism performed on basic units. Experiments on benchmark\ndatasets demonstrate the effectiveness of DNAZEN on various downstream tasks.\n","authors":["Lei Mao","Yuanhe Tian","Yan Song"],"pdf_url":"https://arxiv.org/pdf/2505.02206v1.pdf","comment":"19 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.02198v1","updated":"2025-05-04T17:36:11Z","published":"2025-05-04T17:36:11Z","title":"Student Perspectives on the Benefits and Risks of AI in Education","summary":"  The use of chatbots equipped with artificial intelligence (AI) in educational\nsettings has increased in recent years, showing potential to support teaching\nand learning. However, the adoption of these technologies has raised concerns\nabout their impact on academic integrity, students' ability to problem-solve\nindependently, and potential underlying biases. To better understand students'\nperspectives and experiences with these tools, a survey was conducted at a\nlarge public university in the United States. Through thematic analysis, 262\nundergraduate students' responses regarding their perceived benefits and risks\nof AI chatbots in education were identified and categorized into themes.\n  The results discuss several benefits identified by the students, with\nfeedback and study support, instruction capabilities, and access to information\nbeing the most cited. Their primary concerns included risks to academic\nintegrity, accuracy of information, loss of critical thinking skills, the\npotential development of overreliance, and ethical considerations such as data\nprivacy, system bias, environmental impact, and preservation of human elements\nin education.\n  While student perceptions align with previously discussed benefits and risks\nof AI in education, they show heightened concerns about distinguishing between\nhuman and AI generated work - particularly in cases where authentic work is\nflagged as AI-generated. To address students' concerns, institutions can\nestablish clear policies regarding AI use and develop curriculum around AI\nliteracy. With these in place, practitioners can effectively develop and\nimplement educational systems that leverage AI's potential in areas such as\nimmediate feedback and personalized learning support. This approach can enhance\nthe quality of students' educational experiences while preserving the integrity\nof the learning process with AI.\n","authors":["Griffin Pitts","Viktoria Marcus","Sanaz Motamedi"],"pdf_url":"https://arxiv.org/pdf/2505.02198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01638v2","updated":"2025-05-04T17:21:20Z","published":"2024-06-30T19:36:04Z","title":"LASSI: An LLM-based Automated Self-Correcting Pipeline for Translating\n  Parallel Scientific Codes","summary":"  This paper addresses the problem of providing a novel approach to sourcing\nsignificant training data for LLMs focused on science and engineering. In\nparticular, a crucial challenge is sourcing parallel scientific codes in the\nranges of millions to billions of codes. To tackle this problem, we propose an\nautomated pipeline framework called LASSI, designed to translate between\nparallel programming languages by bootstrapping existing closed- or open-source\nLLMs. LASSI incorporates autonomous enhancement through self-correcting loops\nwhere errors encountered during the compilation and execution of generated code\nare fed back to the LLM through guided prompting for debugging and refactoring.\nWe highlight the bi-directional translation of existing GPU benchmarks between\nOpenMP target offload and CUDA to validate LASSI. The results of evaluating\nLASSI with different application codes across four LLMs demonstrate the\neffectiveness of LASSI for generating executable parallel codes, with 80% of\nOpenMP to CUDA translations and 85% of CUDA to OpenMP translations producing\nthe expected output. We also observe approximately 78% of OpenMP to CUDA\ntranslations and 62% of CUDA to OpenMP translations execute within 10% of or at\na faster runtime than the original benchmark code in the same language.\n","authors":["Matthew T. Dearing","Yiheng Tao","Xingfu Wu","Zhiling Lan","Valerie Taylor"],"pdf_url":"https://arxiv.org/pdf/2407.01638v2.pdf","comment":"8 pages, 1 figure, 7 tables"},{"id":"http://arxiv.org/abs/2505.02192v1","updated":"2025-05-04T17:19:20Z","published":"2025-05-04T17:19:20Z","title":"DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in\n  Video Customization","summary":"  Customized text-to-video generation with pre-trained large-scale models has\nrecently garnered significant attention through focusing on identity and motion\nconsistency. Existing works typically follow the isolated customized paradigm,\nwhere the subject identity or motion dynamics are customized exclusively.\nHowever, this paradigm completely ignores the intrinsic mutual constraints and\nsynergistic interdependencies between identity and motion, resulting in\nidentity-motion conflicts throughout the generation process that systematically\ndegrades. To address this, we introduce DualReal, a novel framework that,\nemploys adaptive joint training to collaboratively construct interdependencies\nbetween dimensions. Specifically, DualReal is composed of two units: (1)\nDual-aware Adaptation dynamically selects a training phase (i.e., identity or\nmotion), learns the current information guided by the frozen dimension prior,\nand employs a regularization strategy to avoid knowledge leakage; (2)\nStageBlender Controller leverages the denoising stages and Diffusion\nTransformer depths to guide different dimensions with adaptive granularity,\navoiding conflicts at various stages and ultimately achieving lossless fusion\nof identity and motion patterns. We constructed a more comprehensive benchmark\nthan existing methods. The experimental results show that DualReal improves\nCLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top\nperformance on nearly all motion quality metrics.\n","authors":["Wenchuan Wang","Mengqi Huang","Yijing Tu","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2505.02192v1.pdf","comment":null}]},"2025-05-03T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2505.01947v1","updated":"2025-05-03T23:48:50Z","published":"2025-05-03T23:48:50Z","title":"Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and\n  Unsupervised-Learning Approach","summary":"  UAVs, commonly referred to as drones, have witnessed a remarkable surge in\npopularity due to their versatile applications. These cyber-physical systems\ndepend on multiple sensor inputs, such as cameras, GPS receivers,\naccelerometers, and gyroscopes, with faults potentially leading to physical\ninstability and serious safety concerns. To mitigate such risks, anomaly\ndetection has emerged as a crucial safeguarding mechanism, capable of\nidentifying the physical manifestations of emerging issues and allowing\noperators to take preemptive action at runtime. Recent anomaly detection\nmethods based on LSTM neural networks have shown promising results, but three\nchallenges persist: the need for models that can generalise across the diverse\nmission profiles of drones; the need for interpretability, enabling operators\nto understand the nature of detected problems; and the need for capturing\ndomain knowledge that is difficult to infer solely from log data. Motivated by\nthese challenges, this paper introduces RADD, an integrated approach to anomaly\ndetection in drones that combines rule mining and unsupervised learning. In\nparticular, we leverage rules (or invariants) to capture expected relationships\nbetween sensors and actuators during missions, and utilise unsupervised\nlearning techniques to cover more subtle relationships that the rules may have\nmissed. We implement this approach using the ArduPilot drone software in the\nGazebo simulator, utilising 44 rules derived across the main phases of drone\nmissions, in conjunction with an ensemble of five unsupervised learning models.\nWe find that our integrated approach successfully detects 93.84% of anomalies\nover six types of faults with a low false positive rate (2.33%), and can be\ndeployed effectively at runtime. Furthermore, RADD outperforms a\nstate-of-the-art LSTM-based method in detecting the different types of faults\nevaluated in our study.\n","authors":["Ivan Tan","Wei Minn","Christopher M. Poskitt","Lwin Khin Shar","Lingxiao Jiang"],"pdf_url":"https://arxiv.org/pdf/2505.01947v1.pdf","comment":"Accepted by the 29th International Conference on Engineering of\n  Complex Computer Systems (ICECCS 2025)"},{"id":"http://arxiv.org/abs/2505.01945v1","updated":"2025-05-03T23:28:46Z","published":"2025-05-03T23:28:46Z","title":"Act Natural! Extending Naturalistic Projection to Multimodal Behavior\n  Scenarios","summary":"  Autonomous agents operating in public spaces must consider how their\nbehaviors might affect the humans around them, even when not directly\ninteracting with them. To this end, it is often beneficial to be predictable\nand appear naturalistic. Existing methods for this purpose use human actor\nintent modeling or imitation learning techniques, but these approaches rarely\ncapture all possible motivations for human behavior and/or require significant\namounts of data. Our work extends a technique for modeling unimodal\nnaturalistic behaviors with an explicit convex set representation, to account\nfor multimodal behavior by using multiple convex sets. This more flexible\nrepresentation provides a higher degree of fidelity in data-driven modeling of\nnaturalistic behavior that arises in real-world scenarios in which human\nbehavior is, in some sense, discrete, e.g. whether or not to yield at a\nroundabout. Equipped with this new set representation, we develop an\noptimization-based filter to project arbitrary trajectories into the set so\nthat they appear naturalistic to humans in the scene, while also satisfying\nvehicle dynamics, actuator limits, etc. We demonstrate our methods on\nreal-world human driving data from the inD (intersection) and rounD\n(roundabout) datasets.\n","authors":["Hamzah I. Khan","David Fridovich-Keil"],"pdf_url":"https://arxiv.org/pdf/2505.01945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01931v1","updated":"2025-05-03T21:49:14Z","published":"2025-05-03T21:49:14Z","title":"Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost\n  Robotics","summary":"  Classical robot navigation often relies on hardcoded state machines and\npurely geometric path planners, limiting a robot's ability to interpret\nhigh-level semantic instructions. In this paper, we first assess GPT-4's\nability to act as a path planner compared to the A* algorithm, then present a\nhybrid planning framework that integrates GPT-4's semantic reasoning with A* on\na low-cost robot platform operating on ROS2 Humble. Our approach eliminates\nexplicit finite state machine (FSM) coding by using prompt-based GPT-4\nreasoning to handle task logic while maintaining the accurate paths computed by\nA*. The GPT-4 module provides semantic understanding of instructions and\nenvironmental cues (e.g., recognizing toxic obstacles or crowded areas to\navoid, or understanding low-battery situations requiring alternate route\nselection), and dynamically adjusts the robot's occupancy grid via obstacle\nbuffering to enforce semantic constraints. We demonstrate multi-step reasoning\nfor sequential tasks, such as first navigating to a resource goal and then\nreaching a final destination safely. Experiments on a Petoi Bittle robot with\nan overhead camera and Raspberry Pi Zero 2W compare classical A* against\nGPT-4-assisted planning. Results show that while A* is faster and more accurate\nfor basic route generation and obstacle avoidance, the GPT-4-integrated system\nachieves high success rates (96-100%) on semantic tasks that are infeasible for\npure geometric planners. This work highlights how affordable robots can exhibit\nintelligent, context-aware behaviors by leveraging large language model\nreasoning with minimal hardware and no fine-tuning.\n","authors":["Jesse Barkley","Abraham George","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2505.01931v1.pdf","comment":"10 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.01893v1","updated":"2025-05-03T19:02:30Z","published":"2025-05-03T19:02:30Z","title":"DriveNetBench: An Affordable and Configurable Single-Camera Benchmarking\n  System for Autonomous Driving Networks","summary":"  Validating autonomous driving neural networks often demands expensive\nequipment and complex setups, limiting accessibility for researchers and\neducators. We introduce DriveNetBench, an affordable and configurable\nbenchmarking system designed to evaluate autonomous driving networks using a\nsingle-camera setup. Leveraging low-cost, off-the-shelf hardware, and a\nflexible software stack, DriveNetBench enables easy integration of various\ndriving models, such as object detection and lane following, while ensuring\nstandardized evaluation in real-world scenarios. Our system replicates common\ndriving conditions and provides consistent, repeatable metrics for comparing\nnetwork performance. Through preliminary experiments with representative vision\nmodels, we illustrate how DriveNetBench effectively measures inference speed\nand accuracy within a controlled test environment. The key contributions of\nthis work include its affordability, its replicability through open-source\nsoftware, and its seamless integration into existing workflows, making\nautonomous vehicle research more accessible.\n","authors":["Ali Al-Bustami","Humberto Ruiz-Ochoa","Jaerock Kwon"],"pdf_url":"https://arxiv.org/pdf/2505.01893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01881v1","updated":"2025-05-03T17:59:26Z","published":"2025-05-03T17:59:26Z","title":"PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in\n  Navigation Applications","summary":"  Robust navigation in diverse environments and domains requires both accurate\nstate estimation and transparent decision making. We present PhysNav-DG, a\nnovel framework that integrates classical sensor fusion with the semantic power\nof vision-language models. Our dual-branch architecture predicts navigation\nactions from multi-sensor inputs while simultaneously generating detailed\nchain-of-thought explanations. A modified Adaptive Kalman Filter dynamically\nadjusts its noise parameters based on environmental context. It leverages\nseveral streams of raw sensor data along with semantic insights from models\nsuch as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the\nMD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,\nautonomous driving, and social navigation tasks with ground-truth actions and\nhuman-validated explanations. Extensive experiments and ablations show that\nPhysNav-DG improves navigation success rates by over 20% and achieves high\nefficiency, with explanations that are both highly grounded and clear. This\nwork connects high-level semantic reasoning and geometric planning for safer\nand more trustworthy autonomous systems.\n","authors":["Trisanth Srinivasan","Santosh Patapati"],"pdf_url":"https://arxiv.org/pdf/2505.01881v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.01862v1","updated":"2025-05-03T16:48:05Z","published":"2025-05-03T16:48:05Z","title":"ReLI: A Language-Agnostic Approach to Human-Robot Interaction","summary":"  Adapting autonomous agents to industrial, domestic, and other daily tasks is\ncurrently gaining momentum. However, in the global or cross-lingual application\ncontexts, ensuring effective interaction with the environment and executing\nunrestricted human task-specified instructions in diverse languages remains an\nunsolved problem. To address this challenge, we propose ReLI, a\nlanguage-agnostic framework designed to enable autonomous agents to converse\nnaturally, semantically reason about the environment, and to perform downstream\ntasks, regardless of the task instruction's linguistic origin. First, we ground\nlarge-scale pre-trained foundation models and transform them into\nlanguage-to-action models that can directly provide common-sense reasoning and\nhigh-level robot control through natural, free-flow human-robot conversational\ninteractions. Further, we perform cross-lingual grounding of the models to\nensure that ReLI generalises across the global languages. To demonstrate the\nReLI's robustness, we conducted extensive simulated and real-world experiments\non various short- and long-horizon tasks, including zero-shot and few-shot\nspatial navigation, scene information retrieval, and query-oriented tasks. We\nbenchmarked the performance on 140 languages involving over 70K multi-turn\nconversations. On average, ReLI achieved over 90%$\\pm$0.2 accuracy in\ncross-lingual instruction parsing and task execution success rates. These\nresults demonstrate the ReLI's potential to enhance natural human-robot\ninteraction in the real world while championing linguistic diversity.\nDemonstrations and resources will be publicly available at\nhttps://linusnep.github.io/ReLI/.\n","authors":["Linus Nwankwo","Bjoern Ellensohn","Ozan Özdenizci","Elmar Rueckert"],"pdf_url":"https://arxiv.org/pdf/2505.01862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00091v2","updated":"2025-05-03T16:45:20Z","published":"2025-04-30T18:02:45Z","title":"CoordField: Coordination Field for Agentic UAV Task Allocation In\n  Low-altitude Urban Scenarios","summary":"  With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV)\nswarms to perform complex tasks in urban environments, system design now faces\nmajor challenges, including efficient semantic understanding, flexible task\nplanning, and the ability to dynamically adjust coordination strategies in\nresponse to evolving environmental conditions and continuously changing task\nrequirements. To address the limitations of existing approaches, this paper\nproposes coordination field agentic system for coordinating heterogeneous UAV\nswarms in complex urban scenarios. In this system, large language models (LLMs)\nis responsible for interpreting high-level human instructions and converting\nthem into executable commands for the UAV swarms, such as patrol and target\ntracking. Subsequently, a Coordination field mechanism is proposed to guide UAV\nmotion and task selection, enabling decentralized and adaptive allocation of\nemergent tasks. A total of 50 rounds of comparative testing were conducted\nacross different models in a 2D simulation space to evaluate their performance.\nExperimental results demonstrate that the proposed system achieves superior\nperformance in terms of task coverage, response time, and adaptability to\ndynamic changes.\n","authors":["Tengchao Zhang","Yonglin Tian","Fei Lin","Jun Huang","Patrik P. Süli","Rui Qin","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2505.00091v2.pdf","comment":"Submitted ITSC 2025"},{"id":"http://arxiv.org/abs/2504.08661v2","updated":"2025-05-03T16:10:16Z","published":"2025-04-11T16:10:58Z","title":"Safe Flow Matching: Robot Motion Planning with Control Barrier Functions","summary":"  Recent advances in generative modeling have led to promising results in robot\nmotion planning, particularly through diffusion and flow matching (FM)-based\nmodels that capture complex, multimodal trajectory distributions. However,\nthese methods are typically trained offline and remain limited when faced with\nnew environments with constraints, often lacking explicit mechanisms to ensure\nsafety during deployment. In this work, we propose Safe Flow Matching\n(SafeFlow), a motion planning framework, for trajectory generation that\nintegrates flow matching with safety guarantees. SafeFlow leverages our\nproposed flow matching barrier functions (FMBF) to ensure the planned\ntrajectories remain within safe regions across the entire planning horizon.\nCrucially, our approach enables training-free, real-time safety enforcement at\ntest time, eliminating the need for retraining. We evaluate SafeFlow on a\ndiverse set of tasks, including planar robot navigation and 7-DoF manipulation,\ndemonstrating superior safety and planning performance compared to\nstate-of-the-art generative planners. Comprehensive resources are available on\nthe project website: https://safeflowmatching.github.io\n","authors":["Xiaobing Dai","Zewen Yang","Dian Yu","Shanshan Zhang","Hamid Sadeghian","Sami Haddadin","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2504.08661v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16693v2","updated":"2025-05-03T12:21:40Z","published":"2025-04-23T13:27:07Z","title":"PIN-WM: Learning Physics-INformed World Models for Non-Prehensile\n  Manipulation","summary":"  While non-prehensile manipulation (e.g., controlled pushing/poking)\nconstitutes a foundational robotic skill, its learning remains challenging due\nto the high sensitivity to complex physical interactions involving friction and\nrestitution. To achieve robust policy learning and generalization, we opt to\nlearn a world model of the 3D rigid body dynamics involved in non-prehensile\nmanipulations and use it for model-based reinforcement learning. We propose\nPIN-WM, a Physics-INformed World Model that enables efficient end-to-end\nidentification of a 3D rigid body dynamical system from visual observations.\nAdopting differentiable physics simulation, PIN-WM can be learned with only\nfew-shot and task-agnostic physical interaction trajectories. Further, PIN-WM\nis learned with observational loss induced by Gaussian Splatting without\nneeding state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM\ninto a group of Digital Cousins via physics-aware randomizations which perturb\nphysics and rendering parameters to generate diverse and meaningful variations\nof the PIN-WM. Extensive evaluations on both simulation and real-world tests\ndemonstrate that PIN-WM, enhanced with physics-aware digital cousins,\nfacilitates learning robust non-prehensile manipulation skills with Sim2Real\ntransfer, surpassing the Real2Sim2Real state-of-the-arts.\n","authors":["Wenxuan Li","Hang Zhao","Zhiyuan Yu","Yu Du","Qin Zou","Ruizhen Hu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2504.16693v2.pdf","comment":"Robotics: Science and Systems 2025"},{"id":"http://arxiv.org/abs/2505.01766v1","updated":"2025-05-03T09:43:30Z","published":"2025-05-03T09:43:30Z","title":"Multimodal Graph Representation Learning for Robust Surgical Workflow\n  Recognition with Adversarial Feature Disentanglement","summary":"  Surgical workflow recognition is vital for automating tasks, supporting\ndecision-making, and training novice surgeons, ultimately improving patient\nsafety and standardizing procedures. However, data corruption can lead to\nperformance degradation due to issues like occlusion from bleeding or smoke in\nsurgical scenes and problems with data storage and transmission. In this case,\nwe explore a robust graph-based multimodal approach to integrating vision and\nkinematic data to enhance accuracy and reliability. Vision data captures\ndynamic surgical scenes, while kinematic data provides precise movement\ninformation, overcoming limitations of visual recognition under adverse\nconditions. We propose a multimodal Graph Representation network with\nAdversarial feature Disentanglement (GRAD) for robust surgical workflow\nrecognition in challenging scenarios with domain shifts or corrupted data.\nSpecifically, we introduce a Multimodal Disentanglement Graph Network that\ncaptures fine-grained visual information while explicitly modeling the complex\nrelationships between vision and kinematic embeddings through graph-based\nmessage modeling. To align feature spaces across modalities, we propose a\nVision-Kinematic Adversarial framework that leverages adversarial training to\nreduce modality gaps and improve feature consistency. Furthermore, we design a\nContextual Calibrated Decoder, incorporating temporal and contextual priors to\nenhance robustness against domain shifts and corrupted data. Extensive\ncomparative and ablation experiments demonstrate the effectiveness of our model\nand proposed modules. Moreover, our robustness experiments show that our method\neffectively handles data corruption during storage and transmission, exhibiting\nexcellent stability and robustness. Our approach aims to advance automated\nsurgical workflow recognition, addressing the complexities and dynamism\ninherent in surgical procedures.\n","authors":["Long Bai","Boyi Ma","Ruohan Wang","Guankun Wang","Beilei Cui","Zhongliang Jiang","Mobarakol Islam","Zhe Min","Jiewen Lai","Nassir Navab","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2505.01766v1.pdf","comment":"Accepted by Information Fusion"},{"id":"http://arxiv.org/abs/2505.01752v1","updated":"2025-05-03T09:02:35Z","published":"2025-05-03T09:02:35Z","title":"NMPCB: A Lightweight and Safety-Critical Motion Control Framework","summary":"  In multi-obstacle environments, real-time performance and safety in robot\nmotion control have long been challenging issues, as conventional methods often\nstruggle to balance the two. In this paper, we propose a novel motion control\nframework composed of a Neural network-based path planner and a Model\nPredictive Control (MPC) controller based on control Barrier function (NMPCB) .\nThe planner predicts the next target point through a lightweight neural network\nand generates a reference trajectory for the controller. In the design of the\ncontroller, we introduce the dual problem of control barrier function (CBF) as\nthe obstacle avoidance constraint, enabling it to ensure robot motion safety\nwhile significantly reducing computation time. The controller directly outputs\ncontrol commands to the robot by tracking the reference trajectory. This\nframework achieves a balance between real-time performance and safety. We\nvalidate the feasibility of the framework through numerical simulations and\nreal-world experiments.\n","authors":["Longze Zheng","Qinghe Liu"],"pdf_url":"https://arxiv.org/pdf/2505.01752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01718v1","updated":"2025-05-03T07:05:19Z","published":"2025-05-03T07:05:19Z","title":"Mitigating Compensatory Movements in Prosthesis Users via Adaptive\n  Collaborative Robotics","summary":"  Prosthesis users can regain partial limb functionality, however, full natural\nlimb mobility is rarely restored, often resulting in compensatory movements\nthat lead to discomfort, inefficiency, and long-term physical strain. To\naddress this issue, we propose a novel human-robot collaboration framework to\nmitigate compensatory mechanisms in upper-limb prosthesis users by exploiting\ntheir residual motion capabilities while respecting task requirements. Our\napproach introduces a personalised mobility model that quantifies\njoint-specific functional limitations and the cost of compensatory movements.\nThis model is integrated into a constrained optimisation framework that\ncomputes optimal user postures for task performance, balancing functionality\nand comfort. The solution guides a collaborative robot to reconfigure the task\nenvironment, promoting effective interaction. We validated the framework using\na new body-powered prosthetic device for single-finger amputation, which\nenhances grasping capabilities through synergistic closure with the hand but\nimposes wrist constraints. Initial experiments with healthy subjects wearing\nthe prosthesis as a supernumerary finger demonstrated that a robotic assistant\nembedding the user-specific mobility model outperformed human partners in\nhandover tasks, improving both the efficiency of the prosthesis user's grasp\nand reducing compensatory movements in functioning joints. These results\nhighlight the potential of collaborative robots as effective workplace and\ncaregiving assistants, promoting inclusion and better integration of prosthetic\ndevices into daily tasks.\n","authors":["Marta Lagomarsino","Robin Arbaud","Francesco Tassi","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2505.01718v1.pdf","comment":"6 pages, 6 figures, IEEE International Conference on Rehabilitation\n  Robotics (ICORR)"},{"id":"http://arxiv.org/abs/2505.01709v1","updated":"2025-05-03T06:17:18Z","published":"2025-05-03T06:17:18Z","title":"RoBridge: A Hierarchical Architecture Bridging Cognition and Execution\n  for General Robotic Manipulation","summary":"  Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation.\n","authors":["Kaidong Zhang","Rongtao Xu","Pengzhen Ren","Junfan Lin","Hefeng Wu","Liang Lin","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2505.01709v1.pdf","comment":"project page: https://abliao.github.io/RoBridge/"},{"id":"http://arxiv.org/abs/2505.01654v1","updated":"2025-05-03T02:17:45Z","published":"2025-05-03T02:17:45Z","title":"T-REX: Vision-Based System for Autonomous Leaf Detection and Grasp\n  Estimation","summary":"  T-Rex (The Robot for Extracting Leaf Samples) is a gantry-based robotic\nsystem developed for autonomous leaf localization, selection, and grasping in\ngreenhouse environments. The system integrates a 6-degree-of-freedom\nmanipulator with a stereo vision pipeline to identify and interact with target\nleaves. YOLOv8 is used for real-time leaf segmentation, and RAFT-Stereo\nprovides dense depth maps, allowing the reconstruction of 3D leaf masks. These\nobservations are processed through a leaf grasping algorithm that selects the\noptimal leaf based on clutter, visibility, and distance, and determines a grasp\npoint by analyzing local surface flatness, top-down approachability, and margin\nfrom edges. The selected grasp point guides a trajectory executed by ROS-based\nmotion controllers, driving a custom microneedle-equipped end-effector to clamp\nthe leaf and simulate tissue sampling. Experiments conducted with artificial\nplants under varied poses demonstrate that the T-Rex system can consistently\ndetect, plan, and perform physical interactions with plant-like targets,\nachieving a grasp success rate of 66.6\\%. This paper presents the system\narchitecture, implementation, and testing of T-Rex as a step toward plant\nsampling automation in Controlled Environment Agriculture (CEA).\n","authors":["Srecharan Selvam","Abhisesh Silwal","George Kantor"],"pdf_url":"https://arxiv.org/pdf/2505.01654v1.pdf","comment":"11 Pages, 10 figures, 2 tables"}]},"2025-05-06T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2505.03738v1","updated":"2025-05-06T17:59:51Z","published":"2025-05-06T17:59:51Z","title":"AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid\n  Whole-Body Control","summary":"  Humanoid robots derive much of their dexterity from hyper-dexterous\nwhole-body movements, enabling tasks that require a large operational\nworkspace: such as picking objects off the ground. However, achieving these\ncapabilities on real humanoids remains challenging due to their high degrees of\nfreedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization\n(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with\ntrajectory optimization for real-time, adaptive whole-body control. To mitigate\ndistribution bias in motion imitation RL, we construct a hybrid AMO dataset and\ntrain a network capable of robust, on-demand adaptation to potentially O.O.D.\ncommands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid\nrobot, demonstrating superior stability and an expanded workspace compared to\nstrong baselines. Finally, we show that AMO's consistent performance supports\nautonomous task execution via imitation learning, underscoring the system's\nversatility and robustness.\n","authors":["Jialong Li","Xuxin Cheng","Tianshu Huang","Shiqi Yang","Ri-Zhao Qiu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03738v1.pdf","comment":"website: https://amo-humanoid.github.io"},{"id":"http://arxiv.org/abs/2505.03729v1","updated":"2025-05-06T17:57:12Z","published":"2025-05-06T17:57:12Z","title":"Visual Imitation Enables Contextual Humanoid Control","summary":"  How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.\n","authors":["Arthur Allshire","Hongsuk Choi","Junyi Zhang","David McAllister","Anthony Zhang","Chung Min Kim","Trevor Darrell","Pieter Abbeel","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2505.03729v1.pdf","comment":"Project website: https://www.videomimic.net/"},{"id":"http://arxiv.org/abs/2505.03728v1","updated":"2025-05-06T17:56:40Z","published":"2025-05-06T17:56:40Z","title":"PyRoki: A Modular Toolkit for Robot Kinematic Optimization","summary":"  Robot motion can have many goals. Depending on the task, we might optimize\nfor pose error, speed, collision, or similarity to a human demonstration.\nMotivated by this, we present PyRoki: a modular, extensible, and cross-platform\ntoolkit for solving kinematic optimization problems. PyRoki couples an\ninterface for specifying kinematic variables and costs with an efficient\nnonlinear least squares optimizer. Unlike existing tools, it is also\ncross-platform: optimization runs natively on CPU, GPU, and TPU. In this paper,\nwe present (i) the design and implementation of PyRoki, (ii) motion retargeting\nand planning case studies that highlight the advantages of PyRoki's modularity,\nand (iii) optimization benchmarking, where PyRoki can be 1.4-1.7x faster and\nconverges to lower errors than cuRobo, an existing GPU-accelerated inverse\nkinematics library.\n","authors":["Chung Min Kim","Brent Yi","Hongsuk Choi","Yi Ma","Ken Goldberg","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2505.03728v1.pdf","comment":"First two authors contributed equally. Code is available at\n  https://pyroki-toolkit.github.io"},{"id":"http://arxiv.org/abs/2505.03725v1","updated":"2025-05-06T17:53:14Z","published":"2025-05-06T17:53:14Z","title":"Meta-Optimization and Program Search using Language Models for Task and\n  Motion Planning","summary":"  Intelligent interaction with the real world requires robotic agents to\njointly reason over high-level plans and low-level controls. Task and motion\nplanning (TAMP) addresses this by combining symbolic planning and continuous\ntrajectory generation. Recently, foundation model approaches to TAMP have\npresented impressive results, including fast planning times and the execution\nof natural language instructions. Yet, the optimal interface between high-level\nplanning and low-level motion generation remains an open question: prior\napproaches are limited by either too much abstraction (e.g., chaining\nsimplified skill primitives) or a lack thereof (e.g., direct joint angle\nprediction). Our method introduces a novel technique employing a form of\nmeta-optimization to address these issues by: (i) using program search over\ntrajectory optimization problems as an interface between a foundation model and\nrobot control, and (ii) leveraging a zero-order method to optimize numerical\nparameters in the foundation model output. Results on challenging object\nmanipulation and drawing tasks confirm that our proposed method improves over\nprior TAMP approaches.\n","authors":["Denis Shcherba","Eckart Cobo-Briesewitz","Cornelius V. Braun","Marc Toussaint"],"pdf_url":"https://arxiv.org/pdf/2505.03725v1.pdf","comment":"20 pages, 8 figures, under review for the 9th Annual Conference on\n  Robot Learning (CoRL 2025)"},{"id":"http://arxiv.org/abs/2505.03702v1","updated":"2025-05-06T17:22:21Z","published":"2025-05-06T17:22:21Z","title":"Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid\n  Geometric-Neural Approach","summary":"  Automating leaf manipulation in agricultural settings faces significant\nchallenges, including the variability of plant morphologies and deformable\nleaves. We propose a novel hybrid geometric-neural approach for autonomous leaf\ngrasping that combines traditional computer vision with neural networks through\nself-supervised learning. Our method integrates YOLOv8 for instance\nsegmentation and RAFT-Stereo for 3D depth estimation to build rich leaf\nrepresentations, which feed into both a geometric feature scoring pipeline and\na neural refinement module (GraspPointCNN). The key innovation is our\nconfidence-weighted fusion mechanism that dynamically balances the contribution\nof each approach based on prediction certainty. Our self-supervised framework\nuses the geometric pipeline as an expert teacher to automatically generate\ntraining data. Experiments demonstrate that our approach achieves an 88.0%\nsuccess rate in controlled environments and 84.7% in real greenhouse\nconditions, significantly outperforming both purely geometric (75.3%) and\nneural (60.2%) methods. This work establishes a new paradigm for agricultural\nrobotics where domain expertise is seamlessly integrated with machine learning\ncapabilities, providing a foundation for fully automated crop monitoring\nsystems.\n","authors":["Srecharan Selvam","Abhishesh Silwal","George Kanter"],"pdf_url":"https://arxiv.org/pdf/2505.03702v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.03695v1","updated":"2025-05-06T17:00:32Z","published":"2025-05-06T17:00:32Z","title":"Frenet Corridor Planner: An Optimal Local Path Planning Framework for\n  Autonomous Driving","summary":"  Motivated by the requirements for effectiveness and efficiency, path-speed\ndecomposition-based trajectory planning methods have widely been adopted for\nautonomous driving applications. While a global route can be pre-computed\noffline, real-time generation of adaptive local paths remains crucial.\nTherefore, we present the Frenet Corridor Planner (FCP), an optimization-based\nlocal path planning strategy for autonomous driving that ensures smooth and\nsafe navigation around obstacles. Modeling the vehicles as safety-augmented\nbounding boxes and pedestrians as convex hulls in the Frenet space, our\napproach defines a drivable corridor by determining the appropriate deviation\nside for static obstacles. Thereafter, a modified space-domain bicycle\nkinematics model enables path optimization for smoothness, boundary clearance,\nand dynamic obstacle risk minimization. The optimized path is then passed to a\nspeed planner to generate the final trajectory. We validate FCP through\nextensive simulations and real-world hardware experiments, demonstrating its\nefficiency and effectiveness.\n","authors":["Faizan M. Tariq","Zheng-Hang Yeh","Avinash Singh","David Isele","Sangjae Bae"],"pdf_url":"https://arxiv.org/pdf/2505.03695v1.pdf","comment":"8 pages, 10 figures - Presented at 2025 IEEE 36th Intelligent\n  Vehicles Symposium (IV)"},{"id":"http://arxiv.org/abs/2505.03694v1","updated":"2025-05-06T16:59:54Z","published":"2025-05-06T16:59:54Z","title":"Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and\n  Avoid","summary":"  Assured safe-separation is essential for achieving seamless high-density\noperation of airborne vehicles in a shared airspace. To equip\nresource-constrained aerial systems with this safety-critical capability, we\npresent ViSafe, a high-speed vision-only airborne collision avoidance system.\nViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by\ntightly integrating a learning-based edge-AI framework with a custom\nmulti-camera hardware prototype designed under SWaP-C constraints. By\nleveraging perceptual input-focused control barrier functions (CBF) to design,\nencode, and enforce safety thresholds, ViSafe can provide provably safe runtime\nguarantees for self-separation in high-speed aerial operations. We evaluate\nViSafe's performance through an extensive test campaign involving both\nsimulated digital twins and real-world flight scenarios. By independently\nvarying agent types, closure rates, interaction geometries, and environmental\nconditions (e.g., weather and lighting), we demonstrate that ViSafe\nconsistently ensures self-separation across diverse scenarios. In\nfirst-of-its-kind real-world high-speed collision avoidance tests with closure\nrates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous\ncollision avoidance, establishing a new standard for safety in high-speed\naerial navigation.\n","authors":["Parv Kapoor","Ian Higgins","Nikhil Keetha","Jay Patrikar","Brady Moon","Zelin Ye","Yao He","Ivan Cisneros","Yaoyu Hu","Changliu Liu","Eunsuk Kang","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2505.03694v1.pdf","comment":"13 pages, RSS 2025 Demo track"},{"id":"http://arxiv.org/abs/2505.03692v1","updated":"2025-05-06T16:54:07Z","published":"2025-05-06T16:54:07Z","title":"Matching Distance and Geometric Distribution Aided Learning Multiview\n  Point Cloud Registration","summary":"  Multiview point cloud registration plays a crucial role in robotics,\nautomation, and computer vision fields. This paper concentrates on pose graph\nconstruction and motion synchronization within multiview registration. Previous\nmethods for pose graph construction often pruned fully connected graphs or\nconstructed sparse graph using global feature aggregated from local\ndescriptors, which may not consistently yield reliable results. To identify\ndependable pairs for pose graph construction, we design a network model that\nextracts information from the matching distance between point cloud pairs. For\nmotion synchronization, we propose another neural network model to calculate\nthe absolute pose in a data-driven manner, rather than optimizing inaccurate\nhandcrafted loss functions. Our model takes into account geometric distribution\ninformation and employs a modified attention mechanism to facilitate flexible\nand reliable feature interaction. Experimental results on diverse indoor and\noutdoor datasets confirm the effectiveness and generalizability of our\napproach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.\n","authors":["Shiqi Li","Jihua Zhu","Yifan Xie","Naiwen Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03673v1","updated":"2025-05-06T16:11:49Z","published":"2025-05-06T16:11:49Z","title":"RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and\n  Multi-Agent Collaboration","summary":"  The dawn of embodied intelligence has ushered in an unprecedented imperative\nfor resilient, cognition-enabled multi-agent collaboration across\nnext-generation ecosystems, revolutionizing paradigms in autonomous\nmanufacturing, adaptive service robotics, and cyber-physical production\narchitectures. However, current robotic systems face significant limitations,\nsuch as limited cross-embodiment adaptability, inefficient task scheduling, and\ninsufficient dynamic error correction. While End-to-end VLA models demonstrate\ninadequate long-horizon planning and task generalization, hierarchical VLA\nmodels suffer from a lack of cross-embodiment and multi-agent coordination\ncapabilities. To address these challenges, we introduce RoboOS, the first\nopen-source embodied system built on a Brain-Cerebellum hierarchical\narchitecture, enabling a paradigm shift from single-agent to multi-agent\nintelligence. Specifically, RoboOS consists of three key components: (1)\nEmbodied Brain Model (RoboBrain), a MLLM designed for global perception and\nhigh-level decision-making; (2) Cerebellum Skill Library, a modular,\nplug-and-play toolkit that facilitates seamless execution of multiple skills;\nand (3) Real-Time Shared Memory, a spatiotemporal synchronization mechanism for\ncoordinating multi-agent states. By integrating hierarchical information flow,\nRoboOS bridges Embodied Brain and Cerebellum Skill Library, facilitating robust\nplanning, scheduling, and error correction for long-horizon tasks, while\nensuring efficient multi-agent collaboration through Real-Time Shared Memory.\nFurthermore, we enhance edge-cloud communication and cloud-based distributed\ninference to facilitate high-frequency interactions and enable scalable\ndeployment. Extensive real-world experiments across various scenarios,\ndemonstrate RoboOS's versatility in supporting heterogeneous embodiments.\nProject website: https://github.com/FlagOpen/RoboOS\n","authors":["Huajie Tan","Xiaoshuai Hao","Minglan Lin","Pengwei Wang","Yaoxu Lyu","Mingyu Cao","Zhongyuan Wang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.03673v1.pdf","comment":"22 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.01998v2","updated":"2025-05-06T16:09:59Z","published":"2025-05-04T06:03:12Z","title":"A Synergistic Framework of Nonlinear Acoustic Computing and\n  Reinforcement Learning for Real-World Human-Robot Interaction","summary":"  This paper introduces a novel framework integrating nonlinear acoustic\ncomputing and reinforcement learning to enhance advanced human-robot\ninteraction under complex noise and reverberation. Leveraging physically\ninformed wave equations (e.g., Westervelt, KZK), the approach captures\nhigher-order phenomena such as harmonic generation and shock formation. By\nembedding these models in a reinforcement learning-driven control loop, the\nsystem adaptively optimizes key parameters (e.g., absorption, beamforming) to\nmitigate multipath interference and non-stationary noise. Experimental\nevaluations, covering far-field localization, weak signal detection, and\nmultilingual speech recognition, demonstrate that this hybrid strategy\nsurpasses traditional linear methods and purely data-driven baselines,\nachieving superior noise suppression, minimal latency, and robust accuracy in\ndemanding real-world scenarios. The proposed system demonstrates broad\napplication prospects in AI hardware, robot, machine audition, artificial\naudition, and brain-machine interfaces.\n","authors":["Xiaoliang Chen","Xin Yu","Le Chang","Yunhe Huang","Jiashuai He","Shibo Zhang","Jin Li","Likai Lin","Ziyu Zeng","Xianling Tu","Shuyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.01998v2.pdf","comment":"34 pages, 11 figures, 10 tables, and 10 equations"},{"id":"http://arxiv.org/abs/2505.00306v2","updated":"2025-05-06T16:08:39Z","published":"2025-05-01T04:58:50Z","title":"J-PARSE: Jacobian-based Projection Algorithm for Resolving Singularities\n  Effectively in Inverse Kinematic Control of Serial Manipulators","summary":"  J-PARSE is a method for smooth first-order inverse kinematic control of a\nserial manipulator near kinematic singularities. The commanded end-effector\nvelocity is interpreted component-wise, according to the available mobility in\neach dimension of the task space. First, a substitute \"Safety\" Jacobian matrix\nis created, keeping the aspect ratio of the manipulability ellipsoid above a\nthreshold value. The desired motion is then projected onto non-singular and\nsingular directions, and the latter projection scaled down by a factor informed\nby the threshold value. A right-inverse of the non-singular Safety Jacobian is\napplied to the modified command. In the absence of joint limits and collisions,\nthis ensures smooth transition into and out of low-rank poses, guaranteeing\nasymptotic stability for target poses within the workspace, and stability for\nthose outside. Velocity control with J-PARSE is benchmarked against the\nLeast-Squares and Damped Least-Squares inversions of the Jacobian, and shows\nhigh accuracy in reaching and leaving singular target poses. By expanding the\navailable workspace of manipulators, the method finds applications in servoing,\nteleoperation, and learning.\n","authors":["Shivani Guptasarma","Matthew Strong","Honghao Zhen","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2505.00306v2.pdf","comment":"18 pages, 25 figures. v1: Fig. 1 replaced with faster-loading version"},{"id":"http://arxiv.org/abs/2502.15037v5","updated":"2025-05-06T15:36:35Z","published":"2025-02-20T20:46:09Z","title":"DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time","summary":"  Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.\n","authors":["Yizhou Chen","Xiaoyue Wu","Yeheng Zong","Yuzhen Chen","Anran Li","Bohao Zhang","Ram Vasudevan"],"pdf_url":"https://arxiv.org/pdf/2502.15037v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03587v1","updated":"2025-05-06T14:48:29Z","published":"2025-05-06T14:48:29Z","title":"Meta-reasoning Using Attention Maps and Its Applications in Cloud\n  Robotics","summary":"  Metareasoning, a branch of AI, focuses on reasoning about reasons. It has the\npotential to enhance robots' decision-making processes in unexpected\nsituations. However, the concept has largely been confined to theoretical\ndiscussions and case-by-case investigations, lacking general and practical\nsolutions when the Value of Computation (VoC) is undefined, which is common in\nunexpected situations. In this work, we propose a revised meta-reasoning\nframework that significantly improves the scalability of the original approach\nin unexpected situations. This is achieved by incorporating semantic attention\nmaps and unsupervised 'attention' updates into the metareasoning processes. To\naccommodate environmental dynamics, 'lines of thought' are used to bridge\ncontext-specific objects with abstracted attentions, while meta-information is\nmonitored and controlled at the meta-level for effective reasoning. The\npracticality of the proposed approach is demonstrated through cloud robots\ndeployed in real-world scenarios, showing improved performance and robustness.\n","authors":["Adrian Lendinez","Renxi Qiu","Lanfranco Zanzi","Dayou Li"],"pdf_url":"https://arxiv.org/pdf/2505.03587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01919v2","updated":"2025-05-06T14:36:22Z","published":"2024-10-02T18:17:19Z","title":"High-order regularization dealing with ill-conditioned robot\n  localization problems","summary":"  In this work, we propose a high-order regularization method to solve the\nill-conditioned problems in robot localization. Numerical solutions to robot\nlocalization problems are often unstable when the problems are ill-conditioned.\nA typical way to solve ill-conditioned problems is regularization, and a\nclassical regularization method is the Tikhonov regularization. It is shown\nthat the Tikhonov regularization is a low-order case of our method. We find\nthat the proposed method is superior to the Tikhonov regularization in\napproximating some ill-conditioned inverse problems, such as some basic robot\nlocalization problems. The proposed method overcomes the over-smoothing problem\nin the Tikhonov regularization as it uses more than one term in the\napproximation of the matrix inverse, and an explanation for the over-smoothing\nof the Tikhonov regularization is given. Moreover, one a priori criterion,\nwhich improves the numerical stability of the ill-conditioned problem, is\nproposed to obtain an optimal regularization matrix. As most of the\nregularization solutions are biased, we also provide two bias-correction\ntechniques for the proposed high-order regularization. The simulation and\nexperimental results using an Ultra-Wideband sensor network in a 3D environment\nare discussed, demonstrating the performance of the proposed method.\n","authors":["Xinghua Liu","Ming Cao"],"pdf_url":"https://arxiv.org/pdf/2410.01919v2.pdf","comment":"This paper has been accepted by IEEE Transactions on Robotics and the\n  final version is available on IEEE Xplore"},{"id":"http://arxiv.org/abs/2505.03565v1","updated":"2025-05-06T14:21:51Z","published":"2025-05-06T14:21:51Z","title":"Thermal-LiDAR Fusion for Robust Tunnel Localization in GNSS-Denied and\n  Low-Visibility Conditions","summary":"  Despite significant progress in autonomous navigation, a critical gap remains\nin ensuring reliable localization in hazardous environments such as tunnels,\nurban disaster zones, and underground structures. Tunnels present a uniquely\ndifficult scenario: they are not only prone to GNSS signal loss, but also\nprovide little features for visual localization due to their repetitive walls\nand poor lighting. These conditions degrade conventional vision-based and\nLiDAR-based systems, which rely on distinguishable environmental features. To\naddress this, we propose a novel sensor fusion framework that integrates a\nthermal camera with a LiDAR to enable robust localization in tunnels and other\nperceptually degraded environments. The thermal camera provides resilience in\nlow-light or smoke conditions, while the LiDAR delivers precise depth\nperception and structural awareness. By combining these sensors, our framework\nensures continuous and accurate localization across diverse and dynamic\nenvironments. We use an Extended Kalman Filter (EKF) to fuse multi-sensor\ninputs, and leverages visual odometry and SLAM (Simultaneous Localization and\nMapping) techniques to process the sensor data, enabling robust motion\nestimation and mapping even in GNSS-denied environments. This fusion of sensor\nmodalities not only enhances system resilience but also provides a scalable\nsolution for cyber-physical systems in connected and autonomous vehicles\n(CAVs). To validate the framework, we conduct tests in a tunnel environment,\nsimulating sensor degradation and visibility challenges. The results\ndemonstrate that our method sustains accurate localization where standard\napproaches deteriorate due to the tunnels featureless geometry. The frameworks\nversatility makes it a promising solution for autonomous vehicles, inspection\nrobots, and other cyber-physical systems operating in constrained, perceptually\npoor environments.\n","authors":["Lukas Schichler","Karin Festl","Selim Solmaz","Daniel Watzenig"],"pdf_url":"https://arxiv.org/pdf/2505.03565v1.pdf","comment":"Submitted to IAVVC 2025"},{"id":"http://arxiv.org/abs/2310.07937v3","updated":"2025-05-06T14:06:58Z","published":"2023-10-11T23:17:43Z","title":"Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation Using\n  Vision Language Models","summary":"  Visual target navigation is a critical capability for autonomous robots\noperating in unknown environments, particularly in human-robot interaction\nscenarios. While classical and learning-based methods have shown promise, most\nexisting approaches lack common-sense reasoning and are typically designed for\nsingle-robot settings, leading to reduced efficiency and robustness in complex\nenvironments. To address these limitations, we introduce Co-NavGPT, a novel\nframework that integrates a Vision Language Model (VLM) as a global planner to\nenable common-sense multi-robot visual target navigation. Co-NavGPT aggregates\nsub-maps from multiple robots with diverse viewpoints into a unified global\nmap, encoding robot states and frontier regions. The VLM uses this information\nto assign frontiers across the robots, facilitating coordinated and efficient\nexploration. Experiments on the Habitat-Matterport 3D (HM3D) demonstrate that\nCo-NavGPT outperforms existing baselines in terms of success rate and\nnavigation efficiency, without requiring task-specific training. Ablation\nstudies further confirm the importance of semantic priors from the VLM. We also\nvalidate the framework in real-world scenarios using quadrupedal robots.\nSupplementary video and code are available at:\nhttps://sites.google.com/view/co-navgpt2.\n","authors":["Bangguo Yu","Qihao Yuan","Kailai Li","Hamidreza Kasaei","Ming Cao"],"pdf_url":"https://arxiv.org/pdf/2310.07937v3.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.03539v1","updated":"2025-05-06T13:51:26Z","published":"2025-05-06T13:51:26Z","title":"Panoramic Out-of-Distribution Segmentation","summary":"  Panoramic imaging enables capturing 360{\\deg} images with an ultra-wide\nField-of-View (FoV) for dense omnidirectional perception. However, current\npanoramic semantic segmentation methods fail to identify outliers, and pinhole\nOut-of-distribution Segmentation (OoS) models perform unsatisfactorily in the\npanoramic domain due to background clutter and pixel distortions. To address\nthese issues, we introduce a new task, Panoramic Out-of-distribution\nSegmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the\nfirst solution, POS, which adapts to the characteristics of panoramic images\nthrough text-guided prompt distribution learning. Specifically, POS integrates\na disentanglement strategy designed to materialize the cross-domain\ngeneralization capability of CLIP. The proposed Prompt-based Restoration\nAttention (PRA) optimizes semantic decoding by prompt guidance and\nself-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL)\nrefines the manifold of per-pixel mask embeddings via semantic prototype\nsupervision. Besides, to compensate for the scarcity of PanOoS datasets, we\nestablish two benchmarks: DenseOoS, which features diverse outliers in complex\nenvironments, and QuadOoS, captured by a quadruped robot with a panoramic\nannular lens system. Extensive experiments demonstrate superior performance of\nPOS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS,\noutperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves\nleading closed-set segmentation capabilities. Code and datasets will be\navailable at https://github.com/MengfeiD/PanOoS.\n","authors":["Mengfei Duan","Kailun Yang","Yuheng Zhang","Yihong Cao","Fei Teng","Kai Luo","Jiaming Zhang","Zhiyong Li","Shutao Li"],"pdf_url":"https://arxiv.org/pdf/2505.03539v1.pdf","comment":"Code and datasets will be available at\n  https://github.com/MengfeiD/PanOoS"},{"id":"http://arxiv.org/abs/2505.03537v1","updated":"2025-05-06T13:47:51Z","published":"2025-05-06T13:47:51Z","title":"Automated Action Generation based on Action Field for Robotic Garment\n  Manipulation","summary":"  Garment manipulation using robotic systems is a challenging task due to the\ndiverse shapes and deformable nature of fabric. In this paper, we propose a\nnovel method for robotic garment manipulation that significantly improves the\naccuracy while reducing computational time compared to previous approaches. Our\nmethod features an action generator that directly interprets scene images and\ngenerates pixel-wise end-effector action vectors using a neural network. The\nnetwork also predicts a manipulation score map that ranks potential actions,\nallowing the system to select the most effective action. Extensive simulation\nexperiments demonstrate that our method achieves higher unfolding and alignment\nperformances and faster computation time than previous approaches. Real-world\nexperiments show that the proposed method generalizes well to different garment\ntypes and successfully flattens garments.\n","authors":["Hu Cheng","Fuyuki Tokuda","Kazuhiro Kosuge"],"pdf_url":"https://arxiv.org/pdf/2505.03537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01862v2","updated":"2025-05-06T13:46:20Z","published":"2025-05-03T16:48:05Z","title":"ReLI: A Language-Agnostic Approach to Human-Robot Interaction","summary":"  Adapting autonomous agents to industrial, domestic, and other daily tasks is\ncurrently gaining momentum. However, in the global or cross-lingual application\ncontexts, ensuring effective interaction with the environment and executing\nunrestricted human task-specified instructions in diverse languages remains an\nunsolved problem. To address this challenge, we propose ReLI, a\nlanguage-agnostic framework designed to enable autonomous agents to converse\nnaturally, semantically reason about the environment, and to perform downstream\ntasks, regardless of the task instruction's linguistic origin. First, we ground\nlarge-scale pre-trained foundation models and transform them into\nlanguage-to-action models that can directly provide common-sense reasoning and\nhigh-level robot control through natural, free-flow human-robot conversational\ninteractions. Further, we perform cross-lingual grounding of the models to\nensure that ReLI generalises across the global languages. To demonstrate the\nReLI's robustness, we conducted extensive simulated and real-world experiments\non various short- and long-horizon tasks, including zero-shot and few-shot\nspatial navigation, scene information retrieval, and query-oriented tasks. We\nbenchmarked the performance on 140 languages involving over 70K multi-turn\nconversations. On average, ReLI achieved over 90%$\\pm$0.2 accuracy in\ncross-lingual instruction parsing and task execution success rates. These\nresults demonstrate the ReLI's potential to enhance natural human-robot\ninteraction in the real world while championing linguistic diversity.\nDemonstrations and resources will be publicly available at\nhttps://linusnep.github.io/ReLI/.\n","authors":["Linus Nwankwo","Bjoern Ellensohn","Ozan Özdenizci","Elmar Rueckert"],"pdf_url":"https://arxiv.org/pdf/2505.01862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03512v1","updated":"2025-05-06T13:22:34Z","published":"2025-05-06T13:22:34Z","title":"Artificial Protozoa Optimizer (APO): A novel bio-inspired metaheuristic\n  algorithm for engineering optimization","summary":"  This study proposes a novel artificial protozoa optimizer (APO) that is\ninspired by protozoa in nature. The APO mimics the survival mechanisms of\nprotozoa by simulating their foraging, dormancy, and reproductive behaviors.\nThe APO was mathematically modeled and implemented to perform the optimization\nprocesses of metaheuristic algorithms. The performance of the APO was verified\nvia experimental simulations and compared with 32 state-of-the-art algorithms.\nWilcoxon signed-rank test was performed for pairwise comparisons of the\nproposed APO with the state-of-the-art algorithms, and Friedman test was used\nfor multiple comparisons. First, the APO was tested using 12 functions of the\n2022 IEEE Congress on Evolutionary Computation benchmark. Considering\npracticality, the proposed APO was used to solve five popular engineering\ndesign problems in a continuous space with constraints. Moreover, the APO was\napplied to solve a multilevel image segmentation task in a discrete space with\nconstraints. The experiments confirmed that the APO could provide highly\ncompetitive results for optimization problems. The source codes of Artificial\nProtozoa Optimizer are publicly available at\nhttps://seyedalimirjalili.com/projects and\nhttps://ww2.mathworks.cn/matlabcentral/fileexchange/162656-artificial-protozoa-optimizer.\n","authors":["Xiaopeng Wang","Vaclav Snasel","Seyedali Mirjalili","Jeng-Shyang Pan","Lingping Kong","Hisham A. Shehadeh"],"pdf_url":"https://arxiv.org/pdf/2505.03512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03500v1","updated":"2025-05-06T13:05:04Z","published":"2025-05-06T13:05:04Z","title":"Task Reconstruction and Extrapolation for $π_0$ using Text Latent","summary":"  Vision-language-action models (VLAs) often achieve high performance on\ndemonstrated tasks but struggle significantly when required to extrapolate,\ncombining skills learned from different tasks in novel ways. For instance, VLAs\nmight successfully put the cream cheese in the bowl and put the bowl on top of\nthe cabinet, yet still fail to put the cream cheese on top of the cabinet. In\nthis work, we demonstrate that behaviors from distinct tasks can be effectively\nrecombined by manipulating the VLA's internal representations at inference\ntime. Concretely, we identify the text latent by averaging the text tokens'\nhidden states across all demonstrated trajectories for a specific base task.\nFor executing an extrapolated task, we can temporally interpolate the text\nlatent of the two base tasks and add it back to the text hidden states, so\nsub-behaviors from the two tasks will be activated sequentially. We evaluate\nthis approach using the newly created libero-ood benchmark, featuring 20 tasks\nextrapolated from standard LIBERO suites. The results on libero-ood show that\nall SOTA VLAs achieve < 15% success rate, while $\\pi0$ with text latent\ninterpolation reaches an 83% success rate. Further qualitative analysis reveals\na tendency for VLAs to exhibit spatial overfitting, mapping object names to\ndemonstrated locations rather than achieving genuine object and goal\nunderstanding. Additionally, we find that decoding the text latent yields\nhuman-unreadable prompts that can nevertheless instruct the VLA to achieve a\n70% success rate on standard LIBERO suites, enabling private instruction or\nbackdoor attacks.\n","authors":["Quanyi Li"],"pdf_url":"https://arxiv.org/pdf/2505.03500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03460v1","updated":"2025-05-06T12:00:49Z","published":"2025-05-06T12:00:49Z","title":"LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal\n  Delivery Based on Agentic UAVs","summary":"  The growing demand for intelligent logistics, particularly fine-grained\nterminal delivery, underscores the need for autonomous UAV (Unmanned Aerial\nVehicle)-based delivery systems. However, most existing last-mile delivery\nstudies rely on ground robots, while current UAV-based Vision-Language\nNavigation (VLN) tasks primarily focus on coarse-grained, long-range goals,\nmaking them unsuitable for precise terminal delivery. To bridge this gap, we\npropose LogisticsVLN, a scalable aerial delivery system built on multimodal\nlarge language models (MLLMs) for autonomous terminal delivery. LogisticsVLN\nintegrates lightweight Large Language Models (LLMs) and Visual-Language Models\n(VLMs) in a modular pipeline for request understanding, floor localization,\nobject detection, and action-decision making. To support research and\nevaluation in this new setting, we construct the Vision-Language Delivery (VLD)\ndataset within the CARLA simulator. Experimental results on the VLD dataset\nshowcase the feasibility of the LogisticsVLN system. In addition, we conduct\nsubtask-level evaluations of each module of our system, offering valuable\ninsights for improving the robustness and real-world deployment of foundation\nmodel-based vision-language delivery systems.\n","authors":["Xinyuan Zhang","Yonglin Tian","Fei Lin","Yue Liu","Jing Ma","Kornélia Sára Szatmáry","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03448v1","updated":"2025-05-06T11:37:27Z","published":"2025-05-06T11:37:27Z","title":"AquaticVision: Benchmarking Visual SLAM in Underwater Environment with\n  Events and Frames","summary":"  Many underwater applications, such as offshore asset inspections, rely on\nvisual inspection and detailed 3D reconstruction. Recent advancements in\nunderwater visual SLAM systems for aquatic environments have garnered\nsignificant attention in marine robotics research. However, existing underwater\nvisual SLAM datasets often lack groundtruth trajectory data, making it\ndifficult to objectively compare the performance of different SLAM algorithms\nbased solely on qualitative results or COLMAP reconstruction. In this paper, we\npresent a novel underwater dataset that includes ground truth trajectory data\nobtained using a motion capture system. Additionally, for the first time, we\nrelease visual data that includes both events and frames for benchmarking\nunderwater visual positioning. By providing event camera data, we aim to\nfacilitate the development of more robust and advanced underwater visual SLAM\nalgorithms. The use of event cameras can help mitigate challenges posed by\nextremely low light or hazy underwater conditions. The webpage of our dataset\nis https://sites.google.com/view/aquaticvision-lias.\n","authors":["Yifan Peng","Yuze Hong","Ziyang Hong","Apple Pui-Yi Chui","Junfeng Wu"],"pdf_url":"https://arxiv.org/pdf/2505.03448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14196v2","updated":"2025-05-06T11:13:28Z","published":"2024-09-21T16:53:56Z","title":"Adversarial and Reactive Traffic Entities for Behavior-Realistic Driving\n  Simulation: A Review","summary":"  Despite advancements in perception and planning for autonomous vehicles\n(AVs), validating their performance remains a significant challenge. The\ndeployment of planning algorithms in real-world environments is often\nineffective due to discrepancies between simulations and real traffic\nconditions. Evaluating AVs planning algorithms in simulation typically involves\nreplaying driving logs from recorded real-world traffic. However, entities\nreplayed from offline data are not reactive, lack the ability to respond to\narbitrary AV behavior, and cannot behave in an adversarial manner to test\ncertain properties of the driving policy. Therefore, simulation with realistic\nand potentially adversarial entities represents a critical task for AV planning\nsoftware validation. In this work, we aim to review current research efforts in\nthe field of traffic simulation, focusing on the application of advanced\ntechniques for modeling realistic and adversarial behaviors of traffic\nentities. The objective of this work is to categorize existing approaches based\non the proposed classes of traffic entity behavior and scenario behavior\ncontrol. Moreover, we collect traffic datasets and examine existing traffic\nsimulations with respect to their employed default traffic entities. Finally,\nwe identify challenges and open questions that hold potential for future\nresearch.\n","authors":["Joshua Ransiek","Philipp Reis","Tobias Schürmann","Eric Sax"],"pdf_url":"https://arxiv.org/pdf/2409.14196v2.pdf","comment":"Submitted to the IEEE for possible publication, 8 pages, 1 figures"},{"id":"http://arxiv.org/abs/2505.03422v1","updated":"2025-05-06T10:59:23Z","published":"2025-05-06T10:59:23Z","title":"LiftFeat: 3D Geometry-Aware Local Feature Matching","summary":"  Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled \\textit{LiftFeat}, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.\n","authors":["Yepeng Liu","Wenpeng Lai","Zhou Zhao","Yuxuan Xiong","Jinchi Zhu","Jun Cheng","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2505.03422v1.pdf","comment":"Accepted at ICRA 2025"},{"id":"http://arxiv.org/abs/2505.03400v1","updated":"2025-05-06T10:28:39Z","published":"2025-05-06T10:28:39Z","title":"Close-Fitting Dressing Assistance Based on State Estimation of Feet and\n  Garments with Semantic-based Visual Attention","summary":"  As the population continues to age, a shortage of caregivers is expected in\nthe future. Dressing assistance, in particular, is crucial for opportunities\nfor social participation. Especially dressing close-fitting garments, such as\nsocks, remains challenging due to the need for fine force adjustments to handle\nthe friction or snagging against the skin, while considering the shape and\nposition of the garment. This study introduces a method uses multi-modal\ninformation including not only robot's camera images, joint angles, joint\ntorques, but also tactile forces for proper force interaction that can adapt to\nindividual differences in humans. Furthermore, by introducing semantic\ninformation based on object concepts, rather than relying solely on RGB data,\nit can be generalized to unseen feet and background. In addition, incorporating\ndepth data helps infer relative spatial relationship between the sock and the\nfoot. To validate its capability for semantic object conceptualization and to\nensure safety, training data were collected using a mannequin, and subsequent\nexperiments were conducted with human subjects. In experiments, the robot\nsuccessfully adapted to previously unseen human feet and was able to put socks\non 10 participants, achieving a higher success rate than Action Chunking with\nTransformer and Diffusion Policy. These results demonstrate that the proposed\nmodel can estimate the state of both the garment and the foot, enabling precise\ndressing assistance for close-fitting garments.\n","authors":["Takuma Tsukakoshi","Tamon Miyake","Tetsuya Ogata","Yushi Wang","Takumi Akaishi","Shigeki Sugano"],"pdf_url":"https://arxiv.org/pdf/2505.03400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03356v1","updated":"2025-05-06T09:26:29Z","published":"2025-05-06T09:26:29Z","title":"Effective Reinforcement Learning Control using Conservative Soft\n  Actor-Critic","summary":"  Reinforcement Learning (RL) has shown great potential in complex control\ntasks, particularly when combined with deep neural networks within the\nActor-Critic (AC) framework. However, in practical applications, balancing\nexploration, learning stability, and sample efficiency remains a significant\nchallenge. Traditional methods such as Soft Actor-Critic (SAC) and Proximal\nPolicy Optimization (PPO) address these issues by incorporating entropy or\nrelative entropy regularization, but often face problems of instability and low\nsample efficiency. In this paper, we propose the Conservative Soft Actor-Critic\n(CSAC) algorithm, which seamlessly integrates entropy and relative entropy\nregularization within the AC framework. CSAC improves exploration through\nentropy regularization while avoiding overly aggressive policy updates with the\nuse of relative entropy regularization. Evaluations on benchmark tasks and\nreal-world robotic simulations demonstrate that CSAC offers significant\nimprovements in stability and efficiency over existing methods. These findings\nsuggest that CSAC provides strong robustness and application potential in\ncontrol tasks under dynamic environments.\n","authors":["Xinyi Yuan","Zhiwei Shang","Wenjun Huang","Yunduan Cui","Di Chen","Meixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.03356v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.00693v2","updated":"2025-05-06T09:24:22Z","published":"2025-05-01T17:55:05Z","title":"Robotic Visual Instruction","summary":"  Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision introduces\nchallenges for robotic task definition such as ambiguity and verbosity.\nMoreover, in some public settings where quiet is required, such as libraries or\nhospitals, verbal communication with robots is inappropriate. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment,enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Project\nwebsite: https://robotic-visual-instruction.github.io/\n","authors":["Yanbang Li","Ziyang Gong","Haoyang Li","Xiaoqi Huang","Haolan Kang","Guangping Bai","Xianzheng Ma"],"pdf_url":"https://arxiv.org/pdf/2505.00693v2.pdf","comment":"Project website: https://robotic-visual-instruction.github.io/"},{"id":"http://arxiv.org/abs/2505.03344v1","updated":"2025-05-06T09:12:37Z","published":"2025-05-06T09:12:37Z","title":"RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic\n  Simulation","summary":"  Achieving both realism and controllability in interactive closed-loop traffic\nsimulation remains a key challenge in autonomous driving. Data-driven\nsimulation methods reproduce realistic trajectories but suffer from covariate\nshift in closed-loop deployment, compounded by simplified dynamics models that\nfurther reduce reliability. Conversely, physics-based simulation methods\nenhance reliable and controllable closed-loop interactions but often lack\nexpert demonstrations, compromising realism. To address these challenges, we\nintroduce a dual-stage AV-centered simulation framework that conducts open-loop\nimitation learning pre-training in a data-driven simulator to capture\ntrajectory-level realism and multimodality, followed by closed-loop\nreinforcement learning fine-tuning in a physics-based simulator to enhance\ncontrollability and mitigate covariate shift. In the fine-tuning stage, we\npropose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that\npreserves the trajectory-level multimodality through a GRPO-style\ngroup-relative advantage formulation, while enhancing controllability and\ntraining stability by replacing KL regularization with the dual-clip mechanism.\nExtensive experiments demonstrate that RIFT significantly improves the realism\nand controllability of generated traffic scenarios, providing a robust platform\nfor evaluating autonomous vehicle performance in diverse and interactive\nscenarios.\n","authors":["Keyu Chen","Wenchao Sun","Hao Cheng","Sifa Zheng"],"pdf_url":"https://arxiv.org/pdf/2505.03344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03331v1","updated":"2025-05-06T09:04:49Z","published":"2025-05-06T09:04:49Z","title":"Miniature multihole airflow sensor for lightweight aircraft over wide\n  speed and angular range","summary":"  An aircraft's airspeed, angle of attack, and angle of side slip are crucial\nto its safety, especially when flying close to the stall regime. Various\nsolutions exist, including pitot tubes, angular vanes, and multihole pressure\nprobes. However, current sensors are either too heavy (>30 g) or require large\nairspeeds (>20 m/s), making them unsuitable for small uncrewed aerial vehicles.\nWe propose a novel multihole pressure probe, integrating sensing electronics in\na single-component structure, resulting in a mechanically robust and\nlightweight sensor (9 g), which we released to the public domain. Since there\nis no consensus on two critical design parameters, tip shape (conical vs\nspherical) and hole spacing (distance between holes), we provide a study on\nmeasurement accuracy and noise generation using wind tunnel experiments. The\nsensor is calibrated using a multivariate polynomial regression model over an\nairspeed range of 3-27 m/s and an angle of attack/sideslip range of +-35{\\deg},\nachieving a mean absolute error of 0.44 m/s and 0.16{\\deg}. Finally, we\nvalidated the sensor in outdoor flights near the stall regime. Our probe\nenabled accurate estimations of airspeed, angle of attack and sideslip during\ndifferent acrobatic manoeuvres. Due to its size and weight, this sensor will\nenable safe flight for lightweight, uncrewed aerial vehicles flying at low\nspeeds close to the stall regime.\n","authors":["Lukas Stuber","Simon Jeger","Raphael Zufferey","Dario Floreano"],"pdf_url":"https://arxiv.org/pdf/2505.03331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19511v3","updated":"2025-05-06T08:57:44Z","published":"2023-10-30T13:10:05Z","title":"Rule-Based Lloyd Algorithm for Multi-Robot Motion Planning and Control\n  with Safety and Convergence Guarantees","summary":"  This paper presents a distributed rule-based Lloyd algorithm (RBL) for\nmulti-robot motion planning and control. The main limitations of the basic\nLoyd-based algorithm (LB) concern deadlock issues and the failure to address\ndynamic constraints effectively. Our contribution is twofold. First, we show\nhow RBL is able to provide safety and convergence to the goal region without\nrelying on communication between robots, nor synchronization between the\nrobots. We considered different dynamic constraints with control inputs\nsaturation. Second, we show that the Lloyd-based algorithm (without rules) can\nbe successfully used as a safety layer for learning-based approaches, leading\nto non-negligible benefits. We further prove the soundness, reliability, and\nscalability of RBL through extensive simulations, comparisons with the state of\nthe art, and experimental validations on small-scale car-like robots,\nunicycle-like robots, omnidirectional robots, and aerial robots on the field.\n","authors":["Manuel Boldrer","Alvaro Serra-Gomez","Lorenzo Lyons","Vit Kratky","Javier Alonso-Mora","Laura Ferranti"],"pdf_url":"https://arxiv.org/pdf/2310.19511v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03296v1","updated":"2025-05-06T08:27:23Z","published":"2025-05-06T08:27:23Z","title":"The Unreasonable Effectiveness of Discrete-Time Gaussian Process\n  Mixtures for Robot Policy Learning","summary":"  We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel\napproach for flexible policy representation and imitation learning in robot\nmanipulation. MiDiGap enables learning from as few as five demonstrations using\nonly camera observations and generalizes across a wide range of challenging\ntasks. It excels at long-horizon behaviors such as making coffee, highly\nconstrained motions such as opening doors, dynamic actions such as scooping\nwith a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns\nthese tasks on a CPU in less than a minute and scales linearly to large\ndatasets. We also develop a rich suite of tools for inference-time steering\nusing evidence such as collision signals and robot kinematic constraints. This\nsteering enables novel generalization capabilities, including obstacle\navoidance and cross-embodiment policy transfer. MiDiGap achieves\nstate-of-the-art performance on diverse few-shot manipulation benchmarks. On\nconstrained RLBench tasks, it improves policy success by 76 percentage points\nand reduces trajectory cost by 67%. On multimodal tasks, it improves policy\nsuccess by 48 percentage points and increases sample efficiency by a factor of\n20. In cross-embodiment transfer, it more than doubles policy success. We make\nthe code publicly available at https://midigap.cs.uni-freiburg.de.\n","authors":["Jan Ole von Hartz","Adrian Röfer","Joschka Boedecker","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2505.03296v1.pdf","comment":"Submitted for publication to IEEE Transaction on Robotics"},{"id":"http://arxiv.org/abs/2505.03295v1","updated":"2025-05-06T08:27:04Z","published":"2025-05-06T08:27:04Z","title":"Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for\n  Reusing Existing Libraries and Interfaces","summary":"  Modern automation systems increasingly rely on modular architectures, with\ncapabilities and skills as one solution approach. Capabilities define the\nfunctions of resources in a machine-readable form and skills provide the\nconcrete implementations that realize those capabilities. However, the\ndevelopment of a skill implementation conforming to a corresponding capability\nremains a time-consuming and challenging task. In this paper, we present a\nmethod that treats capabilities as contracts for skill implementations and\nleverages large language models to generate executable code based on natural\nlanguage user input. A key feature of our approach is the integration of\nexisting software libraries and interface technologies, enabling the generation\nof skill implementations across different target languages. We introduce a\nframework that allows users to incorporate their own libraries and resource\ninterfaces into the code generation process through a retrieval-augmented\ngeneration architecture. The proposed method is evaluated using an autonomous\nmobile robot controlled via Python and ROS 2, demonstrating the feasibility and\nflexibility of the approach.\n","authors":["Luis Miguel Vieira da Silva","Aljosha Köcher","Nicolas König","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2505.03295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03284v1","updated":"2025-05-06T08:12:31Z","published":"2025-05-06T08:12:31Z","title":"OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for\n  3D Semantic Occupancy Prediction","summary":"  The safe operation of autonomous vehicles (AVs) is highly dependent on their\nunderstanding of the surroundings. For this, the task of 3D semantic occupancy\nprediction divides the space around the sensors into voxels, and labels each\nvoxel with both occupancy and semantic information. Recent perception models\nhave used multisensor fusion to perform this task. However, existing\nmultisensor fusion-based approaches focus mainly on using sensor information in\nthe Cartesian coordinate system. This ignores the distribution of the sensor\nreadings, leading to a loss of fine-grained details and performance\ndegradation. In this paper, we propose OccCylindrical that merges and refines\nthe different modality features under cylindrical coordinates. Our method\npreserves more fine-grained geometry detail that leads to better performance.\nExtensive experiments conducted on the nuScenes dataset, including challenging\nrainy and nighttime scenarios, confirm our approach's effectiveness and\nstate-of-the-art performance. The code will be available at:\nhttps://github.com/DanielMing123/OccCylindrical\n","authors":["Zhenxing Ming","Julie Stephany Berrio","Mao Shan","Yaoqi Huang","Hongyu Lyu","Nguyen Hoang Khoi Tran","Tzu-Yun Tseng","Stewart Worrall"],"pdf_url":"https://arxiv.org/pdf/2505.03284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03283v1","updated":"2025-05-06T08:10:02Z","published":"2025-05-06T08:10:02Z","title":"Enabling Robots to Autonomously Search Dynamic Cluttered Post-Disaster\n  Environments","summary":"  Robots will bring search and rescue (SaR) in disaster response to another\nlevel, in case they can autonomously take over dangerous SaR tasks from humans.\nA main challenge for autonomous SaR robots is to safely navigate in cluttered\nenvironments with uncertainties, while avoiding static and moving obstacles. We\npropose an integrated control framework for SaR robots in dynamic, uncertain\nenvironments, including a computationally efficient heuristic motion planning\nsystem that provides a nominal (assuming there are no uncertainties)\ncollision-free trajectory for SaR robots and a robust motion tracking system\nthat steers the robot to track this reference trajectory, taking into account\nthe impact of uncertainties. The control architecture guarantees a balanced\ntrade-off among various SaR objectives, while handling the hard constraints,\nincluding safety. The results of various computer-based simulations, presented\nin this paper, showed significant out-performance (of up to 42.3%) of the\nproposed integrated control architecture compared to two commonly used\nstate-of-the-art methods (Rapidly-exploring Random Tree and Artificial\nPotential Function) in reaching targets (e.g., trapped victims in SaR) safely,\ncollision-free, and in the shortest possible time.\n","authors":["Karlo Rado","Mirko Baglioni","Anahita Jamshidnejad"],"pdf_url":"https://arxiv.org/pdf/2505.03283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12636v3","updated":"2025-05-06T07:45:54Z","published":"2025-04-17T04:45:15Z","title":"A0: An Affordance-Aware Hierarchical Model for General Robotic\n  Manipulation","summary":"  Robotic manipulation faces critical challenges in understanding spatial\naffordances--the \"where\" and \"how\" of object interactions--essential for\ncomplex manipulation tasks like wiping a board or stacking objects. Existing\nmethods, including modular-based and end-to-end approaches, often lack robust\nspatial reasoning capabilities. Unlike recent point-based and flow-based\naffordance methods that focus on dense spatial representations or trajectory\nmodeling, we propose A0, a hierarchical affordance-aware diffusion model that\ndecomposes manipulation tasks into high-level spatial affordance understanding\nand low-level action execution. A0 leverages the Embodiment-Agnostic Affordance\nRepresentation, which captures object-centric spatial affordances by predicting\ncontact points and post-contact trajectories. A0 is pre-trained on 1 million\ncontact points data and fine-tuned on annotated trajectories, enabling\ngeneralization across platforms. Key components include Position Offset\nAttention for motion-aware feature extraction and a Spatial Information\nAggregation Layer for precise coordinate mapping. The model's output is\nexecuted by the action execution module. Experiments on multiple robotic\nsystems (Franka, Kinova, Realman, and Dobot) demonstrate A0's superior\nperformance in complex tasks, showcasing its efficiency, flexibility, and\nreal-world applicability.\n","authors":["Rongtao Xu","Jian Zhang","Minghao Guo","Youpeng Wen","Haoting Yang","Min Lin","Jianzheng Huang","Zhe Li","Kaidong Zhang","Liqiong Wang","Yuxuan Kuang","Meng Cao","Feng Zheng","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2504.12636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03257v1","updated":"2025-05-06T07:37:04Z","published":"2025-05-06T07:37:04Z","title":"Model Predictive Fuzzy Control: A Hierarchical Multi-Agent Control\n  Architecture for Outdoor Search-and-Rescue Robots","summary":"  Autonomous robots deployed in unknown search-and-rescue (SaR) environments\ncan significantly improve the efficiency of the mission by assisting in fast\nlocalisation and rescue of the trapped victims. We propose a novel integrated\nhierarchical control architecture, called model predictive fuzzy control\n(MPFC), for autonomous mission planning of multi-robot SaR systems that should\nefficiently map an unknown environment: We combine model predictive control\n(MPC) and fuzzy logic control (FLC), where the robots are locally controlled by\ncomputationally efficient FLC controllers, and the parameters of these local\ncontrollers are tuned via a centralised MPC controller, in a regular or\nevent-triggered manner. The proposed architecture provides three main\nadvantages: (1) The control decisions are made by the FLC controllers, thus the\nreal-time computation time is affordable. (2) The centralised MPC controller\noptimises the performance criteria with a global and predictive vision of the\nsystem dynamics, and updates the parameters of the FLC controllers accordingly.\n(3) FLC controllers are heuristic by nature and thus do not take into account\noptimality in their decisions, while the tuned parameters via the MPC\ncontroller can indirectly incorporate some level of optimality in local\ndecisions of the robots. A simulation environment for victim detection in a\ndisaster environment was designed in MATLAB using discrete, 2-D grid-based\nmodels. While being comparable from the point of computational efficiency, the\nintegrated MPFC architecture improves the performance of the multi-robot SaR\nsystem compared to decentralised FLC controllers. Moreover, the performance of\nMPFC is comparable to the performance of centralised MPC for path planning of\nSaR robots, whereas MPFC requires significantly less computational resources,\nsince the number of the optimisation variables in the control problem are\nreduced.\n","authors":["Craig Maxwell","Mirko Baglioni","Anahita Jamshidnejad"],"pdf_url":"https://arxiv.org/pdf/2505.03257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03238v1","updated":"2025-05-06T07:07:28Z","published":"2025-05-06T07:07:28Z","title":"RobotxR1: Enabling Embodied Robotic Intelligence on Large Language\n  Models through Closed-Loop Reinforcement Learning","summary":"  Future robotic systems operating in real-world environments will require\non-board embodied intelligence without continuous cloud connection, balancing\ncapabilities with constraints on computational power and memory. This work\npresents an extension of the R1-zero approach, which enables the usage of low\nparameter-count Large Language Models (LLMs) in the robotic domain. The R1-Zero\napproach was originally developed to enable mathematical reasoning in LLMs\nusing static datasets. We extend it to the robotics domain through integration\nin a closed-loop Reinforcement Learning (RL) framework. This extension enhances\nreasoning in Embodied Artificial Intelligence (Embodied AI) settings without\nrelying solely on distillation of large models through Supervised Fine-Tuning\n(SFT). We show that small-scale LLMs can achieve effective reasoning\nperformance by learning through closed-loop interaction with their environment,\nwhich enables tasks that previously required significantly larger models. In an\nautonomous driving setting, a performance gain of 20.2%-points over the\nSFT-based baseline is observed with a Qwen2.5-1.5B model. Using the proposed\ntraining procedure, Qwen2.5-3B achieves a 63.3% control adaptability score,\nsurpassing the 58.5% obtained by the much larger, cloud-bound GPT-4o. These\nresults highlight that practical, on-board deployment of small LLMs is not only\nfeasible but can outperform larger models if trained through environmental\nfeedback, underscoring the importance of an interactive learning framework for\nrobotic Embodied AI, one grounded in practical experience rather than static\nsupervision.\n","authors":["Liam Boyle","Nicolas Baumann","Paviththiren Sivasothilingam","Michele Magno","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2505.03238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03233v1","updated":"2025-05-06T06:59:28Z","published":"2025-05-06T06:59:28Z","title":"GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale\n  Synthetic Action Data","summary":"  Embodied foundation models are gaining increasing attention for their\nzero-shot generalization, scalability, and adaptability to new tasks through\nfew-shot post-training. However, existing models rely heavily on real-world\ndata, which is costly and labor-intensive to collect. Synthetic data offers a\ncost-effective alternative, yet its potential remains largely underexplored. To\nbridge this gap, we explore the feasibility of training Vision-Language-Action\nmodels entirely with large-scale synthetic action data. We curate SynGrasp-1B,\na billion-frame robotic grasping dataset generated in simulation with\nphotorealistic rendering and extensive domain randomization. Building on this,\nwe present GraspVLA, a VLA model pretrained on large-scale synthetic action\ndata as a foundational model for grasping tasks. GraspVLA integrates\nautoregressive perception tasks and flow-matching-based action generation into\na unified Chain-of-Thought process, enabling joint training on synthetic action\ndata and Internet semantics data. This design helps mitigate sim-to-real gaps\nand facilitates the transfer of learned actions to a broader range of\nInternet-covered objects, achieving open-vocabulary generalization in grasping.\nExtensive evaluations across real-world and simulation benchmarks demonstrate\nGraspVLA's advanced zero-shot generalizability and few-shot adaptability to\nspecific human preferences. We will release SynGrasp-1B dataset and pre-trained\nweights to benefit the community.\n","authors":["Shengliang Deng","Mi Yan","Songlin Wei","Haixin Ma","Yuxin Yang","Jiayi Chen","Zhiqi Zhang","Taoyu Yang","Xuheng Zhang","Heming Cui","Zhizheng Zhang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03252v2","updated":"2025-05-06T06:54:15Z","published":"2024-12-04T11:51:50Z","title":"Variable-Speed Teaching-Playback as Real-World Data Augmentation for\n  Imitation Learning","summary":"  Because imitation learning relies on human demonstrations in hard-to-simulate\nsettings, the inclusion of force control in this method has resulted in a\nshortage of training data, even with a simple change in speed. Although the\nfield of data augmentation has addressed the lack of data, conventional methods\nof data augmentation for robot manipulation are limited to simulation-based\nmethods or downsampling for position control. This paper proposes a novel\nmethod of data augmentation that is applicable to force control and preserves\nthe advantages of real-world datasets. We applied teaching-playback at variable\nspeeds as real-world data augmentation to increase both the quantity and\nquality of environmental reactions at variable speeds. An experiment was\nconducted on bilateral control-based imitation learning using a method of\nimitation learning equipped with position-force control. We evaluated the\neffect of real-world data augmentation on two tasks, pick-and-place and wiping,\nat variable speeds, each from two human demonstrations at fixed speed. The\nresults showed a maximum 55% increase in success rate from a simple change in\nspeed of real-world reactions and improved accuracy along the\nduration/frequency command by gathering environmental reactions at variable\nspeeds.\n","authors":["Nozomu Masuya","Hiroshi Sato","Koki Yamane","Takuya Kusume","Sho Sakaino","Toshiaki Tsuji"],"pdf_url":"https://arxiv.org/pdf/2412.03252v2.pdf","comment":"16 pages, 12 figures, 4 tables. This is a preprint of an article\n  whose final and definitive form has been published in ADVANCED ROBOTICS 2025,\n  copyright Taylor & Francis and Robotics Society of Japan, is available online\n  at: http://www.tandfonline.com/10.1080/01691864.2025.2497423;\n  doi:10.1080/01691864.2025.2497423"},{"id":"http://arxiv.org/abs/2409.05392v2","updated":"2025-05-06T06:35:26Z","published":"2024-09-09T07:42:54Z","title":"Leveraging Computation of Expectation Models for Commonsense Affordance\n  Estimation on 3D Scene Graphs","summary":"  This article studies the commonsense object affordance concept for enabling\nclose-to-human task planning and task optimization of embodied robotic agents\nin urban environments. The focus of the object affordance is on reasoning how\nto effectively identify object's inherent utility during the task execution,\nwhich in this work is enabled through the analysis of contextual relations of\nsparse information of 3D scene graphs. The proposed framework develops a\nCorrelation Information (CECI) model to learn probability distributions using a\nGraph Convolutional Network, allowing to extract the commonsense affordance for\nindividual members of a semantic class. The overall framework was\nexperimentally validated in a real-world indoor environment, showcasing the\nability of the method to level with human commonsense. For a video of the\narticle, showcasing the experimental demonstration, please refer to the\nfollowing link: https://youtu.be/BDCMVx2GiQE\n","authors":["Mario A. V. Saucedo","Nikolaos Stathoulopoulos","Akash Patel","Christoforos Kanellakis","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2409.05392v2.pdf","comment":"Accepted at IROS24"},{"id":"http://arxiv.org/abs/2505.03178v1","updated":"2025-05-06T04:41:20Z","published":"2025-05-06T04:41:20Z","title":"RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent\n  Conditional Diffusion","summary":"  Generating safety-critical scenarios in high-fidelity simulations offers a\npromising and cost-effective approach for efficient testing of autonomous\nvehicles. Existing methods typically rely on manipulating a single vehicle's\ntrajectory through sophisticated designed objectives to induce adversarial\ninteractions, often at the cost of realism and scalability. In this work, we\npropose the Risk-Adjustable Driving Environment (RADE), a simulation framework\nthat generates statistically realistic and risk-adjustable traffic scenes.\nBuilt upon a multi-agent diffusion architecture, RADE jointly models the\nbehavior of all agents in the environment and conditions their trajectories on\na surrogate risk measure. Unlike traditional adversarial methods, RADE learns\nrisk-conditioned behaviors directly from data, preserving naturalistic\nmulti-agent interactions with controllable risk levels. To ensure physical\nplausibility, we incorporate a tokenized dynamics check module that efficiently\nfilters generated trajectories using a motion vocabulary. We validate RADE on\nthe real-world rounD dataset, demonstrating that it preserves statistical\nrealism across varying risk levels and naturally increases the likelihood of\nsafety-critical events as the desired risk level grows up. Our results\nhighlight RADE's potential as a scalable and realistic tool for AV safety\nevaluation.\n","authors":["Jiawei Wang","Xintao Yan","Yao Mu","Haowei Sun","Zhong Cao","Henry X. Liu"],"pdf_url":"https://arxiv.org/pdf/2505.03178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03174v1","updated":"2025-05-06T04:38:41Z","published":"2025-05-06T04:38:41Z","title":"Automated Data Curation Using GPS & NLP to Generate Instruction-Action\n  Pairs for Autonomous Vehicle Vision-Language Navigation Datasets","summary":"  Instruction-Action (IA) data pairs are valuable for training robotic systems,\nespecially autonomous vehicles (AVs), but having humans manually annotate this\ndata is costly and time-inefficient. This paper explores the potential of using\nmobile application Global Positioning System (GPS) references and Natural\nLanguage Processing (NLP) to automatically generate large volumes of IA\ncommands and responses without having a human generate or retroactively tag the\ndata. In our pilot data collection, by driving to various destinations and\ncollecting voice instructions from GPS applications, we demonstrate a means to\ncollect and categorize the diverse sets of instructions, further accompanied by\nvideo data to form complete vision-language-action triads. We provide details\non our completely automated data collection prototype system, ADVLAT-Engine. We\ncharacterize collected GPS voice instructions into eight different\nclassifications, highlighting the breadth of commands and referentialities\navailable for curation from freely available mobile applications. Through\nresearch and exploration into the automation of IA data pairs using GPS\nreferences, the potential to increase the speed and volume at which\nhigh-quality IA datasets are created, while minimizing cost, can pave the way\nfor robust vision-language-action (VLA) models to serve tasks in\nvision-language navigation (VLN) and human-interactive autonomous systems.\n","authors":["Guillermo Roque","Erika Maquiling","Jose Giovanni Tapia Lopez","Ross Greer"],"pdf_url":"https://arxiv.org/pdf/2505.03174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18292v3","updated":"2025-05-06T04:16:19Z","published":"2024-12-24T09:00:31Z","title":"Enhancing Multi-Robot Semantic Navigation Through Multimodal\n  Chain-of-Thought Score Collaboration","summary":"  Understanding how humans cooperatively utilize semantic knowledge to explore\nunfamiliar environments and decide on navigation directions is critical for\nhouse service multi-robot systems. Previous methods primarily focused on\nsingle-robot centralized planning strategies, which severely limited\nexploration efficiency. Recent research has considered decentralized planning\nstrategies for multiple robots, assigning separate planning models to each\nrobot, but these approaches often overlook communication costs. In this work,\nwe propose Multimodal Chain-of-Thought Co-Navigation (MCoCoNav), a modular\napproach that utilizes multimodal Chain-of-Thought to plan collaborative\nsemantic navigation for multiple robots. MCoCoNav combines visual perception\nwith Vision Language Models (VLMs) to evaluate exploration value through\nprobabilistic scoring, thus reducing time costs and achieving stable outputs.\nAdditionally, a global semantic map is used as a communication bridge,\nminimizing communication overhead while integrating observational results.\nGuided by scores that reflect exploration trends, robots utilize this map to\nassess whether to explore new frontier points or revisit history nodes.\nExperiments on HM3D_v0.2 and MP3D demonstrate the effectiveness of our\napproach. Our code is available at https://github.com/FrankZxShen/MCoCoNav.git.\n","authors":["Zhixuan Shen","Haonan Luo","Kexun Chen","Fengmao Lv","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2412.18292v3.pdf","comment":"16 pages, 10 figures, Extended Version of accepted AAAI 2025 Paper"},{"id":"http://arxiv.org/abs/2412.11503v2","updated":"2025-05-06T04:15:23Z","published":"2024-12-16T07:25:40Z","title":"Visual-Based Forklift Learning System Enabling Zero-Shot Sim2Real\n  Without Real-World Data","summary":"  Forklifts are used extensively in various industrial settings and are in high\ndemand for automation. In particular, counterbalance forklifts are highly\nversatile and employed in diverse scenarios. However, efforts to automate these\nprocesses are lacking, primarily owing to the absence of a safe and\nperformance-verifiable development environment. This study proposes a learning\nsystem that combines a photorealistic digital learning environment with a\n1/14-scale robotic forklift environment to address this challenge. Inspired by\nthe training-based learning approach adopted by forklift operators, we employ\nan end-to-end vision-based deep reinforcement learning approach. The learning\nis conducted in a digitalized environment created from CAD data, making it safe\nand eliminating the need for real-world data. In addition, we safely validate\nthe method in a physical setting utilizing a 1/14-scale robotic forklift with a\nconfiguration similar to that of a real forklift. We achieved a 60% success\nrate in pallet loading tasks in real experiments using a robotic forklift. Our\napproach demonstrates zero-shot sim2real with a simple method that does not\nrequire heuristic additions. This learning-based approach is considered a first\nstep towards the automation of counterbalance forklifts.\n","authors":["Koshi Oishi","Teruki Kato","Hiroya Makino","Seigo Ito"],"pdf_url":"https://arxiv.org/pdf/2412.11503v2.pdf","comment":"Accepted for publication in: 2025 International Conference on\n  Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2505.03159v1","updated":"2025-05-06T04:12:09Z","published":"2025-05-06T04:12:09Z","title":"Systematic Evaluation of Initial States and Exploration-Exploitation\n  Strategies in PID Auto-Tuning: A Framework-Driven Approach Applied on Mobile\n  Robots","summary":"  PID controllers are widely used in control systems because of their\nsimplicity and effectiveness. Although advanced optimization techniques such as\nBayesian Optimization and Differential Evolution have been applied to address\nthe challenges of automatic tuning of PID controllers, the influence of initial\nsystem states on convergence and the balance between exploration and\nexploitation remains underexplored. Moreover, experimenting the influence\ndirectly on real cyber-physical systems such as mobile robots is crucial for\nderiving realistic insights. In the present paper, a novel framework is\nintroduced to evaluate the impact of systematically varying these factors on\nthe PID auto-tuning processes that utilize Bayesian Optimization and\nDifferential Evolution. Testing was conducted on two distinct PID-controlled\nrobotic platforms, an omnidirectional robot and a differential drive mobile\nrobot, to assess the effects on convergence rate, settling time, rise time, and\novershoot percentage. As a result, the experimental outcomes yield evidence on\nthe effects of the systematic variations, thereby providing an empirical basis\nfor future research studies in the field.\n","authors":["Zaid Ghazal","Ali Al-Bustami","Khouloud Gaaloul","Jaerock Kwon"],"pdf_url":"https://arxiv.org/pdf/2505.03159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04929v2","updated":"2025-05-06T04:11:29Z","published":"2025-03-06T20:00:56Z","title":"Neural Configuration-Space Barriers for Manipulation Planning and\n  Control","summary":"  Planning and control for high-dimensional robot manipulators in cluttered,\ndynamic environments require both computational efficiency and robust safety\nguarantees. Inspired by recent advances in learning configuration-space\ndistance functions (CDFs) as robot body representations, we propose a unified\nframework for motion planning and control that formulates safety constraints as\nCDF barriers. A CDF barrier approximates the local free configuration space,\nsubstantially reducing the number of collision-checking operations during\nmotion planning. However, learning a CDF barrier with a neural network and\nrelying on online sensor observations introduce uncertainties that must be\nconsidered during control synthesis. To address this, we develop a\ndistributionally robust CDF barrier formulation for control that explicitly\naccounts for modeling errors and sensor noise without assuming a known\nunderlying distribution. Simulations and hardware experiments on a 6-DoF xArm\nmanipulator show that our neural CDF barrier formulation enables efficient\nplanning and robust real-time safe control in cluttered and dynamic\nenvironments, relying only on onboard point-cloud observations.\n","authors":["Kehan Long","Ki Myung Brian Lee","Nikola Raicevic","Niyas Attasseri","Melvin Leok","Nikolay Atanasov"],"pdf_url":"https://arxiv.org/pdf/2503.04929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03146v1","updated":"2025-05-06T03:42:16Z","published":"2025-05-06T03:42:16Z","title":"Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot\n  Gait Optimization","summary":"  This paper presents a Long Short-Term Memory network-based Fluid Experiment\nData-Driven model (FED-LSTM) for predicting unsteady, nonlinear hydrodynamic\nforces on the underwater quadruped robot we constructed. Trained on\nexperimental data from leg force and body drag tests conducted in both a\nrecirculating water tank and a towing tank, FED-LSTM outperforms traditional\nEmpirical Formulas (EF) commonly used for flow prediction over flat surfaces.\nThe model demonstrates superior accuracy and adaptability in capturing complex\nfluid dynamics, particularly in straight-line and turning-gait optimizations\nvia the NSGA-II algorithm. FED-LSTM reduces deflection errors during\nstraight-line swimming and improves turn times without increasing the turning\nradius. Hardware experiments further validate the model's precision and\nstability over EF. This approach provides a robust framework for enhancing the\nswimming performance of legged robots, laying the groundwork for future\nadvances in underwater robotic locomotion.\n","authors":["Fei Han","Pengming Guo","Hao Chen","Weikun Li","Jingbo Ren","Naijun Liu","Ning Yang","Dixia Fan"],"pdf_url":"https://arxiv.org/pdf/2505.03146v1.pdf","comment":"This work has been accepted for publication in the IEEE International\n  Conference on Robotics and Automation (ICRA) 2025. The final version will be\n  available in IEEE Xplore (DOI to be assigned upon publication)"},{"id":"http://arxiv.org/abs/2405.18251v4","updated":"2025-05-06T03:11:01Z","published":"2024-05-28T15:02:09Z","title":"Sensor-Based Distributionally Robust Control for Safe Robot Navigation\n  in Dynamic Environments","summary":"  We introduce a novel method for mobile robot navigation in dynamic, unknown\nenvironments, leveraging onboard sensing and distributionally robust\noptimization to impose probabilistic safety constraints. Our method introduces\na distributionally robust control barrier function (DR-CBF) that directly\nintegrates noisy sensor measurements and state estimates to define safety\nconstraints. This approach is applicable to a wide range of control-affine\ndynamics, generalizable to robots with complex geometries, and capable of\noperating at real-time control frequencies. Coupled with a control Lyapunov\nfunction (CLF) for path following, the proposed CLF-DR-CBF control synthesis\nmethod achieves safe, robust, and efficient navigation in challenging\nenvironments. We demonstrate the effectiveness and robustness of our approach\nfor safe autonomous navigation under uncertainty in simulations and real-world\nexperiments with differential-drive robots.\n","authors":["Kehan Long","Yinzhuang Yi","Zhirui Dai","Sylvia Herbert","Jorge Cortés","Nikolay Atanasov"],"pdf_url":"https://arxiv.org/pdf/2405.18251v4.pdf","comment":"Project page: https://existentialrobotics.org/DRO_Safe_Navigation"},{"id":"http://arxiv.org/abs/2505.03128v1","updated":"2025-05-06T03:03:26Z","published":"2025-05-06T03:03:26Z","title":"HCOA*: Hierarchical Class-ordered A* for Navigation in Semantic\n  Environments","summary":"  This paper addresses the problem of robot navigation in mixed geometric and\nsemantic 3D environments. Given a hierarchical representation of the\nenvironment, the objective is to navigate from a start position to a goal while\nminimizing the computational cost. We introduce Hierarchical Class-ordered A*\n(HCOA*), an algorithm that leverages the environmental hierarchy for efficient\npath-planning in semantic graphs, significantly reducing computational effort.\nWe use a total order over the semantic classes and prove theoretical\nperformance guarantees for the algorithm. We propose two approaches for\nhigher-layer node classification based on the node semantics of the lowest\nlayer: a Graph Neural Network-based method and a Majority-Class method. We\nevaluate our approach through simulations on a 3D Scene Graph (3DSG), comparing\nit to the state-of-the-art and assessing its performance against our\nclassification approaches. Results show that HCOA* can find the optimal path\nwhile reducing the number of expanded nodes by 25% and achieving a 16%\nreduction in computational time on the uHumans2 3DSG dataset.\n","authors":["Evangelos Psomiadis","Panagiotis Tsiotras"],"pdf_url":"https://arxiv.org/pdf/2505.03128v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2307.00705v2","updated":"2025-05-06T02:34:19Z","published":"2023-07-03T02:03:29Z","title":"Scratch Team of Single-Rotor Robots and Decentralized Cooperative\n  Transportation with Robot Failure","summary":"  Achieving cooperative transportation by aerial robot teams ensures\nflexibility regarding payloads and robustness against failures, which has\ngarnered significant attention in recent years. This study proposes a flexible\ndecentralized controller for robots and the shapes of payloads in a cooperative\ntransport task using multiple single-rotor robots. The proposed controller is\nrobust to mass and center of mass (COM) fluctuations and robot failures.\nMoreover, it possesses asymptotic stability against dynamics errors.\nAdditionally, the controller supports heterogeneous single-rotor robots. Thus,\nrobots with different specifications and deterioration may be effectively\nutilized for cooperative transportation. This performance is particularly\neffective for robot reuse. To achieve the aforementioned performance, the\ncontroller consists of a parallel structure comprising two controllers: a\nfeedback controller, which renders the system strictly positive real, and a\nnonlinear controller, which renders the object asymptotic to the target. First,\nwe confirm cooperative transportation using 8 and 10 robots for two shapes\nthrough numerical simulation. Subsequently, the cooperative transportation of a\nrectangle payload (with a weight of approximately 3 kg and maximum length of\n1.6 m) is demonstrated using a robot team consisting of three types of robots,\neven under robot failure and fluctuation in the COM.\n","authors":["Koshi Oishi","Yasushi Amano","Jimbo Tomohiko"],"pdf_url":"https://arxiv.org/pdf/2307.00705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19786v3","updated":"2025-05-06T02:24:33Z","published":"2024-09-29T21:17:49Z","title":"Spatio-Temporal Metric-Semantic Mapping for Persistent Orchard\n  Monitoring: Method and Dataset","summary":"  Monitoring orchards at the individual tree or fruit level throughout the\ngrowth season is crucial for plant phenotyping and horticultural resource\noptimization, such as chemical use and yield estimation. We present a 4D\nspatio-temporal metric-semantic mapping system that integrates multi-session\nmeasurements to track fruit growth over time. Our approach combines a LiDAR-RGB\nfusion module for 3D fruit localization with a 4D fruit association method\nleveraging positional, visual, and topology information for improved data\nassociation precision. Evaluated on real orchard data, our method achieves a\n96.9% fruit counting accuracy for 1,790 apples across 60 trees, a mean fruit\nsize estimation error of 1.1 cm, and a 23.7% improvement in 4D data association\nprecision over baselines. We publicly release a multimodal dataset covering\nfive fruit species across their growth seasons at\nhttps://4d-metric-semantic-mapping.org/\n","authors":["Jiuzhou Lei","Ankit Prabhu","Xu Liu","Fernando Cladera","Mehrad Mortazavi","Reza Ehsani","Pratik Chaudhari","Vijay Kumar"],"pdf_url":"https://arxiv.org/pdf/2409.19786v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.01963v2","updated":"2025-05-06T02:10:26Z","published":"2021-11-03T01:10:05Z","title":"Cooperative Transportation using Multiple Single-Rotor Robots and\n  Decentralized Control for Unknown Payloads","summary":"  Cooperative transportation via multiple aerial robots has the potential to\nsupport various payloads and reduce the chances of them being dropped.\nFurthermore, autonomously controlled robots render the system scalable with\nrespect to the payload. In this study, a cooperative transportation system was\ndeveloped using rigidly attached single-rotor robots, and a decentralized\ncontroller was proposed to guarantee asymptotic stability of the error dynamics\nfor unknown strictly positive real systems. A feedback controller was used to\ntransform unstable systems into strictly positive real ones considering the\nshared attachment positions. First, the cooperative transportation of unknown\npayloads with different shapes larger than the carrier robots was investigated\nvia numerical simulations. Second, cooperative transportation of an unknown\npayload (with a weight of approximately 2.7 kg and maximum length of 1.6 m) was\ndemonstrated using eight robots, even under robot failure. Finally, the\nproposed system was shown to be capable of carrying an unknown payload, even if\nthe attachment positions were not shared, that is, even if asymptotic stability\nwas not strictly guaranteed.\n","authors":["Koshi Oishi","Yasushi Amano","Tomohiko Jimbo"],"pdf_url":"https://arxiv.org/pdf/2111.01963v2.pdf","comment":"Accepted for publication in: 2022 International Conference on\n  Robotics and Automation (ICRA), pp. 2024-2030, 2022. doi:\n  10.1109/ICRA46639.2022.9811768"},{"id":"http://arxiv.org/abs/2109.10575v2","updated":"2025-05-06T01:42:02Z","published":"2021-09-22T08:13:32Z","title":"Autonomous Cooperative Transportation System involving Multi-Aerial\n  Robots with Variable Attachment Mechanism","summary":"  Cooperative transportation by multi-aerial robots has the potential to\nsupport various payloads and improve failsafe against dropping. Furthermore,\nchanging the attachment positions of robots according payload characteristics\nincreases the stability of transportation. However, there are almost no\ntransportation systems capable of scaling to the payload weight and size and\nchanging the optimal attachment positions. To address this issue, we propose a\ncooperative transportation system comprising autonomously executable software\nand suitable hardware, covering the entire process, from pre-takeoff setting to\ncontrolled flight. The proposed system decides the formation of the attachment\npositions by prioritizing controllability based on the center of gravity\nobtained from Bayesian estimations with robot pairs. We investigated the\ncooperative transportation of an unknown payload larger than that of whole\ncarrier robots through numerical simulations. Furthermore, we performed\ncooperative transportation of an unknown payload (with a weight of about 3.2 kg\nand maximum length of 1.76 m) using eight robots. The proposed system was found\nto be versatile with regard to handling unknown payloads with different shapes\nand center-of-gravity positions.\n","authors":["Koshi Oishi","Tomohiko Jimbo"],"pdf_url":"https://arxiv.org/pdf/2109.10575v2.pdf","comment":"Accepted for publication in: 2021 IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS), pp. 6322-6328, 2021. doi:\n  10.1109/IROS51168.2021.9636145"},{"id":"http://arxiv.org/abs/2503.10966v2","updated":"2025-05-06T01:23:10Z","published":"2025-03-14T00:21:48Z","title":"Is Your Imitation Learning Policy Better than Mine? Policy Comparison\n  with Near-Optimal Stopping","summary":"  Imitation learning has enabled robots to perform complex, long-horizon tasks\nin challenging dexterous manipulation settings. As new methods are developed,\nthey must be rigorously evaluated and compared against corresponding baselines\nthrough repeated evaluation trials. However, policy comparison is fundamentally\nconstrained by a small feasible sample size (e.g., 10 or 50) due to significant\nhuman effort and limited inference throughput of policies. This paper proposes\na novel statistical framework for rigorously comparing two policies in the\nsmall sample size regime. Prior work in statistical policy comparison relies on\nbatch testing, which requires a fixed, pre-determined number of trials and\nlacks flexibility in adapting the sample size to the observed evaluation data.\nFurthermore, extending the test with additional trials risks inducing\ninadvertent p-hacking, undermining statistical assurances. In contrast, our\nproposed statistical test is sequential, allowing researchers to decide whether\nor not to run more trials based on intermediate results. This adaptively\ntailors the number of trials to the difficulty of the underlying comparison,\nsaving significant time and effort without sacrificing probabilistic\ncorrectness. Extensive numerical simulation and real-world robot manipulation\nexperiments show that our test achieves near-optimal stopping, letting\nresearchers stop evaluation and make a decision in a near-minimal number of\ntrials. Specifically, it reduces the number of evaluation trials by up to 37%\nas compared to state-of-the-art baselines, while preserving the probabilistic\ncorrectness and statistical power of the comparison. Moreover, our method is\nstrongest in the most challenging comparison instances (requiring the most\nevaluation trials); in a multi-task comparison scenario, we save the evaluator\nmore than 200 simulation rollouts.\n","authors":["David Snyder","Asher James Hancock","Apurva Badithela","Emma Dixon","Patrick Miller","Rares Andrei Ambrus","Anirudha Majumdar","Masha Itkina","Haruki Nishimura"],"pdf_url":"https://arxiv.org/pdf/2503.10966v2.pdf","comment":"19 pages, 10 figures, 4 tables. Accepted to RSS 2025"},{"id":"http://arxiv.org/abs/2505.03088v1","updated":"2025-05-06T00:51:13Z","published":"2025-05-06T00:51:13Z","title":"Global Task-aware Fault Detection, Identification For On-Orbit\n  Multi-Spacecraft Collaborative Inspection","summary":"  In this paper, we present a global-to-local task-aware fault detection and\nidentification algorithm to detect failures in a multi-spacecraft system\nperforming a collaborative inspection (referred to as global) task. The\ninspection task is encoded as a cost functional $\\costH$ that informs global\n(task allocation and assignment) and local (agent-level) decision-making. The\nmetric $\\costH$ is a function of the inspection sensor model, and the agent\nfull-pose. We use the cost functional $\\costH$ to design a metric that compares\nthe expected and actual performance to detect the faulty agent using a\nthreshold. We use higher-order cost gradients $\\costH$ to derive a new metric\nto identify the type of fault, including task-specific sensor fault, an\nagent-level actuator, and sensor faults. Furthermore, we propose an approach to\ndesign adaptive thresholds for each fault mentioned above to incorporate the\ntime dependence of the inspection task. We demonstrate the efficacy of the\nproposed method empirically, by simulating and detecting faults (such as\ninspection sensor faults, actuators, and sensor faults) in a low-Earth orbit\ncollaborative spacecraft inspection task using the metrics and the threshold\ndesigned using the global task cost $\\costH$.\n","authors":["Akshita Gupta","Yashwanth Kumar Nakka","Changrak Choi","Amir Rahmani"],"pdf_url":"https://arxiv.org/pdf/2505.03088v1.pdf","comment":"published. 33rd AAS AIAA Conference 2023"},{"id":"http://arxiv.org/abs/2505.03087v1","updated":"2025-05-06T00:49:51Z","published":"2025-05-06T00:49:51Z","title":"Fabrication and Characterization of Additively Manufactured Stretchable\n  Strain Sensors Towards the Shape Sensing of Continuum Robots","summary":"  This letter describes the manufacturing and experimental characterization of\nnovel stretchable strain sensors for continuum robots. The overarching goal of\nthis research is to provide a new solution for the shape sensing of these\ndevices. The sensors are fabricated via direct ink writing, an extrusion-based\nadditive manufacturing technique. Electrically conductive material (i.e., the\n\\textit{ink}) is printed into traces whose electrical resistance varies in\nresponse to mechanical deformation. The principle of operation of stretchable\nstrain sensors is analogous to that of conventional strain gauges, but with a\nsignificantly larger operational window thanks to their ability to withstand\nlarger strain. Among the different conductive materials considered for this\nstudy, we opted to fabricate the sensors with a high-viscosity eutectic\nGallium-Indium ink, which in initial testing exhibited high linearity ($R^2\n\\approx$ 0.99), gauge factor $\\approx$ 1, and negligible drift. Benefits of the\nproposed sensors include (i) ease of fabrication, as they can be conveniently\nprinted in a matter of minutes; (ii) ease of installation, as they can simply\nbe glued to the outside body of a robot; (iii) ease of miniaturization, which\nenables integration into millimiter-sized continuum robots.\n","authors":["Daniel C. Moyer","Wenpeng Wang","Logan S. Karschner","Loris Fichera","Pratap M. Rao"],"pdf_url":"https://arxiv.org/pdf/2505.03087v1.pdf","comment":"Author's manuscript. Accepted for publication in IEEE Robotics and\n  Automation Letters"},{"id":"http://arxiv.org/abs/2505.03077v1","updated":"2025-05-06T00:09:09Z","published":"2025-05-06T00:09:09Z","title":"Latent Adaptive Planner for Dynamic Manipulation","summary":"  This paper presents Latent Adaptive Planner (LAP), a novel approach for\ndynamic nonprehensile manipulation tasks that formulates planning as latent\nspace inference, effectively learned from human demonstration videos. Our\nmethod addresses key challenges in visuomotor policy learning through a\nprincipled variational replanning framework that maintains temporal consistency\nwhile efficiently adapting to environmental changes. LAP employs Bayesian\nupdating in latent space to incrementally refine plans as new observations\nbecome available, striking an optimal balance between computational efficiency\nand real-time adaptability. We bridge the embodiment gap between humans and\nrobots through model-based proportional mapping that regenerates accurate\nkinematic-dynamic joint states and object positions from human demonstrations.\nExperimental evaluations across multiple complex manipulation benchmarks\ndemonstrate that LAP achieves state-of-the-art performance, outperforming\nexisting approaches in success rate, trajectory smoothness, and energy\nefficiency, particularly in dynamic adaptation scenarios. Our approach enables\nrobots to perform complex interactions with human-like adaptability while\nproviding an expandable framework applicable to diverse robotic platforms using\nthe same human demonstration videos.\n","authors":["Donghun Noh","Deqian Kong","Minglu Zhao","Andrew Lizarraga","Jianwen Xie","Ying Nian Wu","Dennis Hong"],"pdf_url":"https://arxiv.org/pdf/2505.03077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06908v4","updated":"2025-05-06T00:07:23Z","published":"2024-05-11T04:33:42Z","title":"To Ask or Not To Ask: Human-in-the-loop Contextual Bandits with\n  Applications in Robot-Assisted Feeding","summary":"  Robot-assisted bite acquisition involves picking up food items with varying\nshapes, compliance, sizes, and textures. Fully autonomous strategies may not\ngeneralize efficiently across this diversity. We propose leveraging feedback\nfrom the care recipient when encountering novel food items. However, frequent\nqueries impose a workload on the user. We formulate human-in-the-loop bite\nacquisition within a contextual bandit framework and introduce LinUCB-QG, a\nmethod that selectively asks for help using a predictive model of querying\nworkload based on query types and timings. This model is trained on data\ncollected in an online study involving 14 participants with mobility\nlimitations, 3 occupational therapists simulating physical limitations, and 89\nparticipants without limitations. We demonstrate that our method better\nbalances task performance and querying workload compared to autonomous and\nalways-querying baselines and adjusts its querying behavior to account for\nhigher workload in users with mobility limitations. We validate this through\nexperiments in a simulated food dataset and a user study with 19 participants,\nincluding one with severe mobility limitations. Please check out our project\nwebsite at: http://emprise.cs.cornell.edu/hilbiteacquisition/\n","authors":["Rohan Banerjee","Rajat Kumar Jenamani","Sidharth Vasudev","Amal Nanavati","Katherine Dimitropoulou","Sarah Dean","Tapomayukh Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2405.06908v4.pdf","comment":"The second and third authors contributed equally. The last two\n  authors advised equally"},{"id":"http://arxiv.org/abs/2409.19617v2","updated":"2025-05-06T23:59:24Z","published":"2024-09-29T09:03:46Z","title":"LiRA: Light-Robust Adversary for Model-based Reinforcement Learning in\n  Real World","summary":"  Model-based reinforcement learning has attracted much attention due to its\nhigh sample efficiency and is expected to be applied to real-world robotic\napplications. In the real world, as unobservable disturbances can lead to\nunexpected situations, robot policies should be taken to improve not only\ncontrol performance but also robustness. Adversarial learning is an effective\nway to improve robustness, but excessive adversary would increase the risk of\nmalfunction, and make the control performance too conservative. Therefore, this\nstudy addresses a new adversarial learning framework to make reinforcement\nlearning robust moderately and not conservative too much. To this end, the\nadversarial learning is first rederived with variational inference. In\naddition, \\textit{light robustness}, which allows for maximizing robustness\nwithin an acceptable performance degradation, is utilized as a constraint. As a\nresult, the proposed framework, so-called LiRA, can automatically adjust\nadversary level, balancing robustness and conservativeness. The expected\nbehaviors of LiRA are confirmed in numerical simulations. In addition, LiRA\nsucceeds in learning a force-reactive gait control of a quadrupedal robot only\nwith real-world data collected less than two hours.\n","authors":["Taisuke Kobayashi"],"pdf_url":"https://arxiv.org/pdf/2409.19617v2.pdf","comment":"21 pages, 17 figures (accepted in Robotics and Autonomous Systems)"},{"id":"http://arxiv.org/abs/2403.11863v2","updated":"2025-05-06T23:56:25Z","published":"2024-03-18T15:17:15Z","title":"Context-aware LLM-based Safe Control Against Latent Risks","summary":"  Autonomous control systems face significant challenges in performing complex\ntasks in the presence of latent risks. To address this, we propose an\nintegrated framework that combines Large Language Models (LLMs), numerical\noptimization, and optimization-based control to facilitate efficient subtask\nlearning while ensuring safety against latent risks. The framework decomposes\ncomplex tasks into a sequence of context-aware subtasks that account for latent\nrisks. These subtasks and their parameters are then refined through a\nmulti-time-scale process: high-layer multi-turn in-context learning, mid-layer\nLLM Chain-of-Thought reasoning and numerical optimization, and low-layer model\npredictive control. The framework iteratively improves decisions by leveraging\nqualitative feedback and optimized trajectory data from lower-layer\noptimization processes and a physics simulator. We validate the proposed\nframework through simulated case studies involving robot arm and autonomous\nvehicle scenarios. The experiments demonstrate that the proposed framework can\nmediate actions based on the context and latent risks and learn complex\nbehaviors efficiently.\n","authors":["Xiyu Deng","Quan Khanh Luu","Anh Van Ho","Yorie Nakahira"],"pdf_url":"https://arxiv.org/pdf/2403.11863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05667v2","updated":"2025-05-06T23:55:42Z","published":"2025-02-08T18:48:42Z","title":"Online Controller Synthesis for Robot Collision Avoidance: A Case Study","summary":"  The inherent uncertainty of dynamic environments poses significant challenges\nfor modeling robot behavior, particularly in tasks such as collision avoidance.\nThis paper presents an online controller synthesis framework tailored for\nrobots equipped with deep learning-based perception components, with a focus on\naddressing distribution shifts. Our approach integrates periodic monitoring and\nrepair mechanisms for the deep neural network perception component, followed by\nuncertainty reassessment. These uncertainty evaluations are injected into a\nparametric discrete-time markov chain, enabling the synthesis of robust\ncontrollers via probabilistic model checking. To ensure high system\navailability during the repair process, we propose a dual-component\nconfiguration that seamlessly transitions between operational states. Through a\ncase study on robot collision avoidance, we demonstrate the efficacy of our\nmethod, showcasing substantial performance improvements over baseline\napproaches. This work provides a comprehensive and scalable solution for\nenhancing the safety and reliability of autonomous systems operating in\nuncertain environments.\n","authors":["Yuheng Fan","Wang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.05667v2.pdf","comment":"The collaborators disagree to publish on this platform"},{"id":"http://arxiv.org/abs/2505.04002v1","updated":"2025-05-06T22:29:07Z","published":"2025-05-06T22:29:07Z","title":"PARC: Physics-based Augmentation with Reinforcement Learning for\n  Character Controllers","summary":"  Humans excel in navigating diverse, complex environments with agile motor\nskills, exemplified by parkour practitioners performing dynamic maneuvers, such\nas climbing up walls and jumping across gaps. Reproducing these agile movements\nwith simulated characters remains challenging, in part due to the scarcity of\nmotion capture data for agile terrain traversal behaviors and the high cost of\nacquiring such data. In this work, we introduce PARC (Physics-based\nAugmentation with Reinforcement Learning for Character Controllers), a\nframework that leverages machine learning and physics-based simulation to\niteratively augment motion datasets and expand the capabilities of terrain\ntraversal controllers. PARC begins by training a motion generator on a small\ndataset consisting of core terrain traversal skills. The motion generator is\nthen used to produce synthetic data for traversing new terrains. However, these\ngenerated motions often exhibit artifacts, such as incorrect contacts or\ndiscontinuities. To correct these artifacts, we train a physics-based tracking\ncontroller to imitate the motions in simulation. The corrected motions are then\nadded to the dataset, which is used to continue training the motion generator\nin the next iteration. PARC's iterative process jointly expands the\ncapabilities of the motion generator and tracker, creating agile and versatile\nmodels for interacting with complex environments. PARC provides an effective\napproach to develop controllers for agile terrain traversal, which bridges the\ngap between the scarcity of motion data and the need for versatile character\ncontrollers.\n","authors":["Michael Xu","Yi Shi","KangKang Yin","Xue Bin Peng"],"pdf_url":"https://arxiv.org/pdf/2505.04002v1.pdf","comment":"SIGGRAPH Conference Papers 2025"},{"id":"http://arxiv.org/abs/2403.14293v2","updated":"2025-05-06T21:25:02Z","published":"2024-03-21T11:00:11Z","title":"Human-Robot Interaction and Perceived Irrationality: A Study of Trust\n  Dynamics and Error Acknowledgment","summary":"  As robots become increasingly integrated into various industries,\nunderstanding how humans respond to robotic failures is critical. This study\nsystematically examines trust dynamics and system design by analyzing human\nreactions to robot failures. We conducted a four-stage survey to explore how\ntrust evolves throughout human-robot interactions. The first stage collected\ndemographic data and initial trust levels. The second stage focused on\npreliminary expectations and perceptions of robotic capabilities. The third\nstage examined interaction details, including robot precision and error\nacknowledgment. Finally, the fourth stage assessed post-interaction\nperceptions, evaluating trust dynamics, forgiveness, and willingness to\nrecommend robotic technologies. Results indicate that trust in robotic systems\nsignificantly increased when robots acknowledged their errors or limitations.\nAdditionally, participants showed greater willingness to suggest robots for\nfuture tasks, highlighting the importance of direct engagement in shaping trust\ndynamics. These findings provide valuable insights for designing more\ntransparent, responsive, and trustworthy robotic systems. By enhancing our\nunderstanding of human-robot interaction (HRI), this study contributes to the\ndevelopment of robotic technologies that foster greater public acceptance and\nadoption.\n","authors":["Ponkoj Chandra Shill","Md. Azizul Hakim"],"pdf_url":"https://arxiv.org/pdf/2403.14293v2.pdf","comment":"8 pages, 8 figures, 1 table, Ongoing Research"},{"id":"http://arxiv.org/abs/2505.03931v1","updated":"2025-05-06T19:09:46Z","published":"2025-05-06T19:09:46Z","title":"NMPC-Lander: Nonlinear MPC with Barrier Function for UAV Landing on a\n  Mobile Platform","summary":"  Quadcopters are versatile aerial robots gaining popularity in numerous\ncritical applications. However, their operational effectiveness is constrained\nby limited battery life and restricted flight range. To address these\nchallenges, autonomous drone landing on stationary or mobile charging and\nbattery-swapping stations has become an essential capability. In this study, we\npresent NMPC-Lander, a novel control architecture that integrates Nonlinear\nModel Predictive Control (NMPC) with Control Barrier Functions (CBF) to achieve\nprecise and safe autonomous landing on both static and dynamic platforms. Our\napproach employs NMPC for accurate trajectory tracking and landing, while\nsimultaneously incorporating CBF to ensure collision avoidance with static\nobstacles. Experimental evaluations on the real hardware demonstrate high\nprecision in landing scenarios, with an average final position error of 9.0 cm\nand 11 cm for stationary and mobile platforms, respectively. Notably,\nNMPC-Lander outperforms the B-spline combined with the A* planning method by\nnearly threefold in terms of position tracking, underscoring its superior\nrobustness and practical effectiveness.\n","authors":["Amber Batool","Faryal Batool","Roohan Ahmed Khan","Muhammad Ahsan Mustafa","Aleksey Fedoseev","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2505.03931v1.pdf","comment":"This manuscript has been submitted to the IEEE International\n  Conference on Systems, Man, and Cybernetics (SMC), 2025"},{"id":"http://arxiv.org/abs/2505.03929v1","updated":"2025-05-06T19:08:53Z","published":"2025-05-06T19:08:53Z","title":"MIHRaGe: A Mixed-Reality Interface for Human-Robot Interaction via\n  Gaze-Oriented Control","summary":"  Individuals with upper limb mobility impairments often require assistive\ntechnologies to perform activities of daily living. While gaze-tracking has\nemerged as a promising method for robotic assistance, existing solutions lack\nsufficient feedback mechanisms, leading to uncertainty in user intent\nrecognition and reduced adaptability. This paper presents the MIHRAGe\ninterface, an integrated system that combines gaze-tracking, robotic\nassistance, and a mixed-reality to create an immersive environment for\ncontrolling the robot using only eye movements. The system was evaluated\nthrough an experimental protocol involving four participants, assessing gaze\naccuracy, robotic positioning precision, and the overall success of a pick and\nplace task. Results showed an average gaze fixation error of 1.46 cm, with\nindividual variations ranging from 1.28 cm to 2.14 cm. The robotic arm\ndemonstrated an average positioning error of +-1.53 cm, with discrepancies\nattributed to interface resolution and calibration constraints. In a pick and\nplace task, the system achieved a success rate of 80%, highlighting its\npotential for improving accessibility in human-robot interaction with visual\nfeedback to the user.\n","authors":["Rafael R. Baptista","Nina R. Gerszberg","Ricardo V. Godoy","Gustavo J. G. Lahr"],"pdf_url":"https://arxiv.org/pdf/2505.03929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03920v1","updated":"2025-05-06T18:50:19Z","published":"2025-05-06T18:50:19Z","title":"Omnidirectional vision sensors based on catadioptric systems with\n  discrete infrared photoreceptors for swarm robotics","summary":"  In this work, we fabricated and studied two designs for omnidirectional\nvision sensors for swarm robotics, based on catadioptric systems consisting of\na mirror with rotational symmetry, eight discrete infrared photodiodes and a\nsingle LED, in order to provide localization and navigation abilities for\nmobile robotic agents. We considered two arrangements for the photodiodes: one\nin which they point upward into the mirror, and one in which they point\noutward, perpendicular to the mirror. To determine which design offers a better\nfield of view on the plane, as well as detection of distance and orientation\nbetween two agents, we developed a test rail with three degrees of freedom to\nexperimentally and systematically measure the signal registered by the\nphotodiodes of a given sensor (in a single readout) from the light emitted by\nanother as functions of the distance and orientation. Afterwards, we processed\nand analyzed the experimental data to develop mathematical models for the mean\nresponse of a photodiode in each design. Finally, by numerically inverting the\nmodels, we compared the two designs in terms of their accuracy. Our results\nshow that the design with the photodiodes pointing upward resolves better the\ndistance, while the other resolves better the orientation of the emitting\nagent, both providing an omnidirectional field of view.\n","authors":["Jose Fernando Contreras-Monsalvo","Victor Dossetti","Blanca Susana Soto-Cruz"],"pdf_url":"https://arxiv.org/pdf/2505.03920v1.pdf","comment":"24 pages, 12 figures"},{"id":"http://arxiv.org/abs/2505.03917v1","updated":"2025-05-06T18:45:50Z","published":"2025-05-06T18:45:50Z","title":"Improving Failure Prediction in Aircraft Fastener Assembly Using\n  Synthetic Data in Imbalanced Datasets","summary":"  Automating aircraft manufacturing still relies heavily on human labor due to\nthe complexity of the assembly processes and customization requirements. One\nkey challenge is achieving precise positioning, especially for large aircraft\nstructures, where errors can lead to substantial maintenance costs or part\nrejection. Existing solutions often require costly hardware or lack\nflexibility. Used in aircraft by the thousands, threaded fasteners, e.g.,\nscrews, bolts, and collars, are traditionally executed by fixed-base robots and\nusually have problems in being deployed in the mentioned manufacturing sites.\nThis paper emphasizes the importance of error detection and classification for\nefficient and safe assembly of threaded fasteners, especially aeronautical\ncollars. Safe assembly of threaded fasteners is paramount since acquiring\nsufficient data for training deep learning models poses challenges due to the\nrarity of failure cases and imbalanced datasets. The paper addresses this by\nproposing techniques like class weighting and data augmentation, specifically\ntailored for temporal series data, to improve classification performance.\nFurthermore, the paper introduces a novel problem-modeling approach,\nemphasizing metrics relevant to collar assembly rather than solely focusing on\naccuracy. This tailored approach enhances the models' capability to handle the\nchallenges of threaded fastener assembly effectively.\n","authors":["Gustavo J. G. Lahr","Ricardo V. Godoy","Thiago H. Segreto","Jose O. Savazzi","Arash Ajoudani","Thiago Boaventura","Glauco A. P. Caurin"],"pdf_url":"https://arxiv.org/pdf/2505.03917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03912v1","updated":"2025-05-06T18:35:07Z","published":"2025-05-06T18:35:07Z","title":"OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation","summary":"  Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.\n","authors":["Can Cui","Pengxiang Ding","Wenxuan Song","Shuanghao Bai","Xinyang Tong","Zirui Ge","Runze Suo","Wanqi Zhou","Yang Liu","Bofang Jia","Han Zhao","Siteng Huang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.05464v3","updated":"2025-05-06T15:48:47Z","published":"2021-03-09T14:54:56Z","title":"Characterizing Trust and Resilience in Distributed Consensus for\n  Cyberphysical Systems","summary":"  This work considers the problem of resilient consensus where stochastic\nvalues of trust between agents are available. Specifically, we derive a unified\nmathematical framework to characterize convergence, deviation of the consensus\nfrom the true consensus value, and expected convergence rate, when there exists\nadditional information of trust between agents. We show that under certain\nconditions on the stochastic trust values and consensus protocol: 1) almost\nsure convergence to a common limit value is possible even when malicious agents\nconstitute more than half of the network connectivity, 2) the deviation of the\nconverged limit, from the case where there is no attack, i.e., the true\nconsensus value, can be bounded with probability that approaches 1\nexponentially, and 3) correct classification of malicious and legitimate agents\ncan be attained in finite time almost surely. Further, the expected convergence\nrate decays exponentially as a function of the quality of the trust\nobservations between agents.\n","authors":["Michal Yemini","Angelia Nedić","Andrea Goldsmith","Stephanie Gil"],"pdf_url":"https://arxiv.org/pdf/2103.05464v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2505.03735v1","updated":"2025-05-06T17:59:31Z","published":"2025-05-06T17:59:31Z","title":"Multi-Agent System for Comprehensive Soccer Understanding","summary":"  Recent advancements in AI-driven soccer understanding have demonstrated rapid\nprogress, yet existing research predominantly focuses on isolated or narrow\ntasks. To bridge this gap, we propose a comprehensive framework for holistic\nsoccer understanding. Specifically, we make the following contributions in this\npaper: (i) we construct SoccerWiki, the first large-scale multimodal soccer\nknowledge base, integrating rich domain knowledge about players, teams,\nreferees, and venues to enable knowledge-driven reasoning; (ii) we present\nSoccerBench, the largest and most comprehensive soccer-specific benchmark,\nfeaturing around 10K standardized multimodal (text, image, video) multi-choice\nQA pairs across 13 distinct understanding tasks, curated through automated\npipelines and manual verification; (iii) we introduce SoccerAgent, a novel\nmulti-agent system that decomposes complex soccer questions via collaborative\nreasoning, leveraging domain expertise from SoccerWiki and achieving robust\nperformance; (iv) extensive evaluations and ablations that benchmark\nstate-of-the-art MLLMs on SoccerBench, highlighting the superiority of our\nproposed agentic system. All data and code are publicly available at:\nhttps://jyrao.github.io/SoccerAgent/.\n","authors":["Jiayuan Rao","Zifeng Li","Haoning Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2505.03735v1.pdf","comment":"Technical Report; Project Page: https://jyrao.github.io/SoccerAgent/"},{"id":"http://arxiv.org/abs/2505.03730v1","updated":"2025-05-06T17:58:02Z","published":"2025-05-06T17:58:02Z","title":"FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios","summary":"  Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/\n","authors":["Shiyi Zhang","Junhao Zhuang","Zhaoyang Zhang","Ying Shan","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2505.03730v1.pdf","comment":"Accepted by Siggraph2025, Project Page:\n  https://shiyi-zh0408.github.io/projectpages/FlexiAct/"},{"id":"http://arxiv.org/abs/2505.03729v1","updated":"2025-05-06T17:57:12Z","published":"2025-05-06T17:57:12Z","title":"Visual Imitation Enables Contextual Humanoid Control","summary":"  How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.\n","authors":["Arthur Allshire","Hongsuk Choi","Junyi Zhang","David McAllister","Anthony Zhang","Chung Min Kim","Trevor Darrell","Pieter Abbeel","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2505.03729v1.pdf","comment":"Project website: https://www.videomimic.net/"},{"id":"http://arxiv.org/abs/2503.15661v2","updated":"2025-05-06T17:43:11Z","published":"2025-03-19T19:26:17Z","title":"UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and\n  Interaction","summary":"  Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks.\n","authors":["Shravan Nayak","Xiangru Jian","Kevin Qinghong Lin","Juan A. Rodriguez","Montek Kalsi","Rabiul Awal","Nicolas Chapados","M. Tamer Özsu","Aishwarya Agrawal","David Vazquez","Christopher Pal","Perouz Taslakian","Spandana Gella","Sai Rajeswar"],"pdf_url":"https://arxiv.org/pdf/2503.15661v2.pdf","comment":"This paper has been accepted to the 41st International Conference on\n  Machine Learning (ICML 2025)"},{"id":"http://arxiv.org/abs/2505.03715v1","updated":"2025-05-06T17:36:49Z","published":"2025-05-06T17:36:49Z","title":"DISARM++: Beyond scanner-free harmonization","summary":"  Harmonization of T1-weighted MR images across different scanners is crucial\nfor ensuring consistency in neuroimaging studies. This study introduces a novel\napproach to direct image harmonization, moving beyond feature standardization\nto ensure that extracted features remain inherently reliable for downstream\nanalysis. Our method enables image transfer in two ways: (1) mapping images to\na scanner-free space for uniform appearance across all scanners, and (2)\ntransforming images into the domain of a specific scanner used in model\ntraining, embedding its unique characteristics. Our approach presents strong\ngeneralization capability, even for unseen scanners not included in the\ntraining phase. We validated our method using MR images from diverse cohorts,\nincluding healthy controls, traveling subjects, and individuals with\nAlzheimer's disease (AD). The model's effectiveness is tested in multiple\napplications, such as brain age prediction (R2 = 0.60 \\pm 0.05), biomarker\nextraction, AD classification (Test Accuracy = 0.86 \\pm 0.03), and diagnosis\nprediction (AUC = 0.95). In all cases, our harmonization technique outperforms\nstate-of-the-art methods, showing improvements in both reliability and\npredictive accuracy. Moreover, our approach eliminates the need for extensive\npreprocessing steps, such as skull-stripping, which can introduce errors by\nmisclassifying brain and non-brain structures. This makes our method\nparticularly suitable for applications that require full-head analysis,\nincluding research on head trauma and cranial deformities. Additionally, our\nharmonization model does not require retraining for new datasets, allowing\nsmooth integration into various neuroimaging workflows. By ensuring\nscanner-invariant image quality, our approach provides a robust and efficient\nsolution for improving neuroimaging studies across diverse settings. The code\nis available at this link.\n","authors":["Luca Caldera","Lara Cavinato","Alessio Cirone","Isabella Cama","Sara Garbarino","Raffaele Lodi","Fabrizio Tagliavini","Anna Nigri","Silvia De Francesco","Andrea Cappozzo","Michele Piana","Francesca Ieva"],"pdf_url":"https://arxiv.org/pdf/2505.03715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01884v2","updated":"2025-05-06T17:26:22Z","published":"2025-05-03T18:18:59Z","title":"Adversarial Robustness of Deep Learning Models for Inland Water Body\n  Segmentation from SAR Images","summary":"  Inland water body segmentation from Synthetic Aperture Radar (SAR) images is\nan important task needed for several applications, such as flood mapping. While\nSAR sensors capture data in all-weather conditions as high-resolution images,\ndifferentiating water and water-like surfaces from SAR images is not\nstraightforward. Inland water bodies, such as large river basins, have complex\ngeometry, which adds to the challenge of segmentation. U-Net is a widely used\ndeep learning model for land-water segmentation of SAR images. In practice,\nmanual annotation is often used to generate the corresponding water masks as\nground truth. Manual annotation of the images is prone to label noise owing to\ndata poisoning attacks, especially due to complex geometry. In this work, we\nsimulate manual errors in the form of adversarial attacks on the U-Net model\nand study the robustness of the model to human errors in annotation. Our\nresults indicate that U-Net can tolerate a certain level of corruption before\nits performance drops significantly. This finding highlights the crucial role\nthat the quality of manual annotations plays in determining the effectiveness\nof the segmentation model. The code and the new dataset, along with adversarial\nexamples for robust training, are publicly available. (GitHub link -\nhttps://github.com/GVCL/IWSeg-SAR-Poison.git)\n","authors":["Siddharth Kothari","Srinivasan Murali","Sankalp Kothari","Ujjwal Verma","Jaya Sreevalsan-Nair"],"pdf_url":"https://arxiv.org/pdf/2505.01884v2.pdf","comment":"21 pages, 15 figures, 2 tables"},{"id":"http://arxiv.org/abs/2505.03703v1","updated":"2025-05-06T17:24:41Z","published":"2025-05-06T17:24:41Z","title":"Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text\n  Representation Learning","summary":"  Vision-language models (VLMs) allow to embed texts and images in a shared\nrepresentation space. However, it has been shown that these models are subject\nto a modality gap phenomenon meaning there exists a clear separation between\nthe embeddings from one modality and another in the embedding space. While this\nmisalignment is detrimental for downstream tasks such as multimodal retrieval,\nmultimodal clustering or zero-shot classification, etc. no generic and\npractical methods have so far been proposed to assess it precisely and even\nreduce it. We therefore propose novel measures and effective techniques\n(spectral- and optimal transport-based methods) to achieve this goal. Extensive\nexperiments conducted on several image-text datasets and models demonstrate\ntheir effectiveness and beneficial effects on downstream tasks. Our code is\navailable at the URL provided in the paper's abstract.\n","authors":["François Role","Sébastien Meyer","Victor Amblard"],"pdf_url":"https://arxiv.org/pdf/2505.03703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07775v5","updated":"2025-05-06T17:24:17Z","published":"2024-12-10T18:59:58Z","title":"Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed\n  GFlowNets","summary":"  While one commonly trains large diffusion models by collecting datasets on\ntarget downstream tasks, it is often desired to align and finetune pretrained\ndiffusion models with some reward functions that are either designed by experts\nor learned from small-scale datasets. Existing post-training methods for reward\nfinetuning of diffusion models typically suffer from lack of diversity in\ngenerated samples, lack of prior preservation, and/or slow convergence in\nfinetuning. In response to this challenge, we take inspiration from recent\nsuccesses in generative flow networks (GFlowNets) and propose a reinforcement\nlearning method for diffusion model finetuning, dubbed Nabla-GFlowNet\n(abbreviated as $\\nabla$-GFlowNet), that leverages the rich signal in reward\ngradients for probabilistic diffusion finetuning. We show that our proposed\nmethod achieves fast yet diversity- and prior-preserving finetuning of Stable\nDiffusion, a large-scale text-conditioned image diffusion model, on different\nrealistic reward functions.\n","authors":["Zhen Liu","Tim Z. Xiao","Weiyang Liu","Yoshua Bengio","Dinghuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.07775v5.pdf","comment":"Technical Report (37 pages, 31 figures), Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2505.03702v1","updated":"2025-05-06T17:22:21Z","published":"2025-05-06T17:22:21Z","title":"Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid\n  Geometric-Neural Approach","summary":"  Automating leaf manipulation in agricultural settings faces significant\nchallenges, including the variability of plant morphologies and deformable\nleaves. We propose a novel hybrid geometric-neural approach for autonomous leaf\ngrasping that combines traditional computer vision with neural networks through\nself-supervised learning. Our method integrates YOLOv8 for instance\nsegmentation and RAFT-Stereo for 3D depth estimation to build rich leaf\nrepresentations, which feed into both a geometric feature scoring pipeline and\na neural refinement module (GraspPointCNN). The key innovation is our\nconfidence-weighted fusion mechanism that dynamically balances the contribution\nof each approach based on prediction certainty. Our self-supervised framework\nuses the geometric pipeline as an expert teacher to automatically generate\ntraining data. Experiments demonstrate that our approach achieves an 88.0%\nsuccess rate in controlled environments and 84.7% in real greenhouse\nconditions, significantly outperforming both purely geometric (75.3%) and\nneural (60.2%) methods. This work establishes a new paradigm for agricultural\nrobotics where domain expertise is seamlessly integrated with machine learning\ncapabilities, providing a foundation for fully automated crop monitoring\nsystems.\n","authors":["Srecharan Selvam","Abhishesh Silwal","George Kanter"],"pdf_url":"https://arxiv.org/pdf/2505.03702v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.13896v4","updated":"2025-05-06T17:17:31Z","published":"2024-06-19T23:53:31Z","title":"SMORE: Simultaneous Map and Object REconstruction","summary":"  We present a method for dynamic surface reconstruction of large-scale urban\nscenes from LiDAR. Depth-based reconstructions tend to focus on small-scale\nobjects or large-scale SLAM reconstructions that treat moving objects as\noutliers. We take a holistic perspective and optimize a compositional model of\na dynamic scene that decomposes the world into rigidly-moving objects and the\nbackground. To achieve this, we take inspiration from recent novel view\nsynthesis methods and frame the reconstruction problem as a global optimization\nover neural surfaces, ego poses, and object poses, which minimizes the error\nbetween composed spacetime surfaces and input LiDAR scans. In contrast to view\nsynthesis methods, which typically minimize 2D errors with gradient descent, we\nminimize a 3D point-to-surface error by coordinate descent, which we decompose\ninto registration and surface reconstruction steps. Each step can be handled\nwell by off-the-shelf methods without any re-training. We analyze the surface\nreconstruction step for rolling-shutter LiDARs, and show that deskewing\noperations common in continuous time SLAM can be applied to dynamic objects as\nwell, improving results over prior art by an order of magnitude. Beyond\npursuing dynamic reconstruction as a goal in and of itself, we propose that\nsuch a system can be used to auto-label partially annotated sequences and\nproduce ground truth annotation for hard-to-label problems such as depth\ncompletion and scene flow. Please see https://anishmadan23.github.io/smore/ for\nmore visual results.\n","authors":["Nathaniel Chodosh","Anish Madan","Simon Lucey","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2406.13896v4.pdf","comment":"3DV 2025,CVPR 2025 4D Vision Workshop"},{"id":"http://arxiv.org/abs/2303.12675v4","updated":"2025-05-06T17:02:55Z","published":"2023-03-22T16:14:39Z","title":"VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector\n  Fonts via Signed Distance Functions","summary":"  Font design is of vital importance in the digital content design and modern\nprinting industry. Developing algorithms capable of automatically synthesizing\nvector fonts can significantly facilitate the font design process. However,\nexisting methods mainly concentrate on raster image generation, and only a few\napproaches can directly synthesize vector fonts. This paper proposes an\nend-to-end trainable method, VecFontSDF, to reconstruct and synthesize\nhigh-quality vector fonts using signed distance functions (SDFs). Specifically,\nbased on the proposed SDF-based implicit shape representation, VecFontSDF\nlearns to model each glyph as shape primitives enclosed by several parabolic\ncurves, which can be precisely converted to quadratic B\\'ezier curves that are\nwidely used in vector font products. In this manner, most image generation\nmethods can be easily extended to synthesize vector fonts. Qualitative and\nquantitative experiments conducted on a publicly-available dataset demonstrate\nthat our method obtains high-quality results on several tasks, including vector\nfont reconstruction, interpolation, and few-shot vector font synthesis,\nmarkedly outperforming the state of the art. Our code and trained models are\navailable at https://xiazeqing.github.io/VecFontSDF.\n","authors":["Zeqing Xia","Bojun Xiong","Zhouhui Lian"],"pdf_url":"https://arxiv.org/pdf/2303.12675v4.pdf","comment":"Accepted to CVPR 2023. Project Page:\n  https://xiazeqing.github.io/VecFontSDF"},{"id":"http://arxiv.org/abs/2505.03692v1","updated":"2025-05-06T16:54:07Z","published":"2025-05-06T16:54:07Z","title":"Matching Distance and Geometric Distribution Aided Learning Multiview\n  Point Cloud Registration","summary":"  Multiview point cloud registration plays a crucial role in robotics,\nautomation, and computer vision fields. This paper concentrates on pose graph\nconstruction and motion synchronization within multiview registration. Previous\nmethods for pose graph construction often pruned fully connected graphs or\nconstructed sparse graph using global feature aggregated from local\ndescriptors, which may not consistently yield reliable results. To identify\ndependable pairs for pose graph construction, we design a network model that\nextracts information from the matching distance between point cloud pairs. For\nmotion synchronization, we propose another neural network model to calculate\nthe absolute pose in a data-driven manner, rather than optimizing inaccurate\nhandcrafted loss functions. Our model takes into account geometric distribution\ninformation and employs a modified attention mechanism to facilitate flexible\nand reliable feature interaction. Experimental results on diverse indoor and\noutdoor datasets confirm the effectiveness and generalizability of our\napproach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.\n","authors":["Shiqi Li","Jihua Zhu","Yifan Xie","Naiwen Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14284v3","updated":"2025-05-06T16:45:30Z","published":"2023-11-24T05:17:01Z","title":"Paragraph-to-Image Generation with Information-Enriched Diffusion Model","summary":"  Text-to-image (T2I) models have recently experienced rapid development,\nachieving astonishing performance in terms of fidelity and textual alignment\ncapabilities. However, given a long paragraph (up to 512 words), these\ngeneration models still struggle to achieve strong alignment and are unable to\ngenerate images depicting complex scenes. In this paper, we introduce an\ninformation-enriched diffusion model for paragraph-to-image generation task,\ntermed ParaDiffusion, which delves into the transference of the extensive\nsemantic comprehension capabilities of large language models to the task of\nimage generation. At its core is using a large language model (e.g., Llama V2)\nto encode long-form text, followed by fine-tuning with LORA to alignthe\ntext-image feature spaces in the generation task. To facilitate the training of\nlong-text semantic alignment, we also curated a high-quality paragraph-image\npair dataset, namely ParaImage. This dataset contains a small amount of\nhigh-quality, meticulously annotated data, and a large-scale synthetic dataset\nwith long text descriptions being generated using a vision-language model.\nExperiments demonstrate that ParaDiffusion outperforms state-of-the-art models\n(SD XL, DeepFloyd IF) on ViLG-300 and ParaPrompts, achieving up to 15% and 45%\nhuman voting rate improvements for visual appeal and text faithfulness,\nrespectively. The code and dataset will be released to foster community\nresearch on long-text alignment.\n","authors":["Weijia Wu","Zhuang Li","Yefei He","Mike Zheng Shou","Chunhua Shen","Lele Cheng","Yan Li","Tingting Gao","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.14284v3.pdf","comment":"The project website is at:\n  https://weijiawu.github.io/ParaDiffusionPage/. Code:\n  https://github.com/weijiawu/ParaDiffusion"},{"id":"http://arxiv.org/abs/2411.15702v2","updated":"2025-05-06T16:30:58Z","published":"2024-11-24T04:07:33Z","title":"Editable-DeepSC: Reliable Cross-Modal Semantic Communications for Facial\n  Editing","summary":"  Real-time computer vision (CV) plays a crucial role in various real-world\napplications, whose performance is highly dependent on communication networks.\nNonetheless, the data-oriented characteristics of conventional communications\noften do not align with the special needs of real-time CV tasks. To alleviate\nthis issue, the recently emerged semantic communications only transmit\ntask-related semantic information and exhibit a promising landscape to address\nthis problem. However, the communication challenges associated with Semantic\nFacial Editing, one of the most important real-time CV applications on social\nmedia, still remain largely unexplored. In this paper, we fill this gap by\nproposing Editable-DeepSC, a novel cross-modal semantic communication approach\nfor facial editing. Firstly, we theoretically discuss different transmission\nschemes that separately handle communications and editings, and emphasize the\nnecessity of Joint Editing-Channel Coding (JECC) via iterative attributes\nmatching, which integrates editings into the communication chain to preserve\nmore semantic mutual information. To compactly represent the high-dimensional\ndata, we leverage inversion methods via pre-trained StyleGAN priors for\nsemantic coding. To tackle the dynamic channel noise conditions, we propose\nSNR-aware channel coding via model fine-tuning. Extensive experiments indicate\nthat Editable-DeepSC can achieve superior editings while significantly saving\nthe transmission bandwidth, even under high-resolution and out-of-distribution\n(OOD) settings.\n","authors":["Bin Chen","Wenbo Yu","Qinshan Zhang","Tianqu Zhuang","Yong Jiang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2411.15702v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03679v1","updated":"2025-05-06T16:25:38Z","published":"2025-05-06T16:25:38Z","title":"CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point\n  Cloud Fusion and Zero-Shot Image Inpainting","summary":"  Segmenting objects in an environment is a crucial task for autonomous driving\nand robotics, as it enables a better understanding of the surroundings of each\nagent. Although camera sensors provide rich visual details, they are vulnerable\nto adverse weather conditions. In contrast, radar sensors remain robust under\nsuch conditions, but often produce sparse and noisy data. Therefore, a\npromising approach is to fuse information from both sensors. In this work, we\npropose a novel framework to enhance camera-only baselines by integrating a\ndiffusion model into a camera-radar fusion architecture. We leverage radar\npoint features to create pseudo-masks using the Segment-Anything model,\ntreating the projected radar points as point prompts. Additionally, we propose\na noise reduction unit to denoise these pseudo-masks, which are further used to\ngenerate inpainted images that complete the missing information in the original\nimages. Our method improves the camera-only segmentation baseline by 2.63% in\nmIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the\nWaterscenes dataset. This demonstrates the effectiveness of our approach for\nsemantic segmentation using camera-radar fusion under adverse weather\nconditions.\n","authors":["Huawei Sun","Bora Kunter Sahin","Georg Stettinger","Maximilian Bernhard","Matthias Schubert","Robert Wille"],"pdf_url":"https://arxiv.org/pdf/2505.03679v1.pdf","comment":"Accepted at RA-L 2025"},{"id":"http://arxiv.org/abs/2505.03667v1","updated":"2025-05-06T16:07:12Z","published":"2025-05-06T16:07:12Z","title":"Distribution-Conditional Generation: From Class Distribution to Creative\n  Generation","summary":"  Text-to-image (T2I) diffusion models are effective at producing semantically\naligned images, but their reliance on training data distributions limits their\nability to synthesize truly novel, out-of-distribution concepts. Existing\nmethods typically enhance creativity by combining pairs of known concepts,\nyielding compositions that, while out-of-distribution, remain linguistically\ndescribable and bounded within the existing semantic space. Inspired by the\nsoft probabilistic outputs of classifiers on ambiguous inputs, we propose\nDistribution-Conditional Generation, a novel formulation that models creativity\nas image synthesis conditioned on class distributions, enabling semantically\nunconstrained creative generation. Building on this, we propose DisTok, an\nencoder-decoder framework that maps class distributions into a latent space and\ndecodes them into tokens of creative concept. DisTok maintains a dynamic\nconcept pool and iteratively sampling and fusing concept pairs, enabling the\ngeneration of tokens aligned with increasingly complex class distributions. To\nenforce distributional consistency, latent vectors sampled from a Gaussian\nprior are decoded into tokens and rendered into images, whose class\ndistributions-predicted by a vision-language model-supervise the alignment\nbetween input distributions and the visual semantics of generated tokens. The\nresulting tokens are added to the concept pool for subsequent composition.\nExtensive experiments demonstrate that DisTok, by unifying\ndistribution-conditioned fusion and sampling-based synthesis, enables efficient\nand flexible token-level generation, achieving state-of-the-art performance\nwith superior text-image alignment and human preference scores.\n","authors":["Fu Feng","Yucheng Xie","Xu Yang","Jing Wang","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2505.03667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03662v1","updated":"2025-05-06T16:05:22Z","published":"2025-05-06T16:05:22Z","title":"Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps\n  from T1-Weighted MRI using CycleGAN Models","summary":"  Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are\nessential for evaluating white matter integrity and structural connectivity in\nneuroimaging. However, the spatial misalignment between FA maps and\ntractography atlases hinders their effective integration into predictive\nmodels. To address this issue, we propose a CycleGAN based approach for\ngenerating FA maps directly from T1-weighted MRI scans, representing the first\napplication of this technique to both healthy and tumour-affected tissues. Our\nmodel, trained on unpaired data, produces high fidelity maps, which have been\nrigorously evaluated using Structural Similarity Index (SSIM) and Peak\nSignal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in\ntumour regions. Radiological assessments further underscore the model's\npotential to enhance clinical workflows by providing an AI-driven alternative\nthat reduces the necessity for additional scans.\n","authors":["Xin Du","Francesca M. Cozzi","Rajesh Jena"],"pdf_url":"https://arxiv.org/pdf/2505.03662v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2505.03654v1","updated":"2025-05-06T16:00:13Z","published":"2025-05-06T16:00:13Z","title":"ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language\n  and Vision Assistant","summary":"  Recent advances in personalized MLLMs enable effective capture of\nuser-specific concepts, supporting both recognition of personalized concepts\nand contextual captioning. However, humans typically explore and reason over\nrelations among objects and individuals, transcending surface-level information\nto achieve more personalized and contextual understanding. To this end,\nexisting methods may face three main limitations: Their training data lacks\nmulti-object sets in which relations among objects are learnable. Building on\nthe limited training data, their models overlook the relations between\ndifferent personalized concepts and fail to reason over them. Their experiments\nmainly focus on a single personalized concept, where evaluations are limited to\nrecognition and captioning tasks. To address the limitations, we present a new\ndataset named ReGraP, consisting of 120 sets of personalized knowledge. Each\nset includes images, KGs, and CoT QA pairs derived from the KGs, enabling more\nstructured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an\nMLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard\ngraph prompting methods are designed to align KGs within the model's semantic\nspace. We establish the ReGraP Benchmark, which contains diverse task types:\nmultiple-choice, fill-in-the-blank, True/False, and descriptive questions in\nboth open- and closed-ended settings. The proposed benchmark is designed to\nevaluate the relational reasoning and knowledge-connection capability of\npersonalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and\nother competitive MLLMs. Results show that the proposed model not only learns\npersonalized knowledge but also performs relational reasoning in responses,\nachieving the SoTA performance compared with the competitive methods. All the\ncodes and datasets are released at: https://github.com/xyfyyds/ReGraP.\n","authors":["Yifan Xiang","Zhenxi Zhang","Bin Li","Yixuan Weng","Shoujun Zhou","Yangfan He","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2505.03654v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2504.17761v3","updated":"2025-05-06T15:58:40Z","published":"2025-04-24T17:25:12Z","title":"Step1X-Edit: A Practical Framework for General Image Editing","summary":"  In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.\n","authors":["Shiyu Liu","Yucheng Han","Peng Xing","Fukun Yin","Rui Wang","Wei Cheng","Jiaqi Liao","Yingming Wang","Honghao Fu","Chunrui Han","Guopeng Li","Yuang Peng","Quan Sun","Jingwei Wu","Yan Cai","Zheng Ge","Ranchen Ming","Lei Xia","Xianfang Zeng","Yibo Zhu","Binxing Jiao","Xiangyu Zhang","Gang Yu","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2504.17761v3.pdf","comment":"code: https://github.com/stepfun-ai/Step1X-Edit"},{"id":"http://arxiv.org/abs/2505.03646v1","updated":"2025-05-06T15:52:14Z","published":"2025-05-06T15:52:14Z","title":"ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders","summary":"  Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples.\n","authors":["Chethan Krishnamurthy Ramanaik","Arjun Roy","Eirini Ntoutsi"],"pdf_url":"https://arxiv.org/pdf/2505.03646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03638v1","updated":"2025-05-06T15:40:14Z","published":"2025-05-06T15:40:14Z","title":"Towards Smart Point-and-Shoot Photography","summary":"  Hundreds of millions of people routinely take photos using their smartphones\nas point and shoot (PAS) cameras, yet very few would have the photography\nskills to compose a good shot of a scene. While traditional PAS cameras have\nbuilt-in functions to ensure a photo is well focused and has the right\nbrightness, they cannot tell the users how to compose the best shot of a scene.\nIn this paper, we present a first of its kind smart point and shoot (SPAS)\nsystem to help users to take good photos. Our SPAS proposes to help users to\ncompose a good shot of a scene by automatically guiding the users to adjust the\ncamera pose live on the scene. We first constructed a large dataset containing\n320K images with camera pose information from 4000 scenes. We then developed an\ninnovative CLIP-based Composition Quality Assessment (CCQA) model to assign\npseudo labels to these images. The CCQA introduces a unique learnable text\nembedding technique to learn continuous word embeddings capable of discerning\nsubtle visual quality differences in the range covered by five levels of\nquality description words {bad, poor, fair, good, perfect}. And finally we have\ndeveloped a camera pose adjustment model (CPAM) which first determines if the\ncurrent view can be further improved and if so it outputs the adjust suggestion\nin the form of two camera pose adjustment angles. The two tasks of CPAM make\ndecisions in a sequential manner and each involves different sets of training\nsamples, we have developed a mixture-of-experts model with a gated loss\nfunction to train the CPAM in an end-to-end manner. We will present extensive\nresults to demonstrate the performances of our SPAS system using publicly\navailable image composition datasets.\n","authors":["Jiawan Li","Fei Zhou","Zhipeng Zhong","Jiongzhi Lin","Guoping Qiu"],"pdf_url":"https://arxiv.org/pdf/2505.03638v1.pdf","comment":"CVPR2025 Accepted"},{"id":"http://arxiv.org/abs/2404.05046v2","updated":"2025-05-06T15:33:55Z","published":"2024-04-07T19:00:45Z","title":"FGAIF: Aligning Large Vision-Language Models with Fine-grained AI\n  Feedback","summary":"  Large Vision-Language Models (LVLMs) have demonstrated proficiency in\ntackling a variety of visual-language tasks. However, current LVLMs suffer from\nmisalignment between text and image modalities which causes three kinds of\nhallucination problems, i.e., object existence, object attribute, and object\nrelationship. To tackle this issue, existing methods mainly utilize\nReinforcement Learning (RL) to align modalities in LVLMs. However, they still\nsuffer from three main limitations: (1) General feedback can not indicate the\nhallucination type contained in the response; (2) Sparse rewards only give the\nsequence-level reward for the whole response; and (3)Annotation cost is\ntime-consuming and labor-intensive. To handle these limitations, we propose an\ninnovative method to align modalities in LVLMs through Fine-Grained Artificial\nIntelligence Feedback (FGAIF), which mainly consists of three steps: AI-based\nFeedback Collection, Fine-grained Reward Model Training, and Reinforcement\nLearning with Fine-grained Reward. Specifically, We first utilize AI tools to\npredict the types of hallucination for each segment in the response and obtain\na collection of fine-grained feedback. Then, based on the collected reward\ndata, three specialized reward models are trained to produce dense rewards.\nFinally, a novel fine-grained feedback module is integrated into the Proximal\nPolicy Optimization (PPO) algorithm. Extensive experiments are conducted on\nhallucination and general benchmarks, demonstrating the superior performance of\nour proposed method. Notably, compared with previous models trained with the\nRL-based aligning method, our proposed method is effective even with fewer\nparameters.\n","authors":["Liqiang Jing","Xinya Du"],"pdf_url":"https://arxiv.org/pdf/2404.05046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03631v1","updated":"2025-05-06T15:29:32Z","published":"2025-05-06T15:29:32Z","title":"Breaking Annotation Barriers: Generalized Video Quality Assessment via\n  Ranking-based Self-Supervision","summary":"  Video quality assessment (VQA) is essential for quantifying perceptual\nquality in various video processing workflows, spanning from camera capture\nsystems to over-the-top streaming platforms. While recent supervised VQA models\nhave made substantial progress, the reliance on manually annotated datasets --\na process that is labor-intensive, costly, and difficult to scale up -- has\nhindered further optimization of their generalization to unseen video content\nand distortions. To bridge this gap, we introduce a self-supervised learning\nframework for VQA to learn quality assessment capabilities from large-scale,\nunlabeled web videos. Our approach leverages a \\textbf{learning-to-rank}\nparadigm to train a large multimodal model (LMM) on video pairs automatically\nlabeled via two manners, including quality pseudo-labeling by existing VQA\nmodels and relative quality ranking based on synthetic distortion simulations.\nFurthermore, we introduce a novel \\textbf{iterative self-improvement training\nstrategy}, where the trained model acts an improved annotator to iteratively\nrefine the annotation quality of training data. By training on a dataset\n$10\\times$ larger than the existing VQA benchmarks, our model: (1) achieves\nzero-shot performance on in-domain VQA benchmarks that matches or surpasses\nsupervised models; (2) demonstrates superior out-of-distribution (OOD)\ngeneralization across diverse video content and distortions; and (3) sets a new\nstate-of-the-art when fine-tuned on human-labeled datasets. Extensive\nexperimental results validate the effectiveness of our self-supervised approach\nin training generalized VQA models. The datasets and code will be publicly\nreleased to facilitate future research.\n","authors":["Linhan Cao","Wei Sun","Kaiwei Zhang","Yicong Peng","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2505.03631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08474v3","updated":"2025-05-06T15:25:00Z","published":"2024-09-13T02:00:16Z","title":"Rethinking Meta-Learning from a Learning Lens","summary":"  Meta-learning seeks to learn a well-generalized model initialization from\ntraining tasks to solve unseen tasks. From the \"learning to learn\" perspective,\nthe quality of the initialization is modeled with one-step gradient decent in\nthe inner loop. However, contrary to theoretical expectations, our empirical\nanalysis reveals that this may expose meta-learning to underfitting. To bridge\nthe gap between theoretical understanding and practical implementation, we\nreconsider meta-learning from the \"Learning\" lens. We propose that the\nmeta-learning model comprises two interrelated components: parameters for model\ninitialization and a meta-layer for task-specific fine-tuning. These components\nwill lead to the risks of overfitting and underfitting depending on tasks, and\ntheir solutions, fewer parameters vs. more meta-layer, are often in conflict.\nTo address this, we aim to regulate the task information the model receives\nwithout modifying the data or model structure. Our theoretical analysis\nindicates that models adapted to different tasks can mutually reinforce each\nother, highlighting the effective information. Based on this insight, we\npropose TRLearner, a plug-and-play method that leverages task relation to\ncalibrate meta-learning. It first extracts task relation matrices and then\napplies relation-aware consistency regularization to guide optimization.\nExtensive theoretical and empirical evaluations demonstrate its effectiveness.\n","authors":["Jingyao Wang","Wenwen Qiang","Changwen Zheng","Hui Xiong","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2409.08474v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12240v3","updated":"2025-05-06T15:23:12Z","published":"2025-04-16T16:45:19Z","title":"Cobra: Efficient Line Art COlorization with BRoAder References","summary":"  The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.\n","authors":["Junhao Zhuang","Lingen Li","Xuan Ju","Zhaoyang Zhang","Chun Yuan","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2504.12240v3.pdf","comment":"Project page with code: https://zhuang2002.github.io/Cobra/"},{"id":"http://arxiv.org/abs/2505.03623v1","updated":"2025-05-06T15:21:36Z","published":"2025-05-06T15:21:36Z","title":"Bounding Box-Guided Diffusion for Synthesizing Industrial Images and\n  Segmentation Map","summary":"  Synthetic dataset generation in Computer Vision, particularly for industrial\napplications, is still underexplored. Industrial defect segmentation, for\ninstance, requires highly accurate labels, yet acquiring such data is costly\nand time-consuming. To address this challenge, we propose a novel\ndiffusion-based pipeline for generating high-fidelity industrial datasets with\nminimal supervision. Our approach conditions the diffusion model on enriched\nbounding box representations to produce precise segmentation masks, ensuring\nrealistic and accurately localized defect synthesis. Compared to existing\nlayout-conditioned generative methods, our approach improves defect consistency\nand spatial accuracy. We introduce two quantitative metrics to evaluate the\neffectiveness of our method and assess its impact on a downstream segmentation\ntask trained on real and synthetic data. Our results demonstrate that\ndiffusion-based synthesis can bridge the gap between artificial and real-world\nindustrial data, fostering more reliable and cost-efficient segmentation\nmodels. The code is publicly available at\nhttps://github.com/covisionlab/diffusion_labeling.\n","authors":["Alessandro Simoni","Francesco Pelosin"],"pdf_url":"https://arxiv.org/pdf/2505.03623v1.pdf","comment":"Accepted at Synthetic Data for Computer Vision Workshop - CVPR 2025"},{"id":"http://arxiv.org/abs/2505.03621v1","updated":"2025-05-06T15:18:38Z","published":"2025-05-06T15:18:38Z","title":"PhysLLM: Harnessing Large Language Models for Cross-Modal Remote\n  Physiological Sensing","summary":"  Remote photoplethysmography (rPPG) enables non-contact physiological\nmeasurement but remains highly susceptible to illumination changes, motion\nartifacts, and limited temporal modeling. Large Language Models (LLMs) excel at\ncapturing long-range dependencies, offering a potential solution but struggle\nwith the continuous, noise-sensitive nature of rPPG signals due to their\ntext-centric design. To bridge this gap, we introduce PhysLLM, a collaborative\noptimization framework that synergizes LLMs with domain-specific rPPG\ncomponents. Specifically, the Text Prototype Guidance (TPG) strategy is\nproposed to establish cross-modal alignment by projecting hemodynamic features\ninto LLM-interpretable semantic space, effectively bridging the\nrepresentational gap between physiological signals and linguistic tokens.\nBesides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for\nresolving signal instability through adaptive time-frequency domain feature\nre-weighting. Finally, rPPG task-specific cues systematically inject\nphysiological priors through physiological statistics, environmental contextual\nanswering, and task description, leveraging cross-modal learning to integrate\nboth visual and textual information, enabling dynamic adaptation to challenging\nscenarios like variable illumination and subject movements. Evaluation on four\nbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,\ndemonstrating superior generalization across lighting variations and motion\nscenarios.\n","authors":["Yiping Xie","Bo Zhao","Mingtong Dai","Jian-Ping Zhou","Yue Sun","Tao Tan","Weicheng Xie","Linlin Shen","Zitong Yu"],"pdf_url":"https://arxiv.org/pdf/2505.03621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02648v2","updated":"2025-05-06T15:18:25Z","published":"2025-05-05T13:50:03Z","title":"MCCD: Multi-Agent Collaboration-based Compositional Diffusion for\n  Complex Text-to-Image Generation","summary":"  Diffusion models have shown excellent performance in text-to-image\ngeneration. Nevertheless, existing methods often suffer from performance\nbottlenecks when handling complex prompts that involve multiple objects,\ncharacteristics, and relations. Therefore, we propose a Multi-agent\nCollaboration-based Compositional Diffusion (MCCD) for text-to-image generation\nfor complex scenes. Specifically, we design a multi-agent collaboration-based\nscene parsing module that generates an agent system comprising multiple agents\nwith distinct tasks, utilizing MLLMs to extract various scene elements\neffectively. In addition, Hierarchical Compositional diffusion utilizes a\nGaussian mask and filtering to refine bounding box regions and enhance objects\nthrough region enhancement, resulting in the accurate and high-fidelity\ngeneration of complex scenes. Comprehensive experiments demonstrate that our\nMCCD significantly improves the performance of the baseline models in a\ntraining-free manner, providing a substantial advantage in complex scene\ngeneration.\n","authors":["Mingcheng Li","Xiaolu Hou","Ziyang Liu","Dingkang Yang","Ziyun Qian","Jiawei Chen","Jinjie Wei","Yue Jiang","Qingyao Xu","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.02648v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03611v1","updated":"2025-05-06T15:09:37Z","published":"2025-05-06T15:09:37Z","title":"Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using\n  Only Real Face Images","summary":"  Face anti-spoofing is a critical technology for ensuring the security of face\nrecognition systems. However, its ability to generalize across diverse\nscenarios remains a significant challenge. In this paper, we attribute the\nlimited generalization ability to two key factors: covariate shift, which\narises from external data collection variations, and semantic shift, which\nresults from substantial differences in emerging attack types. To address both\nchallenges, we propose a novel approach for learning unknown spoof prompts,\nrelying solely on real face images from a single source domain. Our method\ngenerates textual prompts for real faces and potential unknown spoof attacks by\nleveraging the general knowledge embedded in vision-language models, thereby\nenhancing the model's ability to generalize to unseen target domains.\nSpecifically, we introduce a diverse spoof prompt optimization framework to\nlearn effective prompts. This framework constrains unknown spoof prompts within\na relaxed prior knowledge space while maximizing their distance from real face\nimages. Moreover, it enforces semantic independence among different spoof\nprompts to capture a broad range of spoof patterns. Experimental results on\nnine datasets demonstrate that the learned prompts effectively transfer the\nknowledge of vision-language models, enabling state-of-the-art generalization\nability against diverse unknown attack types across unseen target domains\nwithout using any spoof face images.\n","authors":["Fangling Jiang","Qi Li","Weining Wang","Wei Shen","Bing Liu","Zhenan Sun"],"pdf_url":"https://arxiv.org/pdf/2505.03611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03610v1","updated":"2025-05-06T15:09:23Z","published":"2025-05-06T15:09:23Z","title":"Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack\n  Detection","summary":"  3D mask presentation attack detection is crucial for protecting face\nrecognition systems against the rising threat of 3D mask attacks. While most\nexisting methods utilize multimodal features or remote photoplethysmography\n(rPPG) signals to distinguish between real faces and 3D masks, they face\nsignificant challenges, such as the high costs associated with multimodal\nsensors and limited generalization ability. Detection-related text descriptions\noffer concise, universal information and are cost-effective to obtain. However,\nthe potential of vision-language multimodal features for 3D mask presentation\nattack detection remains unexplored. In this paper, we propose a novel\nknowledge-based prompt learning framework to explore the strong generalization\ncapability of vision-language models for 3D mask presentation attack detection.\nSpecifically, our approach incorporates entities and triples from knowledge\ngraphs into the prompt learning process, generating fine-grained, task-specific\nexplicit prompts that effectively harness the knowledge embedded in pre-trained\nvision-language models. Furthermore, considering different input images may\nemphasize distinct knowledge graph elements, we introduce a visual-specific\nknowledge filter based on an attention mechanism to refine relevant elements\naccording to the visual context. Additionally, we leverage causal graph theory\ninsights into the prompt learning process to further enhance the generalization\nability of our method. During training, a spurious correlation elimination\nparadigm is employed, which removes category-irrelevant local image patches\nusing guidance from knowledge-based text features, fostering the learning of\ngeneralized causal prompts that align with category-relevant local patches.\nExperimental results demonstrate that the proposed method achieves\nstate-of-the-art intra- and cross-scenario detection performance on benchmark\ndatasets.\n","authors":["Fangling Jiang","Qi Li","Bing Liu","Weining Wang","Caifeng Shan","Zhenan Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2505.03610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03603v1","updated":"2025-05-06T15:03:58Z","published":"2025-05-06T15:03:58Z","title":"PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model","summary":"  Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose PAHA, an\nend-to-end audio-driven upper-body human animation framework with diffusion\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\nweights based on pose confidence scores, effectively improving visual quality.\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\nimprove the consistency of motion and co-speech audio. Afterwards, we design\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\nSpeech dataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance.\n","authors":["Y. B. Wang","S. Z. Zhou","J. F. Wu","T. Hu","J. N. Zhang","Y. Liu"],"pdf_url":"https://arxiv.org/pdf/2505.03603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03599v1","updated":"2025-05-06T15:01:43Z","published":"2025-05-06T15:01:43Z","title":"From Pixels to Polygons: A Survey of Deep Learning Approaches for\n  Medical Image-to-Mesh Reconstruction","summary":"  Deep learning-based medical image-to-mesh reconstruction has rapidly evolved,\nenabling the transformation of medical imaging data into three-dimensional mesh\nmodels that are critical in computational medicine and in silico trials for\nadvancing our understanding of disease mechanisms, and diagnostic and\ntherapeutic techniques in modern medicine. This survey systematically\ncategorizes existing approaches into four main categories: template models,\nstatistical models, generative models, and implicit models. Each category is\nanalysed in detail, examining their methodological foundations, strengths,\nlimitations, and applicability to different anatomical structures and imaging\nmodalities. We provide an extensive evaluation of these methods across various\nanatomical applications, from cardiac imaging to neurological studies,\nsupported by quantitative comparisons using standard metrics. Additionally, we\ncompile and analyze major public datasets available for medical mesh\nreconstruction tasks and discuss commonly used evaluation metrics and loss\nfunctions. The survey identifies current challenges in the field, including\nrequirements for topological correctness, geometric accuracy, and\nmulti-modality integration. Finally, we present promising future research\ndirections in this domain. This systematic review aims to serve as a\ncomprehensive reference for researchers and practitioners in medical image\nanalysis and computational medicine.\n","authors":["Fengming Lin","Arezoo Zakeri","Yidan Xue","Michael MacRaild","Haoran Dou","Zherui Zhou","Ziwei Zou","Ali Sarrami-Foroushani","Jinming Duan","Alejandro F. Frangi"],"pdf_url":"https://arxiv.org/pdf/2505.03599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03597v1","updated":"2025-05-06T14:59:25Z","published":"2025-05-06T14:59:25Z","title":"Fixed-Length Dense Fingerprint Representation","summary":"  Fixed-length fingerprint representations, which map each fingerprint to a\ncompact and fixed-size feature vector, are computationally efficient and\nwell-suited for large-scale matching. However, designing a robust\nrepresentation that effectively handles diverse fingerprint modalities, pose\nvariations, and noise interference remains a significant challenge. In this\nwork, we propose a fixed-length dense descriptor of fingerprints, and introduce\nFLARE-a fingerprint matching framework that integrates the Fixed-Length dense\ndescriptor with pose-based Alignment and Robust Enhancement. This fixed-length\nrepresentation employs a three-dimensional dense descriptor to effectively\ncapture spatial relationships among fingerprint ridge structures, enabling\nrobust and locally discriminative representations. To ensure consistency within\nthis dense feature space, FLARE incorporates pose-based alignment using\ncomplementary estimation methods, along with dual enhancement strategies that\nrefine ridge clarity while preserving the original fingerprint modality. The\nproposed dense descriptor supports fixed-length representation while\nmaintaining spatial correspondence, enabling fast and accurate similarity\ncomputation. Extensive experiments demonstrate that FLARE achieves superior\nperformance across rolled, plain, latent, and contactless fingerprints,\nsignificantly outperforming existing methods in cross-modality and low-quality\nscenarios. Further analysis validates the effectiveness of the dense descriptor\ndesign, as well as the impact of alignment and enhancement modules on the\naccuracy of dense descriptor matching. Experimental results highlight the\neffectiveness and generalizability of FLARE as a unified and scalable solution\nfor robust fingerprint representation and matching. The implementation and code\nwill be publicly available at https://github.com/Yu-Yy/FLARE.\n","authors":["Zhiyu Pan","Xiongjun Guan","Yongjie Duan","Jianjiang Feng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.03597v1.pdf","comment":"Under review at IEEE Transactions on Information Forensics and\n  Security (TIFS)"},{"id":"http://arxiv.org/abs/2504.07606v2","updated":"2025-05-06T14:55:28Z","published":"2025-04-10T09:57:09Z","title":"Heart Failure Prediction using Modal Decomposition and Masked\n  Autoencoders for Scarce Echocardiography Databases","summary":"  Heart diseases constitute the main cause of international human defunction.\nAccording to the World Health Organization (WHO), approximately 18 million\ndeaths happen each year due to precisely heart diseases. In particular, heart\nfailures (HF) press the healthcare industry to develop systems for their early,\nrapid, and effective prediction. This work presents an automatic system based\non a novel deep learning framework which analyses in real-time echocardiography\nvideo sequences for the challenging and more specific task of heart failure\ntime prediction. This system works in two stages. The first one transforms the\ndata from a database of echocardiography video sequences into a machine\nlearning-compatible collection of annotated images which can be used in the\ntraining phase of any machine learning-based framework, including a deep\nlearning-based one. This stage includes the use of the Higher Order Dynamic\nMode Decomposition (HODMD) algorithm for both data augmentation and feature\nextraction. The second stage builds and trains a Vision Transformer (ViT).\nSelf-supervised learning (SSL) methods, so far barely explored in the\nliterature about heart failure prediction, are adopted to effectively train the\nViT from scratch, even with scarce databases. The designed neural network\nanalyses images from echocardiography sequences to estimate the time in which a\nheart failure will happen. The results obtained show the efficacy of the HODMD\nalgorithm and the superiority of the proposed system with respect to several\nestablished ViT and Convolutional Neural Network (CNN) architectures. The\nsource code will be incorporated into the next version release of the\nModelFLOWs-app software (https://github.com/modelflows/ModelFLOWs-app).\n","authors":["Andrés Bell-Navas","María Villalba-Orero","Enrique Lara-Pezzi","Jesús Garicano-Mena","Soledad Le Clainche"],"pdf_url":"https://arxiv.org/pdf/2504.07606v2.pdf","comment":"39 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:2404.19579"},{"id":"http://arxiv.org/abs/2310.04306v2","updated":"2025-05-06T14:51:07Z","published":"2023-10-06T15:05:41Z","title":"Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware\n  Learning","summary":"  Group-level emotion recognition (GER) is an inseparable part of human\nbehavior analysis, aiming to recognize an overall emotion in a multi-person\nscene. However, the existing methods are devoted to combing diverse emotion\ncues while ignoring the inherent uncertainties under unconstrained\nenvironments, such as congestion and occlusion occurring within a group.\nAdditionally, since only group-level labels are available, inconsistent emotion\npredictions among individuals in one group can confuse the network. In this\npaper, we propose an uncertainty-aware learning (UAL) method to extract more\nrobust representations for GER. By explicitly modeling the uncertainty of each\nindividual, we utilize stochastic embedding drawn from a Gaussian distribution\ninstead of deterministic point embedding. This representation captures the\nprobabilities of different emotions and generates diverse predictions through\nthis stochasticity during the inference stage. Furthermore,\nuncertainty-sensitive scores are adaptively assigned as the fusion weights of\nindividuals' face within each group. Moreover, we develop an image enhancement\nmodule to enhance the model's robustness against severe noise. The overall\nthree-branch model, encompassing face, object, and scene component, is guided\nby a proportional-weighted fusion strategy and integrates the proposed\nuncertainty-aware method to produce the final group-level output. Experimental\nresults demonstrate the effectiveness and generalization ability of our method\nacross three widely used databases.\n","authors":["Qing Zhu","Qirong Mao","Jialin Zhang","Xiaohua Huang","Wenming Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.04306v2.pdf","comment":"11 pages,3 figures"},{"id":"http://arxiv.org/abs/2505.03581v1","updated":"2025-05-06T14:41:42Z","published":"2025-05-06T14:41:42Z","title":"DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer\n  Questions in Dynamic Scenes","summary":"  The analysis of events in dynamic environments poses a fundamental challenge\nin the development of intelligent agents and robots capable of interacting with\nhumans. Current approaches predominantly utilize visual models. However, these\nmethods often capture information implicitly from images, lacking interpretable\nspatial-temporal object representations. To address this issue we introduce\nDyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates\ncompressed spatial-temporal structural observation representation with the\ncognitive capabilities of large language models. The purpose of this\nintegration is to enable advanced question answering based on a sequence of\ntextual scene graphs. Extended evaluations on the STAR and AGQA datasets\nindicate that DyGEnc outperforms existing visual methods by a large margin of\n15-25% in addressing queries regarding the history of human-to-object\ninteractions. Furthermore, the proposed method can be seamlessly extended to\nprocess raw input images utilizing foundational models for extracting explicit\ntextual scene graphs, as substantiated by the results of a robotic experiment\nconducted with a wheeled manipulator platform. We hope that these findings will\ncontribute to the implementation of robust and compressed graph-based robotic\nmemory for long-horizon reasoning. Code is available at\ngithub.com/linukc/DyGEnc.\n","authors":["Sergey Linok","Vadim Semenov","Anastasia Trunova","Oleg Bulichev","Dmitry Yudin"],"pdf_url":"https://arxiv.org/pdf/2505.03581v1.pdf","comment":"8 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2505.03575v1","updated":"2025-05-06T14:34:31Z","published":"2025-05-06T14:34:31Z","title":"Supervised and Unsupervised Textile Classification via Near-Infrared\n  Hyperspectral Imaging and Deep Learning","summary":"  Recycling textile fibers is critical to reducing the environmental impact of\nthe textile industry. Hyperspectral near-infrared (NIR) imaging combined with\nadvanced deep learning algorithms offers a promising solution for efficient\nfiber classification and sorting. In this study, we investigate supervised and\nunsupervised deep learning models and test their generalization capabilities on\ndifferent textile structures. We show that optimized convolutional neural\nnetworks (CNNs) and autoencoder networks achieve robust generalization under\nvarying conditions. These results highlight the potential of hyperspectral\nimaging and deep learning to advance sustainable textile recycling through\naccurate and robust classification.\n","authors":["Maria Kainz","Johannes K. Krondorfer","Malte Jaschik","Maria Jernej","Harald Ganster"],"pdf_url":"https://arxiv.org/pdf/2505.03575v1.pdf","comment":"Accepted at: Proceedings of OCM 2025 - 7th International Conference\n  on Optical Characterization of Materials, March 26-27, 2025, Karlsruhe,\n  Germany, pp. 319-328"},{"id":"http://arxiv.org/abs/2505.03569v1","updated":"2025-05-06T14:27:01Z","published":"2025-05-06T14:27:01Z","title":"Corner Cases: How Size and Position of Objects Challenge\n  ImageNet-Trained Models","summary":"  Backgrounds in images play a major role in contributing to spurious\ncorrelations among different data points. Owing to aesthetic preferences of\nhumans capturing the images, datasets can exhibit positional (location of the\nobject within a given frame) and size (region-of-interest to image ratio)\nbiases for different classes. In this paper, we show that these biases can\nimpact how much a model relies on spurious features in the background to make\nits predictions. To better illustrate our findings, we propose a synthetic\ndataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images\nwith various backgrounds, object positions, and object sizes. By evaluating the\ndataset on different pretrained models, we find that most models rely heavily\non spurious features in the background when the region-of-interest (ROI) to\nimage ratio is small and the object is far from the center of the image.\nMoreover, we also show that current methods that aim to mitigate harmful\nspurious features, do not take into account these factors, hence fail to\nachieve considerable performance gains for worst-group accuracies when the size\nand location of core features in an image change.\n","authors":["Mishal Fatima","Steffen Jung","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2505.03569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03567v1","updated":"2025-05-06T14:25:30Z","published":"2025-05-06T14:25:30Z","title":"Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person\n  Search in Full Images","summary":"  Text-based pedestrian search (TBPS) in full images aims to locate a target\npedestrian in untrimmed images using natural language descriptions. However, in\ncomplex scenes with multiple pedestrians, existing methods are limited by\nuncertainties in detection and matching, leading to degraded performance. To\naddress this, we propose UPD-TBPS, a novel framework comprising three modules:\nMulti-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty\nDecoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts\nmulti-granularity queries to identify potential targets and assigns confidence\nscores to reduce early-stage uncertainty. PUD leverages visual context\ndecoupling and prototype mining to extract features of the target pedestrian\ndescribed in the query. It separates and learns pedestrian prototype\nrepresentations at both the coarse-grained cluster level and the fine-grained\nindividual level, thereby reducing matching uncertainty. ReID evaluates\ncandidates with varying confidence levels, improving detection and retrieval\naccuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the\neffectiveness of our framework.\n","authors":["Zengli Luo","Canlong Zhang","Xiaochun Lu","Zhixin Li","Zhiwen Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03567v1.pdf","comment":"9pages,5figures"},{"id":"http://arxiv.org/abs/2505.03562v1","updated":"2025-05-06T14:13:44Z","published":"2025-05-06T14:13:44Z","title":"Real-Time Person Image Synthesis Using a Flow Matching Model","summary":"  Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance.\n","authors":["Jiwoo Jeong","Kirok Kim","Wooju Kim","Nam-Joon Kim"],"pdf_url":"https://arxiv.org/pdf/2505.03562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03557v1","updated":"2025-05-06T14:11:02Z","published":"2025-05-06T14:11:02Z","title":"Generating Synthetic Data via Augmentations for Improved Facial\n  Resemblance in DreamBooth and InstantID","summary":"  The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications.\n","authors":["Koray Ulusan","Benjamin Kiefer"],"pdf_url":"https://arxiv.org/pdf/2505.03557v1.pdf","comment":"Accepted to CVPR 2025 Workshop \"Synthetic Data for Computer Vision\n  Workshop\", https://syndata4cv.github.io/"},{"id":"http://arxiv.org/abs/2505.03554v1","updated":"2025-05-06T14:05:49Z","published":"2025-05-06T14:05:49Z","title":"Read My Ears! Horse Ear Movement Detection for Equine Affective State\n  Assessment","summary":"  The Equine Facial Action Coding System (EquiFACS) enables the systematic\nannotation of facial movements through distinct Action Units (AUs). It serves\nas a crucial tool for assessing affective states in horses by identifying\nsubtle facial expressions associated with discomfort. However, the field of\nhorse affective state assessment is constrained by the scarcity of annotated\ndata, as manually labelling facial AUs is both time-consuming and costly. To\naddress this challenge, automated annotation systems are essential for\nleveraging existing datasets and improving affective states detection tools. In\nthis work, we study different methods for specific ear AU detection and\nlocalization from horse videos. We leverage past works on deep learning-based\nvideo feature extraction combined with recurrent neural networks for the video\nclassification task, as well as a classic optical flow based approach. We\nachieve 87.5% classification accuracy of ear movement presence on a public\nhorse video dataset, demonstrating the potential of our approach. We discuss\nfuture directions to develop these systems, with the aim of bridging the gap\nbetween automated AU detection and practical applications in equine welfare and\nveterinary diagnostics. Our code will be made publicly available at\nhttps://github.com/jmalves5/read-my-ears.\n","authors":["João Alves","Pia Haubro Andersen","Rikke Gade"],"pdf_url":"https://arxiv.org/pdf/2505.03554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07113v4","updated":"2025-05-06T14:02:10Z","published":"2024-06-11T09:57:04Z","title":"Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene\n  Graph","summary":"  Locating objects described in natural language presents a significant\nchallenge for autonomous agents. Existing CLIP-based open-vocabulary methods\nsuccessfully perform 3D object grounding with simple (bare) queries, but cannot\ncope with ambiguous descriptions that demand an understanding of object\nrelations. To tackle this problem, we propose a modular approach called BBQ\n(Beyond Bare Queries), which constructs 3D scene graph representation with\nmetric and semantic spatial edges and utilizes a large language model as a\nhuman-to-agent interface through our deductive scene reasoning algorithm. BBQ\nemploys robust DINO-powered associations to construct 3D object-centric map and\nan advanced raycasting algorithm with a 2D vision-language model to describe\nthem as graph nodes. On the Replica and ScanNet datasets, we have demonstrated\nthat BBQ takes a leading place in open-vocabulary 3D semantic segmentation\ncompared to other zero-shot methods. Also, we show that leveraging spatial\nrelations is especially effective for scenes containing multiple entities of\nthe same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks,\nour deductive approach demonstrates a significant improvement, enabling objects\ngrounding by complex queries compared to other state-of-the-art methods. The\ncombination of our design choices and software implementation has resulted in\nsignificant data processing speed in experiments on the robot on-board\ncomputer. This promising performance enables the application of our approach in\nintelligent robotics projects. We made the code publicly available at\nhttps://linukc.github.io/BeyondBareQueries/.\n","authors":["Sergey Linok","Tatiana Zemskova","Svetlana Ladanova","Roman Titkov","Dmitry Yudin","Maxim Monastyrny","Aleksei Valenkov"],"pdf_url":"https://arxiv.org/pdf/2406.07113v4.pdf","comment":"6 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2501.13620v5","updated":"2025-05-06T13:59:11Z","published":"2025-01-23T12:42:42Z","title":"A Cognitive Paradigm Approach to Probe the Perception-Reasoning\n  Interface in VLMs","summary":"  A fundamental challenge in artificial intelligence involves understanding the\ncognitive mechanisms underlying visual reasoning in sophisticated models like\nVision-Language Models (VLMs). How do these models integrate visual perception\nwith abstract thought, especially when reasoning across multiple images or\nrequiring fine-grained compositional understanding? Drawing inspiration from\ncognitive science, this paper introduces a structured evaluation framework\nusing diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to\ndissect the perception-reasoning interface in VLMs. We propose three distinct\nevaluation paradigms, mirroring human problem-solving strategies: Direct Visual\nRule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule\nextraction and application), and Componential Analysis (CA; analytical\ndecomposition via task-agnostic textual descriptions). These paradigms\nsystematically vary cognitive load and probe processing stages. Notably, CA\nenables multi-image reasoning evaluation even for single-image architectures\nand isolates reasoning from perception by operating on textual descriptions.\nApplying this framework, we demonstrate that CA, leveraging powerful language\nmodels for reasoning over rich, independently generated descriptions, achieves\nnew state-of-the-art (SOTA) performance on challenging benchmarks including\nBongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm\nreasoning improves significantly when perceptual challenges are mitigated,\nrevealing a critical perception bottleneck. Our framework provides a valuable\ndiagnostic tool and suggests that decoupling perception (via rich,\ntask-agnostic description) from reasoning is a promising direction for robust\nand general visual intelligence.\n","authors":["Mohit Vaishnav","Tanel Tammet"],"pdf_url":"https://arxiv.org/pdf/2501.13620v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03539v1","updated":"2025-05-06T13:51:26Z","published":"2025-05-06T13:51:26Z","title":"Panoramic Out-of-Distribution Segmentation","summary":"  Panoramic imaging enables capturing 360{\\deg} images with an ultra-wide\nField-of-View (FoV) for dense omnidirectional perception. However, current\npanoramic semantic segmentation methods fail to identify outliers, and pinhole\nOut-of-distribution Segmentation (OoS) models perform unsatisfactorily in the\npanoramic domain due to background clutter and pixel distortions. To address\nthese issues, we introduce a new task, Panoramic Out-of-distribution\nSegmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the\nfirst solution, POS, which adapts to the characteristics of panoramic images\nthrough text-guided prompt distribution learning. Specifically, POS integrates\na disentanglement strategy designed to materialize the cross-domain\ngeneralization capability of CLIP. The proposed Prompt-based Restoration\nAttention (PRA) optimizes semantic decoding by prompt guidance and\nself-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL)\nrefines the manifold of per-pixel mask embeddings via semantic prototype\nsupervision. Besides, to compensate for the scarcity of PanOoS datasets, we\nestablish two benchmarks: DenseOoS, which features diverse outliers in complex\nenvironments, and QuadOoS, captured by a quadruped robot with a panoramic\nannular lens system. Extensive experiments demonstrate superior performance of\nPOS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS,\noutperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves\nleading closed-set segmentation capabilities. Code and datasets will be\navailable at https://github.com/MengfeiD/PanOoS.\n","authors":["Mengfei Duan","Kailun Yang","Yuheng Zhang","Yihong Cao","Fei Teng","Kai Luo","Jiaming Zhang","Zhiyong Li","Shutao Li"],"pdf_url":"https://arxiv.org/pdf/2505.03539v1.pdf","comment":"Code and datasets will be available at\n  https://github.com/MengfeiD/PanOoS"},{"id":"http://arxiv.org/abs/2505.03538v1","updated":"2025-05-06T13:50:57Z","published":"2025-05-06T13:50:57Z","title":"RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth\n  Segmentation in CBCT","summary":"  Semi-supervised learning has become a compelling approach for 3D tooth\nsegmentation from CBCT scans, where labeled data is minimal. However, existing\nmethods still face two persistent challenges: limited corrective supervision in\nstructurally ambiguous or mislabeled regions during supervised training and\nperformance degradation caused by unreliable pseudo-labels on unlabeled data.\nTo address these problems, we propose Region-Aware Instructive Learning (RAIL),\na dual-group dual-student, semi-supervised framework. Each group contains two\nstudent models guided by a shared teacher network. By alternating training\nbetween the two groups, RAIL promotes intergroup knowledge transfer and\ncollaborative region-aware instruction while reducing overfitting to the\ncharacteristics of any single model. Specifically, RAIL introduces two\ninstructive mechanisms. Disagreement-Focused Supervision (DFS) Controller\nimproves supervised learning by instructing predictions only within areas where\nstudent outputs diverge from both ground truth and the best student, thereby\nconcentrating supervision on structurally ambiguous or mislabeled areas. In the\nunsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces\nagreement in regions with high model certainty while reducing the effect of\nlow-confidence predictions during training. This helps prevent our model from\nlearning unstable patterns and improves the overall reliability of\npseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets\nshow that RAIL surpasses state-of-the-art methods under limited annotation. Our\ncode will be available at https://github.com/Tournesol-Saturday/RAIL.\n","authors":["Chuyu Zhao","Hao Huang","Jiashuo Guo","Ziyu Shen","Zhongwei Zhou","Jie Liu","Zekuan Yu"],"pdf_url":"https://arxiv.org/pdf/2505.03538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15255v2","updated":"2025-05-06T13:41:55Z","published":"2024-11-22T08:54:16Z","title":"OSMamba: Omnidirectional Spectral Mamba with Dual-Domain Prior Generator\n  for Exposure Correction","summary":"  Exposure correction is a fundamental problem in computer vision and image\nprocessing. Recently, frequency domain-based methods have achieved impressive\nimprovement, yet they still struggle with complex real-world scenarios under\nextreme exposure conditions. This is due to the local convolutional receptive\nfields failing to model long-range dependencies in the spectrum, and the\nnon-generative learning paradigm being inadequate for retrieving lost details\nfrom severely degraded regions. In this paper, we propose Omnidirectional\nSpectral Mamba (OSMamba), a novel exposure correction network that incorporates\nthe advantages of state space models and generative diffusion models to address\nthese limitations. Specifically, OSMamba introduces an omnidirectional spectral\nscanning mechanism that adapts Mamba to the frequency domain to capture\ncomprehensive long-range dependencies in both the amplitude and phase spectra\nof deep image features, hence enhancing illumination correction and structure\nrecovery. Furthermore, we develop a dual-domain prior generator that learns\nfrom well-exposed images to generate a degradation-free diffusion prior\ncontaining correct information about severely under- and over-exposed regions\nfor better detail restoration. Extensive experiments on multiple-exposure and\nmixed-exposure datasets demonstrate that the proposed OSMamba achieves\nstate-of-the-art performance both quantitatively and qualitatively.\n","authors":["Gehui Li","Bin Chen","Chen Zhao","Lei Zhang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.15255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03528v1","updated":"2025-05-06T13:38:35Z","published":"2025-05-06T13:38:35Z","title":"Coop-WD: Cooperative Perception with Weighting and Denoising for Robust\n  V2V Communication","summary":"  Cooperative perception, leveraging shared information from multiple vehicles\nvia vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous\ndriving to alleviate the limitation of single-vehicle perception. Existing\nworks have explored the effects of V2V communication impairments on perception\nprecision, but they lack generalization to different levels of impairments. In\nthis work, we propose a joint weighting and denoising framework, Coop-WD, to\nenhance cooperative perception subject to V2V channel impairments. In this\nframework, the self-supervised contrastive model and the conditional diffusion\nprobabilistic model are adopted hierarchically for vehicle-level and\npixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is\nproposed to selectively deactivate denoising to reduce processing overhead.\nRician fading, non-stationarity, and time-varying distortion are considered.\nSimulation results demonstrate that the proposed Coop-WD outperforms\nconventional benchmarks in all types of channels. Qualitative analysis with\nvisual examples further proves the superiority of our proposed method. The\nproposed Coop-WD-eco achieves up to 50% reduction in computational cost under\nsevere distortion while maintaining comparable accuracy as channel conditions\nimprove.\n","authors":["Chenguang Liu","Jianjun Chen","Yunfei Chen","Yubei He","Zhuangkun Wei","Hongjian Sun","Haiyan Lu","Qi Hao"],"pdf_url":"https://arxiv.org/pdf/2505.03528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03522v1","updated":"2025-05-06T13:35:59Z","published":"2025-05-06T13:35:59Z","title":"Optimization of Module Transferability in Single Image Super-Resolution:\n  Universality Assessment and Cycle Residual Blocks","summary":"  Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.\n","authors":["Haotong Cheng","Zhiqi Zhang","Hao Li","Xinshang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.03522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03510v1","updated":"2025-05-06T13:20:04Z","published":"2025-05-06T13:20:04Z","title":"From Neurons to Computation: Biological Reservoir Computing for Pattern\n  Recognition","summary":"  In this paper, we introduce a novel paradigm for reservoir computing (RC)\nthat leverages a pool of cultured biological neurons as the reservoir\nsubstrate, creating a biological reservoir computing (BRC). This system\noperates similarly to an echo state network (ESN), with the key distinction\nthat the neural activity is generated by a network of cultured neurons, rather\nthan being modeled by traditional artificial computational units. The neuronal\nactivity is recorded using a multi-electrode array (MEA), which enables\nhigh-throughput recording of neural signals. In our approach, inputs are\nintroduced into the network through a subset of the MEA electrodes, while the\nremaining electrodes capture the resulting neural activity. This generates a\nnonlinear mapping of the input data to a high-dimensional biological feature\nspace, where distinguishing between data becomes more efficient and\nstraightforward, allowing a simple linear classifier to perform pattern\nrecognition tasks effectively. To evaluate the performance of our proposed\nsystem, we present an experimental study that includes various input patterns,\nsuch as positional codes, bars with different orientations, and a digit\nrecognition task. The results demonstrate the feasibility of using biological\nneural networks to perform tasks traditionally handled by artificial neural\nnetworks, paving the way for further exploration of biologically-inspired\ncomputing systems, with potential applications in neuromorphic engineering and\nbio-hybrid computing.\n","authors":["Ludovico Iannello","Luca Ciampi","Gabriele Lagani","Fabrizio Tonelli","Eleonora Crocco","Lucio Maria Calcagnile","Angelo Di Garbo","Federico Cremisi","Giuseppe Amato"],"pdf_url":"https://arxiv.org/pdf/2505.03510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03507v1","updated":"2025-05-06T13:15:34Z","published":"2025-05-06T13:15:34Z","title":"Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for\n  Self-Supervised RGB-T Tracking","summary":"  To reduce the reliance on large-scale annotations, self-supervised RGB-T\ntracking approaches have garnered significant attention. However, the omission\nof the object region by erroneous pseudo-label or the introduction of\nbackground noise affects the efficiency of modality fusion, while pseudo-label\nnoise triggered by similar object noise can further affect the tracking\nperformance. In this paper, we propose GDSTrack, a novel approach that\nintroduces dynamic graph fusion and temporal diffusion to address the above\nchallenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the\nmodalities of neighboring frames, treats them as distractor noise, and\nleverages the denoising capability of a generative model. Specifically, by\nconstructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the\nproposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic\nadjacency matrix to guide graph attention, focusing on and fusing the object's\ncoherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features\nfrom neighboring frames as interference, and thus improving robustness against\nsimilar-object noise. Extensive experiments conducted on four public RGB-T\ntracking datasets demonstrate that GDSTrack outperforms the existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/LiShenglana/GDSTrack.\n","authors":["Shenglan Li","Rui Yao","Yong Zhou","Hancheng Zhu","Kunyang Sun","Bing Liu","Zhiwen Shao","Jiaqi Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.03507v1.pdf","comment":"Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2411.18673v4","updated":"2025-05-06T13:11:14Z","published":"2024-11-27T18:49:13Z","title":"AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion\n  Transformers","summary":"  Numerous works have recently integrated 3D camera control into foundational\ntext-to-video models, but the resulting camera control is often imprecise, and\nvideo generation quality suffers. In this work, we analyze camera motion from a\nfirst principles perspective, uncovering insights that enable precise 3D camera\nmanipulation without compromising synthesis quality. First, we determine that\nmotion induced by camera movements in videos is low-frequency in nature. This\nmotivates us to adjust train and test pose conditioning schedules, accelerating\ntraining convergence while improving visual and motion quality. Then, by\nprobing the representations of an unconditional video diffusion transformer, we\nobserve that they implicitly perform camera pose estimation under the hood, and\nonly a sub-portion of their layers contain the camera information. This\nsuggested us to limit the injection of camera conditioning to a subset of the\narchitecture to prevent interference with other video features, leading to a 4x\nreduction of training parameters, improved training speed, and 10% higher\nvisual quality. Finally, we complement the typical dataset for camera control\nlearning with a curated dataset of 20K diverse, dynamic videos with stationary\ncameras. This helps the model distinguish between camera and scene motion and\nimproves the dynamics of generated pose-conditioned videos. We compound these\nfindings to design the Advanced 3D Camera Control (AC3D) architecture, the new\nstate-of-the-art model for generative video modeling with camera control.\n","authors":["Sherwin Bahmani","Ivan Skorokhodov","Guocheng Qian","Aliaksandr Siarohin","Willi Menapace","Andrea Tagliasacchi","David B. Lindell","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2411.18673v4.pdf","comment":"CVPR 2025; Project Page: https://snap-research.github.io/ac3d/"},{"id":"http://arxiv.org/abs/2505.03498v1","updated":"2025-05-06T13:02:40Z","published":"2025-05-06T13:02:40Z","title":"MRI motion correction via efficient residual-guided denoising diffusion\n  probabilistic models","summary":"  Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly\ndegrade image quality and impair quantitative analysis. Conventional mitigation\nstrategies, such as repeated acquisitions or motion tracking, are costly and\nworkflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising\ndiffusion probabilistic model tailored for MRI motion artifact correction.\nMethods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in\nthe forward diffusion process, aligning the noise distribution with\nmotion-corrupted data and enabling an efficient four-step reverse diffusion. A\nU-net backbone enhanced with Swin-Transformer blocks conventional attention\nlayers, improving adaptability across resolutions. Training employs a combined\nl1+l2 loss, which promotes image sharpness and reduces pixel-level errors.\nRes-MoCoDiff was evaluated on synthetic dataset generated using a realistic\nmotion simulation framework and on an in-vivo dataset. Comparative analyses\nwere conducted against established methods, including CycleGAN, Pix2pix, and\nMT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR),\nstructural similarity index measure (SSIM), and normalized mean squared error\n(NMSE). Results: The proposed method demonstrated superior performance in\nremoving motion artifacts across all motion severity levels. Res-MoCoDiff\nconsistently achieved the highest SSIM and the lowest NMSE values, with a PSNR\nof up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling\ntime was reduced to 0.37 seconds per batch of two image slices, compared with\n101.74 seconds for conventional approaches.\n","authors":["Mojtaba Safari","Shansong Wang","Qiang Li","Zach Eidex","Richard L. J. Qiu","Chih-Wei Chang","Hui Mao","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2505.03498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12516v2","updated":"2025-05-06T13:01:27Z","published":"2024-11-19T13:58:20Z","title":"Modular Autonomous Virtualization System for Two-Dimensional\n  Semiconductor Quantum Dot Arrays","summary":"  Arrays of gate-defined semiconductor quantum dots are among the leading\ncandidates for building scalable quantum processors. High-fidelity\ninitialization, control, and readout of spin qubit registers require exquisite\nand targeted control over key Hamiltonian parameters that define the\nelectrostatic environment. However, due to the tight gate pitch, capacitive\ncrosstalk between gates hinders independent tuning of chemical potentials and\ninterdot couplings. While virtual gates offer a practical solution, determining\nall the required cross-capacitance matrices accurately and efficiently in large\nquantum dot registers is an open challenge. Here, we establish a modular\nautomated virtualization system (MAViS) -- a general and modular framework for\nautonomously constructing a complete stack of multilayer virtual gates in real\ntime. Our method employs machine learning techniques to rapidly extract\nfeatures from two-dimensional charge stability diagrams. We then utilize\ncomputer vision and regression models to self-consistently determine all\nrelative capacitive couplings necessary for virtualizing plunger and barrier\ngates in both low- and high-tunnel-coupling regimes. Using MAViS, we\nsuccessfully demonstrate accurate virtualization of a dense two-dimensional\narray comprising ten quantum dots defined in a high-quality Ge/SiGe\nheterostructure. Our work offers an elegant and practical solution for the\nefficient control of large-scale semiconductor quantum dot systems.\n","authors":["Anantha S. Rao","Donovan Buterakos","Barnaby van Straaten","Valentin John","Cécile X. Yu","Stefan D. Oosterhout","Lucas Stehouwer","Giordano Scappucci","Menno Veldhorst","Francesco Borsoi","Justyna P. Zwolak"],"pdf_url":"https://arxiv.org/pdf/2411.12516v2.pdf","comment":"14 pages, 5 figures, 9 pages of supplemental material"},{"id":"http://arxiv.org/abs/2505.03494v1","updated":"2025-05-06T12:56:04Z","published":"2025-05-06T12:56:04Z","title":"UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance\n  and Adaptive Multimodal Feature Fusion","summary":"  Background: Brain tumor segmentation has a significant impact on the\ndiagnosis and treatment of brain tumors. Accurate brain tumor segmentation\nremains challenging due to their irregular shapes, vague boundaries, and high\nvariability. Objective: We propose a brain tumor segmentation method that\ncombines deep learning with prior knowledge derived from a region-growing\nalgorithm. Methods: The proposed method utilizes a multi-scale feature fusion\n(MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale\nfeatures and capture global contextual information. To enhance the model's\nrobustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout)\nstrategy is employed for uncertainty estimation. Results: Extensive experiments\ndemonstrate that the proposed method achieves superior performance on Brain\nTumor Segmentation (BraTS) datasets, significantly outperforming various\nstate-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are\n89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT)\nsegmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019\nvalidation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for\nET, WT, and TC segmentation, respectively. Ablation studies further confirmed\nthe contribution of each module to segmentation accuracy, indicating that each\ncomponent played a vital role in overall performance improvement. Conclusion:\nThis study proposed a novel 3D brain tumor segmentation network based on the\nU-Net architecture. By incorporating the prior knowledge and employing the\nuncertainty estimation method, the robustness and performance were improved.\nThe code for the proposed method is available at\nhttps://github.com/chenzhao2023/UPMAD_Net_BrainSeg.\n","authors":["Zhanyuan Jia","Ni Yao","Danyang Sun","Chuang Han","Yanting Li","Jiaofen Nan","Fubao Zhu","Chen Zhao","Weihua Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.03494v1.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.13754v3","updated":"2025-05-06T12:45:03Z","published":"2025-04-18T15:39:46Z","title":"Towards Accurate and Interpretable Neuroblastoma Diagnosis via\n  Contrastive Multi-scale Pathological Image Analysis","summary":"  Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole-slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole-slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to bridge patch-level predictions to whole-slide\nimage-level classifications seamlessly. We verified the CMSwinKAN on the\npublicly available BreakHis dataset and the PpNTs dataset, which was\nestablished by our hospital. Results demonstrate that CMSwinKAN performs better\nthan existing state-of-the-art pathology-specific models pre-trained on large\ndatasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN.\n","authors":["Zhu Zhu","Shuo Jiang","Jingyuan Zheng","Yawen Li","Yifei Chen","Manli Zhao","Weizhong Gu","Feiwei Qin","Jinhu Wang","Gang Yu"],"pdf_url":"https://arxiv.org/pdf/2504.13754v3.pdf","comment":"10pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.03470v1","updated":"2025-05-06T12:22:45Z","published":"2025-05-06T12:22:45Z","title":"Blending 3D Geometry and Machine Learning for Multi-View Stereopsis","summary":"  Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.\n","authors":["Vibhas Vats","Md. Alimoor Reza","David Crandall","Soon-heung Jung"],"pdf_url":"https://arxiv.org/pdf/2505.03470v1.pdf","comment":"A pre-print -- paper under-review. arXiv admin note: substantial text\n  overlap with arXiv:2310.19583"},{"id":"http://arxiv.org/abs/2505.03463v1","updated":"2025-05-06T12:01:40Z","published":"2025-05-06T12:01:40Z","title":"Nonperiodic dynamic CT reconstruction using backward-warping INR with\n  regularization of diffeomorphism (BIRD)","summary":"  Dynamic computed tomography (CT) reconstruction faces significant challenges\nin addressing motion artifacts, particularly for nonperiodic rapid movements\nsuch as cardiac imaging with fast heart rates. Traditional methods struggle\nwith the extreme limited-angle problems inherent in nonperiodic cases. Deep\nlearning methods have improved performance but face generalization challenges.\nRecent implicit neural representation (INR) techniques show promise through\nself-supervised deep learning, but have critical limitations: computational\ninefficiency due to forward-warping modeling, difficulty balancing DVF\ncomplexity with anatomical plausibility, and challenges in preserving fine\ndetails without additional patient-specific pre-scans. This paper presents a\nnovel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It\naddresses these challenges through four key contributions: (1) backward-warping\ndeformation that enables direct computation of each dynamic voxel with\nsignificantly reduced computational cost, (2) diffeomorphism-based DVF\nregularization that ensures anatomically plausible deformations while\nmaintaining representational capacity, (3) motion-compensated analytical\nreconstruction that enhances fine details without requiring additional\npre-scans, and (4) dimensional-reduction design for efficient 4D coordinate\nencoding. Through various simulations and practical studies, including digital\nand physical phantoms and retrospective patient data, we demonstrate the\neffectiveness of our approach for nonperiodic dynamic CT reconstruction with\nenhanced details and reduced motion artifacts. The proposed framework enables\nmore accurate dynamic CT reconstruction with potential clinical applications,\nsuch as one-beat cardiac reconstruction, cinematic image sequences for\nfunctional imaging, and motion artifact reduction in conventional CT scans.\n","authors":["Muge Du","Zhuozhao Zheng","Wenying Wang","Guotao Quan","Wuliang Shi","Le Shen","Li Zhang","Liang Li","Yinong Liu","Yuxiang Xing"],"pdf_url":"https://arxiv.org/pdf/2505.03463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03445v1","updated":"2025-05-06T11:31:14Z","published":"2025-05-06T11:31:14Z","title":"Polar Coordinate-Based 2D Pose Prior with Neural Distance Field","summary":"  Human pose capture is essential for sports analysis, enabling precise\nevaluation of athletes' movements. While deep learning-based human pose\nestimation (HPE) models from RGB videos have achieved impressive performance on\npublic datasets, their effectiveness in real-world sports scenarios is often\nhindered by motion blur, occlusions, and domain shifts across different pose\nrepresentations. Fine-tuning these models can partially alleviate such\nchallenges but typically requires large-scale annotated data and still\nstruggles to generalize across diverse sports environments. To address these\nlimitations, we propose a 2D pose prior-guided refinement approach based on\nNeural Distance Fields (NDF). Unlike existing approaches that rely solely on\nangular representations of human poses, we introduce a polar coordinate-based\nrepresentation that explicitly incorporates joint connection lengths, enabling\na more accurate correction of erroneous pose estimations. Additionally, we\ndefine a novel non-geodesic distance metric that separates angular and radial\ndiscrepancies, which we demonstrate is better suited for polar representations\nthan traditional geodesic distances. To mitigate data scarcity, we develop a\ngradient-based batch-projection augmentation strategy, which synthesizes\nrealistic pose samples through iterative refinement. Our method is evaluated on\na long jump dataset, demonstrating its ability to improve 2D pose estimation\nacross multiple pose representations, making it robust across different\ndomains. Experimental results show that our approach enhances pose plausibility\nwhile requiring only limited training data. Code is available at:\nhttps://github.com/QGAN2019/polar-NDF.\n","authors":["Qi Gan","Sao Mai Nguyen","Eric Fenaux","Stephan Clémençon","Mounîm El Yacoubi"],"pdf_url":"https://arxiv.org/pdf/2505.03445v1.pdf","comment":"This paper is accepted by CVPRW 2025"},{"id":"http://arxiv.org/abs/2505.03435v1","updated":"2025-05-06T11:19:01Z","published":"2025-05-06T11:19:01Z","title":"Robustness in AI-Generated Detection: Enhancing Resistance to\n  Adversarial Attacks","summary":"  The rapid advancement of generative image technology has introduced\nsignificant security concerns, particularly in the domain of face generation\ndetection. This paper investigates the vulnerabilities of current AI-generated\nface detection systems. Our study reveals that while existing detection methods\noften achieve high accuracy under standard conditions, they exhibit limited\nrobustness against adversarial attacks. To address these challenges, we propose\nan approach that integrates adversarial training to mitigate the impact of\nadversarial examples. Furthermore, we utilize diffusion inversion and\nreconstruction to further enhance detection robustness. Experimental results\ndemonstrate that minor adversarial perturbations can easily bypass existing\ndetection systems, but our method significantly improves the robustness of\nthese systems. Additionally, we provide an in-depth analysis of adversarial and\nbenign examples, offering insights into the intrinsic characteristics of\nAI-generated content. All associated code will be made publicly available in a\ndedicated repository to facilitate further research and verification.\n","authors":["Sun Haoxuan","Hong Yan","Zhan Jiahui","Chen Haoxing","Lan Jun","Zhu Huijia","Wang Weiqiang","Zhang Liqing","Zhang Jianfu"],"pdf_url":"https://arxiv.org/pdf/2505.03435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19894v2","updated":"2025-05-06T11:18:38Z","published":"2024-07-29T11:16:59Z","title":"CardioSyntax: end-to-end SYNTAX score prediction -- dataset, benchmark\n  and method","summary":"  The SYNTAX score has become a widely used measure of coronary disease\nseverity, crucial in selecting the optimal mode of the revascularization\nprocedure. This paper introduces a new medical regression and classification\nproblem - automatically estimating SYNTAX score from coronary angiography. Our\nstudy presents a comprehensive CardioSYNTAX dataset of 3,018 patients for the\nSYNTAX score estimation and coronary dominance classification. The dataset\nfeatures a balanced distribution of individuals with zero and non-zero scores.\nThis dataset includes a first-of-its-kind, complete coronary angiography\nsamples captured through a multi-view X-ray video, allowing one to observe\ncoronary arteries from multiple perspectives. Furthermore, we present a novel,\nfully automatic end-to-end method for estimating the SYNTAX. For such a\ndifficult task, we have achieved a solid coefficient of determination R2 of\n0.51 in score value prediction and 77.3% accuracy for zero score\nclassification.\n","authors":["Alexander Ponomarchuk","Ivan Kruzhilov","Galina Zubkova","Artem Shadrin","Ruslan Utegenov","Ivan Bessonov","Pavel Blinov"],"pdf_url":"https://arxiv.org/pdf/2407.19894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03431v1","updated":"2025-05-06T11:15:59Z","published":"2025-05-06T11:15:59Z","title":"A Fusion-Guided Inception Network for Hyperspectral Image\n  Super-Resolution","summary":"  The fusion of low-spatial-resolution hyperspectral images (HSIs) with\nhigh-spatial-resolution conventional images (e.g., panchromatic or RGB) has\nplayed a significant role in recent advancements in HSI super-resolution.\nHowever, this fusion process relies on the availability of precise alignment\nbetween image pairs, which is often challenging in real-world scenarios. To\nmitigate this limitation, we propose a single-image super-resolution model\ncalled the Fusion-Guided Inception Network (FGIN). Specifically, we first\nemploy a spectral-spatial fusion module to effectively integrate spectral and\nspatial information at an early stage. Next, an Inception-like hierarchical\nfeature extraction strategy is used to capture multiscale spatial dependencies,\nfollowed by a dedicated multi-scale fusion block. To further enhance\nreconstruction quality, we incorporate an optimized upsampling module that\ncombines bilinear interpolation with depthwise separable convolutions.\nExperimental evaluations on two publicly available hyperspectral datasets\ndemonstrate the competitive performance of our method.\n","authors":["Usman Muhammad","Jorma Laaksonen"],"pdf_url":"https://arxiv.org/pdf/2505.03431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03426v1","updated":"2025-05-06T11:06:41Z","published":"2025-05-06T11:06:41Z","title":"Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI\n  Synthesis: Advancing Pretraining and Clinical Applications","summary":"  Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for\ndiagnosing heart diseases and evaluating cardiac health. However, the limited\navailability of large-scale, high-quality CMR datasets poses a major challenge\nto the effective application of artificial intelligence (AI) in this domain.\nEven the amount of unlabeled data and the health status it covers are difficult\nto meet the needs of model pretraining, which hinders the performance of AI\nmodels on downstream tasks. In this study, we present Cardiac Phenotype-Guided\nCMR Generation (CPGG), a novel approach for generating diverse CMR data that\ncovers a wide spectrum of cardiac health status. The CPGG framework consists of\ntwo stages: in the first stage, a generative model is trained using cardiac\nphenotypes derived from CMR data; in the second stage, a masked autoregressive\ndiffusion model, conditioned on these phenotypes, generates high-fidelity CMR\ncine sequences that capture both structural and functional features of the\nheart in a fine-grained manner. We synthesized a massive amount of CMR to\nexpand the pretraining data. Experimental results show that CPGG generates\nhigh-quality synthetic CMR data, significantly improving performance on various\ndownstream tasks, including diagnosis and cardiac phenotypes prediction. These\ngains are demonstrated across both public and private datasets, highlighting\nthe effectiveness of our approach. Code is availabel at\nhttps://anonymous.4open.science/r/CPGG.\n","authors":["Ziyu Li","Yujian Hu","Zhengyao Ding","Yiheng Mao","Haitao Li","Fan Yi","Hongkun Zhang","Zhengxing Huang"],"pdf_url":"https://arxiv.org/pdf/2505.03426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03422v1","updated":"2025-05-06T10:59:23Z","published":"2025-05-06T10:59:23Z","title":"LiftFeat: 3D Geometry-Aware Local Feature Matching","summary":"  Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled \\textit{LiftFeat}, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.\n","authors":["Yepeng Liu","Wenpeng Lai","Zhou Zhao","Yuxuan Xiong","Jinchi Zhu","Jun Cheng","Yongchao Xu"],"pdf_url":"https://arxiv.org/pdf/2505.03422v1.pdf","comment":"Accepted at ICRA 2025"},{"id":"http://arxiv.org/abs/2505.03420v1","updated":"2025-05-06T10:55:21Z","published":"2025-05-06T10:55:21Z","title":"Mitigating Image Captioning Hallucinations in Vision-Language Models","summary":"  Hallucinations in vision-language models (VLMs) hinder reliability and\nreal-world applicability, usually stemming from distribution shifts between\npretraining data and test samples. Existing solutions, such as retraining or\nfine-tuning on additional data, demand significant computational resources and\nlabor-intensive data collection, while ensemble-based methods incur additional\ncosts by introducing auxiliary VLMs. To address these challenges, we propose a\nnovel test-time adaptation framework using reinforcement learning to mitigate\nhallucinations during inference without retraining or any auxiliary VLMs. By\nupdating only the learnable parameters in the layer normalization of the\nlanguage model (approximately 0.003% of the model parameters), our method\nreduces distribution shifts between test samples and pretraining samples. A\nCLIP-based hallucination evaluation model is proposed to provide dual rewards\nto VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in\nhallucination rates on LLaVA and InstructBLIP, respectively. Our approach\noutperforms state-of-the-art baselines with a 68.3% improvement in\nhallucination mitigation, demonstrating its effectiveness.\n","authors":["Fei Zhao","Chengcui Zhang","Runlin Zhang","Tianyang Wang","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2505.03420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03414v1","updated":"2025-05-06T10:41:53Z","published":"2025-05-06T10:41:53Z","title":"Enhancing Target-unspecific Tasks through a Features Matrix","summary":"  Recent developments in prompt learning of large vision-language models have\nsignificantly improved performance in target-specific tasks. However, these\nprompt optimizing methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge having\nstrong promotion on target-unspecific tasks. To alleviate this issue, we\npropose a novel Features Matrix (FM) regularization approach designed to\nenhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks, achieving state-of-the-art performance.\n","authors":["Fangming Cui","Yonggang Zhang","Xuan Wang","Xinmei Tian","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2505.03414v1.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.03412v1","updated":"2025-05-06T10:38:24Z","published":"2025-05-06T10:38:24Z","title":"CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection","summary":"  Internal defect detection constitutes a critical process in ensuring\ncomponent quality, for which anomaly detection serves as an effective solution.\nHowever, existing anomaly detection datasets predominantly focus on surface\ndefects in visible-light images, lacking publicly available X-ray datasets\ntargeting internal defects in components. To address this gap, we construct the\nfirst publicly accessible component X-ray anomaly detection (CXR-AD) dataset,\ncomprising real-world X-ray images. The dataset covers five industrial\ncomponent categories, including 653 normal samples and 561 defect samples with\nprecise pixel-level mask annotations. We systematically analyze the dataset\ncharacteristics and identify three major technical challenges: (1) strong\ncoupling between complex internal structures and defect regions, (2) inherent\nlow contrast and high noise interference in X-ray imaging, and (3) significant\nvariations in defect scales and morphologies. To evaluate dataset complexity,\nwe benchmark three state-of-the-art anomaly detection frameworks\n(feature-based, reconstruction-based, and zero-shot learning methods).\nExperimental results demonstrate a 29.78% average performance degradation on\nCXR-AD compared to MVTec AD, highlighting the limitations of current algorithms\nin handling internal defect detection tasks. To the best of our knowledge,\nCXR-AD represents the first publicly available X-ray dataset for component\nanomaly detection, providing a real-world industrial benchmark to advance\nalgorithm development and enhance precision in internal defect inspection\ntechnologies.\n","authors":["Haoyu Bai","Jie Wang","Gaomin Li","Xuan Li","Xiaohu Zhang","Xia Yang"],"pdf_url":"https://arxiv.org/pdf/2505.03412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03401v1","updated":"2025-05-06T10:29:23Z","published":"2025-05-06T10:29:23Z","title":"DDaTR: Dynamic Difference-aware Temporal Residual Network for\n  Longitudinal Radiology Report Generation","summary":"  Radiology Report Generation (RRG) automates the creation of radiology reports\nfrom medical imaging, enhancing the efficiency of the reporting process.\nLongitudinal Radiology Report Generation (LRRG) extends RRG by incorporating\nthe ability to compare current and prior exams, facilitating the tracking of\ntemporal changes in clinical findings. Existing LRRG approaches only extract\nfeatures from prior and current images using a visual pre-trained encoder,\nwhich are then concatenated to generate the final report. However, these\nmethods struggle to effectively capture both spatial and temporal correlations\nduring the feature extraction process. Consequently, the extracted features\ninadequately capture the information of difference across exams and thus\nunderrepresent the expected progressions, leading to sub-optimal performance in\nLRRG. To address this, we develop a novel dynamic difference-aware temporal\nresidual network (DDaTR). In DDaTR, we introduce two modules at each stage of\nthe visual encoder to capture multi-level spatial correlations. The Dynamic\nFeature Alignment Module (DFAM) is designed to align prior features across\nmodalities for the integrity of prior clinical information. Prompted by the\nenriched prior features, the dynamic difference-aware module (DDAM) captures\nfavorable difference information by identifying relationships across exams.\nFurthermore, our DDaTR employs the dynamic residual network to unidirectionally\ntransmit longitudinal information, effectively modelling temporal correlations.\nExtensive experiments demonstrated superior performance over existing methods\non three benchmarks, proving its efficacy in both RRG and LRRG tasks.\n","authors":["Shanshan Song","Hui Tang","Honglong Yang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.03401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02825v2","updated":"2025-05-06T10:17:58Z","published":"2025-05-05T17:51:56Z","title":"Towards Application-Specific Evaluation of Vision Models: Case Studies\n  in Ecology and Biology","summary":"  Computer vision methods have demonstrated considerable potential to\nstreamline ecological and biological workflows, with a growing number of\ndatasets and models becoming available to the research community. However,\nthese resources focus predominantly on evaluation using machine learning\nmetrics, with relatively little emphasis on how their application impacts\ndownstream analysis. We argue that models should be evaluated using\napplication-specific metrics that directly represent model performance in the\ncontext of its final use case. To support this argument, we present two\ndisparate case studies: (1) estimating chimpanzee abundance and density with\ncamera trap distance sampling when using a video-based behaviour classifier and\n(2) estimating head rotation in pigeons using a 3D posture estimator. We show\nthat even models with strong machine learning performance (e.g., 87% mAP) can\nyield data that leads to discrepancies in abundance estimates compared to\nexpert-derived data. Similarly, the highest-performing models for posture\nestimation do not produce the most accurate inferences of gaze direction in\npigeons. Motivated by these findings, we call for researchers to integrate\napplication-specific metrics in ecological/biological datasets, allowing for\nmodels to be benchmarked in the context of their downstream application and to\nfacilitate better integration of models into application workflows.\n","authors":["Alex Hoi Hang Chan","Otto Brookes","Urs Waldmann","Hemal Naik","Iain D. Couzin","Majid Mirmehdi","Noël Adiko Houa","Emmanuelle Normand","Christophe Boesch","Lukas Boesch","Mimi Arandjelovic","Hjalmar Kühl","Tilo Burghardt","Fumihiro Kano"],"pdf_url":"https://arxiv.org/pdf/2505.02825v2.pdf","comment":"Accepted at CVPR Workshop, CV4Animals 2025"},{"id":"http://arxiv.org/abs/2505.03394v1","updated":"2025-05-06T10:17:32Z","published":"2025-05-06T10:17:32Z","title":"EOPose : Exemplar-based object reposing using Generalized Pose\n  Correspondences","summary":"  Reposing objects in images has a myriad of applications, especially for\ne-commerce where several variants of product images need to be produced\nquickly. In this work, we leverage the recent advances in unsupervised keypoint\ncorrespondence detection between different object images of the same class to\npropose an end-to-end framework for generic object reposing. Our method,\nEOPose, takes a target pose-guidance image as input and uses its keypoint\ncorrespondence with the source object image to warp and re-render the latter\ninto the target pose using a novel three-step approach. Unlike generative\napproaches, our method also preserves the fine-grained details of the object\nsuch as its exact colors, textures, and brand marks. We also prepare a new\ndataset of paired objects based on the Objaverse dataset to train and test our\nnetwork. EOPose produces high-quality reposing output as evidenced by different\nimage quality metrics (PSNR, SSIM and FID). Besides a description of the method\nand the dataset, the paper also includes detailed ablation and user studies to\nindicate the efficacy of the proposed method\n","authors":["Sarthak Mehrotra","Rishabh Jain","Mayur Hemani","Balaji Krishnamurthy","Mausoom Sarkar"],"pdf_url":"https://arxiv.org/pdf/2505.03394v1.pdf","comment":"Accepted in CVPR 2025 AI4CC workshop"},{"id":"http://arxiv.org/abs/2505.03383v1","updated":"2025-05-06T10:02:56Z","published":"2025-05-06T10:02:56Z","title":"Attention-aggregated Attack for Boosting the Transferability of Facial\n  Adversarial Examples","summary":"  Adversarial examples have revealed the vulnerability of deep learning models\nand raised serious concerns about information security. The transfer-based\nattack is a hot topic in black-box attacks that are practical to real-world\nscenarios where the training datasets, parameters, and structure of the target\nmodel are unknown to the attacker. However, few methods consider the\nparticularity of class-specific deep models for fine-grained vision tasks, such\nas face recognition (FR), giving rise to unsatisfactory attacking performance.\nIn this work, we first investigate what in a face exactly contributes to the\nembedding learning of FR models and find that both decisive and auxiliary\nfacial features are specific to each FR model, which is quite different from\nthe biological mechanism of human visual system. Accordingly we then propose a\nnovel attack method named Attention-aggregated Attack (AAA) to enhance the\ntransferability of adversarial examples against FR, which is inspired by the\nattention divergence and aims to destroy the facial features that are critical\nfor the decision-making of other FR models by imitating their attentions on the\nclean face images. Extensive experiments conducted on various FR models\nvalidate the superiority and robust effectiveness of the proposed method over\nexisting methods.\n","authors":["Jian-Wei Li","Wen-Ze Shao"],"pdf_url":"https://arxiv.org/pdf/2505.03383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03380v1","updated":"2025-05-06T10:00:08Z","published":"2025-05-06T10:00:08Z","title":"Reinforced Correlation Between Vision and Language for Precise Medical\n  AI Assistant","summary":"  Medical AI assistants support doctors in disease diagnosis, medical image\nanalysis, and report generation. However, they still face significant\nchallenges in clinical use, including limited accuracy with multimodal content\nand insufficient validation in real-world settings. We propose RCMed, a\nfull-stack AI assistant that improves multimodal alignment in both input and\noutput, enabling precise anatomical delineation, accurate localization, and\nreliable diagnosis through hierarchical vision-language grounding. A\nself-reinforcing correlation mechanism allows visual features to inform\nlanguage context, while language semantics guide pixel-wise attention, forming\na closed loop that refines both modalities. This correlation is enhanced by a\ncolor region description strategy, translating anatomical structures into\nsemantically rich text to learn shape-location-text relationships across\nscales. Trained on 20 million image-mask-description triplets, RCMed achieves\nstate-of-the-art precision in contextualizing irregular lesions and subtle\nanatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It\nachieved a 23.5% relative improvement in cell segmentation from microscopy\nimages over prior methods. RCMed's strong vision-language alignment enables\nexceptional generalization, with state-of-the-art performance in external\nvalidation across 20 clinically significant cancer types, including novel\ntasks. This work demonstrates how integrated multimodal models capture\nfine-grained patterns, enabling human-level interpretation in complex scenarios\nand advancing human-centric AI healthcare.\n","authors":["Haonan Wang","Jiaji Mao","Lehan Wang","Qixiang Zhang","Marawan Elbatel","Yi Qin","Huijun Hu","Baoxun Li","Wenhui Deng","Weifeng Qin","Hongrui Li","Jialin Liang","Jun Shen","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.03380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04280v2","updated":"2025-05-06T09:56:18Z","published":"2024-12-05T16:00:59Z","title":"HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based\n  Image Editing","summary":"  We present HumanEdit, a high-quality, human-rewarded dataset specifically\ndesigned for instruction-guided image editing, enabling precise and diverse\nimage manipulations through open-form language instructions. Previous\nlarge-scale editing datasets often incorporate minimal human feedback, leading\nto challenges in aligning datasets with human preferences. HumanEdit bridges\nthis gap by employing human annotators to construct data pairs and\nadministrators to provide feedback. With meticulously curation, HumanEdit\ncomprises 5,751 images and requires more than 2,500 hours of human effort\nacross four stages, ensuring both accuracy and reliability for a wide range of\nimage editing tasks. The dataset includes six distinct types of editing\ninstructions: Action, Add, Counting, Relation, Remove, and Replace,\nencompassing a broad spectrum of real-world scenarios. All images in the\ndataset are accompanied by masks, and for a subset of the data, we ensure that\nthe instructions are sufficiently detailed to support mask-free editing.\nFurthermore, HumanEdit offers comprehensive diversity and high-resolution $1024\n\\times 1024$ content sourced from various domains, setting a new versatile\nbenchmark for instructional image editing datasets. With the aim of advancing\nfuture research and establishing evaluation benchmarks in the field of image\nediting, we release HumanEdit at\nhttps://huggingface.co/datasets/BryanW/HumanEdit.\n","authors":["Jinbin Bai","Wei Chow","Ling Yang","Xiangtai Li","Juncheng Li","Hanwang Zhang","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2412.04280v2.pdf","comment":"Accepted to CVPR 2025 AI for Content Creation (AI4CC) Workshop. Codes\n  and Supplementary Material: https://github.com/viiika/HumanEdit"},{"id":"http://arxiv.org/abs/2505.03374v1","updated":"2025-05-06T09:49:45Z","published":"2025-05-06T09:49:45Z","title":"Reducing Annotation Burden in Physical Activity Research Using\n  Vision-Language Models","summary":"  Introduction: Data from wearable devices collected in free-living settings,\nand labelled with physical activity behaviours compatible with health research,\nare essential for both validating existing wearable-based measurement\napproaches and developing novel machine learning approaches. One common way of\nobtaining these labels relies on laborious annotation of sequences of images\ncaptured by cameras worn by participants through the course of a day. Methods:\nWe compare the performance of three vision language models and two\ndiscriminative models on two free-living validation studies with 161 and 111\nparticipants, collected in Oxfordshire, United Kingdom and Sichuan, China,\nrespectively, using the Autographer (OMG Life, defunct) wearable camera.\nResults: We found that the best open-source vision-language model (VLM) and\nfine-tuned discriminative model (DM) achieved comparable performance when\npredicting sedentary behaviour from single images on unseen participants in the\nOxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86,\n0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63,\n0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53,\n0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study,\nperformance fell across all intensity categories, with median Cohen's\nkappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM,\nand from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely\navailable computer vision models could help annotate sedentary behaviour,\ntypically the most prevalent activity of daily living, from wearable camera\nimages within similar populations to seen data, reducing the annotation burden.\n","authors":["Abram Schonfeldt","Benjamin Maylor","Xiaofang Chen","Ronald Clark","Aiden Doherty"],"pdf_url":"https://arxiv.org/pdf/2505.03374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03362v1","updated":"2025-05-06T09:37:04Z","published":"2025-05-06T09:37:04Z","title":"3D Surface Reconstruction with Enhanced High-Frequency Details","summary":"  Neural implicit 3D reconstruction can reproduce shapes without 3D\nsupervision, and it learns the 3D scene through volume rendering methods and\nneural implicit representations. Current neural surface reconstruction methods\ntend to randomly sample the entire image, making it difficult to learn\nhigh-frequency details on the surface, and thus the reconstruction results tend\nto be too smooth. We designed a method (FreNeuS) based on high-frequency\ninformation to solve the problem of insufficient surface detail. Specifically,\nFreNeuS uses pixel gradient changes to easily acquire high-frequency regions in\nan image and uses the obtained high-frequency information to guide surface\ndetail reconstruction. High-frequency information is first used to guide the\ndynamic sampling of rays, applying different sampling strategies according to\nvariations in high-frequency regions. To further enhance the focus on surface\ndetails, we have designed a high-frequency weighting method that constrains the\nrepresentation of high-frequency details during the reconstruction process.\nQualitative and quantitative results show that our method can reconstruct fine\nsurface details and obtain better surface reconstruction quality compared to\nexisting methods. In addition, our method is more applicable and can be\ngeneralized to any NeuS-based work.\n","authors":["Shikun Zhang","Yiqun Wang","Cunjian Chen","Yong Li","Qiuhong Ke"],"pdf_url":"https://arxiv.org/pdf/2505.03362v1.pdf","comment":"Accepted by Journal of Visual Communication and Image Representation"},{"id":"http://arxiv.org/abs/2505.03361v1","updated":"2025-05-06T09:30:30Z","published":"2025-05-06T09:30:30Z","title":"Interpretable Zero-shot Learning with Infinite Class Concepts","summary":"  Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images\nwith intermediate class semantics, like human-annotated concepts or class\ndefinitions. An emerging alternative leverages Large-scale Language Models\n(LLMs) to automatically generate class documents. However, these methods often\nface challenges with transparency in the classification process and may suffer\nfrom the notorious hallucination problem in LLMs, resulting in non-visual class\nsemantics. This paper redefines class semantics in ZSL with a focus on\ntransferability and discriminability, introducing a novel framework called\nZero-shot Learning with Infinite Class Concepts (InfZSL). Our approach\nleverages the powerful capabilities of LLMs to dynamically generate an\nunlimited array of phrase-level class concepts. To address the hallucination\nchallenge, we introduce an entropy-based scoring process that incorporates a\n``goodness\" concept selection mechanism, ensuring that only the most\ntransferable and discriminative concepts are selected. Our InfZSL framework not\nonly demonstrates significant improvements on three popular benchmark datasets\nbut also generates highly interpretable, image-grounded concepts. Code will be\nreleased upon acceptance.\n","authors":["Zihan Ye","Shreyank N Gowda","Shiming Chen","Yaochu Jin","Kaizhu Huang","Xiaobo Jin"],"pdf_url":"https://arxiv.org/pdf/2505.03361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00693v2","updated":"2025-05-06T09:24:22Z","published":"2025-05-01T17:55:05Z","title":"Robotic Visual Instruction","summary":"  Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision introduces\nchallenges for robotic task definition such as ambiguity and verbosity.\nMoreover, in some public settings where quiet is required, such as libraries or\nhospitals, verbal communication with robots is inappropriate. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment,enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Project\nwebsite: https://robotic-visual-instruction.github.io/\n","authors":["Yanbang Li","Ziyang Gong","Haoyang Li","Xiaoqi Huang","Haolan Kang","Guangping Bai","Xianzheng Ma"],"pdf_url":"https://arxiv.org/pdf/2505.00693v2.pdf","comment":"Project website: https://robotic-visual-instruction.github.io/"},{"id":"http://arxiv.org/abs/2505.03351v1","updated":"2025-05-06T09:19:16Z","published":"2025-05-06T09:19:16Z","title":"GUAVA: Generalizable Upper Body 3D Gaussian Avatar","summary":"  Reconstructing a high-quality, animatable 3D human avatar with expressive\nfacial and hand motions from a single image has gained significant attention\ndue to its broad application potential. 3D human avatar reconstruction\ntypically requires multi-view or monocular videos and training on individual\nIDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's\nexpressiveness, these methods often focus on body motion but struggle with\nfacial expressions. To address these challenges, we first introduce an\nexpressive human model (EHM) to enhance facial expression capabilities and\ndevelop an accurate tracking method. Based on this template model, we propose\nGUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar\nreconstruction. We leverage inverse texture mapping and projection sampling\ntechniques to infer Ubody (upper-body) Gaussians from a single image. The\nrendered images are refined through a neural refiner. Experimental results\ndemonstrate that GUAVA significantly outperforms previous methods in rendering\nquality and offers significant speed improvements, with reconstruction times in\nthe sub-second range (0.1s), and supports real-time animation and rendering.\n","authors":["Dongbin Zhang","Yunfei Liu","Lijian Lin","Ye Zhu","Yang Li","Minghan Qin","Yu Li","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03351v1.pdf","comment":"Project page: https://eastbeanzhang.github.io/GUAVA/"},{"id":"http://arxiv.org/abs/2505.03350v1","updated":"2025-05-06T09:19:12Z","published":"2025-05-06T09:19:12Z","title":"A Vision-Language Model for Focal Liver Lesion Classification","summary":"  Accurate classification of focal liver lesions is crucial for diagnosis and\ntreatment in hepatology. However, traditional supervised deep learning models\ndepend on large-scale annotated datasets, which are often limited in medical\nimaging. Recently, Vision-Language models (VLMs) such as Contrastive\nLanguage-Image Pre-training model (CLIP) has been applied to image\nclassifications. Compared to the conventional convolutional neural network\n(CNN), which classifiers image based on visual information only, VLM leverages\nmultimodal learning with text and images, allowing it to learn effectively even\nwith a limited amount of labeled data. Inspired by CLIP, we pro-pose a\nLiver-VLM, a model specifically designed for focal liver lesions (FLLs)\nclassification. First, Liver-VLM incorporates class information into the text\nencoder without introducing additional inference overhead. Second, by\ncalculating the pairwise cosine similarities between image and text embeddings\nand optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively\naligns image features with class-level text features. Experimental results on\nMPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the\nstandard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve\n(AUC). Further analysis shows that using a lightweight ResNet18 backbone\nenhances classification performance, particularly under data-constrained\nconditions.\n","authors":["Song Jian","Hu Yuchang","Wang Hui","Chen Yen-Wei"],"pdf_url":"https://arxiv.org/pdf/2505.03350v1.pdf","comment":"9 pages,4 figures, 4 tables,Innovation in Medicine and Healthcare\n  Proceedings of 13th KES-InMed 2025"},{"id":"http://arxiv.org/abs/2504.21561v2","updated":"2025-05-06T09:18:40Z","published":"2025-04-30T12:01:27Z","title":"Iterative Tool Usage Exploration for Multimodal Agents via Step-wise\n  Preference Tuning","summary":"  Multimodal agents, which integrate a controller (e.g., a large language\nmodel) with external tools, have demonstrated remarkable capabilities in\ntackling complex tasks. However, existing agents need to collect a large number\nof expert data for fine-tuning to adapt to new environments. In this paper, we\npropose an online self-exploration method for multimodal agents, namely SPORT,\nvia step-wise preference optimization to refine the trajectories of agents,\nwhich automatically generates tasks and learns from solving the generated\ntasks, without any expert annotation. SPORT operates through four iterative\ncomponents: task synthesis, step sampling, step verification, and preference\ntuning. First, we synthesize multi-modal tasks using language models. Then, we\nintroduce a novel search scheme, where step sampling and step verification are\nexecuted alternately to solve each generated task. We employ a verifier to\nprovide AI feedback to construct step-wise preference data. The data is\nsubsequently used to update the controller's policy through preference tuning,\nproducing a SPORT Agent. By interacting with real environments, the SPORT Agent\nevolves into a more refined and capable system. Evaluation in the GTA and GAIA\nbenchmarks show that the SPORT Agent achieves 6.41\\% and 3.64\\% improvements,\nunderscoring the generalization and effectiveness introduced by our method. The\nproject page is https://SPORT-Agents.github.io.\n","authors":["Pengxiang Li","Zhi Gao","Bofei Zhang","Yapeng Mi","Xiaojian Ma","Chenrui Shi","Tao Yuan","Yuwei Wu","Yunde Jia","Song-Chun Zhu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2504.21561v2.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.03334v1","updated":"2025-05-06T09:07:52Z","published":"2025-05-06T09:07:52Z","title":"From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set\n  Aerial Detection","summary":"  In recent years, language-guided open-world aerial object detection has\ngained significant attention due to its better alignment with real-world\napplication needs. However, due to limited datasets, most existing\nlanguage-guided methods primarily focus on vocabulary, which fails to meet the\ndemands of more fine-grained open-world detection. To address this limitation,\nwe propose constructing a large-scale language-guided open-set aerial detection\ndataset, encompassing three levels of language guidance: from words to phrases,\nand ultimately to sentences. Centered around an open-source large\nvision-language model and integrating image-operation-based preprocessing with\nBERT-based postprocessing, we present the OS-W2S Label Engine, an automatic\nannotation pipeline capable of handling diverse scene annotations for aerial\nimages. Using this label engine, we expand existing aerial detection datasets\nwith rich textual annotations and construct a novel benchmark dataset, called\nMulti-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of\ncurrent remote sensing grounding data and enabling effective open-set aerial\ndetection. Specifically, MI-OAD contains 163,023 images and 2 million\nimage-caption pairs, approximately 40 times larger than comparable datasets. We\nalso employ state-of-the-art open-set methods from the natural image domain,\ntrained on our proposed dataset, to validate the model's open-set detection\ncapabilities. For instance, when trained on our dataset, Grounding DINO\nachieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs\nunder zero-shot transfer conditions. Both the dataset and the label engine will\nbe released publicly.\n","authors":["Guoting Wei","Yu Liu","Xia Yuan","Xizhe Xue","Linlin Guo","Yifan Yang","Chunxia Zhao","Zongwen Bai","Haokui Zhang","Rong Xiao"],"pdf_url":"https://arxiv.org/pdf/2505.03334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02768v2","updated":"2025-05-06T09:02:57Z","published":"2024-09-17T05:17:37Z","title":"Uncertainty-Guided Self-Questioning and Answering for Video-Language\n  Alignment","summary":"  The development of multi-modal models has been rapidly advancing, with some\ndemonstrating remarkable capabilities. However, annotating video-text pairs\nremains expensive and insufficient. Take video question answering (VideoQA)\ntasks as an example, human annotated questions and answers often cover only\npart of the video, since the corresponding text is often short and monotonous,\nleading to underutilization of video. To address this, we propose a\nBootstrapping Video-Language Alignment framework (BoViLA), a self-training\nmethod that augments question samples during training process through LLM-based\nself-questioning and answering, which help model exploit video information and\nthe internal knowledge of LLMs more thoroughly to improve modality alignment.\nHowever, low-quality self-generated questions may instead contaminate the\nperformance, especially in the early stages of training, as we have observed in\nour experiments. To filter bad self-generated questions, we introduce\nEvidential Deep Learning (EDL) to estimate uncertainty and assess the quality\nof self-generated questions by evaluating the modality alignment within the\ncontext. To the best of our knowledge, this work is the first to explore\nLLM-based self-training frameworks for modality alignment. We evaluate BoViLA\non five strong VideoQA benchmarks, where it outperforms several\nstate-of-the-art methods and demonstrate its effectiveness and generality.\nAdditionally, we provide extensive analyses of the self-training framework and\nthe EDL-based uncertainty filtering mechanism. The code will be made available.\n","authors":["Jin Chen","Kaijing Ma","Haojian Huang","Han Fang","Hao Sun","Mehdi Hosseinzadeh","Zhe Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08387v3","updated":"2025-05-06T09:00:02Z","published":"2023-10-12T14:59:22Z","title":"Aligning Data Selection with Performance: Performance-driven\n  Reinforcement Learning for Active Learning in Object Detection","summary":"  Active learning strategies aim to train high-performance models with minimal\nlabeled data by selecting the most informative instances for labeling. However,\nexisting methods for assessing data informativeness often fail to align\ndirectly with task model performance metrics, such as mean average precision\n(mAP) in object detection. This paper introduces Mean-AP Guided Reinforced\nActive Learning for Object Detection (MGRAL), a novel approach that leverages\nthe concept of expected model output changes as informativeness for deep\ndetection networks, directly optimizing the sampling strategy using mAP. MGRAL\nemploys a reinforcement learning agent based on LSTM architecture to\nefficiently navigate the combinatorial challenge of batch sample selection and\nthe non-differentiable nature between performance and selected batches. The\nagent optimizes selection using policy gradient with mAP improvement as the\nreward signal. To address the computational intensity of mAP estimation with\nunlabeled samples, we implement fast look-up tables, ensuring real-world\nfeasibility. We evaluate MGRAL on PASCAL VOC and MS COCO benchmarks across\nvarious backbone architectures. Our approach demonstrates strong performance,\nestablishing a new paradigm in reinforcement learning-based active learning for\nobject detection.\n","authors":["Zhixuan Liang","Xingyu Zeng","Rui Zhao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2310.08387v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03329v1","updated":"2025-05-06T08:56:28Z","published":"2025-05-06T08:56:28Z","title":"FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for\n  Scene Text Editing","summary":"  The task of scene text editing is to modify or add texts on images while\nmaintaining the fidelity of newly generated text and visual coherence with the\nbackground. Recent works based on latent diffusion models (LDM) show improved\ntext editing results, yet still face challenges and often generate inaccurate\nor unrecognizable characters, especially for non-Latin ones (\\eg, Chinese),\nwhich have complex glyph structures. To address these issues, we present\nFLUX-Text, a simple and advanced multilingual scene text editing framework\nbased on FLUX-Fill. Specifically, we carefully investigate glyph conditioning,\nconsidering both visual and textual modalities. To retain the original\ngenerative capabilities of FLUX-Fill while enhancing its understanding and\ngeneration of glyphs, we propose lightweight glyph and text embedding modules.\nOwning to the lightweight design, FLUX-Text is trained only with $100K$\ntraining examples compared to current popular methods trained with 2.9M ones.\nWith no bells and whistles, our method achieves state-of-the-art performance on\ntext editing tasks. Qualitative and quantitative experiments on the public\ndatasets demonstrate that our method surpasses previous works in text fidelity.\n","authors":["Rui Lan","Yancheng Bai","Xu Duan","Mingxing Li","Lei Sun","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2505.03329v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.03327v1","updated":"2025-05-06T08:54:28Z","published":"2025-05-06T08:54:28Z","title":"Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and\n  Self-Supervised Learning","summary":"  Deep learning models have shown encouraging capabilities for mapping\naccurately forests at medium resolution with TanDEM-X interferometric SAR data.\nSuch models, as most of current state-of-the-art deep learning techniques in\nremote sensing, are trained in a fully-supervised way, which requires a large\namount of labeled data for training and validation. In this work, our aim is to\nexploit the high-resolution capabilities of the TanDEM-X mission to map forests\nat 6 m. The goal is to overcome the intrinsic limitations posed by\nmidresolution products, which affect, e.g., the detection of narrow roads\nwithin vegetated areas and the precise delineation of forested regions\ncontours. To cope with the lack of extended reliable reference datasets at such\na high resolution, we investigate self-supervised learning techniques for\nextracting highly informative representations from the input features, followed\nby a supervised training step with a significantly smaller number of reliable\nlabels. A 1 m resolution forest/non-forest reference map over Pennsylvania,\nUSA, allows for comparing different training approaches for the development of\nan effective forest mapping framework with limited labeled samples. We select\nthe best-performing approach over this test region and apply it in a real-case\nforest mapping scenario over the Amazon rainforest, where only very few labeled\ndata at high resolution are available. In this challenging scenario, the\nproposed self-supervised framework significantly enhances the classification\naccuracy with respect to fully-supervised methods, trained using the same\namount of labeled data, representing an extremely promising starting point for\nlarge-scale, very high-resolution forest mapping with TanDEM-X data.\n","authors":["José-Luis Bueso-Bello","Benjamin Chauvel","Daniel Carcereri","Philipp Posovszky","Pietro Milillo","Jennifer Ruiz","Juan-Carlos Fernández-Diaz","Carolina González","Michele Martone","Ronny Hänsch","Paola Rizzoli"],"pdf_url":"https://arxiv.org/pdf/2505.03327v1.pdf","comment":"Preprint submitted to Remote Sensing of Environment"},{"id":"http://arxiv.org/abs/2505.03319v1","updated":"2025-05-06T08:47:14Z","published":"2025-05-06T08:47:14Z","title":"SD-VSum: A Method and Dataset for Script-Driven Video Summarization","summary":"  In this work, we introduce the task of script-driven video summarization,\nwhich aims to produce a summary of the full-length video by selecting the parts\nthat are most relevant to a user-provided script outlining the visual content\nof the desired summary. Following, we extend a recently-introduced large-scale\ndataset for generic video summarization (VideoXum) by producing natural\nlanguage descriptions of the different human-annotated summaries that are\navailable per video. In this way we make it compatible with the introduced\ntask, since the available triplets of ``video, summary and summary\ndescription'' can be used for training a method that is able to produce\ndifferent summaries for a given video, driven by the provided script about the\ncontent of each summary. Finally, we develop a new network architecture for\nscript-driven video summarization (SD-VSum), that relies on the use of a\ncross-modal attention mechanism for aligning and fusing information from the\nvisual and text modalities. Our experimental evaluations demonstrate the\nadvanced performance of SD-VSum against state-of-the-art approaches for\nquery-driven and generic (unimodal and multimodal) summarization from the\nliterature, and document its capacity to produce video summaries that are\nadapted to each user's needs about their content.\n","authors":["Manolis Mylonas","Evlampios Apostolidis","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2505.03319v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.03318v1","updated":"2025-05-06T08:46:41Z","published":"2025-05-06T08:46:41Z","title":"Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning","summary":"  Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.\n","authors":["Yibin Wang","Zhimin Li","Yuhang Zang","Chunyu Wang","Qinglin Lu","Cheng Jin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03318v1.pdf","comment":"project page: https://codegoat24.github.io/UnifiedReward/think"},{"id":"http://arxiv.org/abs/2505.03310v1","updated":"2025-05-06T08:42:39Z","published":"2025-05-06T08:42:39Z","title":"3D Gaussian Splatting Data Compression with Mixture of Priors","summary":"  3D Gaussian Splatting (3DGS) data compression is crucial for enabling\nefficient storage and transmission in 3D scene modeling. However, its\ndevelopment remains limited due to inadequate entropy models and suboptimal\nquantization strategies for both lossless and lossy compression scenarios,\nwhere existing methods have yet to 1) fully leverage hyperprior information to\nconstruct robust conditional entropy models, and 2) apply fine-grained,\nelement-wise quantization strategies for improved compression granularity. In\nthis work, we propose a novel Mixture of Priors (MoP) strategy to\nsimultaneously address these two challenges. Specifically, inspired by the\nMixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior\ninformation through multiple lightweight MLPs to generate diverse prior\nfeatures, which are subsequently integrated into the MoP feature via a gating\nmechanism. To enhance lossless compression, the resulting MoP feature is\nutilized as a hyperprior to improve conditional entropy modeling. Meanwhile,\nfor lossy compression, we employ the MoP feature as guidance information in an\nelement-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine\nQuantization (C2FQ) strategy with a predefined quantization step value.\nSpecifically, we expand the quantization step value into a matrix and\nadaptively refine it from coarse to fine granularity, guided by the MoP\nfeature, thereby obtaining a quantization step matrix that facilitates\nelement-wise quantization. Extensive experiments demonstrate that our proposed\n3DGS data compression framework achieves state-of-the-art performance across\nmultiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and\nTank&Temples.\n","authors":["Lei Liu","Zhenghao Chen","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2505.03310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03303v1","updated":"2025-05-06T08:36:01Z","published":"2025-05-06T08:36:01Z","title":"Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices","summary":"  This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.\n","authors":["Tasnim Shahriar"],"pdf_url":"https://arxiv.org/pdf/2505.03303v1.pdf","comment":"22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis"},{"id":"http://arxiv.org/abs/2305.18708v2","updated":"2025-05-06T08:34:42Z","published":"2023-05-30T03:24:09Z","title":"Infrared Image Deturbulence Restoration Using Degradation\n  Parameter-Assisted Wide & Deep Learning","summary":"  Infrared images captured under turbulent conditions are degraded by complex\ngeometric distortions and blur. We address infrared deturbulence as an image\nrestoration task, proposing DparNet, a parameter-assisted multi-frame network\nwith a wide & deep architecture. DparNet learns a degradation prior (key\nparameter matrix) directly from degraded images without external knowledge. Its\nwide & deep architecture uses these learned parameters to directly modulate\nrestoration, achieving spatially and intensity adaptive results. Evaluated on\ndedicated infrared deturbulence (49,744 images) and visible image denoising\n(109,536 images) datasets, DparNet significantly outperforms State-of-the-Art\n(SOTA) methods in restoration performance and efficiency. Notably, leveraging\nthese parameters improves PSNR by 0.6-1.1 dB with less than 2% increase in\nmodel parameters and computational complexity. Our work demonstrates that\ndegraded images hide key degradation information that can be learned and\nutilized to boost adaptive image restoration.\n","authors":["Yi Lu","Yadong Wang","Xingbo Jiang","Xiangzhi Bai"],"pdf_url":"https://arxiv.org/pdf/2305.18708v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03300v1","updated":"2025-05-06T08:31:32Z","published":"2025-05-06T08:31:32Z","title":"3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds\n  Using Sensor-Intensity-Based 2D Semantic Segmentation","summary":"  Semantic segmentation of 3D LiDAR point clouds, essential for autonomous\ndriving and infrastructure management, is best achieved by supervised learning,\nwhich demands extensive annotated datasets and faces the problem of domain\nshifts. We introduce a new 3D semantic segmentation pipeline that leverages\naligned scenes and state-of-the-art 2D segmentation methods, avoiding the need\nfor direct 3D annotation or reliance on additional modalities such as camera\nimages at inference time. Our approach generates 2D views from LiDAR scans\ncolored by sensor intensity and applies 2D semantic segmentation to these views\nusing a camera-domain pretrained model. The segmented 2D outputs are then\nback-projected onto the 3D points, with a simple voting-based estimator that\nmerges the labels associated to each 3D point. Our main contribution is a\nglobal pipeline for 3D semantic segmentation requiring no prior 3D annotation\nand not other modality for inference, which can be used for pseudo-label\ngeneration. We conduct a thorough ablation study and demonstrate the potential\nof the generated pseudo-labels for the Unsupervised Domain Adaptation task.\n","authors":["Andrew Caunes","Thierry Chateau","Vincent Frémont"],"pdf_url":"https://arxiv.org/pdf/2505.03300v1.pdf","comment":"Accepted to IV2024"},{"id":"http://arxiv.org/abs/2505.03299v1","updated":"2025-05-06T08:29:18Z","published":"2025-05-06T08:29:18Z","title":"Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A\n  Capabilities Encoding Approach","summary":"  Foundation models constitute a significant advancement in computer vision:\nafter a single, albeit costly, training phase, they can address a wide array of\ntasks. In the field of Earth observation, over 75 remote sensing vision\nfoundation models have been developed in the past four years. However, none has\nconsistently outperformed the others across all available downstream tasks. To\nfacilitate their comparison, we propose a cost-effective method for predicting\na model's performance on multiple downstream tasks without the need for\nfine-tuning on each one. This method is based on what we call \"capabilities\nencoding.\" The utility of this novel approach is twofold: we demonstrate its\npotential to simplify the selection of a foundation model for a given new task,\nand we employ it to offer a fresh perspective on the existing literature,\nsuggesting avenues for future research. Codes are available at\nhttps://github.com/pierreadorni/capabilities-encoding.\n","authors":["Pierre Adorni","Minh-Tan Pham","Stéphane May","Sébastien Lefèvre"],"pdf_url":"https://arxiv.org/pdf/2505.03299v1.pdf","comment":"Accepted at the MORSE workshop of CVPR 2025"},{"id":"http://arxiv.org/abs/2407.06606v4","updated":"2025-05-06T08:24:18Z","published":"2024-07-09T07:15:56Z","title":"Tailored Design of Audio-Visual Speech Recognition Models using\n  Branchformers","summary":"  Recent advances in Audio-Visual Speech Recognition (AVSR) have led to\nunprecedented achievements in the field, improving the robustness of this type\nof system in adverse, noisy environments. In most cases, this task has been\naddressed through the design of models composed of two independent encoders,\neach dedicated to a specific modality. However, while recent works have\nexplored unified audio-visual encoders, determining the optimal cross-modal\narchitecture remains an ongoing challenge. Furthermore, such approaches often\nrely on models comprising vast amounts of parameters and high computational\ncost training processes. In this paper, we aim to bridge this research gap by\nintroducing a novel audio-visual framework. Our proposed method constitutes, to\nthe best of our knowledge, the first attempt to harness the flexibility and\ninterpretability offered by encoder architectures, such as the Branchformer, in\nthe design of parameter-efficient AVSR systems. To be more precise, the\nproposed framework consists of two steps: first, estimating audio- and\nvideo-only systems, and then designing a tailored audio-visual unified encoder\nbased on the layer-level branch scores provided by the modality-specific\nmodels. Extensive experiments on English and Spanish AVSR benchmarks covering\nmultiple data conditions and scenarios demonstrated the effectiveness of our\nproposed method. Even when trained on a moderate scale of data, our models\nachieve competitive word error rates (WER) of approximately 2.5\\% for English\nand surpass existing approaches for Spanish, establishing a new benchmark with\nan average WER of around 9.1\\%. These results reflect how our tailored AVSR\nsystem is able to reach state-of-the-art recognition rates while significantly\nreducing the model complexity w.r.t. the prevalent approach in the field. Code\nand pre-trained models are available at\nhttps://github.com/david-gimeno/tailored-avsr.\n","authors":["David Gimeno-Gómez","Carlos-D. Martínez-Hinarejos"],"pdf_url":"https://arxiv.org/pdf/2407.06606v4.pdf","comment":"Accepted in Computer Speech & Language journal of Elsevier"},{"id":"http://arxiv.org/abs/2411.15106v2","updated":"2025-05-06T08:18:58Z","published":"2024-11-22T18:09:27Z","title":"About Time: Advances, Challenges, and Outlooks of Action Understanding","summary":"  We have witnessed impressive advances in video action understanding.\nIncreased dataset sizes, variability, and computation availability have enabled\nleaps in performance and task diversification. Current systems can provide\ncoarse- and fine-grained descriptions of video scenes, extract segments\ncorresponding to queries, synthesize unobserved parts of videos, and predict\ncontext across multiple modalities. This survey comprehensively reviews\nadvances in uni- and multi-modal action understanding across a range of tasks.\nWe focus on prevalent challenges, overview widely adopted datasets, and survey\nseminal works with an emphasis on recent advances. We broadly distinguish\nbetween three temporal scopes: (1) recognition tasks of actions observed in\nfull, (2) prediction tasks for ongoing partially observed actions, and (3)\nforecasting tasks for subsequent unobserved action(s). This division allows us\nto identify specific action modeling and video representation challenges.\nFinally, we outline future directions to address current shortcomings.\n","authors":["Alexandros Stergiou","Ronald Poppe"],"pdf_url":"https://arxiv.org/pdf/2411.15106v2.pdf","comment":"Accepted at the International Journal of Computer Vision (IJCV)"},{"id":"http://arxiv.org/abs/2505.03286v1","updated":"2025-05-06T08:14:07Z","published":"2025-05-06T08:14:07Z","title":"Base-Detail Feature Learning Framework for Visible-Infrared Person\n  Re-Identification","summary":"  Visible-infrared person re-identification (VIReID) provides a solution for\nReID tasks in 24-hour scenarios; however, significant challenges persist in\nachieving satisfactory performance due to the substantial discrepancies between\nvisible (VIS) and infrared (IR) modalities. Existing methods inadequately\nleverage information from different modalities, primarily focusing on digging\ndistinguishing features from modality-shared information while neglecting\nmodality-specific details. To fully utilize differentiated minutiae, we propose\na Base-Detail Feature Learning Framework (BDLF) that enhances the learning of\nboth base and detail knowledge, thereby capitalizing on both modality-shared\nand modality-specific information. Specifically, the proposed BDLF mines detail\nand base features through a lossless detail feature extraction module and a\ncomplementary base embedding generation mechanism, respectively, supported by a\nnovel correlation restriction method that ensures the features gained by BDLF\nenrich both detail and base knowledge across VIS and IR features. Comprehensive\nexperiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the\neffectiveness of BDLF.\n","authors":["Zhihao Gong","Lian Wu","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2505.03286v1.pdf","comment":"9 pages, 5 figures, 2025 34th International Joint Conference on\n  Artificial Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2505.03284v1","updated":"2025-05-06T08:12:31Z","published":"2025-05-06T08:12:31Z","title":"OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for\n  3D Semantic Occupancy Prediction","summary":"  The safe operation of autonomous vehicles (AVs) is highly dependent on their\nunderstanding of the surroundings. For this, the task of 3D semantic occupancy\nprediction divides the space around the sensors into voxels, and labels each\nvoxel with both occupancy and semantic information. Recent perception models\nhave used multisensor fusion to perform this task. However, existing\nmultisensor fusion-based approaches focus mainly on using sensor information in\nthe Cartesian coordinate system. This ignores the distribution of the sensor\nreadings, leading to a loss of fine-grained details and performance\ndegradation. In this paper, we propose OccCylindrical that merges and refines\nthe different modality features under cylindrical coordinates. Our method\npreserves more fine-grained geometry detail that leads to better performance.\nExtensive experiments conducted on the nuScenes dataset, including challenging\nrainy and nighttime scenarios, confirm our approach's effectiveness and\nstate-of-the-art performance. The code will be available at:\nhttps://github.com/DanielMing123/OccCylindrical\n","authors":["Zhenxing Ming","Julie Stephany Berrio","Mao Shan","Yaoqi Huang","Hongyu Lyu","Nguyen Hoang Khoi Tran","Tzu-Yun Tseng","Stewart Worrall"],"pdf_url":"https://arxiv.org/pdf/2505.03284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03239v4","updated":"2025-05-06T07:55:57Z","published":"2024-11-05T16:37:30Z","title":"Decoupling Fine Detail and Global Geometry for Compressed Depth Map\n  Super-Resolution","summary":"  Recovering high-quality depth maps from compressed sources has gained\nsignificant attention due to the limitations of consumer-grade depth cameras\nand the bandwidth restrictions during data transmission. However, current\nmethods still suffer from two challenges. First, bit-depth compression produces\na uniform depth representation in regions with subtle variations, hindering the\nrecovery of detailed information. Second, densely distributed random noise\nreduces the accuracy of estimating the global geometric structure of the scene.\nTo address these challenges, we propose a novel framework, termed\ngeometry-decoupled network (GDNet), for compressed depth map super-resolution\nthat decouples the high-quality depth map reconstruction process by handling\nglobal and detailed geometric features separately. To be specific, we propose\nthe fine geometry detail encoder (FGDE), which is designed to aggregate fine\ngeometry details in high-resolution low-level image features while\nsimultaneously enriching them with complementary information from\nlow-resolution context-level image features. In addition, we develop the global\ngeometry encoder (GGE) that aims at suppressing noise and extracting global\ngeometric information effectively via constructing compact feature\nrepresentation in a low-rank space. We conduct experiments on multiple\nbenchmark datasets, demonstrating that our GDNet significantly outperforms\ncurrent methods in terms of geometric consistency and detail recovery. In the\nECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st\nplace award. Our codes are available at: https://github.com/Ian0926/GDNet.\n","authors":["Huan Zheng","Wencheng Han","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2411.03239v4.pdf","comment":"Accepted by CVPR 2025 & The 1st place award for the ECCV 2024 AIM\n  Compressed Depth Upsampling Challenge"},{"id":"http://arxiv.org/abs/2505.03261v1","updated":"2025-05-06T07:42:24Z","published":"2025-05-06T07:42:24Z","title":"DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor","summary":"  Video Quality Assessment (VQA) aims to evaluate video quality based on\nperceptual distortions and human preferences. Despite the promising performance\nof existing methods using Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs), they often struggle to align closely with human\nperceptions, particularly in diverse real-world scenarios. This challenge is\nexacerbated by the limited scale and diversity of available datasets. To\naddress this limitation, we introduce a novel VQA framework, DiffVQA, which\nharnesses the robust generalization capabilities of diffusion models\npre-trained on extensive datasets. Our framework adapts these models to\nreconstruct identical input frames through a control module. The adapted\ndiffusion model is then used to extract semantic and distortion features from a\nresizing branch and a cropping branch, respectively. To enhance the model's\nability to handle long-term temporal dynamics, a parallel Mamba module is\nintroduced, which extracts temporal coherence augmented features that are\nmerged with the diffusion features to predict the final score. Experiments\nacross multiple datasets demonstrate DiffVQA's superior performance on\nintra-dataset evaluations and its exceptional generalization across datasets.\nThese results confirm that leveraging a diffusion model as a feature extractor\ncan offer enhanced VQA performance compared to CNN and ViT backbones.\n","authors":["Wei-Ting Chen","Yu-Jiet Vong","Yi-Tsung Lee","Sy-Yen Kuo","Qiang Gao","Sizhuo Ma","Jian Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03254v1","updated":"2025-05-06T07:32:24Z","published":"2025-05-06T07:32:24Z","title":"PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for\n  Efficient CNNs","summary":"  Convolutional neural networks (CNNs) are crucial for computer vision tasks on\nresource-constrained devices. Quantization effectively compresses these models,\nreducing storage size and energy cost. However, in modern depthwise-separable\narchitectures, the computational cost is distributed unevenly across its\ncomponents, with pointwise operations being the most expensive. By applying a\ngeneral quantization scheme to this imbalanced cost distribution, existing\nquantization approaches fail to fully exploit potential efficiency gains. To\nthis end, we introduce PROM, a straightforward approach for quantizing modern\ndepthwise-separable convolutional networks by selectively using two distinct\nbit-widths. Specifically, pointwise convolutions are quantized to ternary\nweights, while the remaining modules use 8-bit weights, which is achieved\nthrough a simple quantization-aware training procedure. Additionally, by\nquantizing activations to 8-bit, our method transforms pointwise convolutions\nwith ternary weights into int8 additions, which enjoy broad support across\nhardware platforms and effectively eliminates the need for expensive\nmultiplications. Applying PROM to MobileNetV2 reduces the model's energy cost\nby more than an order of magnitude (23.9x) and its storage size by 2.7x\ncompared to the float16 baseline while retaining similar classification\nperformance on ImageNet. Our method advances the Pareto frontier for energy\nconsumption vs. top-1 accuracy for quantized convolutional models on ImageNet.\nPROM addresses the challenges of quantizing depthwise-separable convolutional\nnetworks to both ternary and 8-bit weights, offering a simple way to reduce\nenergy cost and storage size.\n","authors":["Lukas Meiner","Jens Mehnert","Alexandru Paul Condurache"],"pdf_url":"https://arxiv.org/pdf/2505.03254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02549v2","updated":"2025-05-06T07:22:39Z","published":"2025-05-05T10:36:52Z","title":"Robust Duality Learning for Unsupervised Visible-Infrared Person\n  Re-Identification","summary":"  Unsupervised visible-infrared person re-identification (UVI-ReID) aims to\nretrieve pedestrian images across different modalities without costly\nannotations, but faces challenges due to the modality gap and lack of\nsupervision. Existing methods often adopt self-training with\nclustering-generated pseudo-labels but implicitly assume these labels are\nalways correct. In practice, however, this assumption fails due to inevitable\npseudo-label noise, which hinders model learning. To address this, we introduce\na new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),\ncharacterized by three key challenges: noise overfitting, error accumulation,\nand noisy cluster correspondence. To this end, we propose a novel Robust\nDuality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy\npseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning\nmechanism (RAL) is proposed to dynamically emphasize clean samples while\ndown-weighting noisy ones. Second, to alleviate error accumulation-where the\nmodel reinforces its own mistakes-RoDE employs dual distinct models that are\nalternately trained using pseudo-labels from each other, encouraging diversity\nand preventing collapse. However, this dual-model strategy introduces\nmisalignment between clusters across models and modalities, creating noisy\ncluster correspondence. To resolve this, we introduce Cluster Consistency\nMatching (CCM), which aligns clusters across models and modalities by measuring\ncross-cluster similarity. Extensive experiments on three benchmarks demonstrate\nthe effectiveness of RoDE.\n","authors":["Yongxiang Li","Yuan Sun","Yang Qin","Dezhong Peng","Xi Peng","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2505.02549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06690v2","updated":"2025-05-06T07:21:59Z","published":"2024-12-09T17:32:54Z","title":"FedSynthCT-Brain: A Federated Learning Framework for Multi-Institutional\n  Brain MRI-to-CT Synthesis","summary":"  The generation of Synthetic Computed Tomography (sCT) images has become a\npivotal methodology in modern clinical practice, particularly in the context of\nRadiotherapy (RT) treatment planning. The use of sCT enables the calculation of\ndoses, pushing towards Magnetic Resonance Imaging (MRI) guided radiotherapy\ntreatments. Deep learning methods for MRI-to-sCT have shown promising results,\nbut their reliance on single-centre training dataset limits generalisation\ncapabilities to diverse clinical settings. Moreover, creating centralised\nmulti-centre datasets may pose privacy concerns. To address the aforementioned\nissues, we introduced FedSynthCT-Brain, an approach based on the Federated\nLearning (FL) paradigm for MRI-to-sCT in brain imaging. This is among the first\napplications of FL for MRI-to-sCT, employing a cross-silo horizontal FL\napproach that allows multiple centres to collaboratively train a U-Net-based\ndeep learning model. We validated our method using real multicentre data from\nfour European and American centres, simulating heterogeneous scanner types and\nacquisition modalities, and tested its performance on an independent dataset\nfrom a centre outside the federation. In the case of the unseen centre, the\nfederated model achieved a median Mean Absolute Error (MAE) of $102.0$ HU\nacross 23 patients, with an interquartile range of $96.7-110.5$ HU. The median\n(interquartile range) for the Structural Similarity Index (SSIM) and the Peak\nSignal to Noise Ratio (PNSR) were $0.89 (0.86-0.89)$ and $26.58 (25.52-27.42)$,\nrespectively. The analysis of the results showed acceptable performances of the\nfederated approach, thus highlighting the potential of FL to enhance MRI-to-sCT\nto improve generalisability and advancing safe and equitable clinical\napplications while fostering collaboration and preserving data privacy.\n","authors":["Ciro Benito Raggio","Mathias Krohmer Zabaleta","Nils Skupien","Oliver Blanck","Francesco Cicone","Giuseppe Lucio Cascini","Paolo Zaffino","Lucia Migliorelli","Maria Francesca Spadea"],"pdf_url":"https://arxiv.org/pdf/2412.06690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03242v1","updated":"2025-05-06T07:14:10Z","published":"2025-05-06T07:14:10Z","title":"Seeing the Abstract: Translating the Abstract Language for Vision\n  Language Models","summary":"  Natural language goes beyond dryly describing visual content. It contains\nrich abstract concepts to express feeling, creativity and properties that\ncannot be directly perceived. Yet, current research in Vision Language Models\n(VLMs) has not shed light on abstract-oriented language. Our research breaks\nnew ground by uncovering its wide presence and under-estimated value, with\nextensive analysis. Particularly, we focus our investigation on the fashion\ndomain, a highly-representative field with abstract expressions. By analyzing\nrecent large-scale multimodal fashion datasets, we find that abstract terms\nhave a dominant presence, rivaling the concrete ones, providing novel\ninformation, and being useful in the retrieval task. However, a critical\nchallenge emerges: current general-purpose or fashion-specific VLMs are\npre-trained with databases that lack sufficient abstract words in their text\ncorpora, thus hindering their ability to effectively represent\nabstract-oriented language. We propose a training-free and model-agnostic\nmethod, Abstract-to-Concrete Translator (ACT), to shift abstract\nrepresentations towards well-represented concrete ones in the VLM latent space,\nusing pre-trained models and existing multimodal databases. On the\ntext-to-image retrieval task, despite being training-free, ACT outperforms the\nfine-tuned VLMs in both same- and cross-dataset settings, exhibiting its\neffectiveness with a strong generalization capability. Moreover, the\nimprovement introduced by ACT is consistent with various VLMs, making it a\nplug-and-play solution.\n","authors":["Davide Talon","Federico Girella","Ziyue Liu","Marco Cristani","Yiming Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03242v1.pdf","comment":"Accepted to CVPR25. Project page:\n  https://davidetalon.github.io/fashionact-page/"},{"id":"http://arxiv.org/abs/2501.18630v2","updated":"2025-05-06T07:02:33Z","published":"2025-01-27T18:58:43Z","title":"Deformable Beta Splatting","summary":"  3D Gaussian Splatting (3DGS) has advanced radiance field reconstruction by\nenabling real-time rendering. However, its reliance on Gaussian kernels for\ngeometry and low-order Spherical Harmonics (SH) for color encoding limits its\nability to capture complex geometries and diverse colors. We introduce\nDeformable Beta Splatting (DBS), a deformable and compact approach that\nenhances both geometry and color representation. DBS replaces Gaussian kernels\nwith deformable Beta Kernels, which offer bounded support and adaptive\nfrequency control to capture fine geometric details with higher fidelity while\nachieving better memory efficiency. In addition, we extended the Beta Kernel to\ncolor encoding, which facilitates improved representation of diffuse and\nspecular components, yielding superior results compared to SH-based methods.\nFurthermore, Unlike prior densification techniques that depend on Gaussian\nproperties, we mathematically prove that adjusting regularized opacity alone\nensures distribution-preserved Markov chain Monte Carlo (MCMC), independent of\nthe splatting kernel type. Experimental results demonstrate that DBS achieves\nstate-of-the-art visual quality while utilizing only 45% of the parameters and\nrendering 1.5x faster than 3DGS-MCMC, highlighting the superior performance of\nDBS for real-time radiance field rendering. Interactive demonstrations and\nsource code are available on our project website:\nhttps://rongliu-leo.github.io/beta-splatting/.\n","authors":["Rong Liu","Dylan Sun","Meida Chen","Yue Wang","Andrew Feng"],"pdf_url":"https://arxiv.org/pdf/2501.18630v2.pdf","comment":"SIGGRAPH 2025"},{"id":"http://arxiv.org/abs/2307.11470v4","updated":"2025-05-06T06:31:14Z","published":"2023-07-21T10:10:18Z","title":"Semi-supervised Underwater Image Enhancement Using A Physics-Aware\n  Triple-Stream Network","summary":"  Underwater images normally suffer from degradation due to the transmission\nmedium of water bodies. Both traditional prior-based approaches and deep\nlearning-based methods have been used to address this problem. However, the\ninflexible assumption of the former often impairs their effectiveness in\nhandling diverse underwater scenes, while the generalization of the latter to\nunseen images is usually weakened by insufficient data. In this study, we\nleverage both the physics-based Image Formation Model (IFM) and deep learning\ntechniques for Underwater Image Enhancement (UIE). To this end, we propose a\nnovel Physics-Aware Triple-Stream Underwater Image Enhancement Network, i.e.,\nPATS-UIENet, which comprises a Direct Signal Transmission Estimation Steam\n(D-Stream), a Backscatter Signal Transmission Estimation Steam (B-Stream) and\nan Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE\ntask by explicitly estimating the degradation parameters of a revised IFM. We\nalso adopt an IFM-inspired semi-supervised learning framework, which exploits\nboth the labeled and unlabeled images, to address the issue of insufficient\ndata. To our knowledge, such a physics-aware deep network and the IFM-inspired\nsemi-supervised learning framework have not been used for the UIE task before.\nOur method performs better than, or at least comparably to, sixteen baselines\nacross six testing sets in the degradation estimation and UIE tasks. These\npromising results should be due to the fact that the proposed method can not\nonly model the degradation but also learn the characteristics of diverse\nunderwater scenes.\n","authors":["Hao Qi","Shixuan Xu","Xinghui Dong"],"pdf_url":"https://arxiv.org/pdf/2307.11470v4.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.03220v1","updated":"2025-05-06T06:24:21Z","published":"2025-05-06T06:24:21Z","title":"Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining\n  Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data","summary":"  Hyperspectral images (HSIs) capture rich spectral signatures that reveal\nvital material properties, offering broad applicability across various domains.\nHowever, the scarcity of labeled HSI data limits the full potential of deep\nlearning, especially for transformer-based architectures that require\nlarge-scale training. To address this constraint, we propose Spatial-Frequency\nMasked Image Modeling (SFMIM), a self-supervised pretraining strategy for\nhyperspectral data that utilizes the large portion of unlabeled data. Our\nmethod introduces a novel dual-domain masking mechanism that operates in both\nspatial and frequency domains. The input HSI cube is initially divided into\nnon-overlapping patches along the spatial dimension, with each patch comprising\nthe entire spectrum of its corresponding spatial location. In spatial masking,\nwe randomly mask selected patches and train the model to reconstruct the masked\ninputs using the visible patches. Concurrently, in frequency masking, we remove\nportions of the frequency components of the input spectra and predict the\nmissing frequencies. By learning to reconstruct these masked components, the\ntransformer-based encoder captures higher-order spectral-spatial correlations.\nWe evaluate our approach on three publicly available HSI classification\nbenchmarks and demonstrate that it achieves state-of-the-art performance.\nNotably, our model shows rapid convergence during fine-tuning, highlighting the\nefficiency of our pretraining strategy.\n","authors":["Shaheer Mohamed","Tharindu Fernando","Sridha Sridharan","Peyman Moghadam","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2505.03220v1.pdf","comment":"Preprint to appear in IEEE IGARSS 2025"},{"id":"http://arxiv.org/abs/2502.09608v2","updated":"2025-05-06T06:15:34Z","published":"2025-02-13T18:56:05Z","title":"Instance Segmentation of Scene Sketches Using Natural Image Priors","summary":"  Sketch segmentation involves grouping pixels within a sketch that belong to\nthe same object or instance. It serves as a valuable tool for sketch editing\ntasks, such as moving, scaling, or removing specific components. While image\nsegmentation models have demonstrated remarkable capabilities in recent years,\nsketches present unique challenges for these models due to their sparse nature\nand wide variation in styles. We introduce InkLayer, a method for instance\nsegmentation of raster scene sketches. Our approach adapts state-of-the-art\nimage segmentation and object detection models to the sketch domain by\nemploying class-agnostic fine-tuning and refining segmentation masks using\ndepth cues. Furthermore, our method organizes sketches into sorted layers,\nwhere occluded instances are inpainted, enabling advanced sketch editing\napplications. As existing datasets in this domain lack variation in sketch\nstyles, we construct a synthetic scene sketch segmentation dataset, InkScenes,\nfeaturing sketches with diverse brush strokes and varying levels of detail. We\nuse this dataset to demonstrate the robustness of our approach.\n","authors":["Mia Tang","Yael Vinker","Chuan Yan","Lvmin Zhang","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2502.09608v2.pdf","comment":"Project website: https://inklayer.github.io"},{"id":"http://arxiv.org/abs/2505.02048v2","updated":"2025-05-06T05:56:47Z","published":"2025-05-04T09:57:10Z","title":"Regression is all you need for medical image translation","summary":"  The acquisition of information-rich images within a limited time budget is\ncrucial in medical imaging. Medical image translation (MIT) can help enhance\nand supplement existing datasets by generating synthetic images from acquired\ndata. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have\nachieved remarkable success in natural image generation, their benefits -\ncreativity and image realism - do not necessarily transfer to medical\napplications where highly accurate anatomical information is required. In fact,\nthe imitation of acquisition noise or content hallucination hinder clinical\nutility. Here, we introduce YODA (You Only Denoise once - or Average), a novel\n2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and\nregression paradigms to produce realistic or noise-free outputs. Furthermore,\nwe propose Expectation-Approximation (ExpA) DM sampling, which draws\ninspiration from MRI signal averaging. ExpA-sampling suppresses generated noise\nand, thus, eliminates noise from biasing the evaluation of image quality.\nThrough extensive experiments on four diverse multi-modal datasets - comprising\nmulti-contrast brain MRI and pelvic MRI-CT - we show that diffusion and\nregression sampling yield similar results in practice. As such, the\ncomputational overhead of diffusion sampling does not provide systematic\nbenefits in medical information translation. Building on these insights, we\ndemonstrate that YODA outperforms several state-of-the-art GAN and DM methods.\nNotably, YODA-generated images are shown to be interchangeable with, or even\nsuperior to, physical acquisitions for several downstream tasks. Our findings\nchallenge the presumed advantages of DMs in MIT and pave the way for the\npractical application of MIT in medical imaging.\n","authors":["Sebastian Rassmann","David Kügler","Christian Ewert","Martin Reuter"],"pdf_url":"https://arxiv.org/pdf/2505.02048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01457v2","updated":"2025-05-06T05:52:51Z","published":"2025-05-01T02:40:30Z","title":"A Multi-Granularity Retrieval Framework for Visually-Rich Documents","summary":"  Retrieval-augmented generation (RAG) systems have predominantly focused on\ntext-based retrieval, limiting their effectiveness in handling visually-rich\ndocuments that encompass text, images, tables, and charts. To bridge this gap,\nwe propose a unified multi-granularity multimodal retrieval framework tailored\nfor two benchmark tasks: MMDocIR and M2KR. Our approach integrates hierarchical\nencoding strategies, modality-aware retrieval mechanisms, and vision-language\nmodel (VLM)-based candidate filtering to effectively capture and utilize the\ncomplex interdependencies between textual and visual modalities. By leveraging\noff-the-shelf vision-language models and implementing a training-free hybrid\nretrieval strategy, our framework demonstrates robust performance without the\nneed for task-specific fine-tuning. Experimental evaluations reveal that\nincorporating layout-aware search and VLM-based candidate verification\nsignificantly enhances retrieval accuracy, achieving a top performance score of\n65.56. This work underscores the potential of scalable and reproducible\nsolutions in advancing multimodal document retrieval systems.\n","authors":["Mingjun Xu","Zehui Wang","Hengxing Cai","Renxin Zhong"],"pdf_url":"https://arxiv.org/pdf/2505.01457v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03204v1","updated":"2025-05-06T05:38:17Z","published":"2025-05-06T05:38:17Z","title":"DCS-ST for Classification of Breast Cancer Histopathology Images with\n  Limited Annotations","summary":"  Deep learning methods have shown promise in classifying breast cancer\nhistopathology images, but their performance often declines with limited\nannotated data, a critical challenge in medical imaging due to the high cost\nand expertise required for annotations.\n","authors":["Liu Suxing","Byungwon Min"],"pdf_url":"https://arxiv.org/pdf/2505.03204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03203v1","updated":"2025-05-06T05:38:13Z","published":"2025-05-06T05:38:13Z","title":"PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and\n  Precise Mask Control in Diffusion Models","summary":"  Advanced diffusion models have made notable progress in text-to-image\ncompositional generation. However, it is still a challenge for existing models\nto achieve text-image alignment when confronted with complex text prompts. In\nthis work, we highlight two factors that affect this alignment: the quality of\nthe randomly initialized noise and the reliability of the generated controlling\nmask. We then propose PiCo (Pick-and-Control), a novel training-free approach\nwith two key components to tackle these two factors. First, we develop a noise\nselection module to assess the quality of the random noise and determine\nwhether the noise is suitable for the target text. A fast sampling strategy is\nutilized to ensure efficiency in the noise selection stage. Second, we\nintroduce a referring mask module to generate pixel-level masks and to\nprecisely modulate the cross-attention maps. The referring mask is applied to\nthe standard diffusion process to guide the reasonable interaction between text\nand image features. Extensive experiments have been conducted to verify the\neffectiveness of PiCo in liberating users from the tedious process of random\ngeneration and in enhancing the text-image alignment for diverse text\ndescriptions.\n","authors":["Chang Xie","Chenyi Zhuang","Pan Gao"],"pdf_url":"https://arxiv.org/pdf/2505.03203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03307v2","updated":"2025-05-06T05:35:44Z","published":"2025-03-05T09:39:51Z","title":"Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers","summary":"  For event cameras, current sparse geometric solvers for egomotion estimation\nassume that the rotational displacements are known, such as those provided by\nan IMU. Thus, they can only recover the translational motion parameters.\nRecovering full-DoF motion parameters using a sparse geometric solver is a more\nchallenging task, and has not yet been investigated. In this paper, we propose\nseveral solvers to estimate both rotational and translational velocities within\na unified framework. Our method leverages event manifolds induced by line\nsegments. The problem formulations are based on either an incidence relation\nfor lines or a novel coplanarity relation for normal vectors. We demonstrate\nthe possibility of recovering full-DoF egomotion parameters for both angular\nand linear velocities without requiring extra sensor measurements or motion\npriors. To achieve efficient optimization, we exploit the Adam framework with a\nfirst-order approximation of rotations for quick initialization. Experiments on\nboth synthetic and real-world data demonstrate the effectiveness of our method.\nThe code is available at https://github.com/jizhaox/relpose-event.\n","authors":["Ji Zhao","Banglei Guan","Zibin Liu","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2503.03307v2.pdf","comment":"Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2025"},{"id":"http://arxiv.org/abs/2505.03186v1","updated":"2025-05-06T05:07:11Z","published":"2025-05-06T05:07:11Z","title":"CoGenAV: Versatile Audio-Visual Representation Learning via\n  Contrastive-Generative Synchronization","summary":"  The inherent synchronization between a speaker's lip movements, voice, and\nthe underlying linguistic content offers a rich source of information for\nimproving speech processing tasks, especially in challenging conditions where\ntraditional audio-only systems falter. We introduce CoGenAV, a powerful and\ndata-efficient model designed to learn versatile audio-visual representations\napplicable across a wide range of speech and audio-visual tasks. CoGenAV is\ntrained by optimizing a dual objective derived from natural audio-visual\nsynchrony, contrastive feature alignment and generative text prediction, using\nonly 223 hours of labeled data from the LRS2 dataset. This\ncontrastive-generative synchronization strategy effectively captures\nfundamental cross-modal correlations. We showcase the effectiveness and\nversatility of the learned CoGenAV representations on multiple benchmarks. When\nutilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these\nrepresentations contribute to achieving a state-of-the-art Word Error Rate\n(WER) of 1.27. They also enable strong performance in Visual Speech Recognition\n(VSR) with a WER of 22.0 on LRS2, and significantly improve performance in\nnoisy environments by over 70%. Furthermore, CoGenAV representations benefit\nspeech reconstruction tasks, boosting performance in Speech Enhancement and\nSeparation, and achieve competitive results in audio-visual synchronization\ntasks like Active Speaker Detection (ASD). Our model will be open-sourced to\nfacilitate further development and collaboration within both academia and\nindustry.\n","authors":["Detao Bai","Zhiheng Ma","Xihan Wei","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2505.03186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03184v1","updated":"2025-05-06T05:04:56Z","published":"2025-05-06T05:04:56Z","title":"Interactive Instance Annotation with Siamese Networks","summary":"  Annotating instance masks is time-consuming and labor-intensive. A promising\nsolution is to predict contours using a deep learning model and then allow\nusers to refine them. However, most existing methods focus on in-domain\nscenarios, limiting their effectiveness for cross-domain annotation tasks. In\nthis paper, we propose SiamAnno, a framework inspired by the use of Siamese\nnetworks in object tracking. SiamAnno leverages one-shot learning to annotate\npreviously unseen objects by taking a bounding box as input and predicting\nobject boundaries, which can then be adjusted by annotators. Trained on one\ndataset and tested on another without fine-tuning, SiamAnno achieves\nstate-of-the-art (SOTA) performance across multiple datasets, demonstrating its\nability to handle domain and environment shifts in cross-domain tasks. We also\nprovide more comprehensive results compared to previous work, establishing a\nstrong baseline for future research. To our knowledge, SiamAnno is the first\nmodel to explore Siamese architecture for instance annotation.\n","authors":["Xiang Xu","Ruotong Li","Mengjun Yi","Baile XU","Furao Shen","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.03184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13460v3","updated":"2025-05-06T05:00:15Z","published":"2025-04-18T04:35:35Z","title":"Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization","summary":"  Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.\n","authors":["Hongwei Ji","Wulian Yun","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2504.13460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03176v1","updated":"2025-05-06T04:39:11Z","published":"2025-05-06T04:39:11Z","title":"seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant\n  World Models","summary":"  Current self-supervised algorithms mostly rely on transformations such as\ndata augmentation and masking to learn visual representations. This is achieved\nby inducing invariance or equivariance with respect to these transformations\nafter encoding two views of an image. This dominant two-view paradigm can limit\nthe flexibility of learned representations for downstream adaptation by\ncreating performance trade-offs between invariance-related tasks such as image\nclassification and more fine-grained equivariance-related tasks. In this work,\nwe introduce \\emph{seq-JEPA}, a world modeling paradigm based on\njoint-embedding predictive architecture that leverages architectural inductive\nbiases to resolve this trade-off. Without requiring an additional equivariance\npredictor or loss term, seq-JEPA simultaneously learns two architecturally\nsegregated representations: one equivariant to the specified transformations\nand another invariant to them and suited for tasks such as classification. To\ndo so, our model processes a short sequence of different views (observations)\nof an input image. Each encoded view is concatenated with embeddings\ncorresponding to the relative transformation (action) producing the next\nobservation in the sequence. A transformer encoder outputs an aggregate\nrepresentation of this sequence, which is subsequently conditioned on the\naction leading to the next observation to predict its representation.\nEmpirically, seq-JEPA achieves strong performance on equivariant benchmarks and\nimage classification without sacrificing one for the other. Additionally, our\nframework excels at tasks that inherently require aggregating a sequence of\nobservations, such as path integration across actions and predictive learning\nacross eye movements.\n","authors":["Hafez Ghaemi","Eilif Muller","Shahab Bakhtiari"],"pdf_url":"https://arxiv.org/pdf/2505.03176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03174v1","updated":"2025-05-06T04:38:41Z","published":"2025-05-06T04:38:41Z","title":"Automated Data Curation Using GPS & NLP to Generate Instruction-Action\n  Pairs for Autonomous Vehicle Vision-Language Navigation Datasets","summary":"  Instruction-Action (IA) data pairs are valuable for training robotic systems,\nespecially autonomous vehicles (AVs), but having humans manually annotate this\ndata is costly and time-inefficient. This paper explores the potential of using\nmobile application Global Positioning System (GPS) references and Natural\nLanguage Processing (NLP) to automatically generate large volumes of IA\ncommands and responses without having a human generate or retroactively tag the\ndata. In our pilot data collection, by driving to various destinations and\ncollecting voice instructions from GPS applications, we demonstrate a means to\ncollect and categorize the diverse sets of instructions, further accompanied by\nvideo data to form complete vision-language-action triads. We provide details\non our completely automated data collection prototype system, ADVLAT-Engine. We\ncharacterize collected GPS voice instructions into eight different\nclassifications, highlighting the breadth of commands and referentialities\navailable for curation from freely available mobile applications. Through\nresearch and exploration into the automation of IA data pairs using GPS\nreferences, the potential to increase the speed and volume at which\nhigh-quality IA datasets are created, while minimizing cost, can pave the way\nfor robust vision-language-action (VLA) models to serve tasks in\nvision-language navigation (VLN) and human-interactive autonomous systems.\n","authors":["Guillermo Roque","Erika Maquiling","Jose Giovanni Tapia Lopez","Ross Greer"],"pdf_url":"https://arxiv.org/pdf/2505.03174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03173v1","updated":"2025-05-06T04:38:09Z","published":"2025-05-06T04:38:09Z","title":"RAVU: Retrieval Augmented Video Understanding with Compositional\n  Reasoning over Graph","summary":"  Comprehending long videos remains a significant challenge for Large\nMulti-modal Models (LMMs). Current LMMs struggle to process even minutes to\nhours videos due to their lack of explicit memory and retrieval mechanisms. To\naddress this limitation, we propose RAVU (Retrieval Augmented Video\nUnderstanding), a novel framework for video understanding enhanced by retrieval\nwith compositional reasoning over a spatio-temporal graph. We construct a graph\nrepresentation of the video, capturing both spatial and temporal relationships\nbetween entities. This graph serves as a long-term memory, allowing us to track\nobjects and their actions across time. To answer complex queries, we decompose\nthe queries into a sequence of reasoning steps and execute these steps on the\ngraph, retrieving relevant key information. Our approach enables more accurate\nunderstanding of long videos, particularly for queries that require multi-hop\nreasoning and tracking objects across frames. Our approach demonstrate superior\nperformances with limited retrieved frames (5-10) compared with other SOTA\nmethods and baselines on two major video QA datasets, NExT-QA and EgoSchema.\n","authors":["Sameer Malik","Moyuru Yamada","Ayush Singh","Dishank Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2505.03173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03154v1","updated":"2025-05-06T04:02:47Z","published":"2025-05-06T04:02:47Z","title":"StableMotion: Training Motion Cleanup Models with Unpaired Corrupted\n  Data","summary":"  Motion capture (mocap) data often exhibits visually jarring artifacts due to\ninaccurate sensors and post-processing. Cleaning this corrupted data can\nrequire substantial manual effort from human experts, which can be a costly and\ntime-consuming process. Previous data-driven motion cleanup methods offer the\npromise of automating this cleanup process, but often require in-domain paired\ncorrupted-to-clean training data. Constructing such paired datasets requires\naccess to high-quality, relatively artifact-free motion clips, which often\nnecessitates laborious manual cleanup. In this work, we present StableMotion, a\nsimple yet effective method for training motion cleanup models directly from\nunpaired corrupted datasets that need cleanup. The core component of our method\nis the introduction of motion quality indicators, which can be easily annotated\nthrough manual labeling or heuristic algorithms and enable training of\nquality-aware motion generation models on raw motion data with mixed quality.\nAt test time, the model can be prompted to generate high-quality motions using\nthe quality indicators. Our method can be implemented through a simple\ndiffusion-based framework, leading to a unified motion generate-discriminate\nmodel, which can be used to both identify and fix corrupted frames. We\ndemonstrate that our proposed method is effective for training motion cleanup\nmodels on raw mocap data in production scenarios by applying StableMotion to\nSoccerMocap, a 245-hour soccer mocap dataset containing real-world motion\nartifacts. The trained model effectively corrects a wide range of motion\nartifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.\nSee https://youtu.be/3Y7MMAH02B4 for more results.\n","authors":["Yuxuan Mu","Hung Yu Ling","Yi Shi","Ismael Baira Ojeda","Pengcheng Xi","Chang Shu","Fabio Zinno","Xue Bin Peng"],"pdf_url":"https://arxiv.org/pdf/2505.03154v1.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2505.03153v1","updated":"2025-05-06T03:59:25Z","published":"2025-05-06T03:59:25Z","title":"Robust Fairness Vision-Language Learning for Medical Image Analysis","summary":"  The advent of Vision-Language Models (VLMs) in medical image analysis has the\npotential to help process multimodal inputs and increase performance over\ntraditional inference methods. However, when considering the domain in which\nthese models will be implemented, fairness and robustness are important to\nensure the model stays true for any patient. In this paper, we introduce a\nframework for ensuring robustness and fairness of VLM models. This framework\nmodifies the loss function at training by identifying and adjusting faulty\nimage-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing\nSinkhorn distance to ensure the loss distributions of protected groups do not\ndeviate from the total loss. Experimental testing of our framework shows up to\na 8.6\\% improvement when looking at equity-scaled AUC.\n","authors":["Sparsh Bansal","Mingyang Wu","Xin Wang","Shu Hu"],"pdf_url":"https://arxiv.org/pdf/2505.03153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15326v2","updated":"2025-05-06T03:57:31Z","published":"2025-01-25T21:01:52Z","title":"Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised\n  Data","summary":"  We present RASO, a foundation model designed to Recognize Any Surgical\nObject, offering robust open-set recognition capabilities across a broad range\nof surgical procedures and object classes, in both surgical images and videos.\nRASO leverages a novel weakly-supervised learning framework that generates\ntag-image-text pairs automatically from large-scale unannotated surgical\nlecture videos, significantly reducing the need for manual annotations. Our\nscalable data generation pipeline gathers 2,200 surgical procedures and\nproduces 3.6 million tag annotations across 2,066 unique surgical tags. Our\nexperiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP,\nand 7.2 mAP on four standard surgical benchmarks, respectively, in zero-shot\nsettings, and surpasses state-of-the-art models in supervised surgical action\nrecognition tasks. Code, model, and demo are available at\nhttps://ntlm1686.github.io/raso.\n","authors":["Jiajie Li","Brian R Quaranto","Chenhui Xu","Ishan Mishra","Ruiyang Qin","Dancheng Liu","Peter C W Kim","Jinjun Xiong"],"pdf_url":"https://arxiv.org/pdf/2501.15326v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03149v1","updated":"2025-05-06T03:52:17Z","published":"2025-05-06T03:52:17Z","title":"Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)","summary":"  We introduce an unsupervised motion-compensated image reconstruction\nalgorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging\n(MRI). We express the image volume corresponding to each specific motion phase\nas the deformation of a single static image template. The main contribution of\nthe work is the low-rank model for the compact joint representation of the\nfamily of diffeomorphisms, parameterized by the motion phases. The\ndiffeomorphism at a specific motion phase is obtained by integrating a\nparametric velocity field along a path connecting the reference template phase\nto the motion phase. The velocity field at different phases is represented\nusing a low-rank model. The static template and the low-rank motion model\nparameters are learned directly from the k-space data in an unsupervised\nfashion. The more constrained motion model is observed to offer improved\nrecovery compared to current motion-resolved and motion-compensated algorithms\nfor free-breathing 3D cine MRI.\n","authors":["Joseph William Kettelkamp","Ludovica Romanin","Sarv Priya","Mathews Jacob"],"pdf_url":"https://arxiv.org/pdf/2505.03149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.16915v2","updated":"2025-05-06T03:41:43Z","published":"2025-04-23T17:41:44Z","title":"DreamO: A Unified Framework for Image Customization","summary":"  Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.\n","authors":["Chong Mou","Yanze Wu","Wenxu Wu","Zinan Guo","Pengze Zhang","Yufeng Cheng","Yiming Luo","Fei Ding","Shiwen Zhang","Xinghui Li","Mengtian Li","Songtao Zhao","Jian Zhang","Qian He","Xinglong Wu"],"pdf_url":"https://arxiv.org/pdf/2504.16915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01581v2","updated":"2025-05-06T03:32:16Z","published":"2023-12-04T02:33:53Z","title":"PLUM: Improving Inference Efficiency By Leveraging Repetition-Sparsity\n  Trade-Off","summary":"  Efficient inference of Deep Neural Networks (DNNs) on resource-constrained\nedge devices is essential. Quantization and sparsity are key techniques that\ntranslate to repetition and sparsity within tensors at the hardware-software\ninterface. This paper introduces the concept of repetition-sparsity trade-off\nthat helps explain computational efficiency during inference. We propose PLUM,\na unified co-design framework that integrates DNN inference systems and\nquantization (forward and backward pass) to leverage the repetition-sparsity\ntrade-off to improve inference efficiency. Our results demonstrate that PLUM's\nquantization method is more accurate than binary quantization with the same\nnumber of non-zero weights. Detailed analysis indicates that signed\nbinarization generates a smaller distribution of effectual (non-zero)\nparameters nested within a larger distribution of total parameters of latent\nfull-precision weights for a DNN block. Finally, the proposed PLUM framework\nachieves a 26% speedup on real hardware, doubles energy efficiency, and reduces\ndensity by 2.8x compared to binary methods while retaining top-1 accuracy when\ncompared to prior-art methods for ResNets on ImageNet (by achieving 66.2% top-1\naccuracy), presenting an alternative solution for deploying efficient models in\nresource-limited environments.\n","authors":["Sachit Kuhar","Yash Jain","Alexey Tumanov"],"pdf_url":"https://arxiv.org/pdf/2312.01581v2.pdf","comment":"OpenReview: https://openreview.net/forum?id=IEKtMMSblm"},{"id":"http://arxiv.org/abs/2505.03134v1","updated":"2025-05-06T03:16:56Z","published":"2025-05-06T03:16:56Z","title":"Enhancing Glass Defect Detection with Diffusion Models: Addressing\n  Imbalanced Datasets in Manufacturing Quality Control","summary":"  Visual defect detection in industrial glass manufacturing remains a critical\nchallenge due to the low frequency of defective products, leading to imbalanced\ndatasets that limit the performance of deep learning models and computer vision\nsystems. This paper presents a novel approach using Denoising Diffusion\nProbabilistic Models (DDPMs) to generate synthetic defective glass product\nimages for data augmentation, effectively addressing class imbalance issues in\nmanufacturing quality control and automated visual inspection. The methodology\nsignificantly enhances image classification performance of standard CNN\narchitectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting\nanomalies by increasing the minority class representation. Experimental results\ndemonstrate substantial improvements in key machine learning metrics,\nparticularly in recall for defective samples across all tested deep neural\nnetwork architectures while maintaining perfect precision. The most dramatic\nimprovement was observed in ResNet50V2's overall classification accuracy, which\nincreased from 78 percent to 93 percent when trained with the augmented data.\nThis work provides a scalable, cost-effective approach to enhancing automated\ndefect detection in glass manufacturing that can potentially be extended to\nother industrial quality assurance systems and industries with similar class\nimbalance challenges.\n","authors":["Sajjad Rezvani Boroujeni","Hossein Abedi","Tom Bush"],"pdf_url":"https://arxiv.org/pdf/2505.03134v1.pdf","comment":"12 pages, 7 figures, submitted to Computer and Decision Making An\n  International Journal (COMDEM)"},{"id":"http://arxiv.org/abs/2504.19244v2","updated":"2025-05-06T03:12:50Z","published":"2025-04-27T13:58:12Z","title":"Semantic-Aligned Learning with Collaborative Refinement for Unsupervised\n  VI-ReID","summary":"  Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to\nmatch pedestrian images of the same individual across different modalities\nwithout human annotations for model learning. Previous methods unify\npseudo-labels of cross-modality images through label association algorithms and\nthen design contrastive learning framework for global feature learning.\nHowever, these methods overlook the cross-modality variations in feature\nrepresentation and pseudo-label distributions brought by fine-grained patterns.\nThis insight results in insufficient modality-shared learning when only global\nfeatures are optimized. To address this issue, we propose a Semantic-Aligned\nLearning with Collaborative Refinement (SALCR) framework, which builds up\noptimization objective for specific fine-grained patterns emphasized by each\nmodality, thereby achieving complementary alignment between the label\ndistributions of different modalities. Specifically, we first introduce a Dual\nAssociation with Global Learning (DAGI) module to unify the pseudo-labels of\ncross-modality instances in a bi-directional manner. Afterward, a Fine-Grained\nSemantic-Aligned Learning (FGSAL) module is carried out to explore part-level\nsemantic-aligned patterns emphasized by each modality from cross-modality\ninstances. Optimization objective is then formulated based on the\nsemantic-aligned features and their corresponding label space. To alleviate the\nside-effects arising from noisy pseudo-labels, we propose a Global-Part\nCollaborative Refinement (GPCR) module to mine reliable positive sample sets\nfor the global and part features dynamically and optimize the inter-instance\nrelationships. Extensive experiments demonstrate the effectiveness of the\nproposed method, which achieves superior performances to state-of-the-art\nmethods. Our code is available at\n\\href{https://github.com/FranklinLingfeng/code-for-SALCR}.\n","authors":["De Cheng","Lingfeng He","Nannan Wang","Dingwen Zhang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2504.19244v2.pdf","comment":"Accepted by IJCV 2025"},{"id":"http://arxiv.org/abs/2505.03132v1","updated":"2025-05-06T03:09:15Z","published":"2025-05-06T03:09:15Z","title":"VISLIX: An XAI Framework for Validating Vision Models with Slice\n  Discovery and Analysis","summary":"  Real-world machine learning models require rigorous evaluation before\ndeployment, especially in safety-critical domains like autonomous driving and\nsurveillance. The evaluation of machine learning models often focuses on data\nslices, which are subsets of the data that share a set of characteristics. Data\nslice finding automatically identifies conditions or data subgroups where\nmodels underperform, aiding developers in mitigating performance issues.\nDespite its popularity and effectiveness, data slicing for vision model\nvalidation faces several challenges. First, data slicing often needs additional\nimage metadata or visual concepts, and falls short in certain computer vision\ntasks, such as object detection. Second, understanding data slices is a\nlabor-intensive and mentally demanding process that heavily relies on the\nexpert's domain knowledge. Third, data slicing lacks a human-in-the-loop\nsolution that allows experts to form hypothesis and test them interactively. To\novercome these limitations and better support the machine learning operations\nlifecycle, we introduce VISLIX, a novel visual analytics framework that employs\nstate-of-the-art foundation models to help domain experts analyze slices in\ncomputer vision models. Our approach does not require image metadata or visual\nconcepts, automatically generates natural language insights, and allows users\nto test data slice hypothesis interactively. We evaluate VISLIX with an expert\nstudy and three use cases, that demonstrate the effectiveness of our tool in\nproviding comprehensive insights for validating object detection models.\n","authors":["Xinyuan Yan","Xiwei Xuan","Jorge Piazentin Ono","Jiajing Guo","Vikram Mohanty","Shekar Arvind Kumar","Liang Gou","Bei Wang","Liu Ren"],"pdf_url":"https://arxiv.org/pdf/2505.03132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00746v2","updated":"2025-05-06T03:09:13Z","published":"2025-04-30T09:05:49Z","title":"Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with\n  Sliding-Window Shannon Analysis","summary":"  Vision-language models such as OpenAI GPT-4o can transcribe mathematical\ndocuments directly from images, yet their token-level confidence signals are\nseldom used to pinpoint local recognition mistakes. We present an\nentropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into\na visual ''uncertainty landscape''. By scanning the entropy sequence with a\nfixed-length sliding window, we obtain hotspots that are likely to contain OCR\nerrors such as missing symbols, mismatched braces, or garbled prose. Using a\nsmall, curated set of scanned research pages rendered at several resolutions,\nwe compare the highlighted hotspots with the actual transcription errors\nproduced by GPT-4o. Our analysis shows that the vast majority of true errors\nare indeed concentrated inside the high-entropy regions. This study\ndemonstrates--in a minimally engineered setting--that sliding-window entropy\ncan serve as a practical, lightweight aid for post-editing GPT-based OCR. All\ncode and annotation guidelines are released to encourage replication and\nfurther research.\n","authors":["Alexei Kaltchenko"],"pdf_url":"https://arxiv.org/pdf/2505.00746v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2505.02704v2","updated":"2025-05-06T03:06:28Z","published":"2025-05-05T14:57:16Z","title":"VGLD: Visually-Guided Linguistic Disambiguation for Monocular Depth\n  Scale Recovery","summary":"  We propose a robust method for monocular depth scale recovery. Monocular\ndepth estimation can be divided into two main directions: (1) relative depth\nestimation, which provides normalized or inverse depth without scale\ninformation, and (2) metric depth estimation, which involves recovering depth\nwith absolute scale. To obtain absolute scale information for practical\ndownstream tasks, utilizing textual information to recover the scale of a\nrelative depth map is a highly promising approach. However, since a single\nimage can have multiple descriptions from different perspectives or with\nvarying styles, it has been shown that different textual descriptions can\nsignificantly affect the scale recovery process. To address this issue, our\nmethod, VGLD, stabilizes the influence of textual information by incorporating\nhigh-level semantic information from the corresponding image alongside the\ntextual description. This approach resolves textual ambiguities and robustly\noutputs a set of linear transformation parameters (scalars) that can be\nglobally applied to the relative depth map, ultimately generating depth\npredictions with metric-scale accuracy. We validate our method across several\npopular relative depth models(MiDas, DepthAnything), using both indoor scenes\n(NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions\nas a universal alignment module when trained on multiple datasets, achieving\nstrong performance even in zero-shot scenarios. Code is available at:\nhttps://github.com/pakinwu/VGLD.\n","authors":["Bojin Wu","Jing Chen"],"pdf_url":"https://arxiv.org/pdf/2505.02704v2.pdf","comment":"21 pages, conference"},{"id":"http://arxiv.org/abs/2504.11739v2","updated":"2025-05-06T02:55:26Z","published":"2025-04-16T03:33:25Z","title":"The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for\n  Text-to-Video Generation","summary":"  The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce RAPO, a novel Retrieval-Augmented Prompt Optimization\nframework. In order to address potential inaccuracies and ambiguous details\ngenerated by LLM-generated prompts. RAPO refines the naive prompts through dual\noptimization branches, selecting the superior prompt for T2V generation. The\nfirst branch augments user prompts with diverse modifiers extracted from a\nlearned relational graph, refining them to align with the format of training\nprompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive\nprompt using a pre-trained LLM following a well-defined instruction set.\nExtensive experiments demonstrate that RAPO can effectively enhance both the\nstatic and dynamic dimensions of generated videos, demonstrating the\nsignificance of prompt optimization for user-provided prompts.\n","authors":["Bingjie Gao","Xinyu Gao","Xiaoxue Wu","Yujie Zhou","Yu Qiao","Li Niu","Xinyuan Chen","Yaohui Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11739v2.pdf","comment":"accepted by CVPR2025, Project website:\n  https://whynothaha.github.io/Prompt_optimizer/RAPO.html"},{"id":"http://arxiv.org/abs/2505.03123v1","updated":"2025-05-06T02:41:34Z","published":"2025-05-06T02:41:34Z","title":"STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal\n  Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver\n  Metastasis","summary":"  We propose a multimodal spatiotemporal graph neural network (STG) framework\nto predict colorectal cancer liver metastasis (CRLM) progression. Current\nclinical models do not effectively integrate the tumor's spatial heterogeneity,\ndynamic evolution, and complex multimodal data relationships, limiting their\npredictive accuracy. Our STG framework combines preoperative CT imaging and\nclinical data into a heterogeneous graph structure, enabling joint modeling of\ntumor distribution and temporal evolution through spatial topology and\ncross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal\nneighborhood information and leverages supervised and contrastive learning\nstrategies to enhance the model's ability to capture temporal features and\nimprove robustness. A lightweight version of the model reduces parameter count\nby 78.55%, maintaining near-state-of-the-art performance. The model jointly\noptimizes recurrence risk regression and survival analysis tasks, with\ncontrastive loss improving feature representational discriminability and\ncross-modal consistency. Experimental results on the MSKCC CRLM dataset show a\ntime-adjacent accuracy of 85% and a mean absolute error of 1.1005,\nsignificantly outperforming existing methods. The innovative heterogeneous\ngraph construction and spatiotemporal decoupling mechanism effectively uncover\nthe associations between dynamic tumor microenvironment changes and prognosis,\nproviding reliable quantitative support for personalized treatment decisions.\n","authors":["Yiran Zhu","Wei Yang","Yan su","Zesheng Li","Chengchang Pan","Honggang Qi"],"pdf_url":"https://arxiv.org/pdf/2505.03123v1.pdf","comment":"9 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.03116v1","updated":"2025-05-06T02:12:19Z","published":"2025-05-06T02:12:19Z","title":"TimeTracker: Event-based Continuous Point Tracking for Video Frame\n  Interpolation with Non-linear Motion","summary":"  Video frame interpolation (VFI) that leverages the bio-inspired event cameras\nas guidance has recently shown better performance and memory efficiency than\nthe frame-based methods, thanks to the event cameras' advantages, such as high\ntemporal resolution. A hurdle for event-based VFI is how to effectively deal\nwith non-linear motion, caused by the dynamic changes in motion direction and\nspeed within the scene. Existing methods either use events to estimate sparse\noptical flow or fuse events with image features to estimate dense optical flow.\nUnfortunately, motion errors often degrade the VFI quality as the continuous\nmotion cues from events do not align with the dense spatial information of\nimages in the temporal dimension. In this paper, we find that object motion is\ncontinuous in space, tracking local regions over continuous time enables more\naccurate identification of spatiotemporal feature correlations. In light of\nthis, we propose a novel continuous point tracking-based VFI framework, named\nTimeTracker. Specifically, we first design a Scene-Aware Region Segmentation\n(SARS) module to divide the scene into similar patches. Then, a Continuous\nTrajectory guided Motion Estimation (CTME) module is proposed to track the\ncontinuous motion trajectory of each patch through events. Finally,\nintermediate frames at any given time are generated through global motion\noptimization and frame refinement. Moreover, we collect a real-world dataset\nthat features fast non-linear motion. Extensive experiments show that our\nmethod outperforms prior arts in both motion estimation and frame interpolation\nquality.\n","authors":["Haoyue Liu","Jinghan Xu","Yi Chang","Hanyu Zhou","Haozhi Zhao","Lin Wang","Luxin Yan"],"pdf_url":"https://arxiv.org/pdf/2505.03116v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2505.03114v1","updated":"2025-05-06T02:08:35Z","published":"2025-05-06T02:08:35Z","title":"Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation","summary":"  Accurate MRI-to-CT translation promises the integration of complementary\nimaging information without the need for additional imaging sessions. Given the\npractical challenges associated with acquiring paired MRI and CT scans, the\ndevelopment of robust methods capable of leveraging unpaired datasets is\nessential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT\ntranslation methods, which predominantly rely on cycle consistency and\ncontrastive learning frameworks, frequently encounter challenges in accurately\ntranslating anatomical features that are highly discernible on CT but less\ndistinguishable on MRI, such as bone structures. This limitation renders these\napproaches less suitable for applications in radiation therapy, where precise\nbone representation is essential for accurate treatment planning. To address\nthis challenge, we propose a path- and bone-contour regularized approach for\nunpaired MRI-to-CT translation. In our method, MRI and CT images are projected\nto a shared latent space, where the MRI-to-CT mapping is modeled as a\ncontinuous flow governed by neural ordinary differential equations. The optimal\nmapping is obtained by minimizing the transition path length of the flow. To\nenhance the accuracy of translated bone structures, we introduce a trainable\nneural network to generate bone contours from MRI and implement mechanisms to\ndirectly and indirectly encourage the model to focus on bone contours and their\nadjacent regions. Evaluations conducted on three datasets demonstrate that our\nmethod outperforms existing unpaired MRI-to-CT translation approaches,\nachieving lower overall error rates. Moreover, in a downstream bone\nsegmentation task, our approach exhibits superior performance in preserving the\nfidelity of bone structures. Our code is available at:\nhttps://github.com/kennysyp/PaBoT.\n","authors":["Teng Zhou","Jax Luo","Yuping Sun","Yiheng Tan","Shun Yao","Nazim Haouchine","Scott Raymond"],"pdf_url":"https://arxiv.org/pdf/2505.03114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03113v1","updated":"2025-05-06T02:07:54Z","published":"2025-05-06T02:07:54Z","title":"Image Recognition with Online Lightweight Vision Transformer: A Survey","summary":"  The Transformer architecture has achieved significant success in natural\nlanguage processing, motivating its adaptation to computer vision tasks. Unlike\nconvolutional neural networks, vision transformers inherently capture\nlong-range dependencies and enable parallel processing, yet lack inductive\nbiases and efficiency benefits, facing significant computational and memory\nchallenges that limit its real-world applicability. This paper surveys various\nonline strategies for generating lightweight vision transformers for image\nrecognition, focusing on three key areas: Efficient Component Design, Dynamic\nNetwork, and Knowledge Distillation. We evaluate the relevant exploration for\neach topic on the ImageNet-1K benchmark, analyzing trade-offs among precision,\nparameters, throughput, and more to highlight their respective advantages,\ndisadvantages, and flexibility. Finally, we propose future research directions\nand potential challenges in the lightweighting of vision transformers with the\naim of inspiring further exploration and providing practical guidance for the\ncommunity. Project Page: https://github.com/ajxklo/Lightweight-VIT\n","authors":["Zherui Zhang","Rongtao Xu","Jie Zhou","Changwei Wang","Xingtian Pei","Wenhao Xu","Jiguang Zhang","Li Guo","Longxiang Gao","Wenbo Xu","Shibiao Xu"],"pdf_url":"https://arxiv.org/pdf/2505.03113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02369v2","updated":"2025-05-06T01:56:40Z","published":"2025-05-05T05:13:12Z","title":"Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural\n  Networks","summary":"  Generalizing well in deep neural networks remains a core challenge,\nparticularly due to their tendency to converge to sharp minima that degrade\nrobustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking\nflatter minima but perturbs parameters using the full gradient, which can\ninclude statistically insignificant directions. We propose ZSharp, a simple yet\neffective extension to SAM that applies layer-wise Z-score normalization\nfollowed by percentile-based filtering to retain only statistically significant\ngradient components. This selective perturbation aligns updates with\ncurvature-sensitive directions, enhancing generalization without requiring\narchitectural changes. ZSharp introduces only one additional hyperparameter,\nthe percentile threshold, and remains fully compatible with existing SAM\nvariants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,\nVGG, and Vision Transformers show that ZSharp consistently outperforms SAM and\nits variants in test accuracy, particularly on deeper and transformer-based\nmodels. These results demonstrate that ZSharp is a principled and lightweight\nimprovement for sharpness-aware optimization.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2505.02369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02064v2","updated":"2025-05-06T01:51:03Z","published":"2025-05-04T10:55:21Z","title":"RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and\n  Reasoning through Real-Time Video","summary":"  Multimodal Large Language Models (MLLMs) increasingly excel at perception,\nunderstanding, and reasoning. However, current benchmarks inadequately evaluate\ntheir ability to perform these tasks continuously in dynamic, real-world\nenvironments. To bridge this gap, we introduce RTV-Bench, a fine-grained\nbenchmark for MLLM real-time video analysis. RTV-Bench uses three key\nprinciples: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve\nwith scene changes; (2) Hierarchical Question Structure, combining basic and\nadvanced queries; and (3) Multi-dimensional Evaluation, assessing the ability\nof continuous perception, understanding, and reasoning. RTV-Bench contains 552\ndiverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated\nleading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline\n(Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5,\nInternLM-XComposer2.5-OmniLive) models. Experiment results show open-source\nreal-time models largely outperform offline ones but still trail top\nproprietary models. Our analysis also reveals that larger model size or higher\nframe sampling rates do not significantly boost RTV-Bench performance,\nsometimes causing slight decreases. This underscores the need for better model\narchitectures optimized for video stream processing and long sequences to\nadvance real-time video analysis with MLLMs. Our benchmark toolkit is available\nat: https://github.com/LJungang/RTV-Bench.\n","authors":["Shuhang Xun","Sicheng Tao","Jungang Li","Yibo Shi","Zhixin Lin","Zhanhui Zhu","Yibo Yan","Hanqian Li","Linghao Zhang","Shikang Wang","Yixin Liu","Hanbo Zhang","Ying Ma","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2505.02064v2.pdf","comment":"13 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.03097v1","updated":"2025-05-06T01:14:20Z","published":"2025-05-06T01:14:20Z","title":"Not All Parameters Matter: Masking Diffusion Models for Enhancing\n  Generation Ability","summary":"  The diffusion models, in early stages focus on constructing basic image\nstructures, while the refined details, including local features and textures,\nare generated in later stages. Thus the same network layers are forced to learn\nboth structural and textural information simultaneously, significantly\ndiffering from the traditional deep learning architectures (e.g., ResNet or\nGANs) which captures or generates the image semantic information at different\nlayers. This difference inspires us to explore the time-wise diffusion models.\nWe initially investigate the key contributions of the U-Net parameters to the\ndenoising process and identify that properly zeroing out certain parameters\n(including large parameters) contributes to denoising, substantially improving\nthe generation quality on the fly. Capitalizing on this discovery, we propose a\nsimple yet effective method-termed ``MaskUNet''- that enhances generation\nquality with negligible parameter numbers. Our method fully leverages timestep-\nand sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer\ntwo fine-tuning strategies: a training-based approach and a training-free\napproach, including tailored networks and optimization functions. In zero-shot\ninference on the COCO dataset, MaskUNet achieves the best FID score and further\ndemonstrates its effectiveness in downstream task evaluations. Project page:\nhttps://gudaochangsheng.github.io/MaskUnet-Page/\n","authors":["Lei Wang","Senmao Li","Fei Yang","Jianye Wang","Ziheng Zhang","Yuhan Liu","Yaxing Wang","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2505.03097v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2505.03093v1","updated":"2025-05-06T01:09:07Z","published":"2025-05-06T01:09:07Z","title":"Estimating the Diameter at Breast Height of Trees in a Forest With a\n  Single 360 Camera","summary":"  Forest inventories rely on accurate measurements of the diameter at breast\nheight (DBH) for ecological monitoring, resource management, and carbon\naccounting. While LiDAR-based techniques can achieve centimeter-level\nprecision, they are cost-prohibitive and operationally complex. We present a\nlow-cost alternative that only needs a consumer-grade 360 video camera. Our\nsemi-automated pipeline comprises of (i) a dense point cloud reconstruction\nusing Structure from Motion (SfM) photogrammetry software called Agisoft\nMetashape, (ii) semantic trunk segmentation by projecting Grounded Segment\nAnything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based\ntechnique to estimate cross section shape and DBH. We introduce an interactive\nvisualization tool for inspecting segmented trees and their estimated DBH. On\n61 acquisitions of 43 trees under a variety of conditions, our method attains\nmedian absolute relative errors of 5-9% with respect to \"ground-truth\" manual\nmeasurements. This is only 2-4% higher than LiDAR-based estimates, while\nemploying a single 360 camera that costs orders of magnitude less, requires\nminimal setup, and is widely available.\n","authors":["Siming He","Zachary Osman","Fernando Cladera","Dexter Ong","Nitant Rai","Patrick Corey Green","Vijay Kumar","Pratik Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2505.03093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07012v2","updated":"2025-05-06T00:52:51Z","published":"2024-09-11T04:49:44Z","title":"Towards Predicting Temporal Changes in a Patient's Chest X-ray Images\n  based on Electronic Health Records","summary":"  Chest X-ray (CXR) is an important diagnostic tool widely used in hospitals to\nassess patient conditions and monitor changes over time. Recently, generative\nmodels, specifically diffusion-based models, have shown promise in generating\nrealistic synthetic CXRs. However, these models mainly focus on conditional\ngeneration using single-time-point data, i.e., generating CXRs conditioned on\ntheir corresponding reports from a specific time. This limits their clinical\nutility, particularly for capturing temporal changes. To address this\nlimitation, we propose a novel framework, EHRXDiff, which predicts future CXR\nimages by integrating previous CXRs with subsequent medical events, e.g.,\nprescriptions, lab measures, etc. Our framework dynamically tracks and predicts\ndisease progression based on a latent diffusion model, conditioned on the\nprevious CXR image and a history of medical events. We comprehensively evaluate\nthe performance of our framework across three key aspects, including clinical\nconsistency, demographic consistency, and visual realism. Results show that our\nframework generates high-quality, realistic future images that effectively\ncapture potential temporal changes. This suggests that our framework could be\nfurther developed to support clinical decision-making and provide valuable\ninsights for patient monitoring and treatment planning in the medical field.\nThe code is available at https://github.com/dek924/EHRXDiff.\n","authors":["Daeun Kyung","Junu Kim","Tackeun Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2409.07012v2.pdf","comment":"Accepted at Proc. of Conference on Health, Inference, and Learning\n  (CHIL) 2025 (10 pages for main text, 3 pages for references, 8 pages for\n  supplementary materials)"},{"id":"http://arxiv.org/abs/2502.17648v5","updated":"2025-05-06T00:20:42Z","published":"2025-02-24T20:53:42Z","title":"CalibRefine: Deep Learning-Based Online Automatic Targetless\n  LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement","summary":"  Accurate multi-sensor calibration is essential for deploying robust\nperception systems in applications such as autonomous driving and intelligent\ntransportation. Existing LiDAR-camera calibration methods often rely on\nmanually placed targets, preliminary parameter estimates, or intensive data\npreprocessing, limiting their scalability and adaptability in real-world\nsettings. In this work, we propose a fully automatic, targetless, and online\ncalibration framework, CalibRefine, which directly processes raw LiDAR point\nclouds and camera images. Our approach is divided into four stages: (1) a\nCommon Feature Discriminator that leverages relative spatial positions, visual\nappearance embeddings, and semantic class cues to identify and generate\nreliable LiDAR-camera correspondences, (2) a coarse homography-based\ncalibration that uses the matched feature correspondences to estimate an\ninitial transformation between the LiDAR and camera frames, serving as the\nfoundation for further refinement, (3) an iterative refinement to incrementally\nimprove alignment as additional data frames become available, and (4) an\nattention-based refinement that addresses non-planar distortions by leveraging\na Vision Transformer and cross-attention mechanisms. Extensive experiments on\ntwo urban traffic datasets demonstrate that CalibRefine achieves high-precision\ncalibration with minimal human input, outperforming state-of-the-art targetless\nmethods and matching or surpassing manually tuned baselines. Our results show\nthat robust object-level feature matching, combined with iterative refinement\nand self-supervised attention-based refinement, enables reliable sensor\nalignment in complex real-world conditions without ground-truth matrices or\nelaborate preprocessing. Code is available at\nhttps://github.com/radar-lab/Lidar_Camera_Automatic_Calibration\n","authors":["Lei Cheng","Lihao Guo","Tianya Zhang","Tam Bang","Austin Harris","Mustafa Hajij","Mina Sartipi","Siyang Cao"],"pdf_url":"https://arxiv.org/pdf/2502.17648v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04006v1","updated":"2025-05-06T22:35:54Z","published":"2025-05-06T22:35:54Z","title":"The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from\n  Classical Techniques to Oculomics","summary":"  The unique vascularized anatomy of the human eye, encased in the retina,\nprovides an opportunity to act as a window for human health. The retinal\nstructure assists in assessing the early detection, monitoring of disease\nprogression and intervention for both ocular and non-ocular diseases. The\nadvancement in imaging technology leveraging Artificial Intelligence has seized\nthis opportunity to bridge the gap between the eye and human health. This track\npaves the way for unveiling systemic health insight from the ocular system and\nsurrogating non-invasive markers for timely intervention and identification.\nThe new frontiers of oculomics in ophthalmology cover both ocular and systemic\ndiseases, and getting more attention to explore them. In this survey paper, we\nexplore the evolution of retinal imaging techniques, the dire need for the\nintegration of AI-driven analysis, and the shift of retinal imaging from\nclassical techniques to oculomics. We also discuss some hurdles that may be\nfaced in the progression of oculomics, highlighting the research gaps and\nfuture directions.\n","authors":[" Inamullah","Imran Razzak","Shoaib Jameel"],"pdf_url":"https://arxiv.org/pdf/2505.04006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04003v1","updated":"2025-05-06T22:30:23Z","published":"2025-05-06T22:30:23Z","title":"Prototype-Based Information Compensation Network for Multi-Source Remote\n  Sensing Data Classification","summary":"  Multi-source remote sensing data joint classification aims to provide\naccuracy and reliability of land cover classification by leveraging the\ncomplementary information from multiple data sources. Existing methods confront\ntwo challenges: inter-frequency multi-source feature coupling and inconsistency\nof complementary information exploration. To solve these issues, we present a\nPrototype-based Information Compensation Network (PICNet) for land cover\nclassification based on HSI and SAR/LiDAR data. Specifically, we first design a\nfrequency interaction module to enhance the inter-frequency coupling in\nmulti-source feature extraction. The multi-source features are first decoupled\ninto high- and low-frequency components. Then, these features are recoupled to\nachieve efficient inter-frequency communication. Afterward, we design a\nprototype-based information compensation module to model the global\nmulti-source complementary information. Two sets of learnable modality\nprototypes are introduced to represent the global modality information of\nmulti-source data. Subsequently, cross-modal feature integration and alignment\nare achieved through cross-attention computation between the modality-specific\nprototype vectors and the raw feature representations. Extensive experiments on\nthree public datasets demonstrate the significant superiority of our PICNet\nover state-of-the-art methods. The codes are available at\nhttps://github.com/oucailab/PICNet.\n","authors":["Feng Gao","Sheng Liu","Chuanzheng Gong","Xiaowei Zhou","Jiayi Wang","Junyu Dong","Qian Du"],"pdf_url":"https://arxiv.org/pdf/2505.04003v1.pdf","comment":"Accepted by IEEE TGRS 2025"},{"id":"http://arxiv.org/abs/2505.03991v1","updated":"2025-05-06T22:02:30Z","published":"2025-05-06T22:02:30Z","title":"Action Spotting and Precise Event Detection in Sports: Datasets,\n  Methods, and Challenges","summary":"  Video event detection has become an essential component of sports analytics,\nenabling automated identification of key moments and enhancing performance\nanalysis, viewer engagement, and broadcast efficiency. Recent advancements in\ndeep learning, particularly Convolutional Neural Networks (CNNs) and\nTransformers, have significantly improved accuracy and efficiency in Temporal\nAction Localization (TAL), Action Spotting (AS), and Precise Event Spotting\n(PES). This survey provides a comprehensive overview of these three key tasks,\nemphasizing their differences, applications, and the evolution of\nmethodological approaches. We thoroughly review and categorize existing\ndatasets and evaluation metrics specifically tailored for sports contexts,\nhighlighting the strengths and limitations of each. Furthermore, we analyze\nstate-of-the-art techniques, including multi-modal approaches that integrate\naudio and visual information, methods utilizing self-supervised learning and\nknowledge distillation, and approaches aimed at generalizing across multiple\nsports. Finally, we discuss critical open challenges and outline promising\nresearch directions toward developing more generalized, efficient, and robust\nevent detection frameworks applicable to diverse sports. This survey serves as\na foundation for future research on efficient, generalizable, and multi-modal\nsports event detection.\n","authors":["Hao Xu","Arbind Agrahari Baniya","Sam Well","Mohamed Reda Bouadjenek","Richard Dazeley","Sunil Aryal"],"pdf_url":"https://arxiv.org/pdf/2505.03991v1.pdf","comment":"13 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2504.21226v2","updated":"2025-05-06T22:01:50Z","published":"2025-04-29T23:41:06Z","title":"MemeBLIP2: A novel lightweight multimodal system to detect harmful memes","summary":"  Memes often merge visuals with brief text to share humor or opinions, yet\nsome memes contain harmful messages such as hate speech. In this paper, we\nintroduces MemeBLIP2, a light weight multimodal system that detects harmful\nmemes by combining image and text features effectively. We build on previous\nstudies by adding modules that align image and text representations into a\nshared space and fuse them for better classification. Using BLIP-2 as the core\nvision-language model, our system is evaluated on the PrideMM datasets. The\nresults show that MemeBLIP2 can capture subtle cues in both modalities, even in\ncases with ironic or culturally specific content, thereby improving the\ndetection of harmful material.\n","authors":["Jiaqi Liu","Ran Tong","Aowei Shen","Shuzheng Li","Changlin Yang","Lisha Xu"],"pdf_url":"https://arxiv.org/pdf/2504.21226v2.pdf","comment":"11pages, 3 figures, manucripts in preparation"},{"id":"http://arxiv.org/abs/2505.03974v1","updated":"2025-05-06T20:52:58Z","published":"2025-05-06T20:52:58Z","title":"Deep Learning Framework for Infrastructure Maintenance: Crack Detection\n  and High-Resolution Imaging of Infrastructure Surfaces","summary":"  Recently, there has been an impetus for the application of cutting-edge data\ncollection platforms such as drones mounted with camera sensors for\ninfrastructure asset management. However, the sensor characteristics, proximity\nto the structure, hard-to-reach access, and environmental conditions often\nlimit the resolution of the datasets. A few studies used super-resolution\ntechniques to address the problem of low-resolution images. Nevertheless, these\ntechniques were observed to increase computational cost and false alarms of\ndistress detection due to the consideration of all the infrastructure images\ni.e., positive and negative distress classes. In order to address the\npre-processing of false alarm and achieve efficient super-resolution, this\nstudy developed a framework consisting of convolutional neural network (CNN)\nand efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately\nclassified both the classes. ESPCNN, which is the lightweight super-resolution\ntechnique, generated high-resolution infrastructure image of positive distress\nobtained from CNN. The ESPCNN outperformed bicubic interpolation in all the\nevaluation metrics for super-resolution. Based on the performance metrics, the\ncombination of CNN and ESPCNN was observed to be effective in preprocessing the\ninfrastructure images with negative distress, reducing the computational cost\nand false alarms in the next step of super-resolution. The visual inspection\nshowed that EPSCNN is able to capture crack propagation, complex geometry of\neven minor cracks. The proposed framework is expected to help the highway\nagencies in accurately performing distress detection and assist in efficient\nasset management practices.\n","authors":["Nikhil M. Pawar","Jorge A. Prozzi","Feng Hong","Surya Sarat Chandra Congress"],"pdf_url":"https://arxiv.org/pdf/2505.03974v1.pdf","comment":"Presented :Transportation Research Board 104th Annual Meeting,\n  Washington, D.C"},{"id":"http://arxiv.org/abs/2505.03912v1","updated":"2025-05-06T18:35:07Z","published":"2025-05-06T18:35:07Z","title":"OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation","summary":"  Dual-system VLA (Vision-Language-Action) architectures have become a hot\ntopic in embodied intelligence research, but there is a lack of sufficient\nopen-source work for further performance analysis and optimization. To address\nthis problem, this paper will summarize and compare the structural designs of\nexisting dual-system architectures, and conduct systematic empirical\nevaluations on the core design elements of existing dual-system architectures.\nUltimately, it will provide a low-cost open-source model for further\nexploration. Of course, this project will continue to update with more\nexperimental conclusions and open-source models with improved performance for\neveryone to choose from. Project page: https://openhelix-robot.github.io/.\n","authors":["Can Cui","Pengxiang Ding","Wenxuan Song","Shuanghao Bai","Xinyang Tong","Zirui Ge","Runze Suo","Wanqi Zhou","Yang Liu","Bofang Jia","Han Zhao","Siteng Huang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03896v1","updated":"2025-05-06T18:03:41Z","published":"2025-05-06T18:03:41Z","title":"Novel Extraction of Discriminative Fine-Grained Feature to Improve\n  Retinal Vessel Segmentation","summary":"  Retinal vessel segmentation is a vital early detection method for several\nsevere ocular diseases. Despite significant progress in retinal vessel\nsegmentation with the advancement of Neural Networks, there are still\nchallenges to overcome. Specifically, retinal vessel segmentation aims to\npredict the class label for every pixel within a fundus image, with a primary\nfocus on intra-image discrimination, making it vital for models to extract more\ndiscriminative features. Nevertheless, existing methods primarily focus on\nminimizing the difference between the output from the decoder and the label,\nbut ignore fully using feature-level fine-grained representations from the\nencoder. To address these issues, we propose a novel Attention U-shaped\nKolmogorov-Arnold Network named AttUKAN along with a novel Label-guided\nPixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we\nimplement Attention Gates into Kolmogorov-Arnold Networks to enhance model\nsensitivity by suppressing irrelevant feature activations and model\ninterpretability by non-linear modeling of KAN blocks. Additionally, we also\ndesign a novel Label-guided Pixel-wise Contrastive Loss to supervise our\nproposed AttUKAN to extract more discriminative features by distinguishing\nbetween foreground vessel-pixel pairs and background pairs. Experiments are\nconducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF\nand our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,\n80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and\n66.94% in the above datasets, which are the highest compared to 11 networks for\nretinal vessel segmentation. Quantitative and qualitative results show that our\nAttUKAN achieves state-of-the-art performance and outperforms existing retinal\nvessel segmentation methods. Our code will be available at\nhttps://github.com/stevezs315/AttUKAN.\n","authors":["Shuang Zeng","Chee Hong Lee","Micky C Nnamdi","Wenqi Shi","J Ben Tamo","Lei Zhu","Hangzhou He","Xinliang Zhang","Qian Chen","May D. Wang","Yanye Lu","Qiushi Ren"],"pdf_url":"https://arxiv.org/pdf/2505.03896v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.03739v1","updated":"2025-05-06T17:59:53Z","published":"2025-05-06T17:59:53Z","title":"VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model","summary":"  With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.\n","authors":["Zuwei Long","Yunhang Shen","Chaoyou Fu","Heting Gao","Lijiang Li","Peixian Chen","Mengdan Zhang","Hang Shao","Jian Li","Jinlong Peng","Haoyu Cao","Ke Li","Rongrong Ji","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2505.03739v1.pdf","comment":"Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio"},{"id":"http://arxiv.org/abs/2505.03738v1","updated":"2025-05-06T17:59:51Z","published":"2025-05-06T17:59:51Z","title":"AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid\n  Whole-Body Control","summary":"  Humanoid robots derive much of their dexterity from hyper-dexterous\nwhole-body movements, enabling tasks that require a large operational\nworkspace: such as picking objects off the ground. However, achieving these\ncapabilities on real humanoids remains challenging due to their high degrees of\nfreedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization\n(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with\ntrajectory optimization for real-time, adaptive whole-body control. To mitigate\ndistribution bias in motion imitation RL, we construct a hybrid AMO dataset and\ntrain a network capable of robust, on-demand adaptation to potentially O.O.D.\ncommands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid\nrobot, demonstrating superior stability and an expanded workspace compared to\nstrong baselines. Finally, we show that AMO's consistent performance supports\nautonomous task execution via imitation learning, underscoring the system's\nversatility and robustness.\n","authors":["Jialong Li","Xuxin Cheng","Tianshu Huang","Shiqi Yang","Ri-Zhao Qiu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03738v1.pdf","comment":"website: https://amo-humanoid.github.io"},{"id":"http://arxiv.org/abs/2505.03730v1","updated":"2025-05-06T17:58:02Z","published":"2025-05-06T17:58:02Z","title":"FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios","summary":"  Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/\n","authors":["Shiyi Zhang","Junhao Zhuang","Zhaoyang Zhang","Ying Shan","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2505.03730v1.pdf","comment":"Accepted by Siggraph2025, Project Page:\n  https://shiyi-zh0408.github.io/projectpages/FlexiAct/"},{"id":"http://arxiv.org/abs/2505.01877v2","updated":"2025-05-06T17:51:58Z","published":"2025-05-03T17:42:49Z","title":"Humans can learn to detect AI-generated texts, or at least learn when\n  they can't","summary":"  This study investigates whether individuals can learn to accurately\ndiscriminate between human-written and AI-produced texts when provided with\nimmediate feedback, and if they can use this feedback to recalibrate their\nself-perceived competence. We also explore the specific criteria individuals\nrely upon when making these decisions, focusing on textual style and perceived\nreadability.\n  We used GPT-4o to generate several hundred texts across various genres and\ntext types comparable to Koditex, a multi-register corpus of human-written\ntexts. We then presented randomized text pairs to 255 Czech native speakers who\nidentified which text was human-written and which was AI-generated.\nParticipants were randomly assigned to two conditions: one receiving immediate\nfeedback after each trial, the other receiving no feedback until experiment\ncompletion. We recorded accuracy in identification, confidence levels, response\ntimes, and judgments about text readability along with demographic data and\nparticipants' engagement with AI technologies prior to the experiment.\n  Participants receiving immediate feedback showed significant improvement in\naccuracy and confidence calibration. Participants initially held incorrect\nassumptions about AI-generated text features, including expectations about\nstylistic rigidity and readability. Notably, without feedback, participants\nmade the most errors precisely when feeling most confident -- an issue largely\nresolved among the feedback group.\n  The ability to differentiate between human and AI-generated texts can be\neffectively learned through targeted training with explicit feedback, which\nhelps correct misconceptions about AI stylistic features and readability, as\nwell as potential other variables that were not explored, while facilitating\nmore accurate self-assessment. This finding might be particularly important in\neducational contexts.\n","authors":["Jiří Milička","Anna Marklová","Ondřej Drobil","Eva Pospíšilová"],"pdf_url":"https://arxiv.org/pdf/2505.01877v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15661v2","updated":"2025-05-06T17:43:11Z","published":"2025-03-19T19:26:17Z","title":"UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and\n  Interaction","summary":"  Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks.\n","authors":["Shravan Nayak","Xiangru Jian","Kevin Qinghong Lin","Juan A. Rodriguez","Montek Kalsi","Rabiul Awal","Nicolas Chapados","M. Tamer Özsu","Aishwarya Agrawal","David Vazquez","Christopher Pal","Perouz Taslakian","Spandana Gella","Sai Rajeswar"],"pdf_url":"https://arxiv.org/pdf/2503.15661v2.pdf","comment":"This paper has been accepted to the 41st International Conference on\n  Machine Learning (ICML 2025)"},{"id":"http://arxiv.org/abs/2504.01153v3","updated":"2025-05-06T17:40:04Z","published":"2025-04-01T19:36:14Z","title":"Catch Me if You Search: When Contextual Web Search Results Affect the\n  Detection of Hallucinations","summary":"  While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or `hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N = 560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts.\n","authors":["Mahjabin Nahar","Eun-Ju Lee","Jin Won Park","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2504.01153v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16518v2","updated":"2025-05-06T17:34:16Z","published":"2025-03-16T19:32:17Z","title":"Advancing Human-Machine Teaming: Concepts, Challenges, and Applications","summary":"  Human-Machine Teaming (HMT) is revolutionizing collaboration across domains\nsuch as defense, healthcare, and autonomous systems by integrating AI-driven\ndecision-making, trust calibration, and adaptive teaming. This survey presents\na comprehensive taxonomy of HMT, analyzing theoretical models, including\nreinforcement learning, instance-based learning, and interdependence theory,\nalongside interdisciplinary methodologies. Unlike prior reviews, we examine\nteam cognition, ethical AI, multi-modal interactions, and real-world evaluation\nframeworks. Key challenges include explainability, role allocation, and\nscalable benchmarking. We propose future research in cross-domain adaptation,\ntrust-aware AI, and standardized testbeds. By bridging computational and social\nsciences, this work lays a foundation for resilient, ethical, and scalable HMT\nsystems.\n","authors":["Dian Chen","Han Jun Yoon","Zelin Wan","Nithin Alluru","Sang Won Lee","Richard He","Terrence J. Moore","Frederica F. Nelson","Sunghyun Yoon","Hyuk Lim","Dan Dongseong Kim","Jin-Hee Cho"],"pdf_url":"https://arxiv.org/pdf/2503.16518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03710v1","updated":"2025-05-06T17:32:39Z","published":"2025-05-06T17:32:39Z","title":"Actor-Critics Can Achieve Optimal Sample Efficiency","summary":"  Actor-critic algorithms have become a cornerstone in reinforcement learning\n(RL), leveraging the strengths of both policy-based and value-based methods.\nDespite recent progress in understanding their statistical efficiency, no\nexisting work has successfully learned an $\\epsilon$-optimal policy with a\nsample complexity of $O(1/\\epsilon^2)$ trajectories with general function\napproximation when strategic exploration is necessary.\n  We address this open problem by introducing a novel actor-critic algorithm\nthat attains a sample-complexity of $O(dH^5 \\log|\\mathcal{A}|/\\epsilon^2 + d\nH^4 \\log|\\mathcal{F}|/ \\epsilon^2)$ trajectories, and accompanying $\\sqrt{T}$\nregret when the Bellman eluder dimension $d$ does not increase with $T$ at more\nthan a $\\log T$ rate.\n  Here, $\\mathcal{F}$ is the critic function class, $\\mathcal{A}$ is the action\nspace, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm\nintegrates optimism, off-policy critic estimation targeting the optimal\nQ-function, and rare-switching policy resets.\n  We extend this to the setting of Hybrid RL, showing that initializing the\ncritic with offline data yields sample efficiency gains compared to purely\noffline or online RL. Further, utilizing access to offline data, we provide a\n\\textit{non-optimistic} provably efficient actor-critic algorithm that only\nadditionally requires $N_{\\text{off}} \\geq c_{\\text{off}}^*dH^4/\\epsilon^2$ in\nexchange for omitting optimism, where $c_{\\text{off}}^*$ is the single-policy\nconcentrability coefficient and $N_{\\text{off}}$ is the number of offline\nsamples. This addresses another open problem in the literature. We further\nprovide numerical experiments to support our theoretical findings.\n","authors":["Kevin Tan","Wei Fan","Yuting Wei"],"pdf_url":"https://arxiv.org/pdf/2505.03710v1.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2505.01884v2","updated":"2025-05-06T17:26:22Z","published":"2025-05-03T18:18:59Z","title":"Adversarial Robustness of Deep Learning Models for Inland Water Body\n  Segmentation from SAR Images","summary":"  Inland water body segmentation from Synthetic Aperture Radar (SAR) images is\nan important task needed for several applications, such as flood mapping. While\nSAR sensors capture data in all-weather conditions as high-resolution images,\ndifferentiating water and water-like surfaces from SAR images is not\nstraightforward. Inland water bodies, such as large river basins, have complex\ngeometry, which adds to the challenge of segmentation. U-Net is a widely used\ndeep learning model for land-water segmentation of SAR images. In practice,\nmanual annotation is often used to generate the corresponding water masks as\nground truth. Manual annotation of the images is prone to label noise owing to\ndata poisoning attacks, especially due to complex geometry. In this work, we\nsimulate manual errors in the form of adversarial attacks on the U-Net model\nand study the robustness of the model to human errors in annotation. Our\nresults indicate that U-Net can tolerate a certain level of corruption before\nits performance drops significantly. This finding highlights the crucial role\nthat the quality of manual annotations plays in determining the effectiveness\nof the segmentation model. The code and the new dataset, along with adversarial\nexamples for robust training, are publicly available. (GitHub link -\nhttps://github.com/GVCL/IWSeg-SAR-Poison.git)\n","authors":["Siddharth Kothari","Srinivasan Murali","Sankalp Kothari","Ujjwal Verma","Jaya Sreevalsan-Nair"],"pdf_url":"https://arxiv.org/pdf/2505.01884v2.pdf","comment":"21 pages, 15 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.14131v3","updated":"2025-05-06T17:12:37Z","published":"2025-02-19T22:22:20Z","title":"An Empirical Risk Minimization Approach for Offline Inverse RL and\n  Dynamic Discrete Choice Model","summary":"  We study the problem of estimating Dynamic Discrete Choice (DDC) models, also\nknown as offline Maximum Entropy-Regularized Inverse Reinforcement Learning\n(offline MaxEnt-IRL) in machine learning. The objective is to recover reward or\n$Q^*$ functions that govern agent behavior from offline behavior data. In this\npaper, we propose a globally convergent gradient-based method for solving these\nproblems without the restrictive assumption of linearly parameterized rewards.\nThe novelty of our approach lies in introducing the Empirical Risk Minimization\n(ERM) based IRL/DDC framework, which circumvents the need for explicit state\ntransition probability estimation in the Bellman equation. Furthermore, our\nmethod is compatible with non-parametric estimation techniques such as neural\nnetworks. Therefore, the proposed method has the potential to be scaled to\nhigh-dimensional, infinite state spaces. A key theoretical insight underlying\nour approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL)\ncondition -- a property that, while weaker than strong convexity, is sufficient\nto ensure fast global convergence guarantees. Through a series of synthetic\nexperiments, we demonstrate that our approach consistently outperforms\nbenchmark methods and state-of-the-art alternatives.\n","authors":["Enoch H. Kang","Hema Yoganarasimhan","Lalit Jain"],"pdf_url":"https://arxiv.org/pdf/2502.14131v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10707v2","updated":"2025-05-06T17:04:18Z","published":"2025-03-12T18:36:41Z","title":"CALLM: Understanding Cancer Survivors' Emotions and Intervention\n  Opportunities via Mobile Diaries and Context-Aware Language Models","summary":"  Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support.\n","authors":["Zhiyuan Wang","Katharine E. Daniel","Laura E. Barnes","Philip I. Chow"],"pdf_url":"https://arxiv.org/pdf/2503.10707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03694v1","updated":"2025-05-06T16:59:54Z","published":"2025-05-06T16:59:54Z","title":"Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and\n  Avoid","summary":"  Assured safe-separation is essential for achieving seamless high-density\noperation of airborne vehicles in a shared airspace. To equip\nresource-constrained aerial systems with this safety-critical capability, we\npresent ViSafe, a high-speed vision-only airborne collision avoidance system.\nViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by\ntightly integrating a learning-based edge-AI framework with a custom\nmulti-camera hardware prototype designed under SWaP-C constraints. By\nleveraging perceptual input-focused control barrier functions (CBF) to design,\nencode, and enforce safety thresholds, ViSafe can provide provably safe runtime\nguarantees for self-separation in high-speed aerial operations. We evaluate\nViSafe's performance through an extensive test campaign involving both\nsimulated digital twins and real-world flight scenarios. By independently\nvarying agent types, closure rates, interaction geometries, and environmental\nconditions (e.g., weather and lighting), we demonstrate that ViSafe\nconsistently ensures self-separation across diverse scenarios. In\nfirst-of-its-kind real-world high-speed collision avoidance tests with closure\nrates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous\ncollision avoidance, establishing a new standard for safety in high-speed\naerial navigation.\n","authors":["Parv Kapoor","Ian Higgins","Nikhil Keetha","Jay Patrikar","Brady Moon","Zelin Ye","Yao He","Ivan Cisneros","Yaoyu Hu","Changliu Liu","Eunsuk Kang","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2505.03694v1.pdf","comment":"13 pages, RSS 2025 Demo track"},{"id":"http://arxiv.org/abs/2504.16381v3","updated":"2025-05-06T16:43:13Z","published":"2025-04-23T03:02:29Z","title":"PINN-MEP: Continuous Neural Representations for Minimum-Energy Path\n  Discovery in Molecular Systems","summary":"  Characterizing conformational transitions in physical systems remains a\nfundamental challenge in the computational sciences. Traditional sampling\nmethods like molecular dynamics (MD) or MCMC often struggle with the\nhigh-dimensional nature of molecular systems and the high energy barriers of\ntransitions between stable states. While these transitions are rare events in\nsimulation timescales, they often represent the most biologically significant\nprocesses - for example, the conformational change of an ion channel protein\nfrom its closed to open state, which controls cellular ion flow and is crucial\nfor neural signaling. Such transitions in real systems may take milliseconds to\nseconds but could require months or years of continuous simulation to observe\neven once. We present a method that reformulates transition path generation as\na continuous optimization problem solved through physics-informed neural\nnetworks (PINNs) inspired by string methods for minimum-energy path (MEP)\ngeneration. By representing transition paths as implicit neural functions and\nleveraging automatic differentiation with differentiable molecular dynamics\nforce fields, our method enables the efficient discovery of physically\nrealistic transition pathways without requiring expensive path sampling. We\ndemonstrate our method's effectiveness on two proteins, including an explicitly\nhydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300\natoms.\n","authors":["Magnus Petersen","Roberto Covino"],"pdf_url":"https://arxiv.org/pdf/2504.16381v3.pdf","comment":"Update 06.05.2025: Added some intermediate steps in the derivation of\n  the loss to add clarity. Update 28.04.2025: Added citation and reference to\n  just-released work \"Action-Minimization Meets Generative Modeling: Efficient\n  Transition Path Sampling with the Onsager-Machlup Functional\" by Sanjeev Raja\n  et al. and added an appendix section clarifying some loss derivation steps"},{"id":"http://arxiv.org/abs/2505.03678v1","updated":"2025-05-06T16:23:42Z","published":"2025-05-06T16:23:42Z","title":"Graph Drawing for LLMs: An Empirical Evaluation","summary":"  Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance.\n","authors":["Walter Didimo","Fabrizio Montecchiani","Tommaso Piselli"],"pdf_url":"https://arxiv.org/pdf/2505.03678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14394v2","updated":"2025-05-06T16:18:38Z","published":"2024-07-19T15:16:25Z","title":"TTT: A Temporal Refinement Heuristic for Tenuously Tractable Discrete\n  Time Reachability Problems","summary":"  Reachable set computation is an important tool for analyzing control systems.\nSimulating a control system can show general trends, but a formal tool like\nreachability analysis can provide guarantees of correctness. Reachability\nanalysis for complex control systems, e.g., with nonlinear dynamics and/or a\nneural network controller, is often either slow or overly conservative. To\naddress these challenges, much literature has focused on spatial refinement,\ni.e., tuning the discretization of the input sets and intermediate reachable\nsets. This paper introduces the idea of temporal refinement: automatically\nchoosing when along the horizon of the reachability problem to execute slow\nsymbolic queries which incur less approximation error versus fast concrete\nqueries which incur more approximation error. Temporal refinement can be\ncombined with other refinement approaches as an additional tool to trade off\ntractability and tightness in approximate reachable set computation. We\nintroduce a temporal refinement algorithm and demonstrate its effectiveness at\ncomputing approximate reachable sets for nonlinear systems with neural network\ncontrollers. We calculate reachable sets with varying computational budget and\nshow that our algorithm can generate approximate reachable sets with a similar\namount of error to the baseline in 20-70% less time.\n","authors":["Chelsea Sidrane","Jana Tumova"],"pdf_url":"https://arxiv.org/pdf/2407.14394v2.pdf","comment":"To appear in the proceedings of the American Control Conference (ACC)\n  2025"},{"id":"http://arxiv.org/abs/2406.13216v3","updated":"2025-05-06T16:16:42Z","published":"2024-06-19T04:57:35Z","title":"CombAlign: Enhancing Model Expressiveness in Unsupervised Graph\n  Alignment","summary":"  Unsupervised graph alignment finds the node correspondence between a pair of\nattributed graphs by only exploiting graph structure and node features. One\ncategory of recent studies first computes the node representation and then\nmatches nodes with the largest embedding-based similarity, while the other\ncategory reduces the problem to optimal transport (OT) via Gromov-Wasserstein\nlearning. However, it remains largely unexplored in the model expressiveness,\nas well as how theoretical expressivity impacts prediction accuracy. We\ninvestigate the model expressiveness from two aspects. First, we characterize\nthe model's discriminative power in distinguishing matched and unmatched node\npairs across two graphs. Second, we study the model's capability of\nguaranteeing node matching properties such as one-to-one matching and mutual\nalignment. Motivated by our theoretical analysis, we put forward a hybrid\napproach named CombAlign with stronger expressive power. Specifically, we\nenable cross-dimensional feature interaction for OT-based learning and propose\nan embedding-based method inspired by the Weisfeiler-Lehman test. We also apply\nnon-uniform marginals obtained from the embedding-based modules to OT as priors\nfor more expressiveness. Based on that, we propose a traditional\nalgorithm-based refinement, which combines our OT and embedding-based\npredictions using the ensemble learning strategy and reduces the problem to\nmaximum weight matching. With carefully designed edge weights, we ensure those\nmatching properties and further enhance prediction accuracy. By extensive\nexperiments, we demonstrate a significant improvement of 14.5% in alignment\naccuracy compared to state-of-the-art approaches and confirm the soundness of\nour theoretical analysis.\n","authors":["Songyang Chen","Yu Liu","Lei Zou","Zexuan Wang","Youfang Lin"],"pdf_url":"https://arxiv.org/pdf/2406.13216v3.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.03674v1","updated":"2025-05-06T16:15:24Z","published":"2025-05-06T16:15:24Z","title":"Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts\n  Collaboration Perception, Not Performance","summary":"  In human-agent teams, openly sharing goals is often assumed to enhance\nplanning, collaboration, and effectiveness. However, direct communication of\nthese goals is not always feasible, requiring teammates to infer their\npartner's intentions through actions. Building on this, we investigate whether\nan AI agent's ability to share its inferred understanding of a human teammate's\ngoals can improve task performance and perceived collaboration. Through an\nexperiment comparing three conditions-no recognition (NR), viable goals (VG),\nand viable goals on-demand (VGod) - we find that while goal-sharing information\ndid not yield significant improvements in task performance or overall\nsatisfaction scores, thematic analysis suggests that it supported strategic\nadaptations and subjective perceptions of collaboration. Cognitive load\nassessments revealed no additional burden across conditions, highlighting the\nchallenge of balancing informativeness and simplicity in human-agent\ninteractions. These findings highlight the nuanced trade-off of goal-sharing:\nwhile it fosters trust and enhances perceived collaboration, it can\noccasionally hinder objective performance gains.\n","authors":["Yotam Amitai","Reuth Mirsky","Ofra Amir"],"pdf_url":"https://arxiv.org/pdf/2505.03674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01998v2","updated":"2025-05-06T16:09:59Z","published":"2025-05-04T06:03:12Z","title":"A Synergistic Framework of Nonlinear Acoustic Computing and\n  Reinforcement Learning for Real-World Human-Robot Interaction","summary":"  This paper introduces a novel framework integrating nonlinear acoustic\ncomputing and reinforcement learning to enhance advanced human-robot\ninteraction under complex noise and reverberation. Leveraging physically\ninformed wave equations (e.g., Westervelt, KZK), the approach captures\nhigher-order phenomena such as harmonic generation and shock formation. By\nembedding these models in a reinforcement learning-driven control loop, the\nsystem adaptively optimizes key parameters (e.g., absorption, beamforming) to\nmitigate multipath interference and non-stationary noise. Experimental\nevaluations, covering far-field localization, weak signal detection, and\nmultilingual speech recognition, demonstrate that this hybrid strategy\nsurpasses traditional linear methods and purely data-driven baselines,\nachieving superior noise suppression, minimal latency, and robust accuracy in\ndemanding real-world scenarios. The proposed system demonstrates broad\napplication prospects in AI hardware, robot, machine audition, artificial\naudition, and brain-machine interfaces.\n","authors":["Xiaoliang Chen","Xin Yu","Le Chang","Yunhe Huang","Jiashuai He","Shibo Zhang","Jin Li","Likai Lin","Ziyu Zeng","Xianling Tu","Shuyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.01998v2.pdf","comment":"34 pages, 11 figures, 10 tables, and 10 equations"},{"id":"http://arxiv.org/abs/2505.03668v1","updated":"2025-05-06T16:08:55Z","published":"2025-05-06T16:08:55Z","title":"Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time","summary":"  This paper proposes an integration of temporal logical reasoning and\nPartially Observable Markov Decision Processes (POMDPs) to achieve\ninterpretable decision-making under uncertainty with macro-actions. Our method\nleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus\n(EC) to generate \\emph{persistent} (i.e., constant) macro-actions, which guide\nMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,\nsignificantly reducing inference time while ensuring robust performance. Such\nmacro-actions are learnt via Inductive Logic Programming (ILP) from a few\ntraces of execution (belief-action pairs), thus eliminating the need for\nmanually designed heuristics and requiring only the specification of the POMDP\ntransition model. In the Pocman and Rocksample benchmark scenarios, our learned\nmacro-actions demonstrate increased expressiveness and generality when compared\nto time-independent heuristics, indeed offering substantial computational\nefficiency improvements.\n","authors":["Celeste Veronese","Daniele Meli","Alessandro Farinelli"],"pdf_url":"https://arxiv.org/pdf/2505.03668v1.pdf","comment":"Accepted at 9th Conference on Neurosymbolic Learning and Reasoning"},{"id":"http://arxiv.org/abs/2505.03662v1","updated":"2025-05-06T16:05:22Z","published":"2025-05-06T16:05:22Z","title":"Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps\n  from T1-Weighted MRI using CycleGAN Models","summary":"  Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are\nessential for evaluating white matter integrity and structural connectivity in\nneuroimaging. However, the spatial misalignment between FA maps and\ntractography atlases hinders their effective integration into predictive\nmodels. To address this issue, we propose a CycleGAN based approach for\ngenerating FA maps directly from T1-weighted MRI scans, representing the first\napplication of this technique to both healthy and tumour-affected tissues. Our\nmodel, trained on unpaired data, produces high fidelity maps, which have been\nrigorously evaluated using Structural Similarity Index (SSIM) and Peak\nSignal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in\ntumour regions. Radiological assessments further underscore the model's\npotential to enhance clinical workflows by providing an AI-driven alternative\nthat reduces the necessity for additional scans.\n","authors":["Xin Du","Francesca M. Cozzi","Rajesh Jena"],"pdf_url":"https://arxiv.org/pdf/2505.03662v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2505.03655v1","updated":"2025-05-06T16:00:41Z","published":"2025-05-06T16:00:41Z","title":"Counterfactual Inference for Eliminating Sentiment Bias in Recommender\n  Systems","summary":"  Recommender Systems (RSs) aim to provide personalized recommendations for\nusers. A newly discovered bias, known as sentiment bias, uncovers a common\nphenomenon within Review-based RSs (RRSs): the recommendation accuracy of users\nor items with negative reviews deteriorates compared with users or items with\npositive reviews. Critical users and niche items are disadvantaged by such\nunfair recommendations. We study this problem from the perspective of\ncounterfactual inference with two stages. At the model training stage, we build\na causal graph and model how sentiment influences the final rating score.\nDuring the inference stage, we decouple the direct and indirect effects to\nmitigate the impact of sentiment bias and remove the indirect effect using\ncounterfactual inference. We have conducted extensive experiments, and the\nresults validate that our model can achieve comparable performance on rating\nprediction for better recommendations and effective mitigation of sentiment\nbias. To the best of our knowledge, this is the first work to employ\ncounterfactual inference on sentiment bias mitigation in RSs.\n","authors":["Le Pan","Yuanjiang Cao","Chengkai Huang","Wenjie Zhang","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2505.03655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03654v1","updated":"2025-05-06T16:00:13Z","published":"2025-05-06T16:00:13Z","title":"ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language\n  and Vision Assistant","summary":"  Recent advances in personalized MLLMs enable effective capture of\nuser-specific concepts, supporting both recognition of personalized concepts\nand contextual captioning. However, humans typically explore and reason over\nrelations among objects and individuals, transcending surface-level information\nto achieve more personalized and contextual understanding. To this end,\nexisting methods may face three main limitations: Their training data lacks\nmulti-object sets in which relations among objects are learnable. Building on\nthe limited training data, their models overlook the relations between\ndifferent personalized concepts and fail to reason over them. Their experiments\nmainly focus on a single personalized concept, where evaluations are limited to\nrecognition and captioning tasks. To address the limitations, we present a new\ndataset named ReGraP, consisting of 120 sets of personalized knowledge. Each\nset includes images, KGs, and CoT QA pairs derived from the KGs, enabling more\nstructured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an\nMLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard\ngraph prompting methods are designed to align KGs within the model's semantic\nspace. We establish the ReGraP Benchmark, which contains diverse task types:\nmultiple-choice, fill-in-the-blank, True/False, and descriptive questions in\nboth open- and closed-ended settings. The proposed benchmark is designed to\nevaluate the relational reasoning and knowledge-connection capability of\npersonalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and\nother competitive MLLMs. Results show that the proposed model not only learns\npersonalized knowledge but also performs relational reasoning in responses,\nachieving the SoTA performance compared with the competitive methods. All the\ncodes and datasets are released at: https://github.com/xyfyyds/ReGraP.\n","authors":["Yifan Xiang","Zhenxi Zhang","Bin Li","Yixuan Weng","Shoujun Zhou","Yangfan He","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2505.03654v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.03648v1","updated":"2025-05-06T15:54:52Z","published":"2025-05-06T15:54:52Z","title":"Binding threshold units with artificial oscillatory neurons","summary":"  Artificial Kuramoto oscillatory neurons were recently introduced as an\nalternative to threshold units. Empirical evidence suggests that oscillatory\nunits outperform threshold units in several tasks including unsupervised object\ndiscovery and certain reasoning problems. The proposed coupling mechanism for\nthese oscillatory neurons is heterogeneous, combining a generalized Kuramoto\nequation with standard coupling methods used for threshold units. In this\nresearch note, we present a theoretical framework that clearly distinguishes\noscillatory neurons from threshold units and establishes a coupling mechanism\nbetween them. We argue that, from a biological standpoint, oscillatory and\nthreshold units realise distinct aspects of neural coding: roughly, threshold\nunits model intensity of neuron firing, while oscillatory units facilitate\ninformation exchange by frequency modulation. To derive interaction between\nthese two types of units, we constrain their dynamics by focusing on dynamical\nsystems that admit Lyapunov functions. For threshold units, this leads to\nHopfield associative memory model, and for oscillatory units it yields a\nspecific form of generalized Kuramoto model. The resulting dynamical systems\ncan be naturally coupled to form a Hopfield-Kuramoto associative memory model,\nwhich also admits a Lyapunov function. Various forms of coupling are possible.\nNotably, oscillatory neurons can be employed to implement a low-rank correction\nto the weight matrix of a Hopfield network. This correction can be viewed\neither as a form of Hebbian learning or as a popular LoRA method used for\nfine-tuning of large language models. We demonstrate the practical realization\nof this particular coupling through illustrative toy experiments.\n","authors":["Vladimir Fanaskov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2505.03648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15627v2","updated":"2025-05-06T15:53:34Z","published":"2025-01-26T18:25:26Z","title":"HardML: A Benchmark For Evaluating Data Science And Machine Learning\n  knowledge and reasoning in AI","summary":"  We present HardML, a benchmark designed to evaluate the knowledge and\nreasoning abilities in the fields of data science and machine learning. HardML\ncomprises a diverse set of 100 challenging multiple-choice questions,\nhandcrafted over a period of 6 months, covering the most popular and modern\nbranches of data science and machine learning. These questions are challenging\neven for a typical Senior Machine Learning Engineer to answer correctly. To\nminimize the risk of data contamination, HardML uses mostly original content\ndevised by the author. Current state of the art AI models achieve a 30% error\nrate on this benchmark, which is about 3 times larger than the one achieved on\nthe equivalent, well known MMLU ML. While HardML is limited in scope and not\naiming to push the frontier, primarily due to its multiple choice nature, it\nserves as a rigorous and modern testbed to quantify and track the progress of\ntop AI. While plenty benchmarks and experimentation in LLM evaluation exist in\nother STEM fields like mathematics, physics and chemistry, the subfields of\ndata science and machine learning remain fairly underexplored.\n","authors":["Tidor-Vlad Pricope"],"pdf_url":"https://arxiv.org/pdf/2501.15627v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03646v1","updated":"2025-05-06T15:52:14Z","published":"2025-05-06T15:52:14Z","title":"ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders","summary":"  Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples.\n","authors":["Chethan Krishnamurthy Ramanaik","Arjun Roy","Eirini Ntoutsi"],"pdf_url":"https://arxiv.org/pdf/2505.03646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03643v1","updated":"2025-05-06T15:50:43Z","published":"2025-05-06T15:50:43Z","title":"BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop\n  Systems","summary":"  Learning-enabled planning and control algorithms are increasingly popular,\nbut they often lack rigorous guarantees of performance or safety. We introduce\nan algorithm for computing underapproximate backward reachable sets of\nnonlinear discrete time neural feedback loops. We then use the backward\nreachable sets to check goal-reaching properties. Our algorithm is based on\noverapproximating the system dynamics function to enable computation of\nunderapproximate backward reachable sets through solutions of mixed-integer\nlinear programs. We rigorously analyze the soundness of our algorithm and\ndemonstrate it on a numerical example. Our work expands the class of properties\nthat can be verified for learning-enabled systems.\n","authors":["Chelsea Sidrane","Jana Tumova"],"pdf_url":"https://arxiv.org/pdf/2505.03643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03641v1","updated":"2025-05-06T15:44:42Z","published":"2025-05-06T15:44:42Z","title":"Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and\n  Manipulating Human Perceptual Variability","summary":"  Human decision-making in cognitive tasks and daily life exhibits considerable\nvariability, shaped by factors such as task difficulty, individual preferences,\nand personal experiences. Understanding this variability across individuals is\nessential for uncovering the perceptual and decision-making mechanisms that\nhumans rely on when faced with uncertainty and ambiguity. We present a\ncomputational framework BAM (Boundary Alignment & Manipulation framework) that\ncombines perceptual boundary sampling in ANNs and human behavioral experiments\nto systematically investigate this phenomenon. Our perceptual boundary sampling\nalgorithm generates stimuli along ANN decision boundaries that intrinsically\ninduce significant perceptual variability. The efficacy of these stimuli is\nempirically validated through large-scale behavioral experiments involving 246\nparticipants across 116,715 trials, culminating in the variMNIST dataset\ncontaining 19,943 systematically annotated images. Through personalized model\nalignment and adversarial generation, we establish a reliable method for\nsimultaneously predicting and manipulating the divergent perceptual decisions\nof pairs of participants. This work bridges the gap between computational\nmodels and human individual difference research, providing new tools for\npersonalized perception analysis.\n","authors":["Chen Wei","Chi Zhang","Jiachen Zou","Haotian Deng","Dietmar Heinke","Quanying Liu"],"pdf_url":"https://arxiv.org/pdf/2505.03641v1.pdf","comment":"accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2502.15037v5","updated":"2025-05-06T15:36:35Z","published":"2025-02-20T20:46:09Z","title":"DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time","summary":"  Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.\n","authors":["Yizhou Chen","Xiaoyue Wu","Yeheng Zong","Yuzhen Chen","Anran Li","Bohao Zhang","Ram Vasudevan"],"pdf_url":"https://arxiv.org/pdf/2502.15037v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13558v6","updated":"2025-05-06T15:28:57Z","published":"2025-03-17T02:49:34Z","title":"Survival Analysis with Machine Learning for Predicting Li-ion Battery\n  Remaining Useful Life","summary":"  Battery degradation significantly impacts the reliability and efficiency of\nenergy storage systems, particularly in electric vehicles and industrial\napplications. Predicting the remaining useful life (RUL) of lithium-ion\nbatteries is crucial for optimizing maintenance schedules, reducing costs, and\nimproving safety. Traditional RUL prediction methods often struggle with\nnonlinear degradation patterns and uncertainty quantification. To address these\nchallenges, we propose a hybrid survival analysis framework integrating\nsurvival data reconstruction, survival model learning, and survival probability\nestimation. Our approach transforms battery voltage time series into\ntime-to-failure data using path signatures. The multiple Cox-based survival\nmodels and machine-learning-based methods, such as DeepHit and MTLR, are\nlearned to predict battery failure-free probabilities over time. Experiments\nconducted on the Toyota battery and NASA battery datasets demonstrate the\neffectiveness of our approach, achieving high time-dependent AUC and\nconcordance index (C-Index) while maintaining a low integrated Brier score. The\ndata and source codes for this work are available to the public at\nhttps://github.com/thinkxca/rul.\n","authors":["Jingyuan Xue","Longfei Wei","Dongjing Jiang","Fang Sheng","Russell Greiner","Jianfei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13558v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21260v2","updated":"2025-05-06T15:02:40Z","published":"2024-07-31T00:43:51Z","title":"Bellman Unbiasedness: Tractable and Provably Efficient Distributional\n  Reinforcement Learning with General Value Function Approximation","summary":"  Distributional reinforcement learning improves performance by capturing\nenvironmental stochasticity, but a comprehensive theoretical understanding of\nits effectiveness remains elusive. In addition, the intractable element of the\ninfinite dimensionality of distributions has been overlooked. In this paper, we\npresent a regret analysis of distributional reinforcement learning with general\nvalue function approximation in a finite episodic Markov decision process\nsetting. We first introduce a key notion of $\\textit{Bellman unbiasedness}$\nwhich is essential for exactly learnable and provably efficient distributional\nupdates in an online manner. Among all types of statistical functionals for\nrepresenting infinite-dimensional return distributions, our theoretical results\ndemonstrate that only moment functionals can exactly capture the statistical\ninformation. Secondly, we propose a provably efficient algorithm,\n$\\texttt{SF-LSVI}$, that achieves a tight regret bound of $\\tilde{O}(d_E\nH^{\\frac{3}{2}}\\sqrt{K})$ where $H$ is the horizon, $K$ is the number of\nepisodes, and $d_E$ is the eluder dimension of a function class.\n","authors":["Taehyun Cho","Seungyub Han","Kyungjae Lee","Seokhun Ju","Dohyeong Kim","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2407.21260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03032v2","updated":"2025-05-06T14:58:40Z","published":"2024-10-03T22:29:20Z","title":"CounterQuill: Investigating the Potential of Human-AI Collaboration in\n  Online Counterspeech Writing","summary":"  Online hate speech has become increasingly prevalent on social media\nplatforms, causing harm to individuals and society. While efforts have been\nmade to combat this issue through content moderation, the potential of\nuser-driven counterspeech as an alternative solution remains underexplored.\nExisting counterspeech methods often face challenges such as fear of\nretaliation and skill-related barriers. To address these challenges, we\nintroduce CounterQuill, an AI-mediated system that assists users in composing\neffective and empathetic counterspeech. CounterQuill provides a three-step\nprocess: (1) a learning session to help users understand hate speech and\ncounterspeech; (2) a brainstorming session that guides users in identifying key\nelements of hate speech and exploring counterspeech strategies; and (3) a\nco-writing session that enables users to draft and refine their counterspeech\nwith CounterQuill. We conducted a within-subjects user study with 20\nparticipants to evaluate CounterQuill in comparison to ChatGPT. Results show\nthat CounterQuill's guidance and collaborative writing process provided users a\nstronger sense of ownership over their co-authored counterspeech. Users\nperceived CounterQuill as a writing partner and thus were more willing to post\nthe co-written counterspeech online compared to the one written with ChatGPT.\n","authors":["Xiaohan Ding","Kaike Ping","Uma Sushmitha Gunturi","Buse Carik","Sophia Stil","Lance T Wilhelm","Taufiq Daryanto","James Hawdon","Sang Won Lee","Eugenia H Rho"],"pdf_url":"https://arxiv.org/pdf/2410.03032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04306v2","updated":"2025-05-06T14:51:07Z","published":"2023-10-06T15:05:41Z","title":"Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware\n  Learning","summary":"  Group-level emotion recognition (GER) is an inseparable part of human\nbehavior analysis, aiming to recognize an overall emotion in a multi-person\nscene. However, the existing methods are devoted to combing diverse emotion\ncues while ignoring the inherent uncertainties under unconstrained\nenvironments, such as congestion and occlusion occurring within a group.\nAdditionally, since only group-level labels are available, inconsistent emotion\npredictions among individuals in one group can confuse the network. In this\npaper, we propose an uncertainty-aware learning (UAL) method to extract more\nrobust representations for GER. By explicitly modeling the uncertainty of each\nindividual, we utilize stochastic embedding drawn from a Gaussian distribution\ninstead of deterministic point embedding. This representation captures the\nprobabilities of different emotions and generates diverse predictions through\nthis stochasticity during the inference stage. Furthermore,\nuncertainty-sensitive scores are adaptively assigned as the fusion weights of\nindividuals' face within each group. Moreover, we develop an image enhancement\nmodule to enhance the model's robustness against severe noise. The overall\nthree-branch model, encompassing face, object, and scene component, is guided\nby a proportional-weighted fusion strategy and integrates the proposed\nuncertainty-aware method to produce the final group-level output. Experimental\nresults demonstrate the effectiveness and generalization ability of our method\nacross three widely used databases.\n","authors":["Qing Zhu","Qirong Mao","Jialin Zhang","Xiaohua Huang","Wenming Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.04306v2.pdf","comment":"11 pages,3 figures"},{"id":"http://arxiv.org/abs/2412.01487v4","updated":"2025-05-06T14:49:11Z","published":"2024-12-02T13:39:29Z","title":"FastRM: An efficient and automatic explainability framework for\n  multimodal generative models","summary":"  Large Vision Language Models (LVLMs) have demonstrated remarkable reasoning\ncapabilities over textual and visual inputs. However, these models remain prone\nto generating misinformation. Identifying and mitigating ungrounded responses\nis crucial for developing trustworthy AI. Traditional explainability methods\nsuch as gradient-based relevancy maps, offer insight into the decision process\nof models, but are often computationally expensive and unsuitable for real-time\noutput validation. In this work, we introduce FastRM, an efficient method for\npredicting explainable Relevancy Maps of LVLMs. Furthermore, FastRM provides\nboth quantitative and qualitative assessment of model confidence. Experimental\nresults demonstrate that FastRM achieves a 99.8% reduction in computation time\nand a 44.4% reduction in memory footprint compared to traditional relevancy map\ngeneration. FastRM allows explainable AI to be more practical and scalable,\nthereby promoting its deployment in real-world applications and enabling users\nto more effectively evaluate the reliability of model outputs.\n","authors":["Gabriela Ben-Melech Stan","Estelle Aflalo","Man Luo","Shachar Rosenman","Tiep Le","Sayak Paul","Shao-Yen Tseng","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2412.01487v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03586v1","updated":"2025-05-06T14:47:56Z","published":"2025-05-06T14:47:56Z","title":"Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning\n  Framework for Mitigating Delayed Observation","summary":"  In real-world multi-agent systems (MASs), observation delays are ubiquitous,\npreventing agents from making decisions based on the environment's true state.\nAn individual agent's local observation often consists of multiple components\nfrom other agents or dynamic entities in the environment. These discrete\nobservation components with varying delay characteristics pose significant\nchallenges for multi-agent reinforcement learning (MARL). In this paper, we\nfirst formulate the decentralized stochastic individual delay partially\nobservable Markov decision process (DSID-POMDP) by extending the standard\nDec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL\ntraining framework for addressing stochastic individual delays, along with\nrecommended implementations for its constituent modules. We implement the\nDSID-POMDP's observation generation pattern using standard MARL benchmarks,\nincluding MPE and SMAC. Experiments demonstrate that baseline MARL methods\nsuffer severe performance degradation under fixed and unfixed delays. The\nRDC-enhanced approach mitigates this issue, remarkably achieving ideal\ndelay-free performance in certain delay scenarios while maintaining\ngeneralization capability. Our work provides a novel perspective on multi-agent\ndelayed observation problems and offers an effective solution framework.\n","authors":["Songchen Fu","Siang Chen","Shaojing Zhao","Letian Bai","Ta Li","Yonghong Yan"],"pdf_url":"https://arxiv.org/pdf/2505.03586v1.pdf","comment":"The code will be open-sourced in the RDC-pymarl project under\n  https://github.com/linkjoker1006"},{"id":"http://arxiv.org/abs/2504.05356v2","updated":"2025-05-06T14:46:21Z","published":"2025-04-07T09:26:25Z","title":"DyTTP: Trajectory Prediction with Normalization-Free Transformers","summary":"  Accurate trajectory prediction is a cornerstone for the safe operation of\nautonomous driving systems, where understanding the dynamic behavior of\nsurrounding agents is crucial. Transformer-based architectures have\ndemonstrated significant promise in capturing complex spatio-temporality\ndependencies. However, their reliance on normalization layers can lead to\ncomputation overhead and training instabilities. In this work, we present a\ntwo-fold approach to address these challenges. First, we integrate DynamicTanh\n(DyT), which is the latest method to promote transformers, into the backbone,\nreplacing traditional layer normalization. This modification simplifies the\nnetwork architecture and improves the stability of the inference. We are the\nfirst work to deploy the DyT to the trajectory prediction task. Complementing\nthis, we employ a snapshot ensemble strategy to further boost trajectory\nprediction performance. Using cyclical learning rate scheduling, multiple model\nsnapshots are captured during a single training run. These snapshots are then\naggregated via simple averaging at inference time, allowing the model to\nbenefit from diverse hypotheses without incurring substantial additional\ncomputational cost. Extensive experiments on Argoverse datasets demonstrate\nthat our combined approach significantly improves prediction accuracy,\ninference speed and robustness in diverse driving scenarios. This work\nunderscores the potential of normalization-free transformer designs augmented\nwith lightweight ensemble techniques in advancing trajectory forecasting for\nautonomous vehicles.\n","authors":["JianLin Zhu","HongKuo Niu"],"pdf_url":"https://arxiv.org/pdf/2504.05356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03584v1","updated":"2025-05-06T14:43:49Z","published":"2025-05-06T14:43:49Z","title":"BCause: Human-AI collaboration to improve hybrid mapping and ideation in\n  argumentation-grounded deliberation","summary":"  Public deliberation, as in open discussion of issues of public concern, often\nsuffers from scattered and shallow discourse, poor sensemaking, and a\ndisconnect from actionable policy outcomes. This paper introduces BCause, a\ndiscussion system leveraging generative AI and human-machine collaboration to\ntransform unstructured dialogue around public issues (such as urban living,\npolicy changes, and current socio-economic transformations) into structured,\nactionable democratic processes. We present three innovations: (i) importing\nand transforming unstructured transcripts into argumentative discussions, (ii)\ngeo-deliberated problem-sensing via a Telegram bot for local issue reporting,\nand (iii) smart reporting with customizable widgets (e.g., summaries, topic\nmodelling, policy recommendations, clustered arguments). The system's human-AI\npartnership preserves critical human participation to ensure ethical oversight,\ncontextual relevance, and creative synthesis.\n","authors":["Lucas Anastasiou","Anna De Liddo"],"pdf_url":"https://arxiv.org/pdf/2505.03584v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.03574v1","updated":"2025-05-06T14:34:21Z","published":"2025-05-06T14:34:21Z","title":"LlamaFirewall: An open source guardrail system for building secure AI\n  agents","summary":"  Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails.\n","authors":["Sahana Chennabasappa","Cyrus Nikolaidis","Daniel Song","David Molnar","Stephanie Ding","Shengye Wan","Spencer Whitman","Lauren Deason","Nicholas Doucette","Abraham Montilla","Alekhya Gampa","Beto de Paola","Dominik Gabi","James Crnkovich","Jean-Christophe Testud","Kat He","Rashnil Chaturvedi","Wu Zhou","Joshua Saxe"],"pdf_url":"https://arxiv.org/pdf/2505.03574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03570v1","updated":"2025-05-06T14:29:47Z","published":"2025-05-06T14:29:47Z","title":"OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents","summary":"  In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.\n","authors":["Mariya Davydova","Daniel Jeffries","Patrick Barker","Arturo Márquez Flores","Sinéad Ryan"],"pdf_url":"https://arxiv.org/pdf/2505.03570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13435v3","updated":"2025-05-06T14:21:22Z","published":"2023-12-20T21:24:52Z","title":"The Adaptive Arms Race: Redefining Robustness in AI Security","summary":"  Despite considerable efforts on making them robust, real-world AI-based\nsystems remain vulnerable to decision based attacks, as definitive proofs of\ntheir operational robustness have so far proven intractable. Canonical\nrobustness evaluation relies on adaptive attacks, which leverage complete\nknowledge of the defense and are tailored to bypass it. This work broadens the\nnotion of adaptivity, which we employ to enhance both attacks and defenses,\nshowing how they can benefit from mutual learning through interaction. We\nintroduce a framework for adaptively optimizing black-box attacks and defenses\nunder the competitive game they form. To assess robustness reliably, it is\nessential to evaluate against realistic and worst-case attacks. We thus enhance\nattacks and their evasive arsenal together using RL, apply the same principle\nto defenses, and evaluate them first independently and then jointly under a\nmulti-agent perspective. We find that active defenses, those that dynamically\ncontrol system responses, are an essential complement to model hardening\nagainst decision-based attacks; that these defenses can be circumvented by\nadaptive attacks, something that elicits defenses being adaptive too. Our\nfindings, supported by an extensive theoretical and empirical investigation,\nconfirm that adaptive adversaries pose a serious threat to black-box AI-based\nsystems, rekindling the proverbial arms race. Notably, our approach outperforms\nthe state-of-the-art black-box attacks and defenses, while bringing them\ntogether to render effective insights into the robustness of real-world\ndeployed ML-based systems.\n","authors":["Ilias Tsingenopoulos","Vera Rimmer","Davy Preuveneers","Fabio Pierazzi","Lorenzo Cavallaro","Wouter Joosen"],"pdf_url":"https://arxiv.org/pdf/2312.13435v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03562v1","updated":"2025-05-06T14:13:44Z","published":"2025-05-06T14:13:44Z","title":"Real-Time Person Image Synthesis Using a Flow Matching Model","summary":"  Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance.\n","authors":["Jiwoo Jeong","Kirok Kim","Wooju Kim","Nam-Joon Kim"],"pdf_url":"https://arxiv.org/pdf/2505.03562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03561v1","updated":"2025-05-06T14:13:21Z","published":"2025-05-06T14:13:21Z","title":"Ergodic Generative Flows","summary":"  Generative Flow Networks (GFNs) were initially introduced on directed acyclic\ngraphs to sample from an unnormalized distribution density. Recent works have\nextended the theoretical framework for generative methods allowing more\nflexibility and enhancing application range. However, many challenges remain in\ntraining GFNs in continuous settings and for imitation learning (IL), including\nintractability of flow-matching loss, limited tests of non-acyclic training,\nand the need for a separate reward model in imitation learning. The present\nwork proposes a family of generative flows called Ergodic Generative Flows\n(EGFs) which are used to address the aforementioned issues. First, we leverage\nergodicity to build simple generative flows with finitely many globally defined\ntransformations (diffeomorphisms) with universality guarantees and tractable\nflow-matching loss (FM loss). Second, we introduce a new loss involving\ncross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It\nis designed for IL training without a separate reward model. We evaluate\nIL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using\nthe KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning\nexperiments with a target reward, using the FM loss.\n","authors":["Leo Maxime Brunswic","Mateo Clemente","Rui Heng Yang","Adam Sigal","Amir Rasouli","Yinchuan Li"],"pdf_url":"https://arxiv.org/pdf/2505.03561v1.pdf","comment":"20 pages, 5 figures, 1 table, accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2505.03560v1","updated":"2025-05-06T14:13:20Z","published":"2025-05-06T14:13:20Z","title":"Rapid AI-based generation of coverage paths for dispensing applications","summary":"  Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial\nrole in the design of power electronics and electronic control units. Up to\nnow, this is done manually by experts or by using optimization approaches with\na high computational effort. We propose a novel AI-based approach to generate\ndispense paths for TIM and similar dispensing applications. It is a drop-in\nreplacement for optimization-based approaches. An Artificial Neural Network\n(ANN) receives the target cooling area as input and directly outputs the\ndispense path. Our proposed setup does not require labels and we show its\nfeasibility on multiple target areas. The resulting dispense paths can be\ndirectly transferred to automated manufacturing equipment and do not exhibit\nair entrapments. The approach of using an ANN to predict process parameters for\na desired target state in real-time could potentially be transferred to other\nmanufacturing processes.\n","authors":["Simon Baeuerle","Ian F. Mendonca","Kristof Van Laerhoven","Ralf Mikut","Andreas Steimer"],"pdf_url":"https://arxiv.org/pdf/2505.03560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03557v1","updated":"2025-05-06T14:11:02Z","published":"2025-05-06T14:11:02Z","title":"Generating Synthetic Data via Augmentations for Improved Facial\n  Resemblance in DreamBooth and InstantID","summary":"  The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications.\n","authors":["Koray Ulusan","Benjamin Kiefer"],"pdf_url":"https://arxiv.org/pdf/2505.03557v1.pdf","comment":"Accepted to CVPR 2025 Workshop \"Synthetic Data for Computer Vision\n  Workshop\", https://syndata4cv.github.io/"},{"id":"http://arxiv.org/abs/2310.07937v3","updated":"2025-05-06T14:06:58Z","published":"2023-10-11T23:17:43Z","title":"Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation Using\n  Vision Language Models","summary":"  Visual target navigation is a critical capability for autonomous robots\noperating in unknown environments, particularly in human-robot interaction\nscenarios. While classical and learning-based methods have shown promise, most\nexisting approaches lack common-sense reasoning and are typically designed for\nsingle-robot settings, leading to reduced efficiency and robustness in complex\nenvironments. To address these limitations, we introduce Co-NavGPT, a novel\nframework that integrates a Vision Language Model (VLM) as a global planner to\nenable common-sense multi-robot visual target navigation. Co-NavGPT aggregates\nsub-maps from multiple robots with diverse viewpoints into a unified global\nmap, encoding robot states and frontier regions. The VLM uses this information\nto assign frontiers across the robots, facilitating coordinated and efficient\nexploration. Experiments on the Habitat-Matterport 3D (HM3D) demonstrate that\nCo-NavGPT outperforms existing baselines in terms of success rate and\nnavigation efficiency, without requiring task-specific training. Ablation\nstudies further confirm the importance of semantic priors from the VLM. We also\nvalidate the framework in real-world scenarios using quadrupedal robots.\nSupplementary video and code are available at:\nhttps://sites.google.com/view/co-navgpt2.\n","authors":["Bangguo Yu","Qihao Yuan","Kailai Li","Hamidreza Kasaei","Ming Cao"],"pdf_url":"https://arxiv.org/pdf/2310.07937v3.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.03553v1","updated":"2025-05-06T14:05:12Z","published":"2025-05-06T14:05:12Z","title":"A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model\n  Reasoning","summary":"  Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks.\n","authors":["Kolawole E. Ogunsina","Morayo A. Ogunsina"],"pdf_url":"https://arxiv.org/pdf/2505.03553v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2406.07113v4","updated":"2025-05-06T14:02:10Z","published":"2024-06-11T09:57:04Z","title":"Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene\n  Graph","summary":"  Locating objects described in natural language presents a significant\nchallenge for autonomous agents. Existing CLIP-based open-vocabulary methods\nsuccessfully perform 3D object grounding with simple (bare) queries, but cannot\ncope with ambiguous descriptions that demand an understanding of object\nrelations. To tackle this problem, we propose a modular approach called BBQ\n(Beyond Bare Queries), which constructs 3D scene graph representation with\nmetric and semantic spatial edges and utilizes a large language model as a\nhuman-to-agent interface through our deductive scene reasoning algorithm. BBQ\nemploys robust DINO-powered associations to construct 3D object-centric map and\nan advanced raycasting algorithm with a 2D vision-language model to describe\nthem as graph nodes. On the Replica and ScanNet datasets, we have demonstrated\nthat BBQ takes a leading place in open-vocabulary 3D semantic segmentation\ncompared to other zero-shot methods. Also, we show that leveraging spatial\nrelations is especially effective for scenes containing multiple entities of\nthe same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks,\nour deductive approach demonstrates a significant improvement, enabling objects\ngrounding by complex queries compared to other state-of-the-art methods. The\ncombination of our design choices and software implementation has resulted in\nsignificant data processing speed in experiments on the robot on-board\ncomputer. This promising performance enables the application of our approach in\nintelligent robotics projects. We made the code publicly available at\nhttps://linukc.github.io/BeyondBareQueries/.\n","authors":["Sergey Linok","Tatiana Zemskova","Svetlana Ladanova","Roman Titkov","Dmitry Yudin","Maxim Monastyrny","Aleksei Valenkov"],"pdf_url":"https://arxiv.org/pdf/2406.07113v4.pdf","comment":"6 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.03547v1","updated":"2025-05-06T14:00:41Z","published":"2025-05-06T14:00:41Z","title":"STORY2GAME: Generating (Almost) Everything in an Interactive Fiction\n  Game","summary":"  We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story.\n","authors":["Eric Zhou","Shreyas Basavatia","Moontashir Siam","Zexin Chen","Mark O. Riedl"],"pdf_url":"https://arxiv.org/pdf/2505.03547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13620v5","updated":"2025-05-06T13:59:11Z","published":"2025-01-23T12:42:42Z","title":"A Cognitive Paradigm Approach to Probe the Perception-Reasoning\n  Interface in VLMs","summary":"  A fundamental challenge in artificial intelligence involves understanding the\ncognitive mechanisms underlying visual reasoning in sophisticated models like\nVision-Language Models (VLMs). How do these models integrate visual perception\nwith abstract thought, especially when reasoning across multiple images or\nrequiring fine-grained compositional understanding? Drawing inspiration from\ncognitive science, this paper introduces a structured evaluation framework\nusing diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to\ndissect the perception-reasoning interface in VLMs. We propose three distinct\nevaluation paradigms, mirroring human problem-solving strategies: Direct Visual\nRule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule\nextraction and application), and Componential Analysis (CA; analytical\ndecomposition via task-agnostic textual descriptions). These paradigms\nsystematically vary cognitive load and probe processing stages. Notably, CA\nenables multi-image reasoning evaluation even for single-image architectures\nand isolates reasoning from perception by operating on textual descriptions.\nApplying this framework, we demonstrate that CA, leveraging powerful language\nmodels for reasoning over rich, independently generated descriptions, achieves\nnew state-of-the-art (SOTA) performance on challenging benchmarks including\nBongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm\nreasoning improves significantly when perceptual challenges are mitigated,\nrevealing a critical perception bottleneck. Our framework provides a valuable\ndiagnostic tool and suggests that decoupling perception (via rich,\ntask-agnostic description) from reasoning is a promising direction for robust\nand general visual intelligence.\n","authors":["Mohit Vaishnav","Tanel Tammet"],"pdf_url":"https://arxiv.org/pdf/2501.13620v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18991v2","updated":"2025-05-06T13:47:34Z","published":"2025-03-23T16:40:29Z","title":"HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective\n  Reasoning for LLM Alignment","summary":"  The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness.\n","authors":["Ruoxi Cheng","Haoxuan Ma","Weixin Wang"],"pdf_url":"https://arxiv.org/pdf/2503.18991v2.pdf","comment":"The three authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2505.03522v1","updated":"2025-05-06T13:35:59Z","published":"2025-05-06T13:35:59Z","title":"Optimization of Module Transferability in Single Image Super-Resolution:\n  Universality Assessment and Cycle Residual Blocks","summary":"  Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.\n","authors":["Haotong Cheng","Zhiqi Zhang","Hao Li","Xinshang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.03522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03510v1","updated":"2025-05-06T13:20:04Z","published":"2025-05-06T13:20:04Z","title":"From Neurons to Computation: Biological Reservoir Computing for Pattern\n  Recognition","summary":"  In this paper, we introduce a novel paradigm for reservoir computing (RC)\nthat leverages a pool of cultured biological neurons as the reservoir\nsubstrate, creating a biological reservoir computing (BRC). This system\noperates similarly to an echo state network (ESN), with the key distinction\nthat the neural activity is generated by a network of cultured neurons, rather\nthan being modeled by traditional artificial computational units. The neuronal\nactivity is recorded using a multi-electrode array (MEA), which enables\nhigh-throughput recording of neural signals. In our approach, inputs are\nintroduced into the network through a subset of the MEA electrodes, while the\nremaining electrodes capture the resulting neural activity. This generates a\nnonlinear mapping of the input data to a high-dimensional biological feature\nspace, where distinguishing between data becomes more efficient and\nstraightforward, allowing a simple linear classifier to perform pattern\nrecognition tasks effectively. To evaluate the performance of our proposed\nsystem, we present an experimental study that includes various input patterns,\nsuch as positional codes, bars with different orientations, and a digit\nrecognition task. The results demonstrate the feasibility of using biological\nneural networks to perform tasks traditionally handled by artificial neural\nnetworks, paving the way for further exploration of biologically-inspired\ncomputing systems, with potential applications in neuromorphic engineering and\nbio-hybrid computing.\n","authors":["Ludovico Iannello","Luca Ciampi","Gabriele Lagani","Fabrizio Tonelli","Eleonora Crocco","Lucio Maria Calcagnile","Angelo Di Garbo","Federico Cremisi","Giuseppe Amato"],"pdf_url":"https://arxiv.org/pdf/2505.03510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01861v2","updated":"2025-05-06T13:15:29Z","published":"2025-02-24T09:31:56Z","title":"Towards Enterprise-Ready Computer Using Generalist Agent","summary":"  This paper presents our ongoing work toward developing an enterprise-ready\nComputer Using Generalist Agent (CUGA) system. Our research highlights the\nevolutionary nature of building agentic systems suitable for enterprise\nenvironments. By integrating state-of-the-art agentic AI techniques with a\nsystematic approach to iterative evaluation, analysis, and refinement, we have\nachieved rapid and cost-effective performance gains, notably reaching a new\nstate-of-the-art performance on the WebArena benchmark. We detail our\ndevelopment roadmap, the methodology and tools that facilitated rapid learning\nfrom failures and continuous system refinement, and discuss key lessons learned\nand future challenges for enterprise adoption.\n","authors":["Sami Marreed","Alon Oved","Avi Yaeli","Segev Shlomov","Ido Levy","Aviad Sela","Asaf Adi","Nir Mashkif"],"pdf_url":"https://arxiv.org/pdf/2503.01861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13685v2","updated":"2025-05-06T13:11:19Z","published":"2025-02-19T12:53:55Z","title":"MoM: Linear Sequence Modeling with Mixture-of-Memories","summary":"  Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE.\n","authors":["Jusen Du","Weigao Sun","Disen Lan","Jiaxi Hu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.13685v2.pdf","comment":"Technical report, 16 pages"},{"id":"http://arxiv.org/abs/2504.18376v2","updated":"2025-05-06T13:04:22Z","published":"2025-04-25T14:20:57Z","title":"Pushing the boundary on Natural Language Inference","summary":"  Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality.\n","authors":["Pablo Miralles-González","Javier Huertas-Tato","Alejandro Martín","David Camacho"],"pdf_url":"https://arxiv.org/pdf/2504.18376v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03492v1","updated":"2025-05-06T12:48:38Z","published":"2025-05-06T12:48:38Z","title":"Augmenting Human Cognition through Everyday AR","summary":"  As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding.\n","authors":["Xiaoan Liu"],"pdf_url":"https://arxiv.org/pdf/2505.03492v1.pdf","comment":"3 pages, 4 figures. Position paper accepted to CHI'25 Workshop\n  'Everyday AR through AI-in-the-Loop'"},{"id":"http://arxiv.org/abs/2505.03490v1","updated":"2025-05-06T12:47:24Z","published":"2025-05-06T12:47:24Z","title":"A new membership inference attack that spots memorization in generative\n  and predictive models: Loss-Based with Reference Model algorithm (LBRM)","summary":"  Generative models can unintentionally memorize training data, posing\nsignificant privacy risks. This paper addresses the memorization phenomenon in\ntime series imputation models, introducing the Loss-Based with Reference Model\n(LBRM) algorithm. The LBRM method leverages a reference model to enhance the\naccuracy of membership inference attacks, distinguishing between training and\ntest data. Our contributions are twofold: first, we propose an innovative\nmethod to effectively extract and identify memorized training data,\nsignificantly improving detection accuracy. On average, without fine-tuning,\nthe AUROC improved by approximately 40\\%. With fine-tuning, the AUROC increased\nby approximately 60\\%. Second, we validate our approach through membership\ninference attacks on two types of architectures designed for time series\nimputation, demonstrating the robustness and versatility of the LBRM approach\nin different contexts. These results highlight the significant enhancement in\ndetection accuracy provided by the LBRM approach, addressing privacy risks in\ntime series imputation models.\n","authors":["Faiz Taleb","Ivan Gazeau","Maryline Laurent"],"pdf_url":"https://arxiv.org/pdf/2505.03490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13754v3","updated":"2025-05-06T12:45:03Z","published":"2025-04-18T15:39:46Z","title":"Towards Accurate and Interpretable Neuroblastoma Diagnosis via\n  Contrastive Multi-scale Pathological Image Analysis","summary":"  Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole-slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole-slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to bridge patch-level predictions to whole-slide\nimage-level classifications seamlessly. We verified the CMSwinKAN on the\npublicly available BreakHis dataset and the PpNTs dataset, which was\nestablished by our hospital. Results demonstrate that CMSwinKAN performs better\nthan existing state-of-the-art pathology-specific models pre-trained on large\ndatasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN.\n","authors":["Zhu Zhu","Shuo Jiang","Jingyuan Zheng","Yawen Li","Yifei Chen","Manli Zhao","Weizhong Gu","Feiwei Qin","Jinhu Wang","Gang Yu"],"pdf_url":"https://arxiv.org/pdf/2504.13754v3.pdf","comment":"10pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.10932v3","updated":"2025-05-06T12:37:47Z","published":"2024-08-20T15:15:10Z","title":"The Evolution of Reinforcement Learning in Quantitative Finance: A\n  Survey","summary":"  Reinforcement Learning (RL) has experienced significant advancement over the\npast decade, prompting a growing interest in applications within finance. This\nsurvey critically evaluates 167 publications, exploring diverse RL applications\nand frameworks in finance. Financial markets, marked by their complexity,\nmulti-agent nature, information asymmetry, and inherent randomness, serve as an\nintriguing test-bed for RL. Traditional finance offers certain solutions, and\nRL advances these with a more dynamic approach, incorporating machine learning\nmethods, including transfer learning, meta-learning, and multi-agent solutions.\nThis survey dissects key RL components through the lens of Quantitative\nFinance. We uncover emerging themes, propose areas for future research, and\ncritique the strengths and weaknesses of existing methods.\n","authors":["Nikolaos Pippas","Elliot A. Ludvig","Cagatay Turkay"],"pdf_url":"https://arxiv.org/pdf/2408.10932v3.pdf","comment":"This work is accepted by ACM Computing Surveys on 18 April 2025 and\n  an early access version is already available here:\n  https://dl.acm.org/doi/10.1145/3733714. The arXiv copy (and the ACM CSUR\n  early-access version) is an unedited, pre-print version and it is the\n  author's version of the work"},{"id":"http://arxiv.org/abs/2505.03475v1","updated":"2025-05-06T12:28:50Z","published":"2025-05-06T12:28:50Z","title":"am-ELO: A Stable Framework for Arena-based LLM Evaluation","summary":"  Arena-based evaluation is a fundamental yet significant evaluation paradigm\nfor modern AI models, especially large language models (LLMs). Existing\nframework based on ELO rating system suffers from the inevitable instability\nproblem due to ranking inconsistency and the lack of attention to the varying\nabilities of annotators. In this paper, we introduce a novel stable arena\nframework to address these issues by enhancing the ELO Rating System.\nSpecifically, we replace the iterative update method with a Maximum Likelihood\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\nconsistency and stability of the MLE approach for model ranking. Additionally,\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\nincorporate annotator abilities, enabling the simultaneous estimation of model\nscores and annotator reliability. Experiments demonstrate that this method\nensures stability, proving that this framework offers a more robust, accurate,\nand stable evaluation method for LLMs.\n","authors":["Zirui Liu","Jiatong Li","Yan Zhuang","Qi Liu","Shuanghong Shen","Jie Ouyang","Mingyue Cheng","Shijin Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03475v1.pdf","comment":"ICML2025 Accepted"},{"id":"http://arxiv.org/abs/2412.18116v3","updated":"2025-05-06T12:24:43Z","published":"2024-12-24T02:54:56Z","title":"AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation","summary":"  Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand powerful large language models that are difficult to be deployed\nlocally on end-users' devices, raising huge concerns about user privacy and\ncentralized serving cost. Inspired by the remarkable coding abilities of recent\nsmall language models (SLMs), we propose to convert the UI task automation\nproblem to a code generation problem, which can be effectively solved by an\non-device SLM and efficiently executed with an on-device code interpreter.\nUnlike normal coding tasks that can be extensively pre-trained with public\ndatasets, generating UI automation code is challenging due to the diversity,\ncomplexity, and variability of target apps. Therefore, we adopt a\ndocument-centered approach that automatically builds fine-grained API\ndocumentation for each app and generates diverse task samples based on this\ndocumentation. By guiding the agent with the synthetic documents and task\nsamples, it learns to generate precise and efficient scripts to complete unseen\ntasks. Based on detailed comparisons with state-of-the-art mobile UI agents,\nour approach effectively improves the mobile task automation with significantly\nhigher success rates and lower latency/token consumption. Code is open-sourced\nat https://github.com/MobileLLM/AutoDroid-V2.\n","authors":["Hao Wen","Shizuo Tian","Borislav Pavlov","Wenjie Du","Yixuan Li","Ge Chang","Shanhui Zhao","Jiacheng Liu","Yunxin Liu","Ya-Qin Zhang","Yuanchun Li"],"pdf_url":"https://arxiv.org/pdf/2412.18116v3.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.03470v1","updated":"2025-05-06T12:22:45Z","published":"2025-05-06T12:22:45Z","title":"Blending 3D Geometry and Machine Learning for Multi-View Stereopsis","summary":"  Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.\n","authors":["Vibhas Vats","Md. Alimoor Reza","David Crandall","Soon-heung Jung"],"pdf_url":"https://arxiv.org/pdf/2505.03470v1.pdf","comment":"A pre-print -- paper under-review. arXiv admin note: substantial text\n  overlap with arXiv:2310.19583"},{"id":"http://arxiv.org/abs/2408.04430v3","updated":"2025-05-06T12:19:55Z","published":"2024-08-08T12:57:14Z","title":"The Struggles of LLMs in Cross-lingual Code Clone Detection","summary":"  With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection.\n","authors":["Micheline Bénédicte Moumoula","Abdoul Kader Kabore","Jacques Klein","Tegawendé Bissyande"],"pdf_url":"https://arxiv.org/pdf/2408.04430v3.pdf","comment":"Accepted for publication at the ACM International Conference on the\n  Foundations of Software Engineering (FSE) 2025"},{"id":"http://arxiv.org/abs/2405.15444v4","updated":"2025-05-06T11:52:21Z","published":"2024-05-24T11:20:41Z","title":"HINT: Hypernetwork Approach to Training Weight Interval Regions in\n  Continual Learning","summary":"  Recently, a new Continual Learning (CL) paradigm was presented to control\ncatastrophic forgetting, called Interval Continual Learning (InterContiNet),\nwhich relies on enforcing interval constraints on the neural network parameter\nspace. Unfortunately, InterContiNet training is challenging due to the high\ndimensionality of the weight space, making intervals difficult to manage. To\naddress this issue, we introduce HINT, a technique that employs interval\narithmetic within the embedding space and utilizes a hypernetwork to map these\nintervals to the target network parameter space. We train interval embeddings\nfor consecutive tasks and train a hypernetwork to transform these embeddings\ninto weights of the target network. An embedding for a given task is trained\nalong with the hypernetwork, preserving the response of the target network for\nthe previous task embeddings. Interval arithmetic works with a more manageable,\nlower-dimensional embedding space rather than directly preparing intervals in a\nhigh-dimensional weight space. Our model allows faster and more efficient\ntraining. Furthermore, HINT maintains the guarantee of not forgetting. At the\nend of training, we can choose one universal embedding to produce a single\nnetwork dedicated to all tasks. In such a framework, hypernetwork is used only\nfor training and, finally, we can utilize one set of weights. HINT obtains\nsignificantly better results than InterContiNet and gives SOTA results on\nseveral benchmarks.\n","authors":["Patryk Krukowski","Anna Bielawska","Kamil Książek","Paweł Wawrzyński","Paweł Batorski","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2405.15444v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14519v2","updated":"2025-05-06T11:48:04Z","published":"2025-03-14T11:57:08Z","title":"Content ARCs: Decentralized Content Rights in the Age of Generative AI","summary":"  The rise of Generative AI (GenAI) has sparked significant debate over\nbalancing the interests of creative rightsholders and AI developers. As GenAI\nmodels are trained on vast datasets that often include copyrighted material,\nquestions around fair compensation and proper attribution have become\nincreasingly urgent. To address these challenges, this paper proposes a\nframework called Content ARCs (Authenticity, Rights, Compensation). By\ncombining open standards for provenance and dynamic licensing with data\nattribution, and decentralized technologies, Content ARCs create a mechanism\nfor managing rights and compensating creators for using their work in AI\ntraining. We characterize several nascent works in the AI data licensing space\nwithin Content ARCs and identify where challenges remain to fully implement the\nend-to-end framework.\n","authors":["Kar Balan","Andrew Gilbert","John Collomosse"],"pdf_url":"https://arxiv.org/pdf/2503.14519v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03452v1","updated":"2025-05-06T11:47:52Z","published":"2025-05-06T11:47:52Z","title":"An Analysis of Hyper-Parameter Optimization Methods for Retrieval\n  Augmented Generation","summary":"  Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a\ngiven use case can be complex and expensive. Motivated by this challenge,\nframeworks for RAG hyper-parameter optimization (HPO) have recently emerged,\nyet their effectiveness has not been rigorously benchmarked. To address this\ngap, we present a comprehensive study involving 5 HPO algorithms over 5\ndatasets from diverse domains, including a new one collected for this work on\nreal-world product documentation. Our study explores the largest HPO search\nspace considered to date, with two optimized evaluation metrics. Analysis of\nthe results shows that RAG HPO can be done efficiently, either greedily or with\niterative random search, and that it significantly boosts RAG performance for\nall datasets. For greedy HPO approaches, we show that optimizing models first\nis preferable to the prevalent practice of optimizing sequentially according to\nthe RAG pipeline order.\n","authors":["Matan Orbach","Ohad Eytan","Benjamin Sznajder","Ariel Gera","Odellia Boni","Yoav Kantor","Gal Bloch","Omri Levy","Hadas Abraham","Nitzan Barzilay","Eyal Shnarch","Michael E. Factor","Shila Ofek-Koifman","Paula Ta-Shma","Assaf Toledo"],"pdf_url":"https://arxiv.org/pdf/2505.03452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03451v1","updated":"2025-05-06T11:47:13Z","published":"2025-05-06T11:47:13Z","title":"Detecting Quishing Attacks with Machine Learning Techniques Through QR\n  Code Analysis","summary":"  The rise of QR code based phishing (\"Quishing\") poses a growing cybersecurity\nthreat, as attackers increasingly exploit QR codes to bypass traditional\nphishing defenses. Existing detection methods predominantly focus on URL\nanalysis, which requires the extraction of the QR code payload, and may\ninadvertently expose users to malicious content. Moreover, QR codes can encode\nvarious types of data beyond URLs, such as Wi-Fi credentials and payment\ninformation, making URL-based detection insufficient for broader security\nconcerns. To address these gaps, we propose the first framework for quishing\ndetection that directly analyzes QR code structure and pixel patterns without\nextracting the embedded content. We generated a dataset of phishing and benign\nQR codes and we used it to train and evaluate multiple machine learning models,\nincluding Logistic Regression, Decision Trees, Random Forest, Naive Bayes,\nLightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of\n0.9106, demonstrating the feasibility of QR-centric detection. Through feature\nimportance analysis, we identify key visual indicators of malicious intent and\nrefine our feature set by removing non-informative pixels, improving\nperformance to an AUC of 0.9133 with a reduced feature space. Our findings\nreveal that the structural features of QR code correlate strongly with phishing\nrisk. This work establishes a foundation for quishing mitigation and highlights\nthe potential of direct QR analysis as a critical layer in modern phishing\ndefenses.\n","authors":["Fouad Trad","Ali Chehab"],"pdf_url":"https://arxiv.org/pdf/2505.03451v1.pdf","comment":"Accepted in 8th International Conference on Optimization and Learning\n  (OLA2025)"},{"id":"http://arxiv.org/abs/2505.02537v2","updated":"2025-05-06T11:45:55Z","published":"2025-05-05T10:18:48Z","title":"Advancing Constrained Monotonic Neural Networks: Achieving Universal\n  Approximation Beyond Bounded Activations","summary":"  Conventional techniques for imposing monotonicity in MLPs by construction\ninvolve the use of non-negative weight constraints and bounded activation\nfunctions, which pose well-known optimization challenges. In this work, we\ngeneralize previous theoretical results, showing that MLPs with non-negative\nweight constraint and activations that saturate on alternating sides are\nuniversal approximators for monotonic functions. Additionally, we show an\nequivalence between the saturation side in the activations and the sign of the\nweight constraint. This connection allows us to prove that MLPs with convex\nmonotone activations and non-positive constrained weights also qualify as\nuniversal approximators, in contrast to their non-negative constrained\ncounterparts. Our results provide theoretical grounding to the empirical\neffectiveness observed in previous works while leading to possible\narchitectural simplification. Moreover, to further alleviate the optimization\ndifficulties, we propose an alternative formulation that allows the network to\nadjust its activations according to the sign of the weights. This eliminates\nthe requirement for weight reparameterization, easing initialization and\nimproving training stability. Experimental evaluation reinforces the validity\nof the theoretical results, showing that our novel approach compares favourably\nto traditional monotonic architectures.\n","authors":["Davide Sartor","Alberto Sinigaglia","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2505.02537v2.pdf","comment":"International Conference on Machine Learning"},{"id":"http://arxiv.org/abs/2310.05537v4","updated":"2025-05-06T11:43:52Z","published":"2023-10-09T09:01:25Z","title":"ParFam -- (Neural Guided) Symbolic Regression Based on Continuous Global\n  Optimization","summary":"  The problem of symbolic regression (SR) arises in many different\napplications, such as identifying physical laws or deriving mathematical\nequations describing the behavior of financial markets from given data. Various\nmethods exist to address the problem of SR, often based on genetic programming.\nHowever, these methods are usually complicated and involve various\nhyperparameters. In this paper, we present our new approach ParFam that\nutilizes parametric families of suitable symbolic functions to translate the\ndiscrete symbolic regression problem into a continuous one, resulting in a more\nstraightforward setup compared to current state-of-the-art methods. In\ncombination with a global optimizer, this approach results in a highly\neffective method to tackle the problem of SR. We theoretically analyze the\nexpressivity of ParFam and demonstrate its performance with extensive numerical\nexperiments based on the common SR benchmark suit SRBench, showing that we\nachieve state-of-the-art results. Moreover, we present an extension\nincorporating a pre-trained transformer network DL-ParFam to guide ParFam,\naccelerating the optimization process by up to two magnitudes. Our code and\nresults can be found at https://github.com/Philipp238/parfam.\n","authors":["Philipp Scholl","Katharina Bieker","Hillary Hauger","Gitta Kutyniok"],"pdf_url":"https://arxiv.org/pdf/2310.05537v4.pdf","comment":"Code: https://github.com/Philipp238/parfam"},{"id":"http://arxiv.org/abs/2503.13551v3","updated":"2025-05-06T11:38:24Z","published":"2025-03-16T15:18:40Z","title":"Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models","summary":"  Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate step.\nIn addition, the cost of annotating reasoning processes for reward modeling is\nhigh, making large-scale collection of high-quality data challenging. To\naddress this, we propose a novel reward model approach called the Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps at both fine-grained and coarse-grained levels. HRM excels at assessing\nmulti-step reasoning coherence, especially when flawed steps are later\ncorrected through self-reflection. To further reduce the cost of generating\ntraining data, we introduce a lightweight and effective data augmentation\nstrategy called Hierarchical Node Compression (HNC), which merges two\nconsecutive reasoning steps into one within the tree structure. By applying HNC\nto MCTS-generated reasoning trajectories, we enhance the diversity and\nrobustness of HRM training data while introducing controlled noise with minimal\ncomputational overhead. Empirical results on the PRM800K dataset show that HRM,\ntogether with HNC, provides more stable and reliable evaluations than PRM.\nFurthermore, cross-domain evaluations on the MATH500 and GSM8K datasets\ndemonstrate HRM's strong generalization and robustness across a variety of\nreasoning tasks.\n","authors":["Teng Wang","Zhangyi Jiang","Zhenqi He","Shenyang Tong","Wenhan Yang","Yanan Zheng","Zeyu Li","Zifan He","Hailei Gong"],"pdf_url":"https://arxiv.org/pdf/2503.13551v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03443v1","updated":"2025-05-06T11:30:16Z","published":"2025-05-06T11:30:16Z","title":"Elevating Semantic Exploration: A Novel Approach Utilizing Distributed\n  Repositories","summary":"  Centralized and distributed systems are two main approaches to organizing ICT\ninfrastructure, each with its pros and cons. Centralized systems concentrate\nresources in one location, making management easier but creating single points\nof failure. Distributed systems, on the other hand, spread resources across\nmultiple nodes, offering better scalability and fault tolerance, but requiring\nmore complex management. The choice between them depends on factors like\napplication needs, scalability, and data sensitivity. Centralized systems suit\napplications with limited scalability and centralized control, while\ndistributed systems excel in large-scale environments requiring high\navailability and performance. This paper explores a distributed document\nrepository system developed for the Italian Ministry of Justice, using edge\nrepositories to analyze textual data and metadata, enhancing semantic\nexploration capabilities.\n","authors":["Valerio Bellandi"],"pdf_url":"https://arxiv.org/pdf/2505.03443v1.pdf","comment":"This paper has been accepted at the 6th International Conference on\n  Recent Trends and Applications in Computer Science. It will appear in the\n  proceedings"},{"id":"http://arxiv.org/abs/2505.03439v1","updated":"2025-05-06T11:25:52Z","published":"2025-05-06T11:25:52Z","title":"The Steganographic Potentials of Language Models","summary":"  The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.\n","authors":["Artem Karpov","Tinuade Adeleke","Seong Hah Cho","Natalia Perez-Campanero"],"pdf_url":"https://arxiv.org/pdf/2505.03439v1.pdf","comment":"Published at Building Trust Workshop at ICLR 2025"},{"id":"http://arxiv.org/abs/2505.03434v1","updated":"2025-05-06T11:18:34Z","published":"2025-05-06T11:18:34Z","title":"Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in\n  LLM-Based Agents","summary":"  Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving.\n","authors":["Schaun Wheeler","Olivier Jeunen"],"pdf_url":"https://arxiv.org/pdf/2505.03434v1.pdf","comment":"Accepted to the workshop on Hybrid AI for Human-Centric\n  Personalization (HyPer), co-located with ACM UMAP '25"},{"id":"http://arxiv.org/abs/2505.03427v1","updated":"2025-05-06T11:07:26Z","published":"2025-05-06T11:07:26Z","title":"MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks","summary":"  Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.\n","authors":["Mouath Abu Daoud","Chaimae Abouzahir","Leen Kharouf","Walid Al-Eisawi","Nizar Habash","Farah E. Shamout"],"pdf_url":"https://arxiv.org/pdf/2505.03427v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2505.03426v1","updated":"2025-05-06T11:06:41Z","published":"2025-05-06T11:06:41Z","title":"Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI\n  Synthesis: Advancing Pretraining and Clinical Applications","summary":"  Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for\ndiagnosing heart diseases and evaluating cardiac health. However, the limited\navailability of large-scale, high-quality CMR datasets poses a major challenge\nto the effective application of artificial intelligence (AI) in this domain.\nEven the amount of unlabeled data and the health status it covers are difficult\nto meet the needs of model pretraining, which hinders the performance of AI\nmodels on downstream tasks. In this study, we present Cardiac Phenotype-Guided\nCMR Generation (CPGG), a novel approach for generating diverse CMR data that\ncovers a wide spectrum of cardiac health status. The CPGG framework consists of\ntwo stages: in the first stage, a generative model is trained using cardiac\nphenotypes derived from CMR data; in the second stage, a masked autoregressive\ndiffusion model, conditioned on these phenotypes, generates high-fidelity CMR\ncine sequences that capture both structural and functional features of the\nheart in a fine-grained manner. We synthesized a massive amount of CMR to\nexpand the pretraining data. Experimental results show that CPGG generates\nhigh-quality synthetic CMR data, significantly improving performance on various\ndownstream tasks, including diagnosis and cardiac phenotypes prediction. These\ngains are demonstrated across both public and private datasets, highlighting\nthe effectiveness of our approach. Code is availabel at\nhttps://anonymous.4open.science/r/CPGG.\n","authors":["Ziyu Li","Yujian Hu","Zhengyao Ding","Yiheng Mao","Haitao Li","Fan Yi","Hongkun Zhang","Zhengxing Huang"],"pdf_url":"https://arxiv.org/pdf/2505.03426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10148v2","updated":"2025-05-06T11:03:22Z","published":"2025-02-14T13:23:18Z","title":"Cooperative Multi-Agent Planning with Adaptive Skill Synthesis","summary":"  Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS's strong performance against\nstate-of-the-art MARL baselines across both symmetric and asymmetric scenarios.\nNotably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\\% win rate,\nrepresenting a 30 percentage point advantage over QMIX (27\\%). Project page can\nbe found at https://stellar-entremet-1720bb.netlify.app/.\n","authors":["Zhiyuan Li","Wenshuai Zhao","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2502.10148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03424v1","updated":"2025-05-06T11:03:19Z","published":"2025-05-06T11:03:19Z","title":"Framework GNN-AID: Graph Neural Network Analysis Interpretation and\n  Defense","summary":"  The growing need for Trusted AI (TAI) highlights the importance of\ninterpretability and robustness in machine learning models. However, many\nexisting tools overlook graph data and rarely combine these two aspects into a\nsingle solution. Graph Neural Networks (GNNs) have become a popular approach,\nachieving top results across various tasks. We introduce GNN-AID (Graph Neural\nNetwork Analysis, Interpretation, and Defense), an open-source framework\ndesigned for graph data to address this gap. Built as a Python library, GNN-AID\nsupports advanced trust methods and architectural layers, allowing users to\nanalyze graph datasets and GNN behavior using attacks, defenses, and\ninterpretability methods.\n  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,\nand support for any GNNs through customizable interfaces. It also includes a\nweb interface with tools for graph visualization and no-code features like an\ninteractive model builder, simplifying the exploration and analysis of GNNs.\nThe framework also supports MLOps techniques, ensuring reproducibility and\nresult versioning to track and revisit analyses efficiently.\n  GNN-AID is a flexible tool for developers and researchers. It helps\ndevelopers create, analyze, and customize graph models, while also providing\naccess to prebuilt datasets and models for quick experimentation. Researchers\ncan use the framework to explore advanced topics on the relationship between\ninterpretability and robustness, test defense strategies, and combine methods\nto protect against different types of attacks.\n  We also show how defenses against evasion and poisoning attacks can conflict\nwhen applied to graph data, highlighting the complex connections between\ndefense strategies.\n  GNN-AID is available at\n\\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}\n","authors":["Kirill Lukyanov","Mikhail Drobyshevskiy","Georgii Sazonov","Mikhail Soloviov","Ilya Makarov"],"pdf_url":"https://arxiv.org/pdf/2505.03424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03406v1","updated":"2025-05-06T10:31:54Z","published":"2025-05-06T10:31:54Z","title":"Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs\n  and Retrieval-Augmented Generation","summary":"  This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings.\n","authors":["Mohammad Shoaib Ansari","Mohd Sohail Ali Khan","Shubham Revankar","Aditya Varma","Anil S. Mokhade"],"pdf_url":"https://arxiv.org/pdf/2505.03406v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2505.03401v1","updated":"2025-05-06T10:29:23Z","published":"2025-05-06T10:29:23Z","title":"DDaTR: Dynamic Difference-aware Temporal Residual Network for\n  Longitudinal Radiology Report Generation","summary":"  Radiology Report Generation (RRG) automates the creation of radiology reports\nfrom medical imaging, enhancing the efficiency of the reporting process.\nLongitudinal Radiology Report Generation (LRRG) extends RRG by incorporating\nthe ability to compare current and prior exams, facilitating the tracking of\ntemporal changes in clinical findings. Existing LRRG approaches only extract\nfeatures from prior and current images using a visual pre-trained encoder,\nwhich are then concatenated to generate the final report. However, these\nmethods struggle to effectively capture both spatial and temporal correlations\nduring the feature extraction process. Consequently, the extracted features\ninadequately capture the information of difference across exams and thus\nunderrepresent the expected progressions, leading to sub-optimal performance in\nLRRG. To address this, we develop a novel dynamic difference-aware temporal\nresidual network (DDaTR). In DDaTR, we introduce two modules at each stage of\nthe visual encoder to capture multi-level spatial correlations. The Dynamic\nFeature Alignment Module (DFAM) is designed to align prior features across\nmodalities for the integrity of prior clinical information. Prompted by the\nenriched prior features, the dynamic difference-aware module (DDAM) captures\nfavorable difference information by identifying relationships across exams.\nFurthermore, our DDaTR employs the dynamic residual network to unidirectionally\ntransmit longitudinal information, effectively modelling temporal correlations.\nExtensive experiments demonstrated superior performance over existing methods\non three benchmarks, proving its efficacy in both RRG and LRRG tasks.\n","authors":["Shanshan Song","Hui Tang","Honglong Yang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.03401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03392v1","updated":"2025-05-06T10:15:05Z","published":"2025-05-06T10:15:05Z","title":"Automatic Calibration for Membership Inference Attack on Large Language\n  Models","summary":"  Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}.\n","authors":["Saleh Zare Zade","Yao Qiang","Xiangyu Zhou","Hui Zhu","Mohammad Amin Roshani","Prashant Khanduri","Dongxiao Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.03392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03380v1","updated":"2025-05-06T10:00:08Z","published":"2025-05-06T10:00:08Z","title":"Reinforced Correlation Between Vision and Language for Precise Medical\n  AI Assistant","summary":"  Medical AI assistants support doctors in disease diagnosis, medical image\nanalysis, and report generation. However, they still face significant\nchallenges in clinical use, including limited accuracy with multimodal content\nand insufficient validation in real-world settings. We propose RCMed, a\nfull-stack AI assistant that improves multimodal alignment in both input and\noutput, enabling precise anatomical delineation, accurate localization, and\nreliable diagnosis through hierarchical vision-language grounding. A\nself-reinforcing correlation mechanism allows visual features to inform\nlanguage context, while language semantics guide pixel-wise attention, forming\na closed loop that refines both modalities. This correlation is enhanced by a\ncolor region description strategy, translating anatomical structures into\nsemantically rich text to learn shape-location-text relationships across\nscales. Trained on 20 million image-mask-description triplets, RCMed achieves\nstate-of-the-art precision in contextualizing irregular lesions and subtle\nanatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It\nachieved a 23.5% relative improvement in cell segmentation from microscopy\nimages over prior methods. RCMed's strong vision-language alignment enables\nexceptional generalization, with state-of-the-art performance in external\nvalidation across 20 clinically significant cancer types, including novel\ntasks. This work demonstrates how integrated multimodal models capture\nfine-grained patterns, enabling human-level interpretation in complex scenarios\nand advancing human-centric AI healthcare.\n","authors":["Haonan Wang","Jiaji Mao","Lehan Wang","Qixiang Zhang","Marawan Elbatel","Yi Qin","Huijun Hu","Baoxun Li","Wenhui Deng","Weifeng Qin","Hongrui Li","Jialin Liang","Jun Shen","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.03380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07328v3","updated":"2025-05-06T09:48:44Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Representational Bias and Cross-Cultural Adaptability of\n  Music Generation Models","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v3.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2505.03373v1","updated":"2025-05-06T09:47:53Z","published":"2025-05-06T09:47:53Z","title":"SPAP: Structured Pruning via Alternating Optimization and Penalty\n  Methods","summary":"  The deployment of large language models (LLMs) is often constrained by their\nsubstantial computational and memory demands. While structured pruning presents\na viable approach by eliminating entire network components, existing methods\nsuffer from performance degradation, reliance on heuristic metrics, or\nexpensive finetuning. To address these challenges, we propose SPAP (Structured\nPruning via Alternating Optimization and Penalty Methods), a novel and\nefficient structured pruning framework for LLMs grounded in optimization\ntheory. SPAP formulates the pruning problem through a mixed-integer\noptimization model, employs a penalty method that effectively makes pruning\ndecisions to minimize pruning errors, and introduces an alternating\nminimization algorithm tailored to the splittable problem structure for\nefficient weight updates and performance recovery. Extensive experiments on\nOPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over\nstate-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at\n30% sparsity) and proportional memory reductions. Our work offers a practical,\noptimization-driven solution for pruning LLMs while preserving model\nperformance.\n","authors":["Hanyu Hu","Xiaoming Yuan"],"pdf_url":"https://arxiv.org/pdf/2505.03373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00047v4","updated":"2025-05-06T09:45:34Z","published":"2025-01-28T09:16:28Z","title":"HadamRNN: Binary and Sparse Ternary Orthogonal RNNs","summary":"  Binary and sparse ternary weights in neural networks enable faster\ncomputations and lighter representations, facilitating their use on edge\ndevices with limited computational power. Meanwhile, vanilla RNNs are highly\nsensitive to changes in their recurrent weights, making the binarization and\nternarization of these weights inherently challenging. To date, no method has\nsuccessfully achieved binarization or ternarization of vanilla RNN weights. We\npresent a new approach leveraging the properties of Hadamard matrices to\nparameterize a subset of binary and sparse ternary orthogonal matrices. This\nmethod enables the training of orthogonal RNNs (ORNNs) with binary and sparse\nternary recurrent weights, effectively creating a specific class of binary and\nsparse ternary vanilla RNNs. The resulting ORNNs, called HadamRNN and\nBlock-HadamRNN, are evaluated on benchmarks such as the copy task, permuted and\nsequential MNIST tasks, the IMDB dataset, two GLUE benchmarks, and two IoT\nbenchmarks. Despite binarization or sparse ternarization, these RNNs maintain\nperformance levels comparable to state-of-the-art full-precision models,\nhighlighting the effectiveness of our approach. Notably, our approach is the\nfirst solution with binary recurrent weights capable of tackling the copy task\nover 1000 timesteps.\n","authors":["Armand Foucault","Franck Mamalet","François Malgouyres"],"pdf_url":"https://arxiv.org/pdf/2502.00047v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03369v1","updated":"2025-05-06T09:40:47Z","published":"2025-05-06T09:40:47Z","title":"Validating the Effectiveness of a Large Language Model-based Approach\n  for Identifying Children's Development across Various Free Play Settings in\n  Kindergarten","summary":"  Free play is a fundamental aspect of early childhood education, supporting\nchildren's cognitive, social, emotional, and motor development. However,\nassessing children's development during free play poses significant challenges\ndue to the unstructured and spontaneous nature of the activity. Traditional\nassessment methods often rely on direct observations by teachers, parents, or\nresearchers, which may fail to capture comprehensive insights from free play\nand provide timely feedback to educators. This study proposes an innovative\napproach combining Large Language Models (LLMs) with learning analytics to\nanalyze children's self-narratives of their play experiences. The LLM\nidentifies developmental abilities, while performance scores across different\nplay settings are calculated using learning analytics techniques. We collected\n2,224 play narratives from 29 children in a kindergarten, covering four\ndistinct play areas over one semester. According to the evaluation results from\neight professionals, the LLM-based approach achieved high accuracy in\nidentifying cognitive, motor, and social abilities, with accuracy exceeding 90%\nin most domains. Moreover, significant differences in developmental outcomes\nwere observed across play settings, highlighting each area's unique\ncontributions to specific abilities. These findings confirm that the proposed\napproach is effective in identifying children's development across various free\nplay settings. This study demonstrates the potential of integrating LLMs and\nlearning analytics to provide child-centered insights into developmental\ntrajectories, offering educators valuable data to support personalized learning\nand enhance early childhood education practices.\n","authors":["Yuanyuan Yang","Yuan Shen","Tianchen Sun","Yangbin Xie"],"pdf_url":"https://arxiv.org/pdf/2505.03369v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.03359v1","updated":"2025-05-06T09:29:14Z","published":"2025-05-06T09:29:14Z","title":"Domain Adversarial Training for Mitigating Gender Bias in Speech-based\n  Mental Health Detection","summary":"  Speech-based AI models are emerging as powerful tools for detecting\ndepression and the presence of Post-traumatic stress disorder (PTSD), offering\na non-invasive and cost-effective way to assess mental health. However, these\nmodels often struggle with gender bias, which can lead to unfair and inaccurate\npredictions. In this study, our study addresses this issue by introducing a\ndomain adversarial training approach that explicitly considers gender\ndifferences in speech-based depression and PTSD detection. Specifically, we\ntreat different genders as distinct domains and integrate this information into\na pretrained speech foundation model. We then validate its effectiveness on the\nE-DAIC dataset to assess its impact on performance. Experimental results show\nthat our method notably improves detection performance, increasing the F1-score\nby up to 13.29 percentage points compared to the baseline. This highlights the\nimportance of addressing demographic disparities in AI-driven mental health\nassessment.\n","authors":["June-Woo Kim","Haram Yoon","Wonkyo Oh","Dawoon Jung","Sung-Hoon Yoon","Dae-Jin Kim","Dong-Ho Lee","Sang-Yeol Lee","Chan-Mo Yang"],"pdf_url":"https://arxiv.org/pdf/2505.03359v1.pdf","comment":"Accepted to EMBC 2025"},{"id":"http://arxiv.org/abs/2505.02156v2","updated":"2025-05-06T09:27:31Z","published":"2025-05-04T15:39:58Z","title":"Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents","summary":"  Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode\n$\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four\nthinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on\nreal-time context. Our framework's core innovation, the $\\textbf{A}$daptive\n$\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach.\n","authors":["Minzheng Wang","Yongbin Li","Haobo Wang","Xinghua Zhang","Nan Xu","Bingli Wu","Fei Huang","Haiyang Yu","Wenji Mao"],"pdf_url":"https://arxiv.org/pdf/2505.02156v2.pdf","comment":"Work in Progress. The code and data are available, see\n  https://github.com/MozerWang/AMPO"},{"id":"http://arxiv.org/abs/2505.00693v2","updated":"2025-05-06T09:24:22Z","published":"2025-05-01T17:55:05Z","title":"Robotic Visual Instruction","summary":"  Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision introduces\nchallenges for robotic task definition such as ambiguity and verbosity.\nMoreover, in some public settings where quiet is required, such as libraries or\nhospitals, verbal communication with robots is inappropriate. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment,enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Project\nwebsite: https://robotic-visual-instruction.github.io/\n","authors":["Yanbang Li","Ziyang Gong","Haoyang Li","Xiaoqi Huang","Haolan Kang","Guangping Bai","Xianzheng Ma"],"pdf_url":"https://arxiv.org/pdf/2505.00693v2.pdf","comment":"Project website: https://robotic-visual-instruction.github.io/"},{"id":"http://arxiv.org/abs/2505.03338v1","updated":"2025-05-06T09:10:12Z","published":"2025-05-06T09:10:12Z","title":"Safer Prompts: Reducing IP Risk in Visual Generative AI","summary":"  Visual Generative AI models have demonstrated remarkable capability in\ngenerating high-quality images from simple inputs like text prompts. However,\nbecause these models are trained on images from diverse sources, they risk\nmemorizing and reproducing specific content, raising concerns about\nintellectual property (IP) infringement. Recent advances in prompt engineering\noffer a cost-effective way to enhance generative AI performance. In this paper,\nwe evaluate the effectiveness of prompt engineering techniques in mitigating IP\ninfringement risks in image generation. Our findings show that Chain of Thought\nPrompting and Task Instruction Prompting significantly reduce the similarity\nbetween generated images and the training data of diffusion models, thereby\nlowering the risk of IP infringement.\n","authors":["Lena Reissinger","Yuanyuan Li","Anna-Carolina Haensch","Neeraj Sarna"],"pdf_url":"https://arxiv.org/pdf/2505.03338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03336v1","updated":"2025-05-06T09:08:36Z","published":"2025-05-06T09:08:36Z","title":"Avoid Recommending Out-of-Domain Items: Constrained Generative\n  Recommendation with LLMs","summary":"  Large Language Models (LLMs) have shown promise for generative recommender\nsystems due to their transformative capabilities in user interaction. However,\nensuring they do not recommend out-of-domain (OOD) items remains a challenge.\nWe study two distinct methods to address this issue: RecLM-ret, a\nretrieval-based method, and RecLM-cgen, a constrained generation method. Both\nmethods integrate seamlessly with existing LLMs to ensure in-domain\nrecommendations. Comprehensive experiments on three recommendation datasets\ndemonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing\nLLM-based recommender models in accuracy while eliminating OOD recommendations,\nmaking it the preferred method for adoption. Additionally, RecLM-cgen maintains\nstrong generalist capabilities and is a lightweight plug-and-play module for\neasy integration into LLMs, offering valuable practical benefits for the\ncommunity. Source code is available at https://github.com/microsoft/RecAI\n","authors":["Hao Liao","Wensheng Lu","Jianxun Lian","Mingqi Wu","Shuo Wang","Yong Zhang","Yitian Huang","Mingyang Zhou","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2505.03336v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2505.03335v1","updated":"2025-05-06T09:08:00Z","published":"2025-05-06T09:08:00Z","title":"Absolute Zero: Reinforced Self-play Reasoning with Zero Data","summary":"  Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.\n","authors":["Andrew Zhao","Yiran Wu","Yang Yue","Tong Wu","Quentin Xu","Yang Yue","Matthieu Lin","Shenzhi Wang","Qingyun Wu","Zilong Zheng","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2505.03335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03332v1","updated":"2025-05-06T09:06:18Z","published":"2025-05-06T09:06:18Z","title":"AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning","summary":"  Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks.\n","authors":["Evgeny Markhasin"],"pdf_url":"https://arxiv.org/pdf/2505.03332v1.pdf","comment":"22 pages, 36 pages (references and appendixes)"},{"id":"http://arxiv.org/abs/2410.02768v2","updated":"2025-05-06T09:02:57Z","published":"2024-09-17T05:17:37Z","title":"Uncertainty-Guided Self-Questioning and Answering for Video-Language\n  Alignment","summary":"  The development of multi-modal models has been rapidly advancing, with some\ndemonstrating remarkable capabilities. However, annotating video-text pairs\nremains expensive and insufficient. Take video question answering (VideoQA)\ntasks as an example, human annotated questions and answers often cover only\npart of the video, since the corresponding text is often short and monotonous,\nleading to underutilization of video. To address this, we propose a\nBootstrapping Video-Language Alignment framework (BoViLA), a self-training\nmethod that augments question samples during training process through LLM-based\nself-questioning and answering, which help model exploit video information and\nthe internal knowledge of LLMs more thoroughly to improve modality alignment.\nHowever, low-quality self-generated questions may instead contaminate the\nperformance, especially in the early stages of training, as we have observed in\nour experiments. To filter bad self-generated questions, we introduce\nEvidential Deep Learning (EDL) to estimate uncertainty and assess the quality\nof self-generated questions by evaluating the modality alignment within the\ncontext. To the best of our knowledge, this work is the first to explore\nLLM-based self-training frameworks for modality alignment. We evaluate BoViLA\non five strong VideoQA benchmarks, where it outperforms several\nstate-of-the-art methods and demonstrate its effectiveness and generality.\nAdditionally, we provide extensive analyses of the self-training framework and\nthe EDL-based uncertainty filtering mechanism. The code will be made available.\n","authors":["Jin Chen","Kaijing Ma","Haojian Huang","Han Fang","Hao Sun","Mehdi Hosseinzadeh","Zhe Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03327v1","updated":"2025-05-06T08:54:28Z","published":"2025-05-06T08:54:28Z","title":"Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and\n  Self-Supervised Learning","summary":"  Deep learning models have shown encouraging capabilities for mapping\naccurately forests at medium resolution with TanDEM-X interferometric SAR data.\nSuch models, as most of current state-of-the-art deep learning techniques in\nremote sensing, are trained in a fully-supervised way, which requires a large\namount of labeled data for training and validation. In this work, our aim is to\nexploit the high-resolution capabilities of the TanDEM-X mission to map forests\nat 6 m. The goal is to overcome the intrinsic limitations posed by\nmidresolution products, which affect, e.g., the detection of narrow roads\nwithin vegetated areas and the precise delineation of forested regions\ncontours. To cope with the lack of extended reliable reference datasets at such\na high resolution, we investigate self-supervised learning techniques for\nextracting highly informative representations from the input features, followed\nby a supervised training step with a significantly smaller number of reliable\nlabels. A 1 m resolution forest/non-forest reference map over Pennsylvania,\nUSA, allows for comparing different training approaches for the development of\nan effective forest mapping framework with limited labeled samples. We select\nthe best-performing approach over this test region and apply it in a real-case\nforest mapping scenario over the Amazon rainforest, where only very few labeled\ndata at high resolution are available. In this challenging scenario, the\nproposed self-supervised framework significantly enhances the classification\naccuracy with respect to fully-supervised methods, trained using the same\namount of labeled data, representing an extremely promising starting point for\nlarge-scale, very high-resolution forest mapping with TanDEM-X data.\n","authors":["José-Luis Bueso-Bello","Benjamin Chauvel","Daniel Carcereri","Philipp Posovszky","Pietro Milillo","Jennifer Ruiz","Juan-Carlos Fernández-Diaz","Carolina González","Michele Martone","Ronny Hänsch","Paola Rizzoli"],"pdf_url":"https://arxiv.org/pdf/2505.03327v1.pdf","comment":"Preprint submitted to Remote Sensing of Environment"},{"id":"http://arxiv.org/abs/2505.03319v1","updated":"2025-05-06T08:47:14Z","published":"2025-05-06T08:47:14Z","title":"SD-VSum: A Method and Dataset for Script-Driven Video Summarization","summary":"  In this work, we introduce the task of script-driven video summarization,\nwhich aims to produce a summary of the full-length video by selecting the parts\nthat are most relevant to a user-provided script outlining the visual content\nof the desired summary. Following, we extend a recently-introduced large-scale\ndataset for generic video summarization (VideoXum) by producing natural\nlanguage descriptions of the different human-annotated summaries that are\navailable per video. In this way we make it compatible with the introduced\ntask, since the available triplets of ``video, summary and summary\ndescription'' can be used for training a method that is able to produce\ndifferent summaries for a given video, driven by the provided script about the\ncontent of each summary. Finally, we develop a new network architecture for\nscript-driven video summarization (SD-VSum), that relies on the use of a\ncross-modal attention mechanism for aligning and fusing information from the\nvisual and text modalities. Our experimental evaluations demonstrate the\nadvanced performance of SD-VSum against state-of-the-art approaches for\nquery-driven and generic (unimodal and multimodal) summarization from the\nliterature, and document its capacity to produce video summaries that are\nadapted to each user's needs about their content.\n","authors":["Manolis Mylonas","Evlampios Apostolidis","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2505.03319v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2505.03315v1","updated":"2025-05-06T08:45:44Z","published":"2025-05-06T08:45:44Z","title":"Artificial Behavior Intelligence: Technology, Challenges, and Future\n  Directions","summary":"  Understanding and predicting human behavior has emerged as a core capability\nin various AI application domains such as autonomous driving, smart healthcare,\nsurveillance systems, and social robotics. This paper defines the technical\nframework of Artificial Behavior Intelligence (ABI), which comprehensively\nanalyzes and interprets human posture, facial expressions, emotions, behavioral\nsequences, and contextual cues. It details the essential components of ABI,\nincluding pose estimation, face and emotion recognition, sequential behavior\nanalysis, and context-aware modeling. Furthermore, we highlight the\ntransformative potential of recent advances in large-scale pretrained models,\nsuch as large language models (LLMs), vision foundation models, and multimodal\nintegration models, in significantly improving the accuracy and\ninterpretability of behavior recognition. Our research team has a strong\ninterest in the ABI domain and is actively conducting research, particularly\nfocusing on the development of intelligent lightweight models capable of\nefficiently inferring complex human behaviors. This paper identifies several\ntechnical challenges that must be addressed to deploy ABI in real-world\napplications including learning behavioral intelligence from limited data,\nquantifying uncertainty in complex behavior prediction, and optimizing model\nstructures for low-power, real-time inference. To tackle these challenges, our\nteam is exploring various optimization strategies including lightweight\ntransformers, graph-based recognition architectures, energy-aware loss\nfunctions, and multimodal knowledge distillation, while validating their\napplicability in real-time environments.\n","authors":["Kanghyun Jo","Jehwan Choi","Kwanho Kim","Seongmin Kim","Duy-Linh Nguyen","Xuan-Thuy Vo","Adri Priadana","Tien-Dat Tran"],"pdf_url":"https://arxiv.org/pdf/2505.03315v1.pdf","comment":"9 pages, 6 figures, Pre-print for IWIS2025"},{"id":"http://arxiv.org/abs/2505.03314v1","updated":"2025-05-06T08:44:52Z","published":"2025-05-06T08:44:52Z","title":"Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic\n  Music Generation","summary":"  The recent surge in the popularity of diffusion models for image synthesis\nhas attracted new attention to their potential for generation tasks in other\ndomains. However, their applications to symbolic music generation remain\nlargely under-explored because symbolic music is typically represented as\nsequences of discrete events and standard diffusion models are not well-suited\nfor discrete data. We represent symbolic music as image-like pianorolls,\nfacilitating the use of diffusion models for the generation of symbolic music.\nMoreover, this study introduces a novel diffusion model that incorporates our\nproposed Transformer-Mamba block and learnable wavelet transform.\nClassifier-free guidance is utilised to generate symbolic music with target\nchords. Our evaluation shows that our method achieves compelling results in\nterms of music quality and controllability, outperforming the strong baseline\nin pianoroll generation. Our code is available at\nhttps://github.com/jinchengzhanggg/proffusion.\n","authors":["Jincheng Zhang","György Fazekas","Charalampos Saitis"],"pdf_url":"https://arxiv.org/pdf/2505.03314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08656v2","updated":"2025-05-06T08:36:57Z","published":"2024-10-11T09:28:09Z","title":"radarODE-MTL: A Multi-Task Learning Framework with Eccentric Gradient\n  Alignment for Robust Radar-Based ECG Reconstruction","summary":"  Millimeter-wave radar is promising to provide robust and accurate vital sign\nmonitoring in an unobtrusive manner. However, the radar signal might be\ndistorted in propagation by ambient noise or random body movement, ruining the\nsubtle cardiac activities and destroying the vital sign recovery. In\nparticular, the recovery of electrocardiogram (ECG) signal heavily relies on\nthe deep-learning model and is sensitive to noise. Therefore, this work\ncreatively deconstructs the radar-based ECG recovery into three individual\ntasks and proposes a multi-task learning (MTL) framework, radarODE-MTL, to\nincrease the robustness against consistent and abrupt noises. In addition, to\nalleviate the potential conflicts in optimizing individual tasks, a novel\nmulti-task optimization strategy, eccentric gradient alignment (EGA), is\nproposed to dynamically trim the task-specific gradients based on task\ndifficulties in orthogonal space. The proposed radarODE-MTL with EGA is\nevaluated on the public dataset with prominent improvements in accuracy, and\nthe performance remains consistent under noises. The experimental results\nindicate that radarODE-MTL could reconstruct accurate ECG signals robustly from\nradar signals and imply the application prospect in real-life situations. The\ncode is available at: http://github.com/ZYY0844/radarODE-MTL.\n","authors":["Yuanyuan Zhang","Rui Yang","Yutao Yue","Eng Gee Lim"],"pdf_url":"https://arxiv.org/pdf/2410.08656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03303v1","updated":"2025-05-06T08:36:01Z","published":"2025-05-06T08:36:01Z","title":"Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices","summary":"  This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.\n","authors":["Tasnim Shahriar"],"pdf_url":"https://arxiv.org/pdf/2505.03303v1.pdf","comment":"22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis"},{"id":"http://arxiv.org/abs/2505.02659v2","updated":"2025-05-06T08:34:46Z","published":"2025-05-05T14:05:15Z","title":"A Note on Statistically Accurate Tabular Data Generation Using Large\n  Language Models","summary":"  Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data.\n","authors":["Andrey Sidorenko"],"pdf_url":"https://arxiv.org/pdf/2505.02659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07986v2","updated":"2025-05-06T08:30:46Z","published":"2025-04-07T02:42:07Z","title":"SEAL: Steerable Reasoning Calibration of Large Language Models for Free","summary":"  Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated\ncompelling capabilities for complex reasoning tasks via the extended\nchain-of-thought (CoT) reasoning mechanism. However, recent studies reveal\nsubstantial redundancy in the CoT reasoning traces, which not only increases\ninference latency but also negatively impacts model performance by diverting\nattention to unnecessary reasoning paths. To address this issue, we investigate\nthe internal reasoning structures of LLMs and categorize them into three\nprimary thought types: execution, reflection, and transition thoughts.\nMoreover, our analysis reveals that excessive reflection and transition\nthoughts are strongly correlated with failure cases and these thought\ncategories exhibit clear separation in the latent space. Based on these, we\nintroduce SEAL (Steerable reasoning calibration), a training-free approach that\nseamlessly calibrates the CoT process, improving accuracy while demonstrating\nsignificant efficiency gains. SEAL consists of an offline stage for extracting\nthe reasoning steering vector in the latent space, followed by an on-the-fly\ncalibration of the reasoning trace through representation intervention using\nthe steering vector. Notably, the steering vector exhibits strong\ntransferability across various tasks. Extensive experiments across multiple\nmodels (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,\nGSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%\nimprovement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our\ncode is publicly available at https://github.com/VITA-Group/SEAL.\n","authors":["Runjin Chen","Zhenyu Zhang","Junyuan Hong","Souvik Kundu","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2504.07986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01672v2","updated":"2025-05-06T08:29:33Z","published":"2024-08-03T06:07:15Z","title":"radarODE: An ODE-Embedded Deep Learning Model for Contactless ECG\n  Reconstruction from Millimeter-Wave Radar","summary":"  Radar-based contactless cardiac monitoring has become a popular research\ndirection recently, but the fine-grained electrocardiogram (ECG) signal is\nstill hard to reconstruct from millimeter-wave radar signal. The key obstacle\nis to decouple the cardiac activities in the electrical domain (i.e., ECG) from\nthat in the mechanical domain (i.e., heartbeat), and most existing research\nonly uses pure data-driven methods to map such domain transformation as a black\nbox. Therefore, this work first proposes a signal model for domain\ntransformation, and then a novel deep learning framework called radarODE is\ndesigned to fuse the temporal and morphological features extracted from radar\nsignals and generate ECG. In addition, ordinary differential equations are\nembedded in radarODE as a decoder to provide morphological prior, helping the\nconvergence of the model training and improving the robustness under body\nmovements. After being validated on the dataset, the proposed radarODE achieves\nbetter performance compared with the benchmark in terms of missed detection\nrate, root mean square error, Pearson correlation coefficient with the\nimprovement of 9%, 16% and 19%, respectively. The validation results imply that\nradarODE is capable of recovering ECG signals from radar signals with high\nfidelity and can be potentially implemented in real-life scenarios.\n","authors":["Yuanyuan Zhang","Runwei Guan","Lingxiao Li","Rui Yang","Yutao Yue","Eng Gee Lim"],"pdf_url":"https://arxiv.org/pdf/2408.01672v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03299v1","updated":"2025-05-06T08:29:18Z","published":"2025-05-06T08:29:18Z","title":"Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A\n  Capabilities Encoding Approach","summary":"  Foundation models constitute a significant advancement in computer vision:\nafter a single, albeit costly, training phase, they can address a wide array of\ntasks. In the field of Earth observation, over 75 remote sensing vision\nfoundation models have been developed in the past four years. However, none has\nconsistently outperformed the others across all available downstream tasks. To\nfacilitate their comparison, we propose a cost-effective method for predicting\na model's performance on multiple downstream tasks without the need for\nfine-tuning on each one. This method is based on what we call \"capabilities\nencoding.\" The utility of this novel approach is twofold: we demonstrate its\npotential to simplify the selection of a foundation model for a given new task,\nand we employ it to offer a fresh perspective on the existing literature,\nsuggesting avenues for future research. Codes are available at\nhttps://github.com/pierreadorni/capabilities-encoding.\n","authors":["Pierre Adorni","Minh-Tan Pham","Stéphane May","Sébastien Lefèvre"],"pdf_url":"https://arxiv.org/pdf/2505.03299v1.pdf","comment":"Accepted at the MORSE workshop of CVPR 2025"},{"id":"http://arxiv.org/abs/2505.03296v1","updated":"2025-05-06T08:27:23Z","published":"2025-05-06T08:27:23Z","title":"The Unreasonable Effectiveness of Discrete-Time Gaussian Process\n  Mixtures for Robot Policy Learning","summary":"  We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel\napproach for flexible policy representation and imitation learning in robot\nmanipulation. MiDiGap enables learning from as few as five demonstrations using\nonly camera observations and generalizes across a wide range of challenging\ntasks. It excels at long-horizon behaviors such as making coffee, highly\nconstrained motions such as opening doors, dynamic actions such as scooping\nwith a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns\nthese tasks on a CPU in less than a minute and scales linearly to large\ndatasets. We also develop a rich suite of tools for inference-time steering\nusing evidence such as collision signals and robot kinematic constraints. This\nsteering enables novel generalization capabilities, including obstacle\navoidance and cross-embodiment policy transfer. MiDiGap achieves\nstate-of-the-art performance on diverse few-shot manipulation benchmarks. On\nconstrained RLBench tasks, it improves policy success by 76 percentage points\nand reduces trajectory cost by 67%. On multimodal tasks, it improves policy\nsuccess by 48 percentage points and increases sample efficiency by a factor of\n20. In cross-embodiment transfer, it more than doubles policy success. We make\nthe code publicly available at https://midigap.cs.uni-freiburg.de.\n","authors":["Jan Ole von Hartz","Adrian Röfer","Joschka Boedecker","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2505.03296v1.pdf","comment":"Submitted for publication to IEEE Transaction on Robotics"},{"id":"http://arxiv.org/abs/2505.03295v1","updated":"2025-05-06T08:27:04Z","published":"2025-05-06T08:27:04Z","title":"Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for\n  Reusing Existing Libraries and Interfaces","summary":"  Modern automation systems increasingly rely on modular architectures, with\ncapabilities and skills as one solution approach. Capabilities define the\nfunctions of resources in a machine-readable form and skills provide the\nconcrete implementations that realize those capabilities. However, the\ndevelopment of a skill implementation conforming to a corresponding capability\nremains a time-consuming and challenging task. In this paper, we present a\nmethod that treats capabilities as contracts for skill implementations and\nleverages large language models to generate executable code based on natural\nlanguage user input. A key feature of our approach is the integration of\nexisting software libraries and interface technologies, enabling the generation\nof skill implementations across different target languages. We introduce a\nframework that allows users to incorporate their own libraries and resource\ninterfaces into the code generation process through a retrieval-augmented\ngeneration architecture. The proposed method is evaluated using an autonomous\nmobile robot controlled via Python and ROS 2, demonstrating the feasibility and\nflexibility of the approach.\n","authors":["Luis Miguel Vieira da Silva","Aljosha Köcher","Nicolas König","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2505.03295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01169v2","updated":"2025-05-06T08:22:46Z","published":"2025-05-02T10:17:49Z","title":"Distilling Two-Timed Flow Models by Separately Matching Initial and\n  Terminal Velocities","summary":"  A flow matching model learns a time-dependent vector field $v_t(x)$ that\ngenerates a probability path $\\{ p_t \\}_{0 \\leq t \\leq 1}$ that interpolates\nbetween a well-known noise distribution ($p_0$) and the data distribution\n($p_1$). It can be distilled into a two-timed flow model (TTFM) $\\phi_{s,x}(t)$\nthat can transform a sample belonging to the distribution at an initial time\n$s$ to another belonging to the distribution at a terminal time $t$ in one\nfunction evaluation. We present a new loss function for TTFM distillation\ncalled the \\emph{initial/terminal velocity matching} (ITVM) loss that extends\nthe Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi et al. by\nadding redundant terms to match the initial velocities at time $s$, removing\nthe derivative from the terminal velocity term at time $t$, and using a version\nof the model under training, stabilized by exponential moving averaging (EMA),\nto compute the target terminal average velocity. Preliminary experiments show\nthat our loss leads to better few-step generation performance on multiple types\nof datasets and model architectures over baselines.\n","authors":["Pramook Khungurn","Pratch Piyawongwisal","Sira Sriswasdi","Supasorn Suwajanakorn"],"pdf_url":"https://arxiv.org/pdf/2505.01169v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15106v2","updated":"2025-05-06T08:18:58Z","published":"2024-11-22T18:09:27Z","title":"About Time: Advances, Challenges, and Outlooks of Action Understanding","summary":"  We have witnessed impressive advances in video action understanding.\nIncreased dataset sizes, variability, and computation availability have enabled\nleaps in performance and task diversification. Current systems can provide\ncoarse- and fine-grained descriptions of video scenes, extract segments\ncorresponding to queries, synthesize unobserved parts of videos, and predict\ncontext across multiple modalities. This survey comprehensively reviews\nadvances in uni- and multi-modal action understanding across a range of tasks.\nWe focus on prevalent challenges, overview widely adopted datasets, and survey\nseminal works with an emphasis on recent advances. We broadly distinguish\nbetween three temporal scopes: (1) recognition tasks of actions observed in\nfull, (2) prediction tasks for ongoing partially observed actions, and (3)\nforecasting tasks for subsequent unobserved action(s). This division allows us\nto identify specific action modeling and video representation challenges.\nFinally, we outline future directions to address current shortcomings.\n","authors":["Alexandros Stergiou","Ronald Poppe"],"pdf_url":"https://arxiv.org/pdf/2411.15106v2.pdf","comment":"Accepted at the International Journal of Computer Vision (IJCV)"},{"id":"http://arxiv.org/abs/2505.03281v1","updated":"2025-05-06T08:07:15Z","published":"2025-05-06T08:07:15Z","title":"Physics-inspired Energy Transition Neural Network for Sequence Learning","summary":"  Recently, the superior performance of Transformers has made them a more\nrobust and scalable solution for sequence modeling than traditional recurrent\nneural networks (RNNs). However, the effectiveness of Transformer in capturing\nlong-term dependencies is primarily attributed to their comprehensive\npair-modeling process rather than inherent inductive biases toward sequence\nsemantics. In this study, we explore the capabilities of pure RNNs and reassess\ntheir long-term learning mechanisms. Inspired by the physics energy transition\nmodels that track energy changes over time, we propose a effective recurrent\nstructure called the``Physics-inspired Energy Transition Neural Network\"\n(PETNN). We demonstrate that PETNN's memory mechanism effectively stores\ninformation over long-term dependencies. Experimental results indicate that\nPETNN outperforms transformer-based methods across various sequence tasks.\nFurthermore, owing to its recurrent nature, PETNN exhibits significantly lower\ncomplexity. Our study presents an optimal foundational recurrent architecture\nand highlights the potential for developing effective recurrent neural networks\nin fields currently dominated by Transformer.\n","authors":["Zhou Wu","Junyi An","Baile Xu","Furao Shen","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.03281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03275v1","updated":"2025-05-06T08:05:35Z","published":"2025-05-06T08:05:35Z","title":"RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via\n  Retrieval-Augmented Generation","summary":"  Large language models (LLMs) struggle to effectively utilize a growing number\nof external tools, such as those defined by the Model Context Protocol\n(MCP)\\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We\nintroduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes\nthis challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to\nidentify the most relevant MCP(s) for a given query from an external index\nbefore engaging the LLM. Only the selected tool descriptions are passed to the\nmodel, drastically reducing prompt size and simplifying decision-making.\nExperiments, including an MCP stress test, demonstrate RAG-MCP significantly\ncuts prompt tokens (e.g., by over 50%) and more than triples tool selection\naccuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables\nscalable and accurate tool integration for LLMs.\n","authors":["Tiantian Gan","Qiyao Sun"],"pdf_url":"https://arxiv.org/pdf/2505.03275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16876v3","updated":"2025-05-06T07:58:27Z","published":"2024-09-25T12:42:25Z","title":"Automating Traffic Model Enhancement with AI Research Agent","summary":"  Developing efficient traffic models is crucial for optimizing modern\ntransportation systems. However, current modeling approaches remain\nlabor-intensive and prone to human errors due to their dependence on manual\nworkflows. These processes typically involve extensive literature reviews,\nformula tuning, and iterative testing, which often lead to inefficiencies. To\naddress this, we propose TR-Agent, an AI-powered framework that autonomously\ndevelops and refines traffic models through a closed-loop, iterative process.\nWe structure the research pipeline into four key stages: idea generation,\ntheory formulation, theory evaluation, and iterative optimization, and\nimplement TR-Agent with four corresponding modules. These modules collaborate\nto retrieve knowledge from external sources, generate novel hypotheses,\nimplement and debug models, and evaluate their performance on evaluation\ndatasets. Through iteratively feedback and refinement, TR-Agent improves both\nmodeling efficiency and effectiveness. We validate the framework on three\nrepresentative traffic models: the Intelligent Driver Model (IDM) for\ncar-following behavior, the MOBIL model for lane-changing, and the\nLighthill-Whitham-Richards (LWR) speed-density relationship for macroscopic\ntraffic flow modeling. Experimental results show substantial performance gains\nover the original models. To assess the robustness and generalizability of the\nimprovements, we conduct additional evaluations across multiple real-world\ndatasets, demonstrating consistent performance gains beyond the original\ndevelopment data. Furthermore, TR-Agent produces interpretable explanations for\neach improvement, enabling researchers to easily verify and extend its results.\nThis makes TR-Agent a valuable assistant for traffic modeling refinement and a\npromising tool for broader applications in transportation research.\n","authors":["Xusen Guo","Xinxi Yang","Mingxing Peng","Hongliang Lu","Meixin Zhu","Hai Yang"],"pdf_url":"https://arxiv.org/pdf/2409.16876v3.pdf","comment":"27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2505.03265v1","updated":"2025-05-06T07:57:16Z","published":"2025-05-06T07:57:16Z","title":"Synthline: A Product Line Approach for Synthetic Requirements\n  Engineering Data Generation using Large Language Models","summary":"  While modern Requirements Engineering (RE) heavily relies on natural language\nprocessing and Machine Learning (ML) techniques, their effectiveness is limited\nby the scarcity of high-quality datasets. This paper introduces Synthline, a\nProduct Line (PL) approach that leverages Large Language Models to\nsystematically generate synthetic RE data for classification-based use cases.\nThrough an empirical evaluation conducted in the context of using ML for the\nidentification of requirements specification defects, we investigated both the\ndiversity of the generated data and its utility for training downstream models.\nOur analysis reveals that while synthetic datasets exhibit less diversity than\nreal data, they are good enough to serve as viable training resources.\nMoreover, our evaluation shows that combining synthetic and real data leads to\nsubstantial performance improvements. Specifically, hybrid approaches achieve\nup to 85% improvement in precision and a 2x increase in recall compared to\nmodels trained exclusively on real data. These findings demonstrate the\npotential of PL-based synthetic data generation to address data scarcity in RE.\nWe make both our implementation and generated datasets publicly available to\nsupport reproducibility and advancement in the field.\n","authors":["Abdelkarim El-Hajjami","Camille Salinesi"],"pdf_url":"https://arxiv.org/pdf/2505.03265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05675v4","updated":"2025-05-06T07:57:08Z","published":"2025-01-10T02:57:08Z","title":"Synergizing Large Language Models and Task-specific Models for Time\n  Series Anomaly Detection","summary":"  In anomaly detection, methods based on large language models (LLMs) can\nincorporate expert knowledge by reading professional document, while\ntask-specific small models excel at extracting normal data patterns and\ndetecting value fluctuations from training data of target applications.\nInspired by the human nervous system, where the brain stores expert knowledge\nand the peripheral nervous system and spinal cord handle specific tasks like\nwithdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to\nfacilitate collaboration between LLMs and task-specific models, leveraging the\nstrengths of both models for anomaly detection.\n  In particular, we first formulate the collaboration process and identify two\nkey challenges in the collaboration:\n  (1) the misalignment between the expression domains of the LLMs and\ntask-specific small models, and (2) error accumulation arising from the\npredictions of both models.\n  To address these challenges, we then introduce two key components in CoLLaTe:\na model alignment module and a collaborative loss function. Through theoretical\nanalysis and experimental validation, we demonstrate that these components\neffectively mitigate the identified challenges and achieve better performance\nthan both LLM-based and task-specific models.\n","authors":["Feiyi Chen","Leilei Zhang","Guansong Pang","Roger Zimmermann","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2501.05675v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17739v3","updated":"2025-05-06T07:47:40Z","published":"2024-12-23T17:44:01Z","title":"Fourier Position Embedding: Enhancing Attention's Periodic Extension for\n  Length Generalization","summary":"  Extending the context length of Language Models (LMs) by improving Rotary\nPosition Embedding (RoPE) has become a trend. While existing works mainly\naddress RoPE's limitations within attention mechanism, this paper provides an\nanalysis across nearly all parts of LMs, uncovering their adverse effects on\nlength generalization for RoPE-based attention. Using Discrete Signal\nProcessing theory, we show that RoPE enables periodic attention by implicitly\nachieving Non-Uniform Discrete Fourier Transform. However, this periodicity is\nundermined by the spectral damage caused by: 1) linear layers and activation\nfunctions outside of attention; 2) insufficiently trained frequency components\nbrought by time-domain truncation. Building on our observations, we propose\nFourier Position Embedding (FoPE), which enhances attention's frequency-domain\nproperties to improve both its periodic extension and length generalization.\nFoPE constructs Fourier Series and zero-outs the destructive frequency\ncomponents, increasing model robustness against the spectrum damage.\nExperiments across various model scales and benchmarks show that, within\nvarying context windows, FoPE maintains a more stable performance compared to\nRoPE and ALiBi. Several analyses and ablations bring further support to our\nmethod and theoretical modeling.\n","authors":["Ermo Hua","Che Jiang","Xingtai Lv","Kaiyan Zhang","Ning Ding","Youbang Sun","Biqing Qi","Yuchen Fan","Xuekai Zhu","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.17739v3.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2410.22367v3","updated":"2025-05-06T07:46:11Z","published":"2024-10-28T20:45:52Z","title":"MAMMAL -- Molecular Aligned Multi-Modal Architecture and Language","summary":"  Large language models applied to vast biological datasets have the potential\nto transform biology by uncovering disease mechanisms and accelerating drug\ndevelopment. However, current models are often siloed, trained separately on\nsmall-molecules, proteins, or transcriptomic data, limiting their ability to\ncapture complex, multi-modal interactions. Effective drug discovery requires\ncomputational tools that integrate multiple biological entities while\nsupporting prediction and generation, a challenge existing models struggle to\naddress. For this purpose, we present MAMMAL - Molecular Aligned Multi-Modal\nArchitecture and Language - a versatile method applied to create a multi-task\nfoundation model that learns from large-scale biological datasets across\ndiverse modalities, including proteins, small-molecules, and omics. MAMMAL's\nstructured prompt syntax supports classification, regression, and generation\ntasks while handling token and scalar inputs and outputs. Evaluated on eleven\ndiverse downstream tasks, it reaches a new state of the art (SOTA) in nine\ntasks and is comparable to SOTA in two tasks, all within a unified\narchitecture, unlike prior task-specific models. Additionally, we explored\nAlphafold 3 binding prediction capabilities on antibody-antigen and\nnanobody-antigen complexes showing significantly better classification\nperformance of MAMMAL in 3 out of 4 targets. The model code and pretrained\nweights are publicly available at\nhttps://github.com/BiomedSciAI/biomed-multi-alignment and\nhttps://huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m\n","authors":["Yoel Shoshan","Moshiko Raboh","Michal Ozery-Flato","Vadim Ratner","Alex Golts","Jeffrey K. Weber","Ella Barkan","Simona Rabinovici-Cohen","Sagi Polaczek","Ido Amos","Ben Shapira","Liam Hazan","Matan Ninio","Sivan Ravid","Michael M. Danziger","Yosi Shamay","Sharon Kurant","Joseph A. Morrone","Parthasarathy Suryanarayanan","Michal Rosen-Zvi","Efrat Hexter"],"pdf_url":"https://arxiv.org/pdf/2410.22367v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10498v3","updated":"2025-05-06T07:41:59Z","published":"2025-04-07T13:43:53Z","title":"CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation\n  for Large Language Models","summary":"  The performance of large language models (LLMs) in Q&A task increased\nsubstantially through Retrieval-Augmented Generation (RAG) which brings in\nexternal knowledge. However, the main difficulty lies in balancing the inherent\nself-knowledge of LLMs with external information retrieval (IR). The current\nthreshold-based methods apply one-dimensional static mechanisms with single\ncriterion. As a result, their IR decisions might be irrelevant to the LLMs'\nresponse under difficult queries. To alleviate this problem, we propose\nCognitive Convection of Self-Knowledge (CCSK). Different from traditional\nmethods that maintain single fixed IR activation criteria, CCSK implements a\ndynamic joint decision process via a Siamese Network module and a Response\nQuality Model. The Siamese Network calculates the cosine similarity between the\ncurrent query and the historical queries. The Response Quality Model evaluates\nthe responses of LLMs through LightGBM. The final decision of the CCSK is\nderived from the outputs of the two modules, as well as text features fused\nusing a multi-head attention mechanism. Extensive experiments on real-world\ndatasets show that CCSK significantly enhances the model's effectiveness in\ninformation retrieval.\n","authors":["Jianling Lu","Mingqi Lv","Tieming Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10498v3.pdf","comment":"All authors of this paper have unanimously decided to withdraw its\n  preprint from arXiv. As one of the authors, I cannot unilaterally decide its\n  retention. In accordance with the collective decision, we formally request\n  the complete deletion of the paper from arXiv"},{"id":"http://arxiv.org/abs/2401.02663v2","updated":"2025-05-06T07:36:14Z","published":"2024-01-05T06:45:48Z","title":"Effective backdoor attack on graph neural networks in link prediction\n  tasks","summary":"  Graph Neural Networks (GNNs) are a class of deep learning models capable of\nprocessing graph-structured data, and they have demonstrated significant\nperformance in a variety of real-world applications. Recent studies have found\nthat GNN models are vulnerable to backdoor attacks. When specific patterns\n(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input\ndata, the backdoor embedded in the GNN models is activated, which misclassifies\nthe input data into the target class label specified by the attacker, whereas\nwhen there are no backdoor triggers in the input, the backdoor embedded in the\nGNN models is not activated, and the models work normally. Backdoor attacks are\nhighly stealthy and expose GNN models to serious security risks. Currently,\nresearch on backdoor attacks against GNNs mainly focus on tasks such as graph\nclassification and node classification, and backdoor attacks against link\nprediction tasks are rarely studied. In this paper, we propose a backdoor\nattack against the link prediction tasks based on GNNs and reveal the existence\nof such security vulnerability in GNN models, which make the backdoored GNN\nmodels to incorrectly predict unlinked two nodes as having a link relationship\nwhen a trigger appear. The method uses a single node as the trigger and poison\nselected node pairs in the training graph, and then the backdoor will be\nembedded in the GNN models through the training process. In the inference\nstage, the backdoor in the GNN models can be activated by simply linking the\ntrigger node to the two end nodes of the unlinked node pairs in the input data,\ncausing the GNN models to produce incorrect link prediction results for the\ntarget node pairs.\n","authors":["Jiazhu Dai","Haoyu Sun"],"pdf_url":"https://arxiv.org/pdf/2401.02663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03242v1","updated":"2025-05-06T07:14:10Z","published":"2025-05-06T07:14:10Z","title":"Seeing the Abstract: Translating the Abstract Language for Vision\n  Language Models","summary":"  Natural language goes beyond dryly describing visual content. It contains\nrich abstract concepts to express feeling, creativity and properties that\ncannot be directly perceived. Yet, current research in Vision Language Models\n(VLMs) has not shed light on abstract-oriented language. Our research breaks\nnew ground by uncovering its wide presence and under-estimated value, with\nextensive analysis. Particularly, we focus our investigation on the fashion\ndomain, a highly-representative field with abstract expressions. By analyzing\nrecent large-scale multimodal fashion datasets, we find that abstract terms\nhave a dominant presence, rivaling the concrete ones, providing novel\ninformation, and being useful in the retrieval task. However, a critical\nchallenge emerges: current general-purpose or fashion-specific VLMs are\npre-trained with databases that lack sufficient abstract words in their text\ncorpora, thus hindering their ability to effectively represent\nabstract-oriented language. We propose a training-free and model-agnostic\nmethod, Abstract-to-Concrete Translator (ACT), to shift abstract\nrepresentations towards well-represented concrete ones in the VLM latent space,\nusing pre-trained models and existing multimodal databases. On the\ntext-to-image retrieval task, despite being training-free, ACT outperforms the\nfine-tuned VLMs in both same- and cross-dataset settings, exhibiting its\neffectiveness with a strong generalization capability. Moreover, the\nimprovement introduced by ACT is consistent with various VLMs, making it a\nplug-and-play solution.\n","authors":["Davide Talon","Federico Girella","Ziyue Liu","Marco Cristani","Yiming Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03242v1.pdf","comment":"Accepted to CVPR25. Project page:\n  https://davidetalon.github.io/fashionact-page/"},{"id":"http://arxiv.org/abs/2505.02118v2","updated":"2025-05-06T07:07:13Z","published":"2025-05-04T14:00:04Z","title":"Adversarial Cooperative Rationalization: The Risk of Spurious\n  Correlations in Even Clean Datasets","summary":"  This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct).\n","authors":["Wei Liu","Zhongyu Niu","Lang Gao","Zhiying Deng","Jun Wang","Haozhao Wang","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2505.02118v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2402.02399v2","updated":"2025-05-06T06:56:48Z","published":"2024-02-04T08:23:41Z","title":"FreDF: Learning to Forecast in the Frequency Domain","summary":"  Time series modeling presents unique challenges due to autocorrelation in\nboth historical data and future sequences. While current research predominantly\naddresses autocorrelation within historical data, the correlations among future\nlabels are often overlooked. Specifically, modern forecasting models primarily\nadhere to the Direct Forecast (DF) paradigm, generating multi-step forecasts\nindependently and disregarding label autocorrelation over time. In this work,\nwe demonstrate that the learning objective of DF is biased in the presence of\nlabel autocorrelation. To address this issue, we propose the Frequency-enhanced\nDirect Forecast (FreDF), which mitigates label autocorrelation by learning to\nforecast in the frequency domain, thereby reducing estimation bias. Our\nexperiments show that FreDF significantly outperforms existing state-of-the-art\nmethods and is compatible with a variety of forecast models. Code is available\nat https://github.com/Master-PLC/FreDF.\n","authors":["Hao Wang","Licheng Pan","Zhichao Chen","Degui Yang","Sen Zhang","Yifei Yang","Xinggao Liu","Haoxuan Li","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2402.02399v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2505.02737v2","updated":"2025-05-06T06:44:35Z","published":"2025-05-05T15:40:24Z","title":"Knowledge Graphs for Enhancing Large Language Models in Entity\n  Disambiguation","summary":"  Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance.\n","authors":["Gerard Pons","Besim Bilalli","Anna Queralt"],"pdf_url":"https://arxiv.org/pdf/2505.02737v2.pdf","comment":"Pre-print submitted to ISWC 2024"},{"id":"http://arxiv.org/abs/2503.02950v2","updated":"2025-05-06T06:42:34Z","published":"2025-03-04T19:13:10Z","title":"LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications","summary":"  We introduce LiteWebAgent, an open-source suite for VLM-based web agent\napplications. Our framework addresses a critical gap in the web agent ecosystem\nwith a production-ready solution that combines minimal serverless backend\nconfiguration, intuitive user and browser interfaces, and extensible research\ncapabilities in agent planning, memory, and tree search. For the core\nLiteWebAgent agent framework, we implemented a simple yet effective baseline\nusing recursive function calling, providing with decoupled action generation\nand action grounding. In addition, we integrate advanced research components\nsuch as agent planning, agent workflow memory, and tree search in a modular and\nextensible manner. We then integrate the LiteWebAgent agent framework with\nfrontend and backend as deployed systems in two formats: (1) a production\nVercel-based web application, which provides users with an agent-controlled\nremote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control\nan existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent\nframework is available at https://github.com/PathOnAI/LiteWebAgent, with\ndeployed frontend at https://lite-web-agent.vercel.app/.\n","authors":["Danqing Zhang","Balaji Rama","Jingyi Ni","Shiying He","Fu Zhao","Kunyu Chen","Arnold Chen","Junyu Cao"],"pdf_url":"https://arxiv.org/pdf/2503.02950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02579v2","updated":"2025-05-06T06:26:11Z","published":"2025-05-05T11:30:46Z","title":"EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning","summary":"  Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.\n","authors":["Lingxiao Kong","Cong Yang","Susanne Neufang","Oya Deniz Beyan","Zeyd Boukhers"],"pdf_url":"https://arxiv.org/pdf/2505.02579v2.pdf","comment":"13 pages, 9 figures, submitted to SIGDIAL 2025 conference"},{"id":"http://arxiv.org/abs/2311.03382v2","updated":"2025-05-06T06:25:57Z","published":"2023-11-02T08:46:07Z","title":"Causal Structure Representation Learning of Confounders in Latent Space\n  for Recommendation","summary":"  Inferring user preferences from the historical feedback of users is a\nvaluable problem in recommender systems. Conventional approaches often rely on\nthe assumption that user preferences in the feedback data are equivalent to the\nreal user preferences without additional noise, which simplifies the problem\nmodeling. However, there are various confounders during user-item interactions,\nsuch as weather and even the recommendation system itself. Therefore,\nneglecting the influence of confounders will result in inaccurate user\npreferences and suboptimal performance of the model. Furthermore, the\nunobservability of confounders poses a challenge in further addressing the\nproblem. To address these issues, we refine the problem and propose a more\nrational solution. Specifically, we consider the influence of confounders,\ndisentangle them from user preferences in the latent space, and employ causal\ngraphs to model their interdependencies without specific labels. By cleverly\ncombining local and global causal graphs, we capture the user-specificity of\nconfounders on user preferences. We theoretically demonstrate the\nidentifiability of the obtained causal graph. Finally, we propose our model\nbased on Variational Autoencoders, named Causal Structure representation\nlearning of Confounders in latent space (CSC). We conducted extensive\nexperiments on one synthetic dataset and five real-world datasets,\ndemonstrating the superiority of our model. Furthermore, we demonstrate that\nthe learned causal representations of confounders are controllable, potentially\noffering users fine-grained control over the objectives of their recommendation\nlists with the learned causal graphs.\n","authors":["Hangtong Xu","Yuanbo Xu","Chaozhuo Li","Fuzhen Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.03382v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03217v1","updated":"2025-05-06T06:17:57Z","published":"2025-05-06T06:17:57Z","title":"Accelerating Evolution: Integrating PSO Principles into Real-Coded\n  Genetic Algorithm Crossover","summary":"  This study introduces an innovative crossover operator named Particle Swarm\nOptimization-inspired Crossover (PSOX), which is specifically developed for\nreal-coded genetic algorithms. Departing from conventional crossover approaches\nthat only exchange information between individuals within the same generation,\nPSOX uniquely incorporates guidance from both the current global best solution\nand historical optimal solutions across multiple generations. This novel\nmechanism enables the algorithm to maintain population diversity while\nsimultaneously accelerating convergence toward promising regions of the search\nspace. The effectiveness of PSOX is rigorously evaluated through comprehensive\nexperiments on 15 benchmark test functions with diverse characteristics,\nincluding unimodal, multimodal, and highly complex landscapes. Comparative\nanalysis against five state-of-the-art crossover operators reveals that PSOX\nconsistently delivers superior performance in terms of solution accuracy,\nalgorithmic stability, and convergence speed, especially when combined with an\nappropriate mutation strategy. Furthermore, the study provides an in-depth\ninvestigation of how different mutation rates influence PSOX's performance,\nyielding practical guidelines for parameter tuning when addressing optimization\nproblems with varying landscape properties.\n","authors":["Xiaobo Jin","JiaShu Tu"],"pdf_url":"https://arxiv.org/pdf/2505.03217v1.pdf","comment":"14 pages,2 figures,4 tables"},{"id":"http://arxiv.org/abs/2505.03214v1","updated":"2025-05-06T06:02:42Z","published":"2025-05-06T06:02:42Z","title":"DocSpiral: A Platform for Integrated Assistive Document Annotation\n  through Human-in-the-Spiral","summary":"  Acquiring structured data from domain-specific, image-based documents such as\nscanned reports is crucial for many downstream tasks but remains challenging\ndue to document variability. Many of these documents exist as images rather\nthan as machine-readable text, which requires human annotation to train\nautomated extraction systems. We present DocSpiral, the first\nHuman-in-the-Spiral assistive document annotation platform, designed to address\nthe challenge of extracting structured information from domain-specific,\nimage-based document collections. Our spiral design establishes an iterative\ncycle in which human annotations train models that progressively require less\nmanual intervention. DocSpiral integrates document format normalization,\ncomprehensive annotation interfaces, evaluation metrics dashboard, and API\nendpoints for the development of AI / ML models into a unified workflow.\nExperiments demonstrate that our framework reduces annotation time by at least\n41\\% while showing consistent performance gains across three iterations during\nmodel training. By making this annotation platform freely accessible, we aim to\nlower barriers to AI/ML models development in document processing, facilitating\nthe adoption of large language models in image-based, document-intensive fields\nsuch as geoscience and healthcare. The system is freely available at:\nhttps://app.ai4wa.com. The demonstration video is available:\nhttps://app.ai4wa.com/docs/docspiral/demo.\n","authors":["Qiang Sun","Sirui Li","Tingting Bi","Du Huynh","Mark Reynolds","Yuanyi Luo","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2505.03214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02048v2","updated":"2025-05-06T05:56:47Z","published":"2025-05-04T09:57:10Z","title":"Regression is all you need for medical image translation","summary":"  The acquisition of information-rich images within a limited time budget is\ncrucial in medical imaging. Medical image translation (MIT) can help enhance\nand supplement existing datasets by generating synthetic images from acquired\ndata. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have\nachieved remarkable success in natural image generation, their benefits -\ncreativity and image realism - do not necessarily transfer to medical\napplications where highly accurate anatomical information is required. In fact,\nthe imitation of acquisition noise or content hallucination hinder clinical\nutility. Here, we introduce YODA (You Only Denoise once - or Average), a novel\n2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and\nregression paradigms to produce realistic or noise-free outputs. Furthermore,\nwe propose Expectation-Approximation (ExpA) DM sampling, which draws\ninspiration from MRI signal averaging. ExpA-sampling suppresses generated noise\nand, thus, eliminates noise from biasing the evaluation of image quality.\nThrough extensive experiments on four diverse multi-modal datasets - comprising\nmulti-contrast brain MRI and pelvic MRI-CT - we show that diffusion and\nregression sampling yield similar results in practice. As such, the\ncomputational overhead of diffusion sampling does not provide systematic\nbenefits in medical information translation. Building on these insights, we\ndemonstrate that YODA outperforms several state-of-the-art GAN and DM methods.\nNotably, YODA-generated images are shown to be interchangeable with, or even\nsuperior to, physical acquisitions for several downstream tasks. Our findings\nchallenge the presumed advantages of DMs in MIT and pave the way for the\npractical application of MIT in medical imaging.\n","authors":["Sebastian Rassmann","David Kügler","Christian Ewert","Martin Reuter"],"pdf_url":"https://arxiv.org/pdf/2505.02048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06269v2","updated":"2025-05-06T05:48:07Z","published":"2025-03-08T16:29:45Z","title":"Using Mechanistic Interpretability to Craft Adversarial Attacks against\n  Large Language Models","summary":"  Traditional white-box methods for creating adversarial perturbations against\nLLMs typically rely only on gradient computation from the targeted model,\nignoring the internal mechanisms responsible for attack success or failure.\nConversely, interpretability studies that analyze these internal mechanisms\nlack practical applications beyond runtime interventions. We bridge this gap by\nintroducing a novel white-box approach that leverages mechanistic\ninterpretability techniques to craft practical adversarial inputs.\nSpecifically, we first identify acceptance subspaces - sets of feature vectors\nthat do not trigger the model's refusal mechanisms - then use gradient-based\noptimization to reroute embeddings from refusal subspaces to acceptance\nsubspaces, effectively achieving jailbreaks. This targeted approach\nsignificantly reduces computation cost, achieving attack success rates of\n80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5\nwithin minutes or even seconds, compared to existing techniques that often fail\nor require hours of computation. We believe this approach opens a new direction\nfor both attack research and defense development. Furthermore, it showcases a\npractical application of mechanistic interpretability where other methods are\nless efficient, which highlights its utility. The code and generated datasets\nare available at https://github.com/Sckathach/subspace-rerouting.\n","authors":["Thomas Winninger","Boussad Addad","Katarzyna Kapusta"],"pdf_url":"https://arxiv.org/pdf/2503.06269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03204v1","updated":"2025-05-06T05:38:17Z","published":"2025-05-06T05:38:17Z","title":"DCS-ST for Classification of Breast Cancer Histopathology Images with\n  Limited Annotations","summary":"  Deep learning methods have shown promise in classifying breast cancer\nhistopathology images, but their performance often declines with limited\nannotated data, a critical challenge in medical imaging due to the high cost\nand expertise required for annotations.\n","authors":["Liu Suxing","Byungwon Min"],"pdf_url":"https://arxiv.org/pdf/2505.03204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03196v1","updated":"2025-05-06T05:32:46Z","published":"2025-05-06T05:32:46Z","title":"A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case","summary":"  Large Language Models (LLMs) demonstrate strong potential across a variety of\ntasks in communications and networking due to their advanced reasoning\ncapabilities. However, because different LLMs have different model structures\nand are trained using distinct corpora and methods, they may offer varying\noptimization strategies for the same network issues. Moreover, the limitations\nof an individual LLM's training data, aggravated by the potential maliciousness\nof its hosting device, can result in responses with low confidence or even\nbias. To address these challenges, we propose a blockchain-enabled\ncollaborative framework that connects multiple LLMs into a Trustworthy\nMulti-LLM Network (MultiLLMN). This architecture enables the cooperative\nevaluation and selection of the most reliable and high-quality responses to\ncomplex network optimization problems. Specifically, we begin by reviewing\nrelated work and highlighting the limitations of existing LLMs in collaboration\nand trust, emphasizing the need for trustworthiness in LLM-based systems. We\nthen introduce the workflow and design of the proposed Trustworthy MultiLLMN\nframework. Given the severity of False Base Station (FBS) attacks in B5G and 6G\ncommunication systems and the difficulty of addressing such threats through\ntraditional modeling techniques, we present FBS defense as a case study to\nempirically validate the effectiveness of our approach. Finally, we outline\npromising future research directions in this emerging area.\n","authors":["Haoxiang Luo","Gang Sun","Yinqiu Liu","Dusit Niyato","Hongfang Yu","Mohammed Atiquzzaman","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2505.03196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03193v1","updated":"2025-05-06T05:24:11Z","published":"2025-05-06T05:24:11Z","title":"A study on audio synchronous steganography detection and distributed\n  guide inference model based on sliding spectral features and intelligent\n  inference drive","summary":"  With the rise of short video platforms in global communication, embedding\nsteganographic data in audio synchronization streams has emerged as a new\ncovert communication method. To address the limitations of traditional\ntechniques in detecting synchronized steganography, this paper proposes a\ndetection and distributed guidance reconstruction model based on short video\n\"Yupan\" samples released by China's South Sea Fleet on TikTok. The method\nintegrates sliding spectrum feature extraction and intelligent inference\nmechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is\nused to extract the main frequency trajectory and construct the synchronization\nframe detection model (M1), identifying a frame flag \"FFFFFFFFFFFFFFFFFF80\".\nThe subsequent 32-byte payload is decoded by a structured model (M2) to infer\ndistributed guidance commands. Analysis reveals a low-entropy, repetitive byte\nsequence in the 36 to 45 second audio segment with highly concentrated spectral\nenergy, confirming the presence of synchronization frames. Although plaintext\nsemantics are not restored, the consistency in command field layout suggests\nfeatures of military communication protocols. The multi-segment splicing model\nfurther shows cross-video embedding and centralized decoding capabilities. The\nproposed framework validates the effectiveness of sliding spectral features for\nsynchronized steganography detection and builds an extensible inference model\nfor covert communication analysis and tactical guidance simulation on open\nplatforms.\n","authors":["Wei Meng"],"pdf_url":"https://arxiv.org/pdf/2505.03193v1.pdf","comment":"This paper proposes a novel framework for detecting steganographic\n  content in short video audio streams using sliding spectral features and\n  distributed inference models, combining STFT analysis, entropy-based\n  synchronization, and deep learning-driven decoding strategies"},{"id":"http://arxiv.org/abs/2505.03189v1","updated":"2025-05-06T05:15:12Z","published":"2025-05-06T05:15:12Z","title":"Patterns and Mechanisms of Contrastive Activation Engineering","summary":"  Controlling the behavior of Large Language Models (LLMs) remains a\nsignificant challenge due to their inherent complexity and opacity. While\ntechniques like fine-tuning can modify model behavior, they typically require\nextensive computational resources. Recent work has introduced a class of\ncontrastive activation engineering (CAE) techniques as promising approaches for\nsteering LLM outputs through targeted modifications to their internal\nrepresentations. Applied at inference-time with zero cost, CAE has the\npotential to introduce a new paradigm of flexible, task-specific LLM behavior\ntuning. We analyze the performance of CAE in in-distribution,\nout-of-distribution settings, evaluate drawbacks, and begin to develop\ncomprehensive guidelines for its effective deployment. We find that 1. CAE is\nonly reliably effective when applied to in-distribution contexts. 2. Increasing\nthe number of samples used to generate steering vectors has diminishing returns\nat around 80 samples. 3. Steering vectors are susceptible to adversarial inputs\nthat reverses the behavior that is steered for. 4. Steering vectors harm the\noverall model perplexity. 5. Larger models are more resistant to\nsteering-induced degradation.\n","authors":["Yixiong Hao","Ayush Panda","Stepan Shabalin","Sheikh Abdur Raheem Ali"],"pdf_url":"https://arxiv.org/pdf/2505.03189v1.pdf","comment":"Published at the ICLR 2025 Bi-Align, HAIC, and Building Trust\n  workshops"},{"id":"http://arxiv.org/abs/2502.18438v2","updated":"2025-05-06T05:04:09Z","published":"2025-02-25T18:31:55Z","title":"ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent\n  Diffusion Policies","summary":"  In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in\nTeams), a new framework for generating ToM-conditioned trajectories. It\ncombines a meta-learning mechanism, that performs ToM reasoning over teammates'\nunderlying goals and future behavior, with a multiagent denoising-diffusion\nmodel, that generates plans for an agent and its teammates conditioned on both\nthe agent's goals and its teammates' characteristics, as computed via ToM. We\nimplemented an online planning system that dynamically samples new trajectories\n(replans) from the diffusion model whenever it detects a divergence between a\npreviously generated plan and the current state of the world. We conducted\nseveral experiments using ToMCAT in a simulated cooking domain. Our results\nhighlight the importance of the dynamic replanning mechanism in reducing the\nusage of resources without sacrificing team performance. We also show that\nrecent observations about the world and teammates' behavior collected by an\nagent over the course of an episode combined with ToM inferences are crucial to\ngenerate team-aware plans for dynamic adaptation to teammates, especially when\nno prior information is provided about them.\n","authors":["Pedro Sequeira","Vidyasagar Sadhu","Melinda Gervasio"],"pdf_url":"https://arxiv.org/pdf/2502.18438v2.pdf","comment":"Appears in Proc. of the Adaptive and Learning Agents Workshop (ALA\n  2025), ala-workshop.github.io"},{"id":"http://arxiv.org/abs/2504.13460v3","updated":"2025-05-06T05:00:15Z","published":"2025-04-18T04:35:35Z","title":"Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization","summary":"  Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.\n","authors":["Hongwei Ji","Wulian Yun","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2504.13460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03176v1","updated":"2025-05-06T04:39:11Z","published":"2025-05-06T04:39:11Z","title":"seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant\n  World Models","summary":"  Current self-supervised algorithms mostly rely on transformations such as\ndata augmentation and masking to learn visual representations. This is achieved\nby inducing invariance or equivariance with respect to these transformations\nafter encoding two views of an image. This dominant two-view paradigm can limit\nthe flexibility of learned representations for downstream adaptation by\ncreating performance trade-offs between invariance-related tasks such as image\nclassification and more fine-grained equivariance-related tasks. In this work,\nwe introduce \\emph{seq-JEPA}, a world modeling paradigm based on\njoint-embedding predictive architecture that leverages architectural inductive\nbiases to resolve this trade-off. Without requiring an additional equivariance\npredictor or loss term, seq-JEPA simultaneously learns two architecturally\nsegregated representations: one equivariant to the specified transformations\nand another invariant to them and suited for tasks such as classification. To\ndo so, our model processes a short sequence of different views (observations)\nof an input image. Each encoded view is concatenated with embeddings\ncorresponding to the relative transformation (action) producing the next\nobservation in the sequence. A transformer encoder outputs an aggregate\nrepresentation of this sequence, which is subsequently conditioned on the\naction leading to the next observation to predict its representation.\nEmpirically, seq-JEPA achieves strong performance on equivariant benchmarks and\nimage classification without sacrificing one for the other. Additionally, our\nframework excels at tasks that inherently require aggregating a sequence of\nobservations, such as path integration across actions and predictive learning\nacross eye movements.\n","authors":["Hafez Ghaemi","Eilif Muller","Shahab Bakhtiari"],"pdf_url":"https://arxiv.org/pdf/2505.03176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03173v1","updated":"2025-05-06T04:38:09Z","published":"2025-05-06T04:38:09Z","title":"RAVU: Retrieval Augmented Video Understanding with Compositional\n  Reasoning over Graph","summary":"  Comprehending long videos remains a significant challenge for Large\nMulti-modal Models (LMMs). Current LMMs struggle to process even minutes to\nhours videos due to their lack of explicit memory and retrieval mechanisms. To\naddress this limitation, we propose RAVU (Retrieval Augmented Video\nUnderstanding), a novel framework for video understanding enhanced by retrieval\nwith compositional reasoning over a spatio-temporal graph. We construct a graph\nrepresentation of the video, capturing both spatial and temporal relationships\nbetween entities. This graph serves as a long-term memory, allowing us to track\nobjects and their actions across time. To answer complex queries, we decompose\nthe queries into a sequence of reasoning steps and execute these steps on the\ngraph, retrieving relevant key information. Our approach enables more accurate\nunderstanding of long videos, particularly for queries that require multi-hop\nreasoning and tracking objects across frames. Our approach demonstrate superior\nperformances with limited retrieved frames (5-10) compared with other SOTA\nmethods and baselines on two major video QA datasets, NExT-QA and EgoSchema.\n","authors":["Sameer Malik","Moyuru Yamada","Ayush Singh","Dishank Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2505.03173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03172v1","updated":"2025-05-06T04:32:47Z","published":"2025-05-06T04:32:47Z","title":"Null Counterfactual Factor Interactions for Goal-Conditioned\n  Reinforcement Learning","summary":"  Hindsight relabeling is a powerful tool for overcoming sparsity in\ngoal-conditioned reinforcement learning (GCRL), especially in certain domains\nsuch as navigation and locomotion. However, hindsight relabeling can struggle\nin object-centric domains. For example, suppose that the goal space consists of\na robotic arm pushing a particular target block to a goal location. In this\ncase, hindsight relabeling will give high rewards to any trajectory that does\nnot interact with the block. However, these behaviors are only useful when the\nobject is already at the goal -- an extremely rare case in practice. A dataset\ndominated by these kinds of trajectories can complicate learning and lead to\nfailures. In object-centric domains, one key intuition is that meaningful\ntrajectories are often characterized by object-object interactions such as\npushing the block with the gripper. To leverage this intuition, we introduce\nHindsight Relabeling using Interactions (HInt), which combines interactions\nwith hindsight relabeling to improve the sample efficiency of downstream RL.\nHowever because interactions do not have a consensus statistical definition\ntractable for downstream GCRL, we propose a definition of interactions based on\nthe concept of null counterfactual: a cause object is interacting with a target\nobject if, in a world where the cause object did not exist, the target object\nwould have different transition dynamics. We leverage this definition to infer\ninteractions in Null Counterfactual Interaction Inference (NCII), which uses a\n\"nulling'' operation with a learned model to infer interactions. NCII is able\nto achieve significantly improved interaction inference accuracy in both simple\nlinear dynamics domains and dynamic robotic domains in Robosuite, Robot Air\nHockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.\n","authors":["Caleb Chuck","Fan Feng","Carl Qi","Chang Shi","Siddhant Agarwal","Amy Zhang","Scott Niekum"],"pdf_url":"https://arxiv.org/pdf/2505.03172v1.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2505.03171v1","updated":"2025-05-06T04:32:17Z","published":"2025-05-06T04:32:17Z","title":"CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics","summary":"  Neurosymbolic approaches integrating large language models with formal\nreasoning have recently achieved human-level performance on mathematics\ncompetition problems in algebra, geometry and number theory. In comparison,\ncombinatorics remains a challenging domain, characterized by a lack of\nappropriate benchmarks and theorem libraries. To address this gap, we introduce\nCombiBench, a comprehensive benchmark comprising 100 combinatorial problems,\neach formalized in Lean~4 and paired with its corresponding informal statement.\nThe problem set covers a wide spectrum of difficulty levels, ranging from\nmiddle school to IMO and university level, and span over ten combinatorial\ntopics. CombiBench is suitable for testing IMO solving capabilities since it\nincludes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its\nstatement contain an images). Furthermore, we provide a comprehensive and\nstandardized evaluation framework, dubbed Fine-Eval (for\n$\\textbf{F}$ill-in-the-blank $\\textbf{in}$ L$\\textbf{e}$an Evaluation), for\nformal mathematics. It accommodates not only proof-based problems but also, for\nthe first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval\nas the evaluation method and Kimina Lean Server as the backend, we benchmark\nseveral LLMs on CombiBench and observe that their capabilities for formally\nsolving combinatorial problems remain limited. Among all models tested (none of\nwhich has been trained for this particular task), Kimina-Prover attains the\nbest results, solving 7 problems (out of 100) under both ``with solution'' and\n``without solution'' scenarios. We open source the benchmark dataset alongside\nwith the code of the proposed evaluation method at\nhttps://github.com/MoonshotAI/CombiBench/.\n","authors":["Junqi Liu","Xiaohan Lin","Jonas Bayer","Yael Dillies","Weijie Jiang","Xiaodan Liang","Roman Soletskyi","Haiming Wang","Yunzhou Xie","Beibei Xiong","Zhengfeng Yang","Jujian Zhang","Lihong Zhi","Jia Li","Zhengying Liu"],"pdf_url":"https://arxiv.org/pdf/2505.03171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03156v1","updated":"2025-05-06T04:03:11Z","published":"2025-05-06T04:03:11Z","title":"Soft Best-of-n Sampling for Model Alignment","summary":"  Best-of-$n$ (BoN) sampling is a practical approach for aligning language\nmodel outputs with human preferences without expensive fine-tuning. BoN\nsampling is performed by generating $n$ responses to a prompt and then\nselecting the sample that maximizes a reward function. BoN yields high reward\nvalues in practice at a distortion cost, as measured by the KL-divergence\nbetween the sampled and original distribution. This distortion is coarsely\ncontrolled by varying the number of samples: larger $n$ yields a higher reward\nat a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a\ngeneralization of BoN that allows for smooth interpolation between the original\ndistribution and reward-maximizing distribution through a temperature parameter\n$\\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$\nsampling converges sharply to the optimal tilted distribution at a rate of\n$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete\noutputs, we analyze an additive reward model that reveals the fundamental\nlimitations of blockwise sampling.\n","authors":["Claudio Mayrink Verdun","Alex Oesterling","Himabindu Lakkaraju","Flavio P. Calmon"],"pdf_url":"https://arxiv.org/pdf/2505.03156v1.pdf","comment":"Accepted for presentation at the 2025 IEEE International Symposium on\n  Information Theory (ISIT 2025)"},{"id":"http://arxiv.org/abs/2505.03154v1","updated":"2025-05-06T04:02:47Z","published":"2025-05-06T04:02:47Z","title":"StableMotion: Training Motion Cleanup Models with Unpaired Corrupted\n  Data","summary":"  Motion capture (mocap) data often exhibits visually jarring artifacts due to\ninaccurate sensors and post-processing. Cleaning this corrupted data can\nrequire substantial manual effort from human experts, which can be a costly and\ntime-consuming process. Previous data-driven motion cleanup methods offer the\npromise of automating this cleanup process, but often require in-domain paired\ncorrupted-to-clean training data. Constructing such paired datasets requires\naccess to high-quality, relatively artifact-free motion clips, which often\nnecessitates laborious manual cleanup. In this work, we present StableMotion, a\nsimple yet effective method for training motion cleanup models directly from\nunpaired corrupted datasets that need cleanup. The core component of our method\nis the introduction of motion quality indicators, which can be easily annotated\nthrough manual labeling or heuristic algorithms and enable training of\nquality-aware motion generation models on raw motion data with mixed quality.\nAt test time, the model can be prompted to generate high-quality motions using\nthe quality indicators. Our method can be implemented through a simple\ndiffusion-based framework, leading to a unified motion generate-discriminate\nmodel, which can be used to both identify and fix corrupted frames. We\ndemonstrate that our proposed method is effective for training motion cleanup\nmodels on raw mocap data in production scenarios by applying StableMotion to\nSoccerMocap, a 245-hour soccer mocap dataset containing real-world motion\nartifacts. The trained model effectively corrects a wide range of motion\nartifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.\nSee https://youtu.be/3Y7MMAH02B4 for more results.\n","authors":["Yuxuan Mu","Hung Yu Ling","Yi Shi","Ismael Baira Ojeda","Pengcheng Xi","Chang Shu","Fabio Zinno","Xue Bin Peng"],"pdf_url":"https://arxiv.org/pdf/2505.03154v1.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2505.03149v1","updated":"2025-05-06T03:52:17Z","published":"2025-05-06T03:52:17Z","title":"Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)","summary":"  We introduce an unsupervised motion-compensated image reconstruction\nalgorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging\n(MRI). We express the image volume corresponding to each specific motion phase\nas the deformation of a single static image template. The main contribution of\nthe work is the low-rank model for the compact joint representation of the\nfamily of diffeomorphisms, parameterized by the motion phases. The\ndiffeomorphism at a specific motion phase is obtained by integrating a\nparametric velocity field along a path connecting the reference template phase\nto the motion phase. The velocity field at different phases is represented\nusing a low-rank model. The static template and the low-rank motion model\nparameters are learned directly from the k-space data in an unsupervised\nfashion. The more constrained motion model is observed to offer improved\nrecovery compared to current motion-resolved and motion-compensated algorithms\nfor free-breathing 3D cine MRI.\n","authors":["Joseph William Kettelkamp","Ludovica Romanin","Sarv Priya","Mathews Jacob"],"pdf_url":"https://arxiv.org/pdf/2505.03149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18916v2","updated":"2025-05-06T03:37:38Z","published":"2025-04-26T13:15:40Z","title":"UnifyFL: Enabling Decentralized Cross-Silo Federated Learning","summary":"  Federated Learning (FL) is a decentralized machine learning (ML) paradigm in\nwhich models are trained on private data across several devices called clients\nand combined at a single node called an aggregator rather than aggregating the\ndata itself. Many organizations employ FL to have better privacy-aware\nML-driven decision-making capabilities. However, organizations often operate\nindependently rather than collaborate to enhance their FL capabilities due to\nthe lack of an effective mechanism for collaboration. The challenge lies in\nbalancing trust and resource efficiency. One approach relies on trusting a\nthird-party aggregator to consolidate models from all organizations (multilevel\nFL), but this requires trusting an entity that may be biased or unreliable.\nAlternatively, organizations can bypass a third party by sharing their local\nmodels directly, which requires significant computational resources for\nvalidation. Both approaches reflect a fundamental trade-off between trust and\nresource constraints, with neither offering an ideal solution. In this work, we\ndevelop a trust-based cross-silo FL framework called UnifyFL, which uses\ndecentralized orchestration and distributed storage. UnifyFL provides\nflexibility to the participating organizations and presents synchronous and\nasynchronous modes to handle stragglers. Our evaluation on a diverse testbed\nshows that UnifyFL achieves a performance comparable to the ideal multilevel\ncentralized FL while allowing trust and optimal use of resources.\n","authors":["Sarang S","Druva Dhakshinamoorthy","Aditya Shiva Sharma","Yuvraj Singh Bhadauria","Siddharth Chaitra Vivek","Arihant Bansal","Arnab K. Paul"],"pdf_url":"https://arxiv.org/pdf/2504.18916v2.pdf","comment":"12 pages, 7 figures, 7 tables. Accepted at the 26th ACM/IFIP\n  International Middleware Conference (MIDDLEWARE 2025)"},{"id":"http://arxiv.org/abs/2312.01581v2","updated":"2025-05-06T03:32:16Z","published":"2023-12-04T02:33:53Z","title":"PLUM: Improving Inference Efficiency By Leveraging Repetition-Sparsity\n  Trade-Off","summary":"  Efficient inference of Deep Neural Networks (DNNs) on resource-constrained\nedge devices is essential. Quantization and sparsity are key techniques that\ntranslate to repetition and sparsity within tensors at the hardware-software\ninterface. This paper introduces the concept of repetition-sparsity trade-off\nthat helps explain computational efficiency during inference. We propose PLUM,\na unified co-design framework that integrates DNN inference systems and\nquantization (forward and backward pass) to leverage the repetition-sparsity\ntrade-off to improve inference efficiency. Our results demonstrate that PLUM's\nquantization method is more accurate than binary quantization with the same\nnumber of non-zero weights. Detailed analysis indicates that signed\nbinarization generates a smaller distribution of effectual (non-zero)\nparameters nested within a larger distribution of total parameters of latent\nfull-precision weights for a DNN block. Finally, the proposed PLUM framework\nachieves a 26% speedup on real hardware, doubles energy efficiency, and reduces\ndensity by 2.8x compared to binary methods while retaining top-1 accuracy when\ncompared to prior-art methods for ResNets on ImageNet (by achieving 66.2% top-1\naccuracy), presenting an alternative solution for deploying efficient models in\nresource-limited environments.\n","authors":["Sachit Kuhar","Yash Jain","Alexey Tumanov"],"pdf_url":"https://arxiv.org/pdf/2312.01581v2.pdf","comment":"OpenReview: https://openreview.net/forum?id=IEKtMMSblm"},{"id":"http://arxiv.org/abs/2505.03135v1","updated":"2025-05-06T03:19:51Z","published":"2025-05-06T03:19:51Z","title":"Holmes: Automated Fact Check with Large Language Models","summary":"  The rise of Internet connectivity has accelerated the spread of\ndisinformation, threatening societal trust, decision-making, and national\nsecurity. Disinformation has evolved from simple text to complex multimodal\nforms combining images and text, challenging existing detection methods.\nTraditional deep learning models struggle to capture the complexity of\nmultimodal disinformation. Inspired by advances in AI, this study explores\nusing Large Language Models (LLMs) for automated disinformation detection. The\nempirical study shows that (1) LLMs alone cannot reliably assess the\ntruthfulness of claims; (2) providing relevant evidence significantly improves\ntheir performance; (3) however, LLMs cannot autonomously search for accurate\nevidence. To address this, we propose Holmes, an end-to-end framework featuring\na novel evidence retrieval method that assists LLMs in collecting high-quality\nevidence. Our approach uses (1) LLM-powered summarization to extract key\ninformation from open sources and (2) a new algorithm and metrics to evaluate\nevidence quality. Holmes enables LLMs to verify claims and generate\njustifications effectively. Experiments show Holmes achieves 88.3% accuracy on\ntwo open-source datasets and 90.2% in real-time verification tasks. Notably,\nour improved evidence retrieval boosts fact-checking accuracy by 30.8% over\nexisting methods\n","authors":["Haoran Ou","Gelei Deng","Xingshuo Han","Jie Zhang","Xinlei He","Han Qiu","Shangwei Guo","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.03135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03132v1","updated":"2025-05-06T03:09:15Z","published":"2025-05-06T03:09:15Z","title":"VISLIX: An XAI Framework for Validating Vision Models with Slice\n  Discovery and Analysis","summary":"  Real-world machine learning models require rigorous evaluation before\ndeployment, especially in safety-critical domains like autonomous driving and\nsurveillance. The evaluation of machine learning models often focuses on data\nslices, which are subsets of the data that share a set of characteristics. Data\nslice finding automatically identifies conditions or data subgroups where\nmodels underperform, aiding developers in mitigating performance issues.\nDespite its popularity and effectiveness, data slicing for vision model\nvalidation faces several challenges. First, data slicing often needs additional\nimage metadata or visual concepts, and falls short in certain computer vision\ntasks, such as object detection. Second, understanding data slices is a\nlabor-intensive and mentally demanding process that heavily relies on the\nexpert's domain knowledge. Third, data slicing lacks a human-in-the-loop\nsolution that allows experts to form hypothesis and test them interactively. To\novercome these limitations and better support the machine learning operations\nlifecycle, we introduce VISLIX, a novel visual analytics framework that employs\nstate-of-the-art foundation models to help domain experts analyze slices in\ncomputer vision models. Our approach does not require image metadata or visual\nconcepts, automatically generates natural language insights, and allows users\nto test data slice hypothesis interactively. We evaluate VISLIX with an expert\nstudy and three use cases, that demonstrate the effectiveness of our tool in\nproviding comprehensive insights for validating object detection models.\n","authors":["Xinyuan Yan","Xiwei Xuan","Jorge Piazentin Ono","Jiajing Guo","Vikram Mohanty","Shekar Arvind Kumar","Liang Gou","Bei Wang","Liu Ren"],"pdf_url":"https://arxiv.org/pdf/2505.03132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23895v4","updated":"2025-05-06T03:04:20Z","published":"2025-03-31T09:46:35Z","title":"Dynamic Parametric Retrieval Augmented Generation for Test-time\n  Knowledge Enhancement","summary":"  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.\n","authors":["Yuqiao Tan","Shizhu He","Huanxuan Liao","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.23895v4.pdf","comment":"preprint. Code is available at https://github.com/Trae1ounG/DyPRAG"},{"id":"http://arxiv.org/abs/2505.01584v2","updated":"2025-05-06T02:36:10Z","published":"2025-05-02T21:03:03Z","title":"Understanding and Exploiting Plasticity for Non-stationary Network\n  Resource Adaptation","summary":"  Adapting to non-stationary network conditions presents significant challenges\nfor resource adaptation. However, current solutions primarily rely on\nstationary assumptions. While data-driven reinforcement learning approaches\noffer promising solutions for handling network dynamics, our systematic\ninvestigation reveals a critical limitation: neural networks suffer from\nplasticity loss, significantly impeding their ability to adapt to evolving\nnetwork conditions. Through theoretical analysis of neural propagation\nmechanisms, we demonstrate that existing dormant neuron metrics inadequately\ncharacterize neural plasticity loss. To address this limitation, we have\ndeveloped the Silent Neuron theory, which provides a more comprehensive\nframework for understanding plasticity degradation. Based on these theoretical\ninsights, we propose the Reset Silent Neuron (ReSiN), which preserves neural\nplasticity through strategic neuron resets guided by both forward and backward\npropagation states. In our implementation of an adaptive video streaming\nsystem, ReSiN has shown significant improvements over existing solutions,\nachieving up to 168% higher bitrate and 108% better quality of experience (QoE)\nwhile maintaining comparable smoothness. Furthermore, ReSiN consistently\noutperforms in stationary environments, demonstrating its robust adaptability\nacross different network conditions.\n","authors":["Zhiqiang He","Zhi Liu"],"pdf_url":"https://arxiv.org/pdf/2505.01584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03108v1","updated":"2025-05-06T02:01:27Z","published":"2025-05-06T02:01:27Z","title":"Is AI currently capable of identifying wild oysters? A comparison of\n  human annotators against the AI model, ODYSSEE","summary":"  Oysters are ecologically and commercially important species that require\nfrequent monitoring to track population demographics (e.g. abundance, growth,\nmortality). Current methods of monitoring oyster reefs often require\ndestructive sampling methods and extensive manual effort. Therefore, they are\nsuboptimal for small-scale or sensitive environments. A recent alternative, the\nODYSSEE model, was developed to use deep learning techniques to identify live\noysters using video or images taken in the field of oyster reefs to assess\nabundance. The validity of this model in identifying live oysters on a reef was\ncompared to expert and non-expert annotators. In addition, we identified\npotential sources of prediction error. Although the model can make inferences\nsignificantly faster than expert and non-expert annotators (39.6 s, $2.34 \\pm\n0.61$ h, $4.50 \\pm 1.46$ h, respectively), the model overpredicted the number\nof live oysters, achieving lower accuracy (63\\%) in identifying live oysters\ncompared to experts (74\\%) and non-experts (75\\%) alike. Image quality was an\nimportant factor in determining the accuracy of the model and the annotators.\nBetter quality images improved human accuracy and worsened model accuracy.\nAlthough ODYSSEE was not sufficiently accurate, we anticipate that future\ntraining on higher-quality images, utilizing additional live imagery, and\nincorporating additional annotation training classes will greatly improve the\nmodel's predictive power based on the results of this analysis. Future research\nshould address methods that improve the detection of living vs. dead oysters.\n","authors":["Brendan Campbell","Alan Williams","Kleio Baxevani","Alyssa Campbell","Rushabh Dhoke","Rileigh E. Hudock","Xiaomin Lin","Vivek Mange","Bernhard Neuberger","Arjun Suresh","Alhim Vera","Arthur Trembanis","Herbert G. Tanner","Edward Hale"],"pdf_url":"https://arxiv.org/pdf/2505.03108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02369v2","updated":"2025-05-06T01:56:40Z","published":"2025-05-05T05:13:12Z","title":"Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural\n  Networks","summary":"  Generalizing well in deep neural networks remains a core challenge,\nparticularly due to their tendency to converge to sharp minima that degrade\nrobustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking\nflatter minima but perturbs parameters using the full gradient, which can\ninclude statistically insignificant directions. We propose ZSharp, a simple yet\neffective extension to SAM that applies layer-wise Z-score normalization\nfollowed by percentile-based filtering to retain only statistically significant\ngradient components. This selective perturbation aligns updates with\ncurvature-sensitive directions, enhancing generalization without requiring\narchitectural changes. ZSharp introduces only one additional hyperparameter,\nthe percentile threshold, and remains fully compatible with existing SAM\nvariants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,\nVGG, and Vision Transformers show that ZSharp consistently outperforms SAM and\nits variants in test accuracy, particularly on deeper and transformer-based\nmodels. These results demonstrate that ZSharp is a principled and lightweight\nimprovement for sharpness-aware optimization.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2505.02369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03105v1","updated":"2025-05-06T01:49:44Z","published":"2025-05-06T01:49:44Z","title":"Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI\n  Knowledge Co-Creation","summary":"  Scientific knowledge creation is fundamentally transforming as humans and AI\nsystems evolve beyond tool-user relationships into co-evolutionary epistemic\npartnerships. When AlphaFold revolutionized protein structure prediction,\nresearchers described engaging with an epistemic partner that reshaped how they\nconceptualized fundamental relationships. This article introduces Cognitio\nEmergens (CE), a framework addressing critical limitations in existing models\nthat focus on static roles or narrow metrics while failing to capture how\nscientific understanding emerges through recursive human-AI interaction over\ntime. CE integrates three components addressing these limitations: Agency\nConfigurations describing how authority distributes between humans and AI\n(Directed, Contributory, Partnership), with partnerships dynamically\noscillating between configurations rather than following linear progression;\nEpistemic Dimensions capturing six specific capabilities emerging through\ncollaboration across Discovery, Integration, and Projection axes, creating\ndistinctive \"capability signatures\" that guide development; and Partnership\nDynamics identifying forces shaping how these relationships evolve,\nparticularly the risk of epistemic alienation where researchers lose\ninterpretive control over knowledge they formally endorse. Drawing from\nautopoiesis theory, social systems theory, and organizational modularity, CE\nreveals how knowledge co-creation emerges through continuous negotiation of\nroles, values, and organizational structures. By reconceptualizing human-AI\nscientific collaboration as fundamentally co-evolutionary, CE offers a balanced\nperspective that neither uncritically celebrates nor unnecessarily fears AI's\nevolving role, instead providing conceptual tools for cultivating partnerships\nthat maintain meaningful human participation while enabling transformative\nscientific breakthroughs.\n","authors":["Xule Lin"],"pdf_url":"https://arxiv.org/pdf/2505.03105v1.pdf","comment":"62 pages (31 appendix pages for guidance), 2 figures"},{"id":"http://arxiv.org/abs/2401.16646v2","updated":"2025-05-06T01:43:38Z","published":"2024-01-30T00:40:49Z","title":"Incoherent Probability Judgments in Large Language Models","summary":"  Autoregressive Large Language Models (LLMs) trained for next-word prediction\nhave demonstrated remarkable proficiency at producing coherent text. But are\nthey equally adept at forming coherent probability judgments? We use\nprobabilistic identities and repeated judgments to assess the coherence of\nprobability judgments made by LLMs. Our results show that the judgments\nproduced by these models are often incoherent, displaying human-like systematic\ndeviations from the rules of probability theory. Moreover, when prompted to\njudge the same event, the mean-variance relationship of probability judgments\nproduced by LLMs shows an inverted-U-shaped like that seen in humans. We\npropose that these deviations from rationality can be explained by linking\nautoregressive LLMs to implicit Bayesian inference and drawing parallels with\nthe Bayesian Sampler model of human probability judgments.\n","authors":["Jian-Qiao Zhu","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2401.16646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19313v2","updated":"2025-05-06T01:26:28Z","published":"2024-05-29T17:37:14Z","title":"Language Models Trained to do Arithmetic Predict Human Risky and\n  Intertemporal Choice","summary":"  The observed similarities in the behavior of humans and Large Language Models\n(LLMs) have prompted researchers to consider the potential of using LLMs as\nmodels of human cognition. However, several significant challenges must be\naddressed before LLMs can be legitimately regarded as cognitive models. For\ninstance, LLMs are trained on far more data than humans typically encounter,\nand may have been directly trained on human data in specific cognitive tasks or\naligned with human preferences. Consequently, the origins of these behavioral\nsimilarities are not well understood. In this paper, we propose a novel way to\nenhance the utility of LLMs as cognitive models. This approach involves (i)\nleveraging computationally equivalent tasks that both an LLM and a rational\nagent need to master for solving a cognitive problem and (ii) examining the\nspecific task distributions required for an LLM to exhibit human-like\nbehaviors. We apply this approach to decision-making -- specifically risky and\nintertemporal choice -- where the key computationally equivalent task is the\narithmetic of expected value calculations. We show that an LLM pretrained on an\necologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts\nhuman behavior better than many traditional cognitive models. Pretraining LLMs\non ecologically valid arithmetic datasets is sufficient to produce a strong\ncorrespondence between these models and human decision-making. Our results also\nsuggest that LLMs used as cognitive models should be carefully investigated via\nablation studies of the pretraining data.\n","authors":["Jian-Qiao Zhu","Haijiang Yan","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2405.19313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04021v1","updated":"2025-05-06T23:38:33Z","published":"2025-05-06T23:38:33Z","title":"Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving","summary":"  Serving large language models (LLMs) is expensive, especially for providers\nhosting many models, making cost reduction essential. The unique workload\npatterns of serving multiple LLMs (i.e., multi-LLM serving) create new\nopportunities and challenges for this task. The long-tail popularity of models\nand their long idle periods present opportunities to improve utilization\nthrough GPU sharing. However, existing GPU sharing systems lack the ability to\nadjust their resource allocation and sharing policies at runtime, making them\nineffective at meeting latency service-level objectives (SLOs) under rapidly\nfluctuating workloads.\n  This paper presents Prism, a multi-LLM serving system that unleashes the full\npotential of GPU sharing to achieve both cost efficiency and SLO attainment. At\nits core, Prism tackles a key limitation of existing\nsystems$\\unicode{x2014}$the lack of $\\textit{cross-model memory coordination}$,\nwhich is essential for flexibly sharing GPU memory across models under dynamic\nworkloads. Prism achieves this with two key designs. First, it supports\non-demand memory allocation by dynamically mapping physical to virtual memory\npages, allowing flexible memory redistribution among models that space- and\ntime-share a GPU. Second, it improves memory efficiency through a two-level\nscheduling policy that dynamically adjusts sharing strategies based on models'\nruntime demands. Evaluations on real-world traces show that Prism achieves more\nthan $2\\times$ cost savings and $3.3\\times$ SLO attainment compared to\nstate-of-the-art systems.\n","authors":["Shan Yu","Jiarong Xing","Yifan Qiao","Mingyuan Ma","Yangmin Li","Yang Wang","Shuo Yang","Zhiqiang Xie","Shiyi Cao","Ke Bao","Ion Stoica","Harry Xu","Ying Sheng"],"pdf_url":"https://arxiv.org/pdf/2505.04021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04019v1","updated":"2025-05-06T23:32:16Z","published":"2025-05-06T23:32:16Z","title":"Extending Decision Predicate Graphs for Comprehensive Explanation of\n  Isolation Forest","summary":"  The need to explain predictive models is well-established in modern machine\nlearning. However, beyond model interpretability, understanding pre-processing\nmethods is equally essential. Understanding how data modifications impact model\nperformance improvements and potential biases and promoting a reliable pipeline\nis mandatory for developing robust machine learning solutions. Isolation Forest\n(iForest) is a widely used technique for outlier detection that performs well.\nIts effectiveness increases with the number of tree-based learners. However,\nthis also complicates the explanation of outlier selection and the decision\nboundaries for inliers. This research introduces a novel Explainable AI (XAI)\nmethod, tackling the problem of global explainability. In detail, it aims to\noffer a global explanation for outlier detection to address its opaque nature.\nOur approach is based on the Decision Predicate Graph (DPG), which clarifies\nthe logic of ensemble methods and provides both insights and a graph-based\nmetric to explain how samples are identified as outliers using the proposed\nInlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's\nexplainability and provides a comprehensive view of the decision-making\nprocess, detailing which features contribute to outlier identification and how\nthe model utilizes them. This method advances the state-of-the-art by providing\ninsights into decision boundaries and a comprehensive view of holistic feature\nusage in outlier identification. -- thus promoting a fully explainable machine\nlearning pipeline.\n","authors":["Matteo Ceschin","Leonardo Arrighi","Luca Longo","Sylvio Barbon Junior"],"pdf_url":"https://arxiv.org/pdf/2505.04019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04016v1","updated":"2025-05-06T23:29:43Z","published":"2025-05-06T23:29:43Z","title":"SLOT: Structuring the Output of Large Language Models","summary":"  Structured outputs are essential for large language models (LLMs) in critical\napplications like agents and information extraction. Despite their\ncapabilities, LLMs often generate outputs that deviate from predefined schemas,\nsignificantly hampering reliable application development. We present SLOT\n(Structured LLM Output Transformer), a model-agnostic approach that transforms\nunstructured LLM outputs into precise structured formats. While existing\nsolutions predominantly rely on constrained decoding techniques or are tightly\ncoupled with specific models, SLOT employs a fine-tuned lightweight language\nmodel as a post-processing layer, achieving flexibility across various LLMs and\nschema specifications. We introduce a systematic pipeline for data curation and\nsynthesis alongside a formal evaluation methodology that quantifies both schema\naccuracy and content fidelity. Our results demonstrate that fine-tuned\nMistral-7B model with constrained decoding achieves near perfect schema\naccuracy (99.5%) and content similarity (94.0%), outperforming\nClaude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,\nrespectively). Notably, even compact models like Llama-3.2-1B can match or\nexceed the structured output capabilities of much larger proprietary models\nwhen equipped with SLOT, enabling reliable structured generation in\nresource-constrained environments.\n","authors":["Darren Yow-Bang Wang","Zhengyuan Shen","Soumya Smruti Mishra","Zhichao Xu","Yifei Teng","Haibo Ding"],"pdf_url":"https://arxiv.org/pdf/2505.04016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04015v1","updated":"2025-05-06T23:26:25Z","published":"2025-05-06T23:26:25Z","title":"MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning\n  Models","summary":"  This paper proposes MergeGuard, a novel methodology for mitigation of AI\nTrojan attacks. Trojan attacks on AI models cause inputs embedded with triggers\nto be misclassified to an adversary's target class, posing a significant threat\nto model usability trained by an untrusted third party. The core of MergeGuard\nis a new post-training methodology for linearizing and merging fully connected\nlayers which we show simultaneously improves model generalizability and\nperformance. Our Proof of Concept evaluation on Transformer models demonstrates\nthat MergeGuard maintains model accuracy while decreasing trojan attack success\nrate, outperforming commonly used (post-training) Trojan mitigation by\nfine-tuning methodologies.\n","authors":["Soheil Zibakhsh Shabgahi","Yaman Jandali","Farinaz Koushanfar"],"pdf_url":"https://arxiv.org/pdf/2505.04015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04002v1","updated":"2025-05-06T22:29:07Z","published":"2025-05-06T22:29:07Z","title":"PARC: Physics-based Augmentation with Reinforcement Learning for\n  Character Controllers","summary":"  Humans excel in navigating diverse, complex environments with agile motor\nskills, exemplified by parkour practitioners performing dynamic maneuvers, such\nas climbing up walls and jumping across gaps. Reproducing these agile movements\nwith simulated characters remains challenging, in part due to the scarcity of\nmotion capture data for agile terrain traversal behaviors and the high cost of\nacquiring such data. In this work, we introduce PARC (Physics-based\nAugmentation with Reinforcement Learning for Character Controllers), a\nframework that leverages machine learning and physics-based simulation to\niteratively augment motion datasets and expand the capabilities of terrain\ntraversal controllers. PARC begins by training a motion generator on a small\ndataset consisting of core terrain traversal skills. The motion generator is\nthen used to produce synthetic data for traversing new terrains. However, these\ngenerated motions often exhibit artifacts, such as incorrect contacts or\ndiscontinuities. To correct these artifacts, we train a physics-based tracking\ncontroller to imitate the motions in simulation. The corrected motions are then\nadded to the dataset, which is used to continue training the motion generator\nin the next iteration. PARC's iterative process jointly expands the\ncapabilities of the motion generator and tracker, creating agile and versatile\nmodels for interacting with complex environments. PARC provides an effective\napproach to develop controllers for agile terrain traversal, which bridges the\ngap between the scarcity of motion data and the need for versatile character\ncontrollers.\n","authors":["Michael Xu","Yi Shi","KangKang Yin","Xue Bin Peng"],"pdf_url":"https://arxiv.org/pdf/2505.04002v1.pdf","comment":"SIGGRAPH Conference Papers 2025"},{"id":"http://arxiv.org/abs/2504.21226v2","updated":"2025-05-06T22:01:50Z","published":"2025-04-29T23:41:06Z","title":"MemeBLIP2: A novel lightweight multimodal system to detect harmful memes","summary":"  Memes often merge visuals with brief text to share humor or opinions, yet\nsome memes contain harmful messages such as hate speech. In this paper, we\nintroduces MemeBLIP2, a light weight multimodal system that detects harmful\nmemes by combining image and text features effectively. We build on previous\nstudies by adding modules that align image and text representations into a\nshared space and fuse them for better classification. Using BLIP-2 as the core\nvision-language model, our system is evaluated on the PrideMM datasets. The\nresults show that MemeBLIP2 can capture subtle cues in both modalities, even in\ncases with ironic or culturally specific content, thereby improving the\ndetection of harmful material.\n","authors":["Jiaqi Liu","Ran Tong","Aowei Shen","Shuzheng Li","Changlin Yang","Lisha Xu"],"pdf_url":"https://arxiv.org/pdf/2504.21226v2.pdf","comment":"11pages, 3 figures, manucripts in preparation"},{"id":"http://arxiv.org/abs/2505.03989v1","updated":"2025-05-06T21:53:44Z","published":"2025-05-06T21:53:44Z","title":"An alignment safety case sketch based on debate","summary":"  If AI systems match or exceed human capabilities on a wide range of tasks, it\nmay become difficult for humans to efficiently judge their actions -- making it\nhard to use human feedback to steer them towards desirable traits. One proposed\nsolution is to leverage another superhuman system to point out flaws in the\nsystem's outputs via a debate. This paper outlines the value of debate for AI\nsafety, as well as the assumptions and further research required to make debate\nwork. It does so by sketching an ``alignment safety case'' -- an argument that\nan AI system will not autonomously take actions which could lead to egregious\nharm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D\nagent inside an AI company sabotaging research, for example by producing false\nresults. To prevent this, the agent is trained via debate, subject to\nexploration guarantees, to teach the system to be honest. Honesty is maintained\nthroughout deployment via online training. The safety case rests on four key\nclaims: (1) the agent has become good at the debate game, (2) good performance\nin the debate game implies that the system is mostly honest, (3) the system\nwill not become significantly less honest during deployment, and (4) the\ndeployment context is tolerant of some errors. We identify open research\nproblems that, if solved, could render this a compelling argument that an AI\nsystem is safe.\n","authors":["Marie Davidsen Buhl","Jacob Pfau","Benjamin Hilton","Geoffrey Irving"],"pdf_url":"https://arxiv.org/pdf/2505.03989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.17669v2","updated":"2025-05-06T21:45:48Z","published":"2025-04-24T15:38:20Z","title":"Towards a HIPAA Compliant Agentic AI System in Healthcare","summary":"  Agentic AI systems powered by Large Language Models (LLMs) as their\nfoundational reasoning engine, are transforming clinical workflows such as\nmedical report generation and clinical summarization by autonomously analyzing\nsensitive healthcare data and executing decisions with minimal human oversight.\nHowever, their adoption demands strict compliance with regulatory frameworks\nsuch as Health Insurance Portability and Accountability Act (HIPAA),\nparticularly when handling Protected Health Information (PHI). This\nwork-in-progress paper introduces a HIPAA-compliant Agentic AI framework that\nenforces regulatory compliance through dynamic, context-aware policy\nenforcement. Our framework integrates three core mechanisms: (1)\nAttribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid\nPHI sanitization pipeline combining regex patterns and BERT-based model to\nminimize leakage, and (3) immutable audit trails for compliance verification.\n","authors":["Subash Neupane","Sudip Mittal","Shahram Rahimi"],"pdf_url":"https://arxiv.org/pdf/2504.17669v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03988v1","updated":"2025-05-06T21:41:20Z","published":"2025-05-06T21:41:20Z","title":"Can Large Language Models Predict Parallel Code Performance?","summary":"  Accurate determination of the performance of parallel GPU code typically\nrequires execution-time profiling on target hardware -- an increasingly\nprohibitive step due to limited access to high-end GPUs. This paper explores\nwhether Large Language Models (LLMs) can offer an alternative approach for GPU\nperformance prediction without relying on hardware. We frame the problem as a\nroofline classification task: given the source code of a GPU kernel and the\nhardware specifications of a target GPU, can an LLM predict whether the GPU\nkernel is compute-bound or bandwidth-bound?\n  For this study, we build a balanced dataset of 340 GPU kernels, obtained from\nHeCBench benchmark and written in CUDA and OpenMP, along with their\nground-truth labels obtained via empirical GPU profiling. We evaluate LLMs\nacross four scenarios: (1) with access to profiling data of the kernel source,\n(2) zero-shot with source code only, (3) few-shot with code and label pairs,\nand (4) fine-tuned on a small custom dataset.\n  Our results show that state-of-the-art LLMs have a strong understanding of\nthe Roofline model, achieving 100% classification accuracy when provided with\nexplicit profiling data. We also find that reasoning-capable LLMs significantly\noutperform standard LLMs in zero- and few-shot settings, achieving up to 64%\naccuracy on GPU source codes, without profiling information. Lastly, we find\nthat LLM fine-tuning will require much more data than what we currently have\navailable.\n  This work is among the first to use LLMs for source-level roofline\nperformance prediction via classification, and illustrates their potential to\nguide optimization efforts when runtime profiling is infeasible. Our findings\nsuggest that with better datasets and prompt strategies, LLMs could become\npractical tools for HPC performance analysis and performance portability.\n","authors":["Gregory Bolet","Giorgis Georgakoudis","Harshitha Menon","Konstantinos Parasyris","Niranjan Hasabnis","Hayden Estes","Kirk W. Cameron","Gal Oren"],"pdf_url":"https://arxiv.org/pdf/2505.03988v1.pdf","comment":"5 pages, 4 figures, accepted to AI4Sys Workshop at HPDC 2025"},{"id":"http://arxiv.org/abs/2505.03985v1","updated":"2025-05-06T21:27:07Z","published":"2025-05-06T21:27:07Z","title":"LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach\n  with Large Language Models Integration","summary":"  Emergency response services are critical to public safety, with 9-1-1\ncall-takers playing a key role in ensuring timely and effective emergency\noperations. To ensure call-taking performance consistency, quality assurance is\nimplemented to evaluate and refine call-takers' skillsets. However, traditional\nhuman-led evaluations struggle with high call volumes, leading to low coverage\nand delayed assessments. We introduce LogiDebrief, an AI-driven framework that\nautomates traditional 9-1-1 call debriefing by integrating Signal-Temporal\nLogic (STL) with Large Language Models (LLMs) for fully-covered rigorous\nperformance evaluation. LogiDebrief formalizes call-taking requirements as\nlogical specifications, enabling systematic assessment of 9-1-1 calls against\nprocedural guidelines. It employs a three-step verification process: (1)\ncontextual understanding to identify responder types, incident classifications,\nand critical conditions; (2) STL-based runtime checking with LLM integration to\nensure compliance; and (3) automated aggregation of results into quality\nassurance reports. Beyond its technical contributions, LogiDebrief has\ndemonstrated real-world impact. Successfully deployed at Metro Nashville\nDepartment of Emergency Communications, it has assisted in debriefing 1,701\nreal-world calls, saving 311.85 hours of active engagement. Empirical\nevaluation with real-world data confirms its accuracy, while a case study and\nextensive user study highlight its effectiveness in enhancing call-taking\nperformance.\n","authors":["Zirong Chen","Ziyan An","Jennifer Reynolds","Kristin Mullen","Stephen Martini","Meiyi Ma"],"pdf_url":"https://arxiv.org/pdf/2505.03985v1.pdf","comment":"Accepted at IJCAI-2025"},{"id":"http://arxiv.org/abs/2505.03983v1","updated":"2025-05-06T21:10:37Z","published":"2025-05-06T21:10:37Z","title":"Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via\n  Autospeculation","summary":"  Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful\ntools for generative modeling. However, their sequential computation\nrequirements lead to significant inference-time bottlenecks. In this work, we\nutilize the connection between DDPMs and Stochastic Localization to prove that,\nunder an appropriate reparametrization, the increments of DDPM satisfy an\nexchangeability property. This general insight enables near-black-box\nadaptation of various performance optimization techniques from autoregressive\nmodels to the diffusion setting. To demonstrate this, we introduce\n\\emph{Autospeculative Decoding} (ASD), an extension of the widely used\nspeculative decoding algorithm to DDPMs that does not require any auxiliary\ndraft models. Our theoretical analysis shows that ASD achieves a $\\tilde{O}\n(K^{\\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM.\nWe also demonstrate that a practical implementation of autospeculative decoding\naccelerates DDPM inference significantly in various domains.\n","authors":["Hengyuan Hu","Aniket Das","Dorsa Sadigh","Nima Anari"],"pdf_url":"https://arxiv.org/pdf/2505.03983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03981v1","updated":"2025-05-06T21:08:27Z","published":"2025-05-06T21:08:27Z","title":"X-Reasoner: Towards Generalizable Reasoning Across Modalities and\n  Domains","summary":"  Recent proprietary models (e.g., o3) have begun to demonstrate strong\nmultimodal reasoning capabilities. Yet, most existing open-source research\nconcentrates on training text-only reasoning models, with evaluations limited\nto mainly mathematical and general-domain tasks. Therefore, it remains unclear\nhow to effectively extend reasoning capabilities beyond text input and general\ndomains. This paper explores a fundamental research question: Is reasoning\ngeneralizable across modalities and domains? Our findings support an\naffirmative answer: General-domain text-based post-training can enable such\nstrong generalizable reasoning. Leveraging this finding, we introduce\nX-Reasoner, a vision-language model post-trained solely on general-domain text\nfor generalizable reasoning, using a two-stage approach: an initial supervised\nfine-tuning phase with distilled long chain-of-thoughts, followed by\nreinforcement learning with verifiable rewards. Experiments show that\nX-Reasoner successfully transfers reasoning capabilities to both multimodal and\nout-of-domain settings, outperforming existing state-of-the-art models trained\nwith in-domain and multimodal data across various general and medical\nbenchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in\nspecialized domains can be further enhanced through continued training on\ndomain-specific text-only data. Building upon this, we introduce\nX-Reasoner-Med, a medical-specialized variant that achieves new state of the\nart on numerous text-only and multimodal medical benchmarks.\n","authors":["Qianchu Liu","Sheng Zhang","Guanghui Qin","Timothy Ossowski","Yu Gu","Ying Jin","Sid Kiblawi","Sam Preston","Mu Wei","Paul Vozila","Tristan Naumann","Hoifung Poon"],"pdf_url":"https://arxiv.org/pdf/2505.03981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08751v3","updated":"2025-05-06T20:54:29Z","published":"2023-09-15T20:27:47Z","title":"Diverse Audio Embeddings -- Bringing Features Back Outperforms CLAP!","summary":"  With the advent of modern AI architectures, a shift has happened towards\nend-to-end architectures. This pivot has led to neural architectures being\ntrained without domain-specific biases/knowledge, optimized according to the\ntask. We in this paper, learn audio embeddings via diverse feature\nrepresentations, in this case, domain-specific. For the case of audio\nclassification over hundreds of categories of sound, we learn robust separate\nembeddings for diverse audio properties such as pitch, timbre, and neural\nrepresentation, along with also learning it via an end-to-end architecture. We\nobserve handcrafted embeddings, e.g., pitch and timbre-based, although on their\nown, are not able to beat a fully end-to-end representation, yet adding these\ntogether with end-to-end embedding helps us, significantly improve performance.\nThis work would pave the way to bring some domain expertise with end-to-end\nmodels to learn robust, diverse representations, surpassing the performance of\njust training end-to-end models.\n","authors":["Prateek Verma"],"pdf_url":"https://arxiv.org/pdf/2309.08751v3.pdf","comment":"6 pages, 1 figure, 2 table"},{"id":"http://arxiv.org/abs/2505.03974v1","updated":"2025-05-06T20:52:58Z","published":"2025-05-06T20:52:58Z","title":"Deep Learning Framework for Infrastructure Maintenance: Crack Detection\n  and High-Resolution Imaging of Infrastructure Surfaces","summary":"  Recently, there has been an impetus for the application of cutting-edge data\ncollection platforms such as drones mounted with camera sensors for\ninfrastructure asset management. However, the sensor characteristics, proximity\nto the structure, hard-to-reach access, and environmental conditions often\nlimit the resolution of the datasets. A few studies used super-resolution\ntechniques to address the problem of low-resolution images. Nevertheless, these\ntechniques were observed to increase computational cost and false alarms of\ndistress detection due to the consideration of all the infrastructure images\ni.e., positive and negative distress classes. In order to address the\npre-processing of false alarm and achieve efficient super-resolution, this\nstudy developed a framework consisting of convolutional neural network (CNN)\nand efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately\nclassified both the classes. ESPCNN, which is the lightweight super-resolution\ntechnique, generated high-resolution infrastructure image of positive distress\nobtained from CNN. The ESPCNN outperformed bicubic interpolation in all the\nevaluation metrics for super-resolution. Based on the performance metrics, the\ncombination of CNN and ESPCNN was observed to be effective in preprocessing the\ninfrastructure images with negative distress, reducing the computational cost\nand false alarms in the next step of super-resolution. The visual inspection\nshowed that EPSCNN is able to capture crack propagation, complex geometry of\neven minor cracks. The proposed framework is expected to help the highway\nagencies in accurately performing distress detection and assist in efficient\nasset management practices.\n","authors":["Nikhil M. Pawar","Jorge A. Prozzi","Feng Hong","Surya Sarat Chandra Congress"],"pdf_url":"https://arxiv.org/pdf/2505.03974v1.pdf","comment":"Presented :Transportation Research Board 104th Annual Meeting,\n  Washington, D.C"},{"id":"http://arxiv.org/abs/2505.00661v2","updated":"2025-05-06T20:44:01Z","published":"2025-05-01T17:02:27Z","title":"On the generalization of language models from in-context learning and\n  finetuning: a controlled study","summary":"  Large language models exhibit exciting capabilities, yet can show\nsurprisingly narrow generalization from finetuning. E.g. they can fail to\ngeneralize to simple reversals of relations they are trained on, or fail to\nmake simple logical deductions based on trained information. These failures to\ngeneralize from fine-tuning can hinder practical application of these models.\nOn the other hand, language models' in-context learning shows different\ninductive biases, and can generalize better in some cases. Here, we explore\nthese differences in generalization between in-context- and fine-tuning-based\nlearning. To do so, we constructed several novel datasets to evaluate and\nimprove models' abilities to generalize from finetuning data. The datasets are\ndesigned to create clean tests of generalization, by isolating the knowledge in\nthe dataset from that in pretraining. We expose pretrained large models to\ncontrolled subsets of the information in these datasets -- either in context,\nor through fine-tuning -- and evaluate their performance on test sets that\nrequire various types of generalization. We find overall that in data-matched\nsettings, in-context learning can generalize more flexibly than fine-tuning\n(though we also find some qualifications of prior findings, such as cases when\nfine-tuning can generalize to reversals embedded in a larger structure of\nknowledge). We build on these findings to propose a method to enable improved\ngeneralization from fine-tuning: adding in-context inferences to finetuning\ndata. We show that this method improves generalization across various splits of\nour datasets and other benchmarks. Our results have implications for\nunderstanding the inductive biases of different modes of learning in language\nmodels, and practically improving their performance.\n","authors":["Andrew K. Lampinen","Arslan Chaudhry","Stephanie C. Y. Chan","Cody Wild","Diane Wan","Alex Ku","Jörg Bornschein","Razvan Pascanu","Murray Shanahan","James L. McClelland"],"pdf_url":"https://arxiv.org/pdf/2505.00661v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03961v1","updated":"2025-05-06T20:23:25Z","published":"2025-05-06T20:23:25Z","title":"The Power of Stories: Narrative Priming Shapes How LLM Agents\n  Collaborate and Compete","summary":"  According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment.\n","authors":["Gerrit Großmann","Larisa Ivanova","Sai Leela Poduru","Mohaddeseh Tabrizian","Islam Mesabah","David A. Selby","Sebastian J. Vollmer"],"pdf_url":"https://arxiv.org/pdf/2505.03961v1.pdf","comment":"16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents"},{"id":"http://arxiv.org/abs/2505.03947v1","updated":"2025-05-06T19:51:41Z","published":"2025-05-06T19:51:41Z","title":"Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents","summary":"  One of the primary aspirations in reinforcement learning research is\ndeveloping general-purpose agents capable of rapidly adapting to and mastering\nnovel tasks. While RL gaming agents have mastered many Atari games, they remain\nslow and costly to train for each game. In this work, we demonstrate that\nlatest reasoning LLMs with out-of-domain RL post-training can play a\nchallenging Atari game called Frogger under a zero-shot setting. We then\ninvestigate the effect of in-context learning and the amount of reasoning\neffort on LLM performance. Lastly, we demonstrate a way to bootstrap\ntraditional RL method with LLM demonstrations, which significantly improves\ntheir performance and sample efficiency. Our implementation is open sourced at\nhttps://github.com/AlienKevin/frogger.\n","authors":["Xiang Li","Yiyang Hao","Doug Fulop"],"pdf_url":"https://arxiv.org/pdf/2505.03947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03946v1","updated":"2025-05-06T19:50:37Z","published":"2025-05-06T19:50:37Z","title":"Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High\n  Performance Computing Scheduling on Multi-User Systems","summary":"  Resource allocation in High Performance Computing (HPC) environments presents\na complex and multifaceted challenge for job scheduling algorithms. Beyond the\nefficient allocation of system resources, schedulers must account for and\noptimize multiple performance metrics, including job wait time and system\nutilization. While traditional rule-based scheduling algorithms dominate the\ncurrent deployments of HPC systems, the increasing heterogeneity and scale of\nthose systems is expected to challenge the efficiency and flexibility of those\nalgorithms in minimizing job wait time and maximizing utilization. Recent\nresearch efforts have focused on leveraging advancements in Reinforcement\nLearning (RL) to develop more adaptable and intelligent scheduling strategies.\nRecent RL-based scheduling approaches have explored a range of algorithms, from\nDeep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently,\nhybrid methods that integrate Graph Neural Networks with RL techniques.\nHowever, a common limitation across these methods is their reliance on\nrelatively small datasets, and these methods face scalability issues when using\nlarge datasets. This study introduces a novel RL-based scheduler utilizing the\nDecentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm,\nwhich supports large-scale distributed training across multiple workers without\nrequiring parameter synchronization at every step. By eliminating reliance on\ncentralized updates to a shared policy, the DD-PPO scheduler enhances\nscalability, training efficiency, and sample utilization. The validation\ndataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO\nperformance between traditional and advanced scheduling approaches, and the\nexperimental results demonstrate improved scheduling performance in comparison\nto both rule-based schedulers and existing RL-based scheduling algorithms.\n","authors":["Matthew Sgambati","Aleksandar Vakanski","Matthew Anderson"],"pdf_url":"https://arxiv.org/pdf/2505.03946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03945v1","updated":"2025-05-06T19:45:13Z","published":"2025-05-06T19:45:13Z","title":"AI-Driven Security in Cloud Computing: Enhancing Threat Detection,\n  Automated Response, and Cyber Resilience","summary":"  Cloud security concerns have been greatly realized in recent years due to the\nincrease of complicated threats in the computing world. Many traditional\nsolutions do not work well in real-time to detect or prevent more complex\nthreats. Artificial intelligence is today regarded as a revolution in\ndetermining a protection plan for cloud data architecture through machine\nlearning, statistical visualization of computing infrastructure, and detection\nof security breaches followed by counteraction. These AI-enabled systems make\nwork easier as more network activities are scrutinized, and any anomalous\nbehavior that might be a precursor to a more serious breach is prevented. This\npaper examines ways AI can enhance cloud security by applying predictive\nanalytics, behavior-based security threat detection, and AI-stirring\nencryption. It also outlines the problems of the previous security models and\nhow AI overcomes them. For a similar reason, issues like data privacy, biases\nin the AI model, and regulatory compliance are also covered. So, AI improves\nthe protection of cloud computing contexts; however, more efforts are needed in\nthe subsequent phases to extend the technology's reliability, modularity, and\nethical aspects. This means that AI can be blended with other new computing\ntechnologies, including blockchain, to improve security frameworks further. The\npaper discusses the current trends in securing cloud data architecture using AI\nand presents further research and application directions.\n","authors":["Shamnad Mohamed Shaffi","Sunish Vengathattil","Jezeena Nikarthil Sidhick","Resmi Vijayan"],"pdf_url":"https://arxiv.org/pdf/2505.03945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03941v1","updated":"2025-05-06T19:38:07Z","published":"2025-05-06T19:38:07Z","title":"GRAML: Dynamic Goal Recognition As Metric Learning","summary":"  Goal Recognition (GR) is the problem of recognizing an agent's objectives\nbased on observed actions. Recent data-driven approaches for GR alleviate the\nneed for costly, manually crafted domain models. However, these approaches can\nonly reason about a pre-defined set of goals, and time-consuming training is\nneeded for new emerging goals. To keep this model-learning automated while\nenabling quick adaptation to new goals, this paper introduces GRAML: Goal\nRecognition As Metric Learning. GRAML uses a Siamese network to treat GR as a\ndeep metric learning task, employing an RNN that learns a metric over an\nembedding space, where the embeddings for observation traces leading to\ndifferent goals are distant, and embeddings of traces leading to the same goals\nare close. This metric is especially useful when adapting to new goals, even if\ngiven just one example observation trace per goal. Evaluated on a versatile set\nof environments, GRAML shows speed, flexibility, and runtime improvements over\nthe state-of-the-art GR while maintaining accurate recognition.\n","authors":["Matan Shamir","Reuth Mirsky"],"pdf_url":"https://arxiv.org/pdf/2505.03941v1.pdf","comment":"Accepted for publication in International Joint Conference on\n  Artificial Intelligence (IJCAI) 2025"},{"id":"http://arxiv.org/abs/2505.03899v1","updated":"2025-05-06T18:09:54Z","published":"2025-05-06T18:09:54Z","title":"A Graphical Global Optimization Framework for Parameter Estimation of\n  Statistical Models with Nonconvex Regularization Functions","summary":"  Optimization problems with norm-bounding constraints arise in a variety of\napplications, including portfolio optimization, machine learning, and feature\nselection. A common approach to these problems involves relaxing the norm\nconstraint via Lagrangian relaxation, transforming it into a regularization\nterm in the objective function. A particularly challenging class includes the\nzero-norm function, which promotes sparsity in statistical parameter\nestimation. Most existing exact methods for solving these problems introduce\nbinary variables and artificial bounds to reformulate them as\nhigher-dimensional mixed-integer programs, solvable by standard solvers. Other\nexact approaches exploit specific structural properties of the objective,\nmaking them difficult to generalize across different problem types. Alternative\nmethods employ nonconvex penalties with favorable statistical characteristics,\nbut these are typically addressed using heuristic or local optimization\ntechniques due to their structural complexity. In this paper, we propose a\nnovel graph-based method to globally solve optimization problems involving\ngeneralized norm-bounding constraints. Our approach encompasses standard\n$\\ell_p$-norms for $p \\in [0, \\infty)$ and nonconvex penalties such as SCAD and\nMCP. We leverage decision diagrams to construct strong convex relaxations\ndirectly in the original variable space, eliminating the need for auxiliary\nvariables or artificial bounds. Integrated into a spatial branch-and-cut\nframework, our method guarantees convergence to the global optimum. We\ndemonstrate its effectiveness through preliminary computational experiments on\nbenchmark sparse linear regression problems involving complex nonconvex\npenalties, which are not tractable using existing global optimization\ntechniques.\n","authors":["Danial Davarnia","Mohammadreza Kiaghadi"],"pdf_url":"https://arxiv.org/pdf/2505.03899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03896v1","updated":"2025-05-06T18:03:41Z","published":"2025-05-06T18:03:41Z","title":"Novel Extraction of Discriminative Fine-Grained Feature to Improve\n  Retinal Vessel Segmentation","summary":"  Retinal vessel segmentation is a vital early detection method for several\nsevere ocular diseases. Despite significant progress in retinal vessel\nsegmentation with the advancement of Neural Networks, there are still\nchallenges to overcome. Specifically, retinal vessel segmentation aims to\npredict the class label for every pixel within a fundus image, with a primary\nfocus on intra-image discrimination, making it vital for models to extract more\ndiscriminative features. Nevertheless, existing methods primarily focus on\nminimizing the difference between the output from the decoder and the label,\nbut ignore fully using feature-level fine-grained representations from the\nencoder. To address these issues, we propose a novel Attention U-shaped\nKolmogorov-Arnold Network named AttUKAN along with a novel Label-guided\nPixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we\nimplement Attention Gates into Kolmogorov-Arnold Networks to enhance model\nsensitivity by suppressing irrelevant feature activations and model\ninterpretability by non-linear modeling of KAN blocks. Additionally, we also\ndesign a novel Label-guided Pixel-wise Contrastive Loss to supervise our\nproposed AttUKAN to extract more discriminative features by distinguishing\nbetween foreground vessel-pixel pairs and background pairs. Experiments are\nconducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF\nand our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,\n80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and\n66.94% in the above datasets, which are the highest compared to 11 networks for\nretinal vessel segmentation. Quantitative and qualitative results show that our\nAttUKAN achieves state-of-the-art performance and outperforms existing retinal\nvessel segmentation methods. Our code will be available at\nhttps://github.com/stevezs315/AttUKAN.\n","authors":["Shuang Zeng","Chee Hong Lee","Micky C Nnamdi","Wenqi Shi","J Ben Tamo","Lei Zhu","Hangzhou He","Xinliang Zhang","Qian Chen","May D. Wang","Yanye Lu","Qiushi Ren"],"pdf_url":"https://arxiv.org/pdf/2505.03896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03867v1","updated":"2025-05-06T17:13:29Z","published":"2025-05-06T17:13:29Z","title":"Scratch Copilot: Supporting Youth Creative Coding with AI","summary":"  Creative coding platforms like Scratch have democratized programming for\nchildren, yet translating imaginative ideas into functional code remains a\nsignificant hurdle for many young learners. While AI copilots assist adult\nprogrammers, few tools target children in block-based environments. Building on\nprior research \\cite{druga_how_2021,druga2023ai, druga2023scratch}, we present\nCognimates Scratch Copilot: an AI-powered assistant integrated into a\nScratch-like environment, providing real-time support for ideation, code\ngeneration, debugging, and asset creation. This paper details the system\narchitecture and findings from an exploratory qualitative evaluation with 18\ninternational children (ages 7--12). Our analysis reveals how the AI Copilot\nsupported key creative coding processes, particularly aiding ideation and\ndebugging. Crucially, it also highlights how children actively negotiated the\nuse of AI, demonstrating strong agency by adapting or rejecting suggestions to\nmaintain creative control. Interactions surfaced design tensions between\nproviding helpful scaffolding and fostering independent problem-solving, as\nwell as learning opportunities arising from navigating AI limitations and\nerrors. Findings indicate Cognimates Scratch Copilot's potential to enhance\ncreative self-efficacy and engagement. Based on these insights, we propose\ninitial design guidelines for AI coding assistants that prioritize youth agency\nand critical interaction alongside supportive scaffolding.\n","authors":["Stefania Druga","Amy J. Ko"],"pdf_url":"https://arxiv.org/pdf/2505.03867v1.pdf","comment":"5 figures, 14 pages"},{"id":"http://arxiv.org/abs/2505.03864v1","updated":"2025-05-06T16:40:39Z","published":"2025-05-06T16:40:39Z","title":"From Glue-Code to Protocols: A Critical Analysis of A2A and MCP\n  Integration for Scalable Agent Systems","summary":"  Artificial intelligence is rapidly evolving towards multi-agent systems where\nnumerous AI agents collaborate and interact with external tools. Two key open\nstandards, Google's Agent to Agent (A2A) protocol for inter-agent communication\nand Anthropic's Model Context Protocol (MCP) for standardized tool access,\npromise to overcome the limitations of fragmented, custom integration\napproaches. While their potential synergy is significant, this paper argues\nthat effectively integrating A2A and MCP presents unique, emergent challenges\nat their intersection, particularly concerning semantic interoperability\nbetween agent tasks and tool capabilities, the compounded security risks\narising from combined discovery and execution, and the practical governance\nrequired for the envisioned \"Agent Economy\". This work provides a critical\nanalysis, moving beyond a survey to evaluate the practical implications and\ninherent difficulties of combining these horizontal and vertical integration\nstandards. We examine the benefits (e.g., specialization, scalability) while\ncritically assessing their dependencies and trade-offs in an integrated\ncontext. We identify key challenges increased by the integration, including\nnovel security vulnerabilities, privacy complexities, debugging difficulties\nacross protocols, and the need for robust semantic negotiation mechanisms. In\nsummary, A2A+MCP offers a vital architectural foundation, but fully realizing\nits potential requires substantial advancements to manage the complexities of\ntheir combined operation.\n","authors":["Qiaomu Li","Ying Xie"],"pdf_url":"https://arxiv.org/pdf/2505.03864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03863v1","updated":"2025-05-06T16:33:06Z","published":"2025-05-06T16:33:06Z","title":"Data-Driven Falsification of Cyber-Physical Systems","summary":"  Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as\nhealthcare, avionics, and autonomous vehicles. Formal verification of their\noperational safety is, therefore, of utmost importance. In this paper, we\naddress the falsification problem, where the focus is on searching for an\nunsafe execution in the system instead of proving their absence. The\ncontribution of this paper is a framework that (a) connects the falsification\nof CPS with the falsification of deep neural networks (DNNs) and (b) leverages\nthe inherent interpretability of Decision Trees for faster falsification of\nCPS. This is achieved by: (1) building a surrogate model of the CPS under test,\neither as a DNN model or a Decision Tree, (2) application of various DNN\nfalsification tools to falsify CPS, and (3) a novel falsification algorithm\nguided by the explanations of safety violations of the CPS model extracted from\nits Decision Tree surrogate. The proposed framework has the potential to\nexploit a repertoire of \\emph{adversarial attack} algorithms designed to\nfalsify robustness properties of DNNs, as well as state-of-the-art\nfalsification algorithms for DNNs. Although the presented methodology is\napplicable to systems that can be executed/simulated in general, we demonstrate\nits effectiveness, particularly in CPS. We show that our framework, implemented\nas a tool \\textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS\nthat have linear and non-linear dynamics. Decision tree-guided falsification\nshows promising results in efficiently finding multiple counterexamples in the\nARCH-COMP 2024 falsification benchmarks~\\cite{khandait2024arch}.\n","authors":["Atanu Kundu","Sauvik Gon","Rajarshi Ray"],"pdf_url":"https://arxiv.org/pdf/2505.03863v1.pdf","comment":null}]},"2025-05-07T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2505.04619v1","updated":"2025-05-07T17:59:28Z","published":"2025-05-07T17:59:28Z","title":"Merging and Disentangling Views in Visual Reinforcement Learning for\n  Robotic Manipulation","summary":"  Vision is well-known for its use in manipulation, especially using visual\nservoing. To make it robust, multiple cameras are needed to expand the field of\nview. That is computationally challenging. Merging multiple views and using\nQ-learning allows the design of more effective representations and optimization\nof sample efficiency. Such a solution might be expensive to deploy. To mitigate\nthis, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently\nmerges views to increase sample efficiency while augmenting with single-view\nfeatures to allow lightweight deployment and ensure robust policies. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad\n","authors":["Abdulaziz Almuzairee","Rohan Patil","Dwait Bhatt","Henrik I. Christensen"],"pdf_url":"https://arxiv.org/pdf/2505.04619v1.pdf","comment":"For project website and code, see https://aalmuzairee.github.io/mad"},{"id":"http://arxiv.org/abs/2505.04583v1","updated":"2025-05-07T17:21:45Z","published":"2025-05-07T17:21:45Z","title":"Modeling Personalized Difficulty of Rehabilitation Exercises Using\n  Causal Trees","summary":"  Rehabilitation robots are often used in game-like interactions for\nrehabilitation to increase a person's motivation to complete rehabilitation\nexercises. By adjusting exercise difficulty for a specific user throughout the\nexercise interaction, robots can maximize both the user's rehabilitation\noutcomes and the their motivation throughout the exercise. Previous approaches\nhave assumed exercises have generic difficulty values that apply to all users\nequally, however, we identified that stroke survivors have varied and unique\nperceptions of exercise difficulty. For example, some stroke survivors found\nreaching vertically more difficult than reaching farther but lower while others\nfound reaching farther more challenging than reaching vertically. In this\npaper, we formulate a causal tree-based method to calculate exercise difficulty\nbased on the user's performance. We find that this approach accurately models\nexercise difficulty and provides a readily interpretable model of why that\nexercise is difficult for both users and caretakers.\n","authors":["Nathaniel Dennler","Zhonghao Shi","Uksang Yoo","Stefanos Nikolaidis","Maja Matarić"],"pdf_url":"https://arxiv.org/pdf/2505.04583v1.pdf","comment":"Accepted to IEEE/RAS-EMBS International Conference on Rehabilitation\n  Robotics (ICORR 2025)"},{"id":"http://arxiv.org/abs/2505.04572v1","updated":"2025-05-07T17:07:09Z","published":"2025-05-07T17:07:09Z","title":"Stow: Robotic Packing of Items into Fabric Pods","summary":"  This paper presents a compliant manipulation system capable of placing items\nonto densely packed shelves. The wide diversity of items and strict business\nrequirements for high producing rates and low defect generation have prohibited\nwarehouse robotics from performing this task. Our innovations in hardware,\nperception, decision-making, motion planning, and control have enabled this\nsystem to perform over 500,000 stows in a large e-commerce fulfillment center.\nThe system achieves human levels of packing density and speed while\nprioritizing work on overhead shelves to enhance the safety of humans working\nalongside the robots.\n","authors":["Nicolas Hudson","Josh Hooks","Rahul Warrier","Curt Salisbury","Ross Hartley","Kislay Kumar","Bhavana Chandrashekhar","Paul Birkmeyer","Bosch Tang","Matt Frost","Shantanu Thakar","Tony Piaskowy","Petter Nilsson","Josh Petersen","Neel Doshi","Alan Slatter","Ankit Bhatia","Cassie Meeker","Yuechuan Xue","Dylan Cox","Alex Kyriazis","Bai Lou","Nadeem Hasan","Asif Rana","Nikhil Chacko","Ruinian Xu","Siamak Faal","Esi Seraj","Mudit Agrawal","Kevin Jamieson","Alessio Bisagni","Valerie Samzun","Christine Fuller","Alex Keklak","Alex Frenkel","Lillian Ratliff","Aaron Parness"],"pdf_url":"https://arxiv.org/pdf/2505.04572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12616v2","updated":"2025-05-07T17:02:51Z","published":"2025-04-17T03:43:20Z","title":"Graph-based Path Planning with Dynamic Obstacle Avoidance for Autonomous\n  Parking","summary":"  Safe and efficient path planning in parking scenarios presents a significant\nchallenge due to the presence of cluttered environments filled with static and\ndynamic obstacles. To address this, we propose a novel and computationally\nefficient planning strategy that seamlessly integrates the predictions of\ndynamic obstacles into the planning process, ensuring the generation of\ncollision-free paths. Our approach builds upon the conventional Hybrid A star\nalgorithm by introducing a time-indexed variant that explicitly accounts for\nthe predictions of dynamic obstacles during node exploration in the graph, thus\nenabling dynamic obstacle avoidance. We integrate the time-indexed Hybrid A\nstar algorithm within an online planning framework to compute local paths at\neach planning step, guided by an adaptively chosen intermediate goal. The\nproposed method is validated in diverse parking scenarios, including\nperpendicular, angled, and parallel parking. Through simulations, we showcase\nour approach's potential in greatly improving the efficiency and safety when\ncompared to the state of the art spline-based planning method for parking\nsituations.\n","authors":["Farhad Nawaz","Minjun Sung","Darshan Gadginmath","Jovin D'sa","Sangjae Bae","David Isele","Nadia Figueroa","Nikolai Matni","Faizan M. Tariq"],"pdf_url":"https://arxiv.org/pdf/2504.12616v2.pdf","comment":"IEEE Intelligent Vehicles Symposium 2025"},{"id":"http://arxiv.org/abs/2505.04565v1","updated":"2025-05-07T16:57:51Z","published":"2025-05-07T16:57:51Z","title":"Hierarchical Task Decomposition for Execution Monitoring and Error\n  Recovery: Understanding the Rationale Behind Task Demonstrations","summary":"  Multi-step manipulation tasks where robots interact with their environment\nand must apply process forces based on the perceived situation remain\nchallenging to learn and prone to execution errors. Accurately simulating these\ntasks is also difficult. Hence, it is crucial for robust task performance to\nlearn how to coordinate end-effector pose and applied force, monitor execution,\nand react to deviations. To address these challenges, we propose a learning\napproach that directly infers both low- and high-level task representations\nfrom user demonstrations on the real system. We developed an unsupervised task\nsegmentation algorithm that combines intention recognition and feature\nclustering to infer the skills of a task. We leverage the inferred\ncharacteristic features of each skill in a novel unsupervised anomaly detection\napproach to identify deviations from the intended task execution. Together,\nthese components form a comprehensive framework capable of incrementally\nlearning task decisions and new behaviors as new situations arise. Compared to\nstate-of-the-art learning techniques, our approach significantly reduces the\nrequired amount of training data and computational complexity while efficiently\nlearning complex in-contact behaviors and recovery strategies. Our proposed\ntask segmentation and anomaly detection approaches outperform state-of-the-art\nmethods on force-based tasks evaluated on two different robotic systems.\n","authors":["Christoph Willibald","Dongheui Lee"],"pdf_url":"https://arxiv.org/pdf/2505.04565v1.pdf","comment":"The paper has been accepted for publication by the International\n  Journal of Robotics Research (IJRR), 26 pages"},{"id":"http://arxiv.org/abs/2505.04548v1","updated":"2025-05-07T16:30:56Z","published":"2025-05-07T16:30:56Z","title":"Accelerating Audio Research with Robotic Dummy Heads","summary":"  This work introduces a robotic dummy head that fuses the acoustic realism of\nconventional audiological mannequins with the mobility of robots. The proposed\ndevice is capable of moving, talking, and listening as people do, and can be\nused to automate spatially-stationary audio experiments, thus accelerating the\npace of audio research. Critically, the device may also be used as a moving\nsound source in dynamic experiments, due to its quiet motor. This feature\ndifferentiates our work from previous robotic acoustic research platforms.\nValidation that the robot enables high quality audio data collection is\nprovided through various experiments and acoustic measurements. These\nexperiments also demonstrate how the robot might be used to study adaptive\nbinaural beamforming. Design files are provided as open-source to stimulate\nnovel audio research.\n","authors":["Austin Lu","Kanad Sarkar","Yongjie Zhuang","Leo Lin","Ryan M Corey","Andrew C Singer"],"pdf_url":"https://arxiv.org/pdf/2505.04548v1.pdf","comment":"WASPAA 2025"},{"id":"http://arxiv.org/abs/2505.04493v1","updated":"2025-05-07T15:17:38Z","published":"2025-05-07T15:17:38Z","title":"Model-Based AI planning and Execution Systems for Robotics","summary":"  Model-based planning and execution systems offer a principled approach to\nbuilding flexible autonomous robots that can perform diverse tasks by\nautomatically combining a host of basic skills. This idea is almost as old as\nmodern robotics. Yet, while diverse general-purpose reasoning architectures\nhave been proposed since, general-purpose systems that are integrated with\nmodern robotic platforms have emerged only recently, starting with the\ninfluential ROSPlan system. Since then, a growing number of model-based systems\nfor robot task-level control have emerged. In this paper, we consider the\ndiverse design choices and issues existing systems attempt to address, the\ndifferent solutions proposed so far, and suggest avenues for future\ndevelopment.\n","authors":["Or Wertheim","Ronen I. Brafman"],"pdf_url":"https://arxiv.org/pdf/2505.04493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04491v1","updated":"2025-05-07T15:12:41Z","published":"2025-05-07T15:12:41Z","title":"Estimating Dynamic Soft Continuum Robot States From Boundaries","summary":"  Accurate state estimation is essential for effective control of robots. For\nsoft robots, this task is particularly challenging because their states are\ninherently infinite-dimensional functions due to the robots' continuous\ndeformability. Traditional sensing techniques, however, can only provide\ndiscrete measurements. Recently, a dynamic state estimation method known as a\nboundary observer was introduced, which leverages Cosserat rod theory to\nrecover all infinite-dimensional states by measuring only the velocity twist at\nthe robot's tip. In this work, we present a novel boundary observer that can\nalso recover infinite-dimensional dynamic states, but instead relies on\nmeasuring the internal wrench at the robot's base. This design exploits the\nduality between the velocity twist at the tip and the internal wrench at the\nbase, with both types of boundary observers being inspired by principles of\nenergy dissipation. Despite the mathematical duality, the proposed approach\noffers a distinct advantage: it requires only a 6-axis force/torque sensor\nembedded at the base, eliminating the need for external sensing systems such as\nmotion capture cameras. Moreover, combining both tip- and base-based techniques\nenhances energy dissipation, accelerates convergence, and improves estimation\naccuracy. We validate the proposed algorithms through both simulation studies\nand experiments based on tendon-driven continuum robots. Our results\ndemonstrate that all boundary observers converge to the ground truth within 3\nseconds, even with significantly deviated initial conditions. Furthermore, they\nrecover from unknown perturbations and effectively track high-frequency\nvibrations. We also show that combining the dual techniques further improves\nconvergence speed and accuracy. Finally, the computational efficiency of these\nalgorithms indicates their feasibility for real-time state estimation.\n","authors":["Tongjia Zheng","Jessica Burgner-Kahrs"],"pdf_url":"https://arxiv.org/pdf/2505.04491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04480v1","updated":"2025-05-07T14:51:43Z","published":"2025-05-07T14:51:43Z","title":"TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven\n  Evolution","summary":"  Trajectory prediction is a crucial task in modeling human behavior,\nespecially in fields as social robotics and autonomous vehicle navigation.\nTraditional heuristics based on handcrafted rules often lack accuracy, while\nrecently proposed deep learning approaches suffer from computational cost, lack\nof explainability, and generalization issues that limit their practical\nadoption. In this paper, we introduce TrajEvo, a framework that leverages Large\nLanguage Models (LLMs) to automatically design trajectory prediction\nheuristics. TrajEvo employs an evolutionary algorithm to generate and refine\nprediction heuristics from past trajectory data. We introduce a\nCross-Generation Elite Sampling to promote population diversity and a\nStatistics Feedback Loop allowing the LLM to analyze alternative predictions.\nOur evaluations show TrajEvo outperforms previous heuristic methods on the\nETH-UCY datasets, and remarkably outperforms both heuristics and deep learning\nmethods when generalizing to the unseen SDD dataset. TrajEvo represents a first\nstep toward automated design of fast, explainable, and generalizable trajectory\nprediction heuristics. We make our source code publicly available to foster\nfuture research at https://github.com/ai4co/trajevo.\n","authors":["Zhikai Zhao","Chuanbo Hua","Federico Berto","Kanghoon Lee","Zihan Ma","Jiachen Li","Jinkyoo Park"],"pdf_url":"https://arxiv.org/pdf/2505.04480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04438v1","updated":"2025-05-07T14:07:01Z","published":"2025-05-07T14:07:01Z","title":"Do We Still Need to Work on Odometry for Autonomous Driving?","summary":"  Over the past decades, a tremendous amount of work has addressed the topic of\nego-motion estimation of moving platforms based on various proprioceptive and\nexteroceptive sensors. At the cost of ever-increasing computational load and\nsensor complexity, odometry algorithms have reached impressive levels of\naccuracy with minimal drift in various conditions. In this paper, we question\nthe need for more research on odometry for autonomous driving by assessing the\naccuracy of one of the simplest algorithms: the direct integration of wheel\nencoder data and yaw rate measurements from a gyroscope. We denote this\nalgorithm as Odometer-Gyroscope (OG) odometry. This work shows that OG odometry\ncan outperform current state-of-the-art radar-inertial SE(2) odometry for a\nfraction of the computational cost in most scenarios. For example, the OG\nodometry is on top of the Boreas leaderboard with a relative translation error\nof 0.20%, while the second-best method displays an error of 0.26%.\nLidar-inertial approaches can provide more accurate estimates, but the\ncomputational load is three orders of magnitude higher than the OG odometry. To\nfurther the analysis, we have pushed the limits of the OG odometry by purposely\nviolating its fundamental no-slip assumption using data collected during a\nheavy snowstorm with different driving behaviours. Our conclusion shows that a\nsignificant amount of slippage is required to result in non-satisfactory pose\nestimates from the OG odometry.\n","authors":["Cedric Le Gentil","Daniil Lisus","Timothy D. Barfoot"],"pdf_url":"https://arxiv.org/pdf/2505.04438v1.pdf","comment":"ICRA 2025, Field Robotics workshop"},{"id":"http://arxiv.org/abs/2410.03072v2","updated":"2025-05-07T13:04:49Z","published":"2024-10-04T01:31:13Z","title":"Multi-Robot Motion Planning with Diffusion Models","summary":"  Diffusion models have recently been successfully applied to a wide range of\nrobotics applications for learning complex multi-modal behaviors from data.\nHowever, prior works have mostly been confined to single-robot and small-scale\nenvironments due to the high sample complexity of learning multi-robot\ndiffusion models. In this paper, we propose a method for generating\ncollision-free multi-robot trajectories that conform to underlying data\ndistributions while using only single-robot data. Our algorithm, Multi-robot\nMulti-model planning Diffusion (MMD), does so by combining learned diffusion\nmodels with classical search-based techniques -- generating data-driven motions\nunder collision constraints. Scaling further, we show how to compose multiple\ndiffusion models to plan in large environments where a single diffusion model\nfails to generalize well. We demonstrate the effectiveness of our approach in\nplanning for dozens of robots in a variety of simulated scenarios motivated by\nlogistics environments. View video demonstrations and code at:\nhttps://multi-robot-diffusion.github.io/.\n","authors":["Yorai Shaoul","Itamar Mishani","Shivam Vats","Jiaoyang Li","Maxim Likhachev"],"pdf_url":"https://arxiv.org/pdf/2410.03072v2.pdf","comment":"The first three authors contributed equally to this work. Published\n  at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.05939v2","updated":"2025-05-07T13:01:35Z","published":"2025-03-07T21:15:13Z","title":"Evaluation Framework for Sensor Configuration Impact on Deep\n  Learning-Based Perception","summary":"  Current research on automotive perception systems predominantly focusses on\neither improving the performance of sensor technology or enhancing the\nperception functions in isolation. High-level perception functions are\nincreasingly based on deep learning (DL) models due to their improved\nperformance and generalisability compared to traditional algorithms. Despite\nthe vital need to evaluate the performance of DL-based perception functions\nunder real-world conditions using onboard sensor inputs, there is a lack of\nframeworks to implement such systematic evaluations. This paper presents a\nversatile framework to evaluate the impact of perception sensor modalities and\nparameter settings on DL-based perception functions. Using a simulation\nenvironment, the framework facilitates sensor modality selection and parameter\ntuning under different operational design domain conditions. Its effectiveness\nis demonstrated through a case study involving a state-of-the-art surround\ntrajectory prediction model, highlighting performance differences across the\nsensor modalities radar and camera. Different settings for the parameter,\nhorizontal field of view (HFOV) were evaluated to identify the optimal\nconfiguration. The results indicate that a radar sensor with a narrow HFOV is\nthe most suitable configuration for the evaluated perception algorithm. The\nproposed framework offers a holistic approach to the design of the perception\nsensor suite, significantly contributing to the development of robust\nperception systems for automated driving systems.\n","authors":["A Gamage","V Donzella"],"pdf_url":"https://arxiv.org/pdf/2503.05939v2.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.04323v1","updated":"2025-05-07T11:12:58Z","published":"2025-05-07T11:12:58Z","title":"A Case Study on the Application of Digital Twins for Enhancing CPS\n  Operations","summary":"  To ensure the availability and reduce the downtime of complex cyber-physical\nsystems across different domains, e.g., agriculture and manufacturing, fault\ntolerance mechanisms are implemented which are complex in both their\ndevelopment and operation. In addition, cyber-physical systems are often\nconfronted with limited hardware resources or are legacy systems, both often\nhindering the addition of new functionalities directly on the onboard hardware.\nDigital Twins can be adopted to offload expensive computations, as well as\nproviding support through fault tolerance mechanisms, thus decreasing costs and\noperational downtime of cyber-physical systems. In this paper, we show the\nfeasibility of a Digital Twin used for enhancing cyber-physical system\noperations, specifically through functional augmentation and increased fault\ntolerance, in an industry-oriented use case.\n","authors":["Irina Muntean","Mirgita Frasheri","Tiziano Munaro"],"pdf_url":"https://arxiv.org/pdf/2505.04323v1.pdf","comment":"In Proceedings ASQAP 2025, arXiv:2505.02873"},{"id":"http://arxiv.org/abs/2505.04258v1","updated":"2025-05-07T09:03:26Z","published":"2025-05-07T09:03:26Z","title":"RGB-Event Fusion with Self-Attention for Collision Prediction","summary":"  Ensuring robust and real-time obstacle avoidance is critical for the safe\noperation of autonomous robots in dynamic, real-world environments. This paper\nproposes a neural network framework for predicting the time and collision\nposition of an unmanned aerial vehicle with a dynamic object, using RGB and\nevent-based vision sensors. The proposed architecture consists of two separate\nencoder branches, one for each modality, followed by fusion by self-attention\nto improve prediction accuracy. To facilitate benchmarking, we leverage the\nABCD [8] dataset collected that enables detailed comparisons of single-modality\nand fusion-based approaches. At the same prediction throughput of 50Hz, the\nexperimental results show that the fusion-based model offers an improvement in\nprediction accuracy over single-modality approaches of 1% on average and 10%\nfor distances beyond 0.5m, but comes at the cost of +71% in memory and + 105%\nin FLOPs. Notably, the event-based model outperforms the RGB model by 4% for\nposition and 26% for time error at a similar computational cost, making it a\ncompetitive alternative. Additionally, we evaluate quantized versions of the\nevent-based models, applying 1- to 8-bit quantization to assess the trade-offs\nbetween predictive performance and computational efficiency. These findings\nhighlight the trade-offs of multi-modal perception using RGB and event-based\ncameras in robotic applications.\n","authors":["Pietro Bonazzi","Christian Vogt","Michael Jost","Haotong Qin","Lyes Khacef","Federico Paredes-Valles","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2505.04258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04257v1","updated":"2025-05-07T09:02:09Z","published":"2025-05-07T09:02:09Z","title":"Automating Box Folding: Sequence Extraction and Ranking Methodologies","summary":"  Box folding represents a crucial challenge for automated packaging systems.\nThis work bridges the gap between existing methods for folding sequence\nextraction and approaches focused on the adaptability of automated systems to\nspecific box types. An innovative method is proposed to identify and rank\nfolding sequences, enabling the transformation of a box from an initial state\nto a desired final configuration. The system evaluates and ranks these\nsequences based on their feasibility and compatibility with available hardware,\nproviding recommendations for real-world implementations. Finally, an\nillustrative use case is presented, where a robot performs the folding of a\nbox.\n","authors":["Giuseppe Fabio Preziosa","Davide Ferloni","Andrea Maria Zanchettin","Marco Faroni","Paolo Rocco"],"pdf_url":"https://arxiv.org/pdf/2505.04257v1.pdf","comment":"Accepted at IFAC ROBOTICS 2025"},{"id":"http://arxiv.org/abs/2505.01709v2","updated":"2025-05-07T08:37:17Z","published":"2025-05-03T06:17:18Z","title":"RoBridge: A Hierarchical Architecture Bridging Cognition and Execution\n  for General Robotic Manipulation","summary":"  Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation.\n","authors":["Kaidong Zhang","Rongtao Xu","Pengzhen Ren","Junfan Lin","Hefeng Wu","Liang Lin","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2505.01709v2.pdf","comment":"project page: https://abliao.github.io/RoBridge/"},{"id":"http://arxiv.org/abs/2502.10156v3","updated":"2025-05-07T08:31:03Z","published":"2025-02-14T13:36:00Z","title":"MonoForce: Learnable Image-conditioned Physics Engine","summary":"  We propose a novel model for the prediction of robot trajectories on rough\noffroad terrain from the onboard camera images. This model enforces the laws of\nclassical mechanics through a physics-aware neural symbolic layer while\npreserving the ability to learn from large-scale data as it is end-to-end\ndifferentiable. The proposed hybrid model integrates a black-box component that\npredicts robot-terrain interaction forces with a neural-symbolic layer. This\nlayer includes a differentiable physics engine that computes the robot's\ntrajectory by querying these forces at the points of contact with the terrain.\nAs the proposed architecture comprises substantial geometrical and physics\npriors, the resulting model can also be seen as a learnable physics engine\nconditioned on real images that delivers $10^4$ trajectories per second. We\nargue and empirically demonstrate that this architecture reduces the\nsim-to-real gap and mitigates out-of-distribution sensitivity. The\ndifferentiability, in conjunction with the rapid simulation speed, makes the\nmodel well-suited for various applications including model predictive control,\ntrajectory shooting, supervised and reinforcement learning or SLAM. The codes\nand data are publicly available.\n","authors":["Ruslan Agishev","Karel Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2502.10156v3.pdf","comment":"Code: https://github.com/ctu-vras/monoforce"},{"id":"http://arxiv.org/abs/2505.04231v1","updated":"2025-05-07T08:27:52Z","published":"2025-05-07T08:27:52Z","title":"Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving\n  in Smart Intersections","summary":"  Unsignalized intersections pose significant safety and efficiency challenges\ndue to complex traffic flows. This paper proposes a novel roadside unit\n(RSU)-centric cooperative driving system leveraging global perception and\nvehicle-to-infrastructure (V2I) communication. The core of the system is an\nRSU-based decision-making module using a two-stage hybrid reinforcement\nlearning (RL) framework. At first, policies are pre-trained offline using\nconservative Q-learning (CQL) combined with behavior cloning (BC) on collected\ndataset. Subsequently, these policies are fine-tuned in the simulation using\nmulti-agent proximal policy optimization (MAPPO), aligned with a self-attention\nmechanism to effectively solve inter-agent dependencies. RSUs perform real-time\ninference based on the trained models to realize vehicle control via V2I\ncommunications. Extensive experiments in CARLA environment demonstrate high\neffectiveness of the proposed system, by: \\textit{(i)} achieving failure rates\nbelow 0.03\\% in coordinating three connected and autonomous vehicles (CAVs)\nthrough complex intersection scenarios, significantly outperforming the\ntraditional Autoware control method, and \\textit{(ii)} exhibiting strong\nrobustness across varying numbers of controlled agents and shows promising\ngeneralization capabilities on other maps.\n","authors":["Taoyuan Yu","Kui Wang","Zongdian Li","Tao Yu","Kei Sakaguchi"],"pdf_url":"https://arxiv.org/pdf/2505.04231v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2505.04228v1","updated":"2025-05-07T08:26:53Z","published":"2025-05-07T08:26:53Z","title":"Low Resolution Next Best View for Robot Packing","summary":"  Automating the packing of objects with robots is a key challenge in\nindustrial automation, where efficient object perception plays a fundamental\nrole. This paper focuses on scenarios where precise 3D reconstruction is not\nrequired, prioritizing cost-effective and scalable solutions. The proposed\nLow-Resolution Next Best View (LR-NBV) algorithm leverages a utility function\nthat balances pose redundancy and acquisition density, ensuring efficient\nobject reconstruction. Experimental validation demonstrates that LR-NBV\nconsistently outperforms standard NBV approaches, achieving comparable accuracy\nwith significantly fewer poses. This method proves highly suitable for\napplications requiring efficiency, scalability, and adaptability without\nrelying on high-precision sensing.\n","authors":["Giuseppe Fabio Preziosa","Chiara Castellano","Andrea Maria Zanchettin","Marco Faroni","Paolo Rocco"],"pdf_url":"https://arxiv.org/pdf/2505.04228v1.pdf","comment":"Paper accepted at IFAC ROBOTICS 2025"},{"id":"http://arxiv.org/abs/2505.04193v1","updated":"2025-05-07T07:41:29Z","published":"2025-05-07T07:41:29Z","title":"Trajectory Entropy Reinforcement Learning for Predictable and Robust\n  Control","summary":"  Simplicity is a critical inductive bias for designing data-driven\ncontrollers, especially when robustness is important. Despite the impressive\nresults of deep reinforcement learning in complex control tasks, it is prone to\ncapturing intricate and spurious correlations between observations and actions,\nleading to failure under slight perturbations to the environment. To tackle\nthis problem, in this work we introduce a novel inductive bias towards simple\npolicies in reinforcement learning. The simplicity inductive bias is introduced\nby minimizing the entropy of entire action trajectories, corresponding to the\nnumber of bits required to describe information in action trajectories after\nthe agent observes state trajectories. Our reinforcement learning agent,\nTrajectory Entropy Reinforcement Learning, is optimized to minimize the\ntrajectory entropy while maximizing rewards. We show that the trajectory\nentropy can be effectively estimated by learning a variational parameterized\naction prediction model, and use the prediction model to construct an\ninformation-regularized reward function. Furthermore, we construct a practical\nalgorithm that enables the joint optimization of models, including the policy\nand the prediction model. Experimental evaluations on several high-dimensional\nlocomotion tasks show that our learned policies produce more cyclical and\nconsistent action trajectories, and achieve superior performance, and\nrobustness to noise and dynamic changes than the state-of-the-art.\n","authors":["Bang You","Chenxu Wang","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2505.04193v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2505.04182v1","updated":"2025-05-07T07:28:16Z","published":"2025-05-07T07:28:16Z","title":"Beyond Task Performance: Human Experience in Human-Robot Collaboration","summary":"  Human interaction experience plays a crucial role in the effectiveness of\nhuman-machine collaboration, especially as interactions in future systems\nprogress towards tighter physical and functional integration. While automation\ndesign has been shown to impact task performance, its influence on human\nexperience metrics such as flow, sense of agency (SoA), and embodiment remains\nunderexplored. This study investigates how variations in automation design\naffect these psychological experience measures and examines correlations\nbetween subjective experience and physiological indicators. A user study was\nconducted in a simulated wood workshop, where participants collaborated with a\nlightweight robot under four automation levels. The results of the study\nindicate that medium automation levels enhance flow, SoA and embodiment,\nstriking a balance between support and user autonomy. In contrast, higher\nautomation, despite optimizing task performance, diminishes perceived flow and\nagency. Furthermore, we observed that grip force might be considered as a\nreal-time proxy of SoA, while correlations with heart rate variability were\ninconclusive. The findings underscore the necessity for automation strategies\nthat integrate human- centric metrics, aiming to optimize both performance and\nuser experience in collaborative robotic systems\n","authors":["Sean Kille","Jan Heinrich Robens","Philipp Dahlinger","Alejandra Rodriguez-Velasquez","Simon Rothfuß","Balint Varga","Andreas Lindenmann","Gerhard Neumann","Sven Matthiesen","Andrea Kiesel","Sören Hohmann"],"pdf_url":"https://arxiv.org/pdf/2505.04182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04162v1","updated":"2025-05-07T06:24:34Z","published":"2025-05-07T06:24:34Z","title":"SCU-Hand: Soft Conical Universal Robotic Hand for Scooping Granular\n  Media from Containers of Various Sizes","summary":"  Automating small-scale experiments in materials science presents challenges\ndue to the heterogeneous nature of experimental setups. This study introduces\nthe SCU-Hand (Soft Conical Universal Robot Hand), a novel end-effector designed\nto automate the task of scooping powdered samples from various container sizes\nusing a robotic arm. The SCU-Hand employs a flexible, conical structure that\nadapts to different container geometries through deformation, maintaining\nconsistent contact without complex force sensing or machine learning-based\ncontrol methods. Its reconfigurable mechanism allows for size adjustment,\nenabling efficient scooping from diverse container types. By combining soft\nrobotics principles with a sheet-morphing design, our end-effector achieves\nhigh flexibility while retaining the necessary stiffness for effective powder\nmanipulation. We detail the design principles, fabrication process, and\nexperimental validation of the SCU-Hand. Experimental validation showed that\nthe scooping capacity is about 20% higher than that of a commercial tool, with\na scooping performance of more than 95% for containers of sizes between 67 mm\nto 110 mm. This research contributes to laboratory automation by offering a\ncost-effective, easily implementable solution for automating tasks such as\nmaterials synthesis and characterization processes.\n","authors":["Tomoya Takahashi","Cristian C. Beltran-Hernandez","Yuki Kuroda","Kazutoshi Tanaka","Masashi Hamaya","Yoshitaka Ushiku"],"pdf_url":"https://arxiv.org/pdf/2505.04162v1.pdf","comment":"2025 IEEE International Conference on Robotics and Automation\n  (ICRA2025). Preprint. Accepted January 2025"},{"id":"http://arxiv.org/abs/2504.19186v2","updated":"2025-05-07T06:07:08Z","published":"2025-04-27T10:20:32Z","title":"LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place\n  Recognition","summary":"  In autonomous driving, place recognition is critical for global localization\nin GPS-denied environments. LiDAR and radar-based place recognition methods\nhave garnered increasing attention, as LiDAR provides precise ranging, whereas\nradar excels in adverse weather resilience. However, effectively leveraging\nLiDAR-radar fusion for place recognition remains challenging. The noisy and\nsparse nature of radar data limits its potential to further improve recognition\naccuracy. In addition, heterogeneous radar configurations complicate the\ndevelopment of unified cross-modality fusion frameworks. In this paper, we\npropose LRFusionPR, which improves recognition accuracy and robustness by\nfusing LiDAR with either single-chip or scanning radar. Technically, a\ndual-branch network is proposed to fuse different modalities within the unified\npolar coordinate bird's eye view (BEV) representation. In the fusion branch,\ncross-attention is utilized to perform cross-modality feature interactions. The\nknowledge from the fusion branch is simultaneously transferred to the\ndistillation branch, which takes radar as its only input to further improve the\nrobustness. Ultimately, the descriptors from both branches are concatenated,\nproducing the multimodal global descriptor for place retrieval. Extensive\nevaluations on multiple datasets demonstrate that our LRFusionPR achieves\naccurate place recognition, while maintaining robustness under varying weather\nconditions. Our open-source code will be released at\nhttps://github.com/QiZS-BIT/LRFusionPR.\n","authors":["Zhangshuo Qi","Luqi Cheng","Zijie Zhou","Guangming Xiong"],"pdf_url":"https://arxiv.org/pdf/2504.19186v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.04141v1","updated":"2025-05-07T05:45:33Z","published":"2025-05-07T05:45:33Z","title":"NAMO-LLM: Efficient Navigation Among Movable Obstacles with Large\n  Language Model Guidance","summary":"  Several planners have been proposed to compute robot paths that reach desired\ngoal regions while avoiding obstacles. However, these methods fail when all\npathways to the goal are blocked. In such cases, the robot must reason about\nhow to reconfigure the environment to access task-relevant regions - a problem\nknown as Navigation Among Movable Objects (NAMO). While various solutions to\nthis problem have been developed, they often struggle to scale to highly\ncluttered environments. To address this, we propose NAMO-LLM, a sampling-based\nplanner that searches over robot and obstacle configurations to compute\nfeasible plans specifying which obstacles to move, where, and in what order.\nIts key novelty is a non-uniform sampling strategy guided by Large Language\nModels (LLMs) biasing the tree construction toward directions more likely to\nyield a solution. We show that NAMO-LLM is probabilistically complete and\ndemonstrate through experiments that it efficiently scales to cluttered\nenvironments, outperforming related works in both runtime and plan quality.\n","authors":["Yuqing Zhang","Yiannis Kantaros"],"pdf_url":"https://arxiv.org/pdf/2505.04141v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.03729v2","updated":"2025-05-07T05:42:27Z","published":"2025-05-06T17:57:12Z","title":"Visual Imitation Enables Contextual Humanoid Control","summary":"  How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.\n","authors":["Arthur Allshire","Hongsuk Choi","Junyi Zhang","David McAllister","Anthony Zhang","Chung Min Kim","Trevor Darrell","Pieter Abbeel","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2505.03729v2.pdf","comment":"Project website: https://www.videomimic.net/"},{"id":"http://arxiv.org/abs/2502.18015v3","updated":"2025-05-07T05:34:19Z","published":"2025-02-25T09:23:52Z","title":"$\\texttt{SPIN}$: distilling $\\texttt{Skill-RRT}$ for long-horizon\n  prehensile and non-prehensile manipulation","summary":"  Current robots struggle with long-horizon manipulation tasks requiring\nsequences of prehensile and non-prehensile skills, contact-rich interactions,\nand long-term reasoning. We present $\\texttt{SPIN}$ ($\\textbf{S}$kill\n$\\textbf{P}$lanning to $\\textbf{IN}$ference), a framework that distills a\ncomputationally intensive planning algorithm into a policy via imitation\nlearning. We propose $\\texttt{Skill-RRT}$, an extension of RRT that\nincorporates skill applicability checks and intermediate object pose sampling\nfor solving such long-horizon problems. To chain independently trained skills,\nwe introduce $\\textit{connectors}$, goal-conditioned policies trained to\nminimize object disturbance during transitions. High-quality demonstrations are\ngenerated with $\\texttt{Skill-RRT}$ and distilled through noise-based replay in\norder to reduce online computation time. The resulting policy, trained entirely\nin simulation, transfers zero-shot to the real world and achieves over 80%\nsuccess across three challenging long-horizon manipulation tasks and\noutperforms state-of-the-art hierarchical RL and planning methods.\n","authors":["Haewon Jung","Donguk Lee","Haecheol Park","JunHyeop Kim","Beomjoon Kim"],"pdf_url":"https://arxiv.org/pdf/2502.18015v3.pdf","comment":"Project website: https://sites.google.com/view/skill-rrt"},{"id":"http://arxiv.org/abs/2503.22240v2","updated":"2025-05-07T04:53:51Z","published":"2025-03-28T08:42:54Z","title":"Bimanual Regrasp Planning and Control for Active Reduction of Object\n  Pose Uncertainty","summary":"  Precisely grasping an object is a challenging task due to pose uncertainties.\nConventional methods have used cameras and fixtures to reduce object\nuncertainty. They are effective but require intensive preparation, such as\ndesigning jigs based on the object geometry and calibrating cameras with\nhigh-precision tools fabricated using lasers. In this study, we propose a\nmethod to reduce the uncertainty of the position and orientation of a grasped\nobject without using a fixture or a camera. Our method is based on the concept\nthat the flat finger pads of a parallel gripper can reduce uncertainty along\nits opening/closing direction through flat surface contact. Three orthogonal\ngrasps by parallel grippers with flat finger pads collectively constrain an\nobject's position and orientation to a unique state. Guided by the concepts, we\ndevelop a regrasp planning and admittance control approach that sequentially\nfinds and leverages three orthogonal grasps of two robotic arms to actively\nreduce uncertainties in the object pose. We evaluated the proposed method on\ndifferent initial object uncertainties and verified that it had good\nrepeatability. The deviation levels of the experimental trials were on the same\norder of magnitude as those of an optical tracking system, demonstrating strong\nrelative inference performance.\n","authors":["Ryuta Nagahama","Weiwei Wan","Zhengtao Hu","Kensuke Harada"],"pdf_url":"https://arxiv.org/pdf/2503.22240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17767v3","updated":"2025-05-07T03:38:59Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Structures in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated structures as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-structures/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v3.pdf","comment":"Accepted to RSS 2025. Project webpage:\n  https://arjung128.github.io/opening-articulated-structures/"},{"id":"http://arxiv.org/abs/2504.08280v2","updated":"2025-05-07T03:30:04Z","published":"2025-04-11T06:25:11Z","title":"PNE-SGAN: Probabilistic NDT-Enhanced Semantic Graph Attention Network\n  for LiDAR Loop Closure Detection","summary":"  LiDAR loop closure detection (LCD) is crucial for consistent Simultaneous\nLocalization and Mapping (SLAM) but faces challenges in robustness and\naccuracy. Existing methods, including semantic graph approaches, often suffer\nfrom coarse geometric representations and lack temporal robustness against\nnoise, dynamics, and viewpoint changes. We introduce PNE-SGAN, a Probabilistic\nNDT-Enhanced Semantic Graph Attention Network, to overcome these limitations.\nPNE-SGAN enhances semantic graphs by using Normal Distributions Transform (NDT)\ncovariance matrices as rich, discriminative geometric node features, processed\nvia a Graph Attention Network (GAT). Crucially, it integrates graph similarity\nscores into a probabilistic temporal filtering framework (modeled as an\nHMM/Bayes filter), incorporating uncertain odometry for motion modeling and\nutilizing forward-backward smoothing to effectively handle ambiguities.\nEvaluations on challenging KITTI sequences (00 and 08) demonstrate\nstate-of-the-art performance, achieving Average Precision of 96.2\\% and 95.1\\%,\nrespectively. PNE-SGAN significantly outperforms existing methods, particularly\nin difficult bidirectional loop scenarios where others falter. By synergizing\ndetailed NDT geometry with principled probabilistic temporal reasoning,\nPNE-SGAN offers a highly accurate and robust solution for LiDAR LCD, enhancing\nSLAM reliability in complex, large-scale environments.\n","authors":["Xiong Li","Shulei Liu","Xingning Chen","Yisong Wu","Dong Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.08280v2.pdf","comment":"We discovered a critical implementation bug in Section 4\n  (probabilistic NDT-based semantic graph attention module) that invalidates\n  the results shown in Figures 3-4"},{"id":"http://arxiv.org/abs/2505.04095v1","updated":"2025-05-07T03:18:59Z","published":"2025-05-07T03:18:59Z","title":"Scalable Aerial GNSS Localization for Marine Robots","summary":"  Accurate localization is crucial for water robotics, yet traditional onboard\nGlobal Navigation Satellite System (GNSS) approaches are difficult or\nineffective due to signal reflection on the water's surface and its high cost\nof aquatic GNSS receivers. Existing approaches, such as inertial navigation,\nDoppler Velocity Loggers (DVL), SLAM, and acoustic-based methods, face\nchallenges like error accumulation and high computational complexity.\nTherefore, a more efficient and scalable solution remains necessary. This paper\nproposes an alternative approach that leverages an aerial drone equipped with\nGNSS localization to track and localize a marine robot once it is near the\nsurface of the water. Our results show that this novel adaptation enables\naccurate single and multi-robot marine robot localization.\n","authors":["Shuo Wen","Edwin Meriaux","Mariana Sosa Guzmán","Charlotte Morissette","Chloe Si","Bobak Baghi","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2505.04095v1.pdf","comment":"International Conference on Robotics and Automation 2025 Workshop\n  Robots in the Wild"},{"id":"http://arxiv.org/abs/2505.00831v2","updated":"2025-05-07T02:00:27Z","published":"2025-05-01T19:44:36Z","title":"SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation","summary":"  Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics.\n","authors":["Quang P. M. Pham","Khoi T. N. Nguyen","Nhi H. Doan","Cuong A. Pham","Kentaro Inui","Dezhen Song"],"pdf_url":"https://arxiv.org/pdf/2505.00831v2.pdf","comment":"Paper is under review"},{"id":"http://arxiv.org/abs/2501.04597v2","updated":"2025-05-07T23:28:22Z","published":"2025-01-08T16:25:32Z","title":"FrontierNet: Learning Visual Cues to Explore","summary":"  Exploration of unknown environments is crucial for autonomous robots; it\nallows them to actively reason and decide on what new data to acquire for\ndifferent tasks, such as mapping, object discovery, and environmental\nassessment. Existing solutions, such as frontier-based exploration approaches,\nrely heavily on 3D map operations, which are limited by map quality and, more\ncritically, often overlook valuable context from visual cues. This work aims at\nleveraging 2D visual cues for efficient autonomous exploration, addressing the\nlimitations of extracting goal poses from a 3D map. We propose a visual-only\nfrontier-based exploration system, with FrontierNet as its core component.\nFrontierNet is a learning-based model that (i) proposes frontiers, and (ii)\npredicts their information gain, from posed RGB images enhanced by monocular\ndepth priors. Our approach provides an alternative to existing 3D-dependent\ngoal-extraction approaches, achieving a 15\\% improvement in early-stage\nexploration efficiency, as validated through extensive simulations and\nreal-world experiments. The project is available at\nhttps://github.com/cvg/FrontierNet.\n","authors":["Boyang Sun","Hanzhi Chen","Stefan Leutenegger","Cesar Cadena","Marc Pollefeys","Hermann Blum"],"pdf_url":"https://arxiv.org/pdf/2501.04597v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07848v3","updated":"2025-05-07T22:19:07Z","published":"2024-11-12T15:01:40Z","title":"Zero-shot Object-Centric Instruction Following: Integrating Foundation\n  Models with Traditional Navigation","summary":"  Large scale scenes such as multifloor homes can be robustly and efficiently\nmapped with a 3D graph of landmarks estimated jointly with robot poses in a\nfactor graph, a technique commonly used in commercial robots such as drones and\nrobot vacuums. In this work, we propose Language-Inferred Factor Graph for\nInstruction Following (LIFGIF), a zero-shot method to ground natural language\ninstructions in such a map. LIFGIF also includes a policy for following natural\nlanguage navigation instructions in a novel environment while the map is\nconstructed, enabling robust navigation performance in the physical world. To\nevaluate LIFGIF, we present a new dataset, Object-Centric VLN (OC-VLN), in\norder to evaluate grounding of object-centric natural language navigation\ninstructions. We compare to two state-of-the-art zero-shot baselines from\nrelated tasks, Object Goal Navigation and Vision Language Navigation, to\ndemonstrate that LIFGIF outperforms them across all our evaluation metrics on\nOCVLN. Finally, we successfully demonstrate the effectiveness of LIFGIF for\nperforming zero-shot object-centric instruction following in the real world on\na Boston Dynamics Spot robot.\n","authors":["Sonia Raychaudhuri","Duy Ta","Katrina Ashton","Angel X. Chang","Jiuguang Wang","Bernadette Bucher"],"pdf_url":"https://arxiv.org/pdf/2411.07848v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08889v3","updated":"2025-05-07T22:07:55Z","published":"2024-09-13T14:55:20Z","title":"Extending the Benefits of Parallel Elasticity across Multiple Actuation\n  Tasks: A Geometric and Optimization-Based Approach","summary":"  A spring in parallel with an effort source (e.g., electric motor or human\nmuscle) can reduce its energy consumption and effort (i.e., torque or force)\ndepending on the spring stiffness, spring preload, and actuation task. However,\nselecting the spring stiffness and preload that guarantees effort or energy\nreduction for an arbitrary set of tasks is a design challenge. This work\nformulates a convex optimization problem to guarantee that a parallel spring\nreduces the root-mean-square source effort or energy consumption for multiple\ntasks. Specifically, we guarantee the benefits across multiple tasks by\nenforcing a set of convex quadratic constraints in our optimization variables,\nthe parallel spring stiffness and preload. These quadratic constraints are\nequivalent to ellipses in the stiffness and preload plane; any combination of\nstiffness and preload inside the ellipse represents a parallel spring that\nminimizes effort source or energy consumption with respect to an actuator\nwithout a spring. This geometric interpretation intuitively guides the\nstiffness and preload selection process. We analytically and experimentally\nprove the convex quadratic function of the spring stiffness and preload. As\napplications, we analyze the stiffness and preload selection of a parallel\nspring for a knee exoskeleton using human muscle as the effort source and a\nprosthetic ankle powered by electric motors. The source code associated with\nour framework is available as supplemental open-source software.\n","authors":["Kang Yang","Myia Dickens","James Schmiedeler","Edgar Bolívar-Nieto"],"pdf_url":"https://arxiv.org/pdf/2409.08889v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04831v1","updated":"2025-05-07T22:07:42Z","published":"2025-05-07T22:07:42Z","title":"Steerable Scene Generation with Post Training and Inference-Time Search","summary":"  Training robots in simulation requires diverse 3D scenes that reflect the\nspecific challenges of downstream tasks. However, scenes that satisfy strict\ntask requirements, such as high-clutter environments with plausible spatial\narrangement, are rare and costly to curate manually. Instead, we generate\nlarge-scale scene data using procedural models that approximate realistic\nenvironments for robotic manipulation, and adapt it to task-specific goals. We\ndo this by training a unified diffusion-based generative model that predicts\nwhich objects to place from a fixed asset library, along with their SE(3)\nposes. This model serves as a flexible scene prior that can be adapted using\nreinforcement learning-based post training, conditional generation, or\ninference-time search, steering generation toward downstream objectives even\nwhen they differ from the original data distribution. Our method enables\ngoal-directed scene synthesis that respects physical feasibility and scales\nacross scene types. We introduce a novel MCTS-based inference-time search\nstrategy for diffusion models, enforce feasibility via projection and\nsimulation, and release a dataset of over 44 million SE(3) scenes spanning five\ndiverse environments. Website with videos, code, data, and model weights:\nhttps://steerable-scene-generation.github.io/\n","authors":["Nicholas Pfaff","Hongkai Dai","Sergey Zakharov","Shun Iwase","Russ Tedrake"],"pdf_url":"https://arxiv.org/pdf/2505.04831v1.pdf","comment":"Project website: https://steerable-scene-generation.github.io/"},{"id":"http://arxiv.org/abs/2407.06454v4","updated":"2025-05-07T21:13:46Z","published":"2024-07-08T23:35:36Z","title":"Simplification of Robotic System Model Analysis by Petri Net Meta-Model\n  Property Transfer","summary":"  This paper presents a simplification of robotic system model analysis due to\nthe transfer of Robotic System Hierarchical Petri Net (RSHPN) meta-model\nproperties onto the model of a designed system. Key contributions include: 1)\nanalysis of RSHPN meta-model properties; 2) decomposition of RSHPN analysis\ninto analysis of individual Petri nets, thus the reduction of state space\nexplosion; and 3) transfer of RSHPN meta-model properties onto the produced\nmodels, hence elimination of the need for full re-analysis of the RSHPN model\nwhen creating new robotic systems. Only task-dependent parts of the model need\nto be analysed. This approach streamlines the analysis thus reducing the design\ntime. Moreover, it produces a specification which is a solid foundation for the\nimplementation of the system. The obtained results highlight the potential of\nPetri nets as a valuable formal framework for analysing robotic system\nproperties.\n","authors":["Maksym Figat","Cezary Zieliński"],"pdf_url":"https://arxiv.org/pdf/2407.06454v4.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2503.05057v2","updated":"2025-05-07T20:42:16Z","published":"2025-03-07T00:27:03Z","title":"Prismatic-Bending Transformable (PBT) Joint for a Modular, Foldable\n  Manipulator with Enhanced Reachability and Dexterity","summary":"  Robotic manipulators, traditionally designed with classical joint-link\narticulated structures, excel in industrial applications but face challenges in\nhuman-centered and general-purpose tasks requiring greater dexterity and\nadaptability. To address these challenges, we propose the Prismatic-Bending\nTransformable (PBT) Joint, a novel, scissors-inspired mechanism with\ndirectional maintenance capability that provides bending, rotation, and\nelongation/contraction within a single module. This design enables\ntransformable kinematic chains that are modular, reconfigurable, and scalable\nfor diverse tasks. We detail the mechanical design, optimization, kinematic and\ndynamic modeling, and experimental validation of the PBT joint, demonstrating\nits integration into foldable, modular robotic manipulators. The PBT joint\nfunctions as a single stock keeping unit (SKU), enabling manipulators to be\nconstructed entirely from standardized PBT joints. It also serves as a modular\nextension for existing systems, such as wrist modules, streamlining design,\ndeployment, transportation, and maintenance. Three joint sizes have been\ndeveloped and tested, showcasing enhanced dexterity, reachability, and\nadaptability, particularly in confined and cluttered spaces. This work presents\na promising approach to robotic manipulator development, providing a compact\nand versatile solution for operation in dynamic and constrained environments.\n","authors":["Jianshu Zhou","Junda Huang","Boyuan Liang","Xiang Zhang","Xin Ma","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2503.05057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04763v1","updated":"2025-05-07T19:42:56Z","published":"2025-05-07T19:42:56Z","title":"Data-Dependent Hidden Markov Model with Off-Road State Determination and\n  Real-Time Viterbi Algorithm for Lane Determination in Autonomous Vehicles","summary":"  Lane determination and lane sequence determination are important components\nfor many Connected and Automated Vehicle (CAV) applications. Lane determination\nhas been solved using Hidden Markov Model (HMM) among other methods. The\nexisting HMM literature for lane sequence determination uses empirical\ndefinitions with user-modified parameters to calculate HMM probabilities. The\nprobability definitions in the literature can cause breaks in the HMM due to\nthe inability to directly calculate probabilities of off-road positions,\nrequiring post-processing of data. This paper develops a time-varying HMM using\nthe physical properties of the roadway and vehicle, and the stochastic\nproperties of the sensors. This approach yields emission and transition\nprobability models conditioned on the sensor data without parameter tuning. It\nalso accounts for the probability that the vehicle is not in any roadway lane\n(e.g., on the shoulder or making a U-turn), which eliminates the need for\npost-processing to deal with breaks in the HMM processing. This approach\nrequires adapting the Viterbi algorithm and the HMM to be conditioned on the\nsensor data, which are then used to generate the most-likely sequence of lanes\nthe vehicle has traveled. The proposed approach achieves an average accuracy of\n95.9%. Compared to the existing literature, this provides an average increase\nof 2.25% by implementing the proposed transition probability and an average\nincrease of 5.1% by implementing both the proposed transition and emission\nprobabilities.\n","authors":["Mike Stas","Wang Hu","Jay A. Farrell"],"pdf_url":"https://arxiv.org/pdf/2505.04763v1.pdf","comment":"15 pages, 5 figures, 5 tables, Journal"},{"id":"http://arxiv.org/abs/2502.09960v2","updated":"2025-05-07T19:15:09Z","published":"2025-02-14T07:36:26Z","title":"Global-Local Interface with Selective Direct and Singularity-Avoiding\n  Motion Mapping for Intuitive Teleoperation","summary":"  This paper presents the Global-Local Teleoperation Interface, a hierarchical\nframework that enhances human-robot interaction by decoupling large-scale\nmanipulator positioning from fine end-effector manipulation. The global\ncomponent enables efficient workspace traversal, while the local component\nfacilitates precise and dexterous control. To address slave-side kinematic\nsingularities-especially during fine manipulation, we propose a\nsingularity-avoiding motion mapping strategy that enhances both stability and\nintuitiveness. We further introduce the concept of an operational Jacobian to\ncharacterize the smoothness of joint motion under local control. The G-L\ninterface is implemented in two variants: Direct Mapping and\nSingularity-Avoiding Mapping, and is validated through hardware experiments\ninvolving precision tasks and complex motion. Results show substantial\nimprovements in task success rate, efficiency, and user experience over\nconventional global or local-only systems.\n","authors":["Jianshu Zhou","Boyuan Liang","Junda Huang","Ian Zhang","Zhengyang Liu","Pieter Abbeel","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2502.09960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01521v2","updated":"2025-05-07T18:57:25Z","published":"2025-02-03T17:00:19Z","title":"Toward Task Generalization via Memory Augmentation in Meta-Reinforcement\n  Learning","summary":"  Agents trained via reinforcement learning (RL) often struggle to perform well\non tasks that differ from those encountered during training. This limitation\npresents a challenge to the broader deployment of RL in diverse and dynamic\ntask settings. In this work, we introduce memory augmentation, a memory-based\nRL approach to improve task generalization. Our approach leverages\ntask-structured augmentations to simulate plausible out-of-distribution\nscenarios and incorporates memory mechanisms to enable context-aware policy\nadaptation. Trained on a predefined set of tasks, our policy demonstrates the\nability to generalize to unseen tasks through memory augmentation without\nrequiring additional interactions with the environment. Through extensive\nsimulation experiments and real-world hardware evaluations on legged locomotion\ntasks, we demonstrate that our approach achieves zero-shot generalization to\nunseen tasks while maintaining robust in-distribution performance and high\nsample efficiency.\n","authors":["Kaixi Bao","Chenhao Li","Yarden As","Andreas Krause","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2502.01521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04725v1","updated":"2025-05-07T18:33:23Z","published":"2025-05-07T18:33:23Z","title":"Geometric Fault-Tolerant Neural Network Tracking Control of Unknown\n  Systems on Matrix Lie Groups","summary":"  We present a geometric neural network-based tracking controller for systems\nevolving on matrix Lie groups under unknown dynamics, actuator faults, and\nbounded disturbances. Leveraging the left-invariance of the tangent bundle of\nmatrix Lie groups, viewed as an embedded submanifold of the vector space\n$\\R^{N\\times N}$, we propose a set of learning rules for neural network weights\nthat are intrinsically compatible with the Lie group structure and do not\nrequire explicit parameterization. Exploiting the geometric properties of Lie\ngroups, this approach circumvents parameterization singularities and enables a\nglobal search for optimal weights. The ultimate boundedness of all error\nsignals -- including the neural network weights, the coordinate-free\nconfiguration error function, and the tracking velocity error -- is established\nusing Lyapunov's direct method. To validate the effectiveness of the proposed\nmethod, we provide illustrative simulation results for decentralized formation\ncontrol of multi-agent systems on the Special Euclidean group.\n","authors":["Robin Chhabra","Farzaneh Abdollahi"],"pdf_url":"https://arxiv.org/pdf/2505.04725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04722v1","updated":"2025-05-07T18:20:45Z","published":"2025-05-07T18:20:45Z","title":"Fitts' List Revisited: An Empirical Study on Function Allocation in a\n  Two-Agent Physical Human-Robot Collaborative Position/Force Task","summary":"  In this letter, we investigate whether the classical function allocation\nholds for physical Human-Robot Collaboration, which is important for providing\ninsights for Industry 5.0 to guide how to best augment rather than replace\nworkers. This study empirically tests the applicability of Fitts' List within\nphysical Human-Robot Collaboration, by conducting a user study (N=26,\nwithin-subject design) to evaluate four distinct allocations of position/force\ncontrol between human and robot in an abstract blending task. We hypothesize\nthat the function in which humans control the position achieves better\nperformance and receives higher user ratings. When allocating position control\nto the human and force control to the robot, compared to the opposite case, we\nobserved a significant improvement in preventing overblending. This was also\nperceived better in terms of physical demand and overall system acceptance,\nwhile participants experienced greater autonomy, more engagement and less\nfrustration. An interesting insight was that the supervisory role (when the\nrobot controls both position and force control) was rated second best in terms\nof subjective acceptance. Another surprising insight was that if position\ncontrol was delegated to the robot, the participants perceived much lower\nautonomy than when the force control was delegated to the robot. These findings\nempirically support applying Fitts' principles to static function allocation\nfor physical collaboration, while also revealing important nuanced user\nexperience trade-offs, particularly regarding perceived autonomy when\ndelegating position control.\n","authors":["Nicky Mol","J. Micah Prendergast","David A. Abbink","Luka Peternel"],"pdf_url":"https://arxiv.org/pdf/2505.04722v1.pdf","comment":"10 pages, 6 figures, under review for publication in IEEE Robotics\n  and Automation Letters (RA-L)"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.22330v2","updated":"2025-05-07T17:59:59Z","published":"2024-10-29T17:59:45Z","title":"Vision-Language Models Create Cross-Modal Task Representations","summary":"  Autoregressive vision-language models (VLMs) can handle many tasks within a\nsingle model, yet the representations that enable this capability remain\nopaque. We find that VLMs align conceptually equivalent inputs into a shared\ntask vector, which is invariant to modality (text, image) and format (examples,\ninstruction), and may simplify VLM processing. We measure this alignment via\ncross-modal transfer -- the ability of a task vector derived in one modality to\ntrigger the correct generation in another -- on a range of tasks and model\narchitectures. Although the task vector is highly compressed, we find that this\nsingle vector outperforms prompting the model with the full task information,\nunique to this cross-modal case. Furthermore, we show that task vectors can be\ntransferred from a base language model to its fine-tuned vision-language\ncounterpart, and that they can be derived solely from instructions without the\nneed for examples. Taken together, our findings shed light on how VLMs\ninternally process task information, and how they map different modalities into\ncommon semantic representations. Project page:\nhttps://vlm-cross-modal-reps.github.io.\n","authors":["Grace Luo","Trevor Darrell","Amir Bar"],"pdf_url":"https://arxiv.org/pdf/2410.22330v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.04623v1","updated":"2025-05-07T17:59:49Z","published":"2025-05-07T17:59:49Z","title":"EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\n  Reinforcement Learning","summary":"  Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research.\n","authors":["Zhenghao Xing","Xiaowei Hu","Chi-Wing Fu","Wenhai Wang","Jifeng Dai","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2505.04623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04622v1","updated":"2025-05-07T17:59:46Z","published":"2025-05-07T17:59:46Z","title":"PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer","summary":"  Shape primitive abstraction, which decomposes complex 3D shapes into simple\ngeometric elements, plays a crucial role in human visual cognition and has\nbroad applications in computer vision and graphics. While recent advances in 3D\ncontent generation have shown remarkable progress, existing primitive\nabstraction methods either rely on geometric optimization with limited semantic\nunderstanding or learn from small-scale, category-specific datasets, struggling\nto generalize across diverse shape categories. We present PrimitiveAnything, a\nnovel framework that reformulates shape primitive abstraction as a primitive\nassembly generation task. PrimitiveAnything includes a shape-conditioned\nprimitive transformer for auto-regressive generation and an ambiguity-free\nparameterization scheme to represent multiple types of primitives in a unified\nmanner. The proposed framework directly learns the process of primitive\nassembly from large-scale human-crafted abstractions, enabling it to capture\nhow humans decompose complex shapes into primitive elements. Through extensive\nexperiments, we demonstrate that PrimitiveAnything can generate high-quality\nprimitive assemblies that better align with human perception while maintaining\ngeometric fidelity across diverse shape categories. It benefits various 3D\napplications and shows potential for enabling primitive-based user-generated\ncontent (UGC) in games. Project page: https://primitiveanything.github.io\n","authors":["Jingwen Ye","Yuze He","Yanning Zhou","Yiqin Zhu","Kaiwen Xiao","Yong-Jin Liu","Wei Yang","Xiao Han"],"pdf_url":"https://arxiv.org/pdf/2505.04622v1.pdf","comment":"SIGGRAPH 2025. 14 pages, 15 figures"},{"id":"http://arxiv.org/abs/2505.04620v1","updated":"2025-05-07T17:59:32Z","published":"2025-05-07T17:59:32Z","title":"On Path to Multimodal Generalist: General-Level and General-Bench","summary":"  The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/\n","authors":["Hao Fei","Yuan Zhou","Juncheng Li","Xiangtai Li","Qingshan Xu","Bobo Li","Shengqiong Wu","Yaoting Wang","Junbao Zhou","Jiahao Meng","Qingyu Shi","Zhiyuan Zhou","Liangtao Shi","Minghe Gao","Daoan Zhang","Zhiqi Ge","Weiming Wu","Siliang Tang","Kaihang Pan","Yaobo Ye","Haobo Yuan","Tao Zhang","Tianjie Ju","Zixiang Meng","Shilin Xu","Liyu Jia","Wentao Hu","Meng Luo","Jiebo Luo","Tat-Seng Chua","Shuicheng Yan","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.04620v1.pdf","comment":"ICML'25, 305 pages, 115 tables, 177 figures, project page:\n  https://generalist.top/"},{"id":"http://arxiv.org/abs/2505.04619v1","updated":"2025-05-07T17:59:28Z","published":"2025-05-07T17:59:28Z","title":"Merging and Disentangling Views in Visual Reinforcement Learning for\n  Robotic Manipulation","summary":"  Vision is well-known for its use in manipulation, especially using visual\nservoing. To make it robust, multiple cameras are needed to expand the field of\nview. That is computationally challenging. Merging multiple views and using\nQ-learning allows the design of more effective representations and optimization\nof sample efficiency. Such a solution might be expensive to deploy. To mitigate\nthis, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently\nmerges views to increase sample efficiency while augmenting with single-view\nfeatures to allow lightweight deployment and ensure robust policies. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad\n","authors":["Abdulaziz Almuzairee","Rohan Patil","Dwait Bhatt","Henrik I. Christensen"],"pdf_url":"https://arxiv.org/pdf/2505.04619v1.pdf","comment":"For project website and code, see https://aalmuzairee.github.io/mad"},{"id":"http://arxiv.org/abs/2505.04616v1","updated":"2025-05-07T17:58:25Z","published":"2025-05-07T17:58:25Z","title":"Person Recognition at Altitude and Range: Fusion of Face, Body Shape and\n  Gait","summary":"  We address the problem of whole-body person recognition in unconstrained\nenvironments. This problem arises in surveillance scenarios such as those in\nthe IARPA Biometric Recognition and Identification at Altitude and Range\n(BRIAR) program, where biometric data is captured at long standoff distances,\nelevated viewing angles, and under adverse atmospheric conditions (e.g.,\nturbulence and high wind velocity). To this end, we propose FarSight, a unified\nend-to-end system for person recognition that integrates complementary\nbiometric cues across face, gait, and body shape modalities. FarSight\nincorporates novel algorithms across four core modules: multi-subject detection\nand tracking, recognition-aware video restoration, modality-specific biometric\nfeature encoding, and quality-guided multi-modal fusion. These components are\ndesigned to work cohesively under degraded image conditions, large pose and\nscale variations, and cross-domain gaps. Extensive experiments on the BRIAR\ndataset, one of the most comprehensive benchmarks for long-range, multi-modal\nbiometric recognition, demonstrate the effectiveness of FarSight. Compared to\nour preliminary system, this system achieves a 34.1% absolute gain in 1:1\nverification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set\nidentification (Rank-20), and a 34.3% reduction in open-set identification\nerrors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE\nFace in Video Evaluation (FIVE), which conducts standardized face recognition\ntesting on the BRIAR dataset. These results establish FarSight as a\nstate-of-the-art solution for operational biometric recognition in challenging\nreal-world conditions.\n","authors":["Feng Liu","Nicholas Chimitt","Lanqing Guo","Jitesh Jain","Aditya Kane","Minchul Kim","Wes Robbins","Yiyang Su","Dingqiang Ye","Xingguang Zhang","Jie Zhu","Siddharth Satyakam","Christopher Perry","Stanley H. Chan","Arun Ross","Humphrey Shi","Zhangyang Wang","Anil Jain","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2505.04616v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2505.04612v1","updated":"2025-05-07T17:56:15Z","published":"2025-05-07T17:56:15Z","title":"FastMap: Revisiting Dense and Scalable Structure from Motion","summary":"  We propose FastMap, a new global structure from motion method focused on\nspeed and simplicity. Previous methods like COLMAP and GLOMAP are able to\nestimate high-precision camera poses, but suffer from poor scalability when the\nnumber of matched keypoint pairs becomes large. We identify two key factors\nleading to this problem: poor parallelization and computationally expensive\noptimization steps. To overcome these issues, we design an SfM framework that\nrelies entirely on GPU-friendly operations, making it easily parallelizable.\nMoreover, each optimization step runs in time linear to the number of image\npairs, independent of keypoint pairs or 3D points. Through extensive\nexperiments, we show that FastMap is one to two orders of magnitude faster than\nCOLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.\n","authors":["Jiahao Li","Haochen Wang","Muhammad Zubair Irshad","Igor Vasiljevic","Matthew R. Walter","Vitor Campagnolo Guizilini","Greg Shakhnarovich"],"pdf_url":"https://arxiv.org/pdf/2505.04612v1.pdf","comment":"Project webpage: https://jiahao.ai/fastmap"},{"id":"http://arxiv.org/abs/2505.04601v1","updated":"2025-05-07T17:48:35Z","published":"2025-05-07T17:48:35Z","title":"OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision\n  Encoders for Multimodal Learning","summary":"  OpenAI's CLIP, released in early 2021, have long been the go-to choice of\nvision encoder for building multimodal foundation models. Although recent\nalternatives such as SigLIP have begun to challenge this status quo, to our\nknowledge none are fully open: their training data remains proprietary and/or\ntheir training recipes are not released. This paper fills this gap with\nOpenVision, a fully-open, cost-effective family of vision encoders that match\nor surpass the performance of OpenAI's CLIP when integrated into multimodal\nframeworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for\ntraining framework and Recap-DataComp-1B for training data -- while revealing\nmultiple key insights in enhancing encoder quality and showcasing practical\nbenefits in advancing multimodal models. By releasing vision encoders spanning\nfrom 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible\ntrade-off between capacity and efficiency in building multimodal models: larger\nmodels deliver enhanced multimodal performance, while smaller versions enable\nlightweight, edge-ready multimodal deployments.\n","authors":["Xianhang Li","Yanqing Liu","Haoqin Tu","Hongru Zhu","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2505.04601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17774v3","updated":"2025-05-07T17:44:11Z","published":"2024-06-25T17:59:06Z","title":"Uncertainty for SVBRDF Acquisition using Frequency Analysis","summary":"  This paper aims to quantify uncertainty for SVBRDF acquisition in multi-view\ncaptures. Under uncontrolled illumination and unstructured viewpoints, there is\nno guarantee that the observations contain enough information to reconstruct\nthe appearance properties of a captured object. We study this ambiguity, or\nuncertainty, using entropy and accelerate the analysis by using the frequency\ndomain, rather than the domain of incoming and outgoing viewing angles. The\nresult is a method that computes a map of uncertainty over an entire object\nwithin a millisecond. We find that the frequency model allows us to recover\nSVBRDF parameters with competitive performance, that the accelerated entropy\ncomputation matches results with a physically-based path tracer, and that there\nis a positive correlation between error and uncertainty. We then show that the\nuncertainty map can be applied to improve SVBRDF acquisition using capture\nguidance, sharing information on the surface, and using a diffusion model to\ninpaint uncertain regions. Our code is available at\nhttps://github.com/rubenwiersma/svbrdf_uncertainty.\n","authors":["Ruben Wiersma","Julien Philip","Miloš Hašan","Krishna Mullia","Fujun Luan","Elmar Eisemann","Valentin Deschaintre"],"pdf_url":"https://arxiv.org/pdf/2406.17774v3.pdf","comment":"Project page: https://svbrdf-uncertainty.github.io"},{"id":"http://arxiv.org/abs/2410.04634v3","updated":"2025-05-07T17:42:51Z","published":"2024-10-06T21:42:53Z","title":"Is What You Ask For What You Get? Investigating Concept Associations in\n  Text-to-Image Models","summary":"  Text-to-image (T2I) models are increasingly used in impactful real-life\napplications. As such, there is a growing need to audit these models to ensure\nthat they generate desirable, task-appropriate images. However, systematically\ninspecting the associations between prompts and generated content in a\nhuman-understandable way remains challenging. To address this, we propose\nConcept2Concept, a framework where we characterize conditional distributions of\nvision language models using interpretable concepts and metrics that can be\ndefined in terms of these concepts. This characterization allows us to use our\nframework to audit models and prompt-datasets. To demonstrate, we investigate\nseveral case studies of conditional distributions of prompts, such as\nuser-defined distributions or empirical, real-world distributions. Lastly, we\nimplement Concept2Concept as an open-source interactive visualization tool to\nfacilitate use by non-technical end-users. A demo is available at\nhttps://tinyurl.com/Concept2ConceptDemo.\n","authors":["Salma S. Abdel Magid","Weiwei Pan","Simon Warchol","Grace Guo","Junsik Kim","Mahia Rahman","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2410.04634v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04596v1","updated":"2025-05-07T17:37:53Z","published":"2025-05-07T17:37:53Z","title":"Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera\n  Surveillance Systems","summary":"  This paper presents a novel approach for optimizing the scheduling and\ncontrol of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments.\nThe proposed method integrates Kalman filters for motion prediction with a\ndynamic network flow model to enhance real-time video capture efficiency. By\nassigning Kalman filters to tracked objects, the system predicts future\nlocations, enabling precise scheduling of camera tasks. This prediction-driven\napproach is formulated as a network flow optimization, ensuring scalability and\nadaptability to various surveillance scenarios. To further reduce redundant\nmonitoring, we also incorporate group-tracking nodes, allowing multiple objects\nto be captured within a single camera focus when appropriate. In addition, a\nvalue-based system is introduced to prioritize camera actions, focusing on the\ntimely capture of critical events. By adjusting the decay rates of these values\nover time, the system ensures prompt responses to tasks with imminent\ndeadlines. Extensive simulations demonstrate that this approach improves\ncoverage, reduces average wait times, and minimizes missed events compared to\ntraditional master-slave camera systems. Overall, our method significantly\nenhances the efficiency, scalability, and effectiveness of surveillance\nsystems, particularly in dynamic and crowded environments.\n","authors":["Mohammad Merati","David Castañón"],"pdf_url":"https://arxiv.org/pdf/2505.04596v1.pdf","comment":"7 pages, 3 Figures, Accepted at AIRC 2025"},{"id":"http://arxiv.org/abs/2505.04594v1","updated":"2025-05-07T17:37:23Z","published":"2025-05-07T17:37:23Z","title":"MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection","summary":"  Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets.\n","authors":["Zhihao Zhang","Abhinav Kumar","Girish Chandar Ganesan","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2505.04594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04590v1","updated":"2025-05-07T17:32:49Z","published":"2025-05-07T17:32:49Z","title":"TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral\n  Grids for Gradient-Based Mesh Optimization","summary":"  We introduce TetWeave, a novel isosurface representation for gradient-based\nmesh optimization that jointly optimizes the placement of a tetrahedral grid\nused for Marching Tetrahedra and a novel directional signed distance at each\npoint. TetWeave constructs tetrahedral grids on-the-fly via Delaunay\ntriangulation, enabling increased flexibility compared to predefined grids. The\nextracted meshes are guaranteed to be watertight, two-manifold and\nintersection-free. The flexibility of TetWeave enables a resampling strategy\nthat places new points where reconstruction error is high and allows to\nencourage mesh fairness without compromising on reconstruction error. This\nleads to high-quality, adaptive meshes that require minimal memory usage and\nfew parameters to optimize. Consequently, TetWeave exhibits near-linear memory\nscaling relative to the vertex count of the output mesh - a substantial\nimprovement over predefined grids. We demonstrate the applicability of TetWeave\nto a broad range of challenging tasks in computer graphics and vision, such as\nmulti-view 3D reconstruction, mesh compression and geometric texture\ngeneration.\n","authors":["Alexandre Binninger","Ruben Wiersma","Philipp Herholz","Olga Sorkine-Hornung"],"pdf_url":"https://arxiv.org/pdf/2505.04590v1.pdf","comment":"ACM Trans. Graph. 44, 4. SIGGRAPH 2025. 19 pages, 21 figures"},{"id":"http://arxiv.org/abs/2505.04586v1","updated":"2025-05-07T17:27:51Z","published":"2025-05-07T17:27:51Z","title":"Active Sampling for MRI-based Sequential Decision Making","summary":"  Despite the superior diagnostic capability of Magnetic Resonance Imaging\n(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and\ncomplexity. To enable such a future by reducing the magnetic field strength,\none key approach will be to improve sampling strategies. Previous work has\nshown that it is possible to make diagnostic decisions directly from k-space\nwith fewer samples. Such work shows that single diagnostic decisions can be\nmade, but if we aspire to see MRI as a true PoC, multiple and sequential\ndecisions are necessary while minimizing the number of samples acquired. We\npresent a novel multi-objective reinforcement learning framework enabling\ncomprehensive, sequential, diagnostic evaluation from undersampled k-space\ndata. Our approach during inference actively adapts to sequential decisions to\noptimally sample. To achieve this, we introduce a training methodology that\nidentifies the samples that contribute the best to each diagnostic objective\nusing a step-wise weighting reward function. We evaluate our approach in two\nsequential knee pathology assessment tasks: ACL sprain detection and cartilage\nthickness loss assessment. Our framework achieves diagnostic performance\ncompetitive with various policy-based benchmarks on disease detection, severity\nquantification, and overall sequential diagnosis, while substantially saving\nk-space samples. Our approach paves the way for the future of MRI as a\ncomprehensive and affordable PoC device. Our code is publicly available at\nhttps://github.com/vios-s/MRI_Sequential_Active_Sampling\n","authors":["Yuning Du","Jingshuai Liu","Rohan Dharmakumar","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2505.04586v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.04575v1","updated":"2025-05-07T17:12:15Z","published":"2025-05-07T17:12:15Z","title":"Componential Prompt-Knowledge Alignment for Domain Incremental Learning","summary":"  Domain Incremental Learning (DIL) aims to learn from non-stationary data\nstreams across domains while retaining and utilizing past knowledge. Although\nprompt-based methods effectively store multi-domain knowledge in prompt\nparameters and obtain advanced performance through cross-domain prompt fusion,\nwe reveal an intrinsic limitation: component-wise misalignment between\ndomain-specific prompts leads to conflicting knowledge integration and degraded\npredictions. This arises from the random positioning of knowledge components\nwithin prompts, where irrelevant component fusion introduces interference.To\naddress this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a\nnovel prompt-based DIL method that introduces component-aware prompt-knowledge\nalignment during training, significantly improving both the learning and\ninference capacity of the model. KA-Prompt operates in two phases: (1) Initial\nComponential Structure Configuring, where a set of old prompts containing\nknowledge relevant to the new domain are mined via greedy search, which is then\nexploited to initialize new prompts to achieve reusable knowledge transfer and\nestablish intrinsic alignment between new and old prompts. (2) Online Alignment\nPreservation, which dynamically identifies the target old prompts and applies\nadaptive componential consistency constraints as new prompts evolve. Extensive\nexperiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.\nOur source code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-KA-Prompt\n","authors":["Kunlun Xu","Xu Zou","Gang Hua","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.04575v1.pdf","comment":"Accpted by ICML2025"},{"id":"http://arxiv.org/abs/2412.06661v2","updated":"2025-05-07T16:57:47Z","published":"2024-12-09T17:00:20Z","title":"Efficiency Meets Fidelity: A Novel Quantization Framework for Stable\n  Diffusion","summary":"  Text-to-image generation via Stable Diffusion models (SDM) have demonstrated\nremarkable capabilities. However, their computational intensity, particularly\nin the iterative denoising process, hinders real-time deployment in\nlatency-sensitive applications. While Recent studies have explored\npost-training quantization (PTQ) and quantization-aware training (QAT) methods\nto compress Diffusion models, existing methods often overlook the consistency\nbetween results generated by quantized models and those from floating-point\nmodels. This consistency is paramount for professional applications where both\nefficiency and output reliability are essential. To ensure that quantized SDM\ngenerates high-quality and consistent images, we propose an efficient\nquantization framework for SDM. Our framework introduces a Serial-to-Parallel\npipeline that simultaneously maintains training-inference consistency and\nensures optimization stability. Building upon this foundation, we further\ndevelop several techniques including multi-timestep activation quantization,\ntime information precalculation, inter-layer distillation, and selective\nfreezing, to achieve high-fidelity generation in comparison to floating-point\nmodels while maintaining quantization efficiency.\n  Through comprehensive evaluation across multiple Stable Diffusion variants\n(v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over\nstate-of-the-art approaches with shorter training times. Under W4A8\nquantization settings, we achieve significant improvements in both distribution\nsimilarity and visual fidelity, while preserving a high image quality.\n","authors":["Shuaiting Li","Juncan Deng","Zeyu Wang","Kedong Xu","Rongtao Deng","Hong Gu","Haibin Shen","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.06661v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04666v3","updated":"2025-05-07T16:55:43Z","published":"2025-01-08T18:25:50Z","title":"Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise\n  Scheduling","summary":"  Given an isolated garment image in a canonical product view and a separate\nimage of a person, the virtual try-on task aims to generate a new image of the\nperson wearing the target garment. Prior virtual try-on works face two major\nchallenges in achieving this goal: a) the paired (human, garment) training data\nhas limited availability; b) generating textures on the human that perfectly\nmatch that of the prompted garment is difficult, often resulting in distorted\ntext and faded textures. Our work explores ways to tackle these issues through\nboth synthetic data as well as model refinement. We introduce a garment\nextraction model that generates (human, synthetic garment) pairs from a single\nimage of a clothed individual. The synthetic pairs can then be used to augment\nthe training of virtual try-on. We also propose an Error-Aware Refinement-based\nSchr\\\"odinger Bridge (EARSB) that surgically targets localized generation\nerrors for correcting the output of a base virtual try-on model. To identify\nlikely errors, we propose a weakly-supervised error classifier that localizes\nregions for refinement, subsequently augmenting the Schr\\\"odinger Bridge's\nnoise schedule with its confidence heatmap. Experiments on VITON-HD and\nDressCode-Upper demonstrate that our synthetic data augmentation enhances the\nperformance of prior work, while EARSB improves the overall image quality. In\nuser studies, our model is preferred by the users in an average of 59% of\ncases.\n","authors":["Nannan Li","Kevin J. Shih","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2501.04666v3.pdf","comment":"Accepted in CVPR 2025"},{"id":"http://arxiv.org/abs/2411.04997v4","updated":"2025-05-07T16:51:33Z","published":"2024-11-07T18:59:16Z","title":"LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation","summary":"  CLIP is a foundational multimodal model that aligns image and text features\ninto a shared representation space via contrastive learning on large-scale\nimage-text pairs. Its effectiveness primarily stems from the use of natural\nlanguage as rich supervision. Motivated by the remarkable advancements in large\nlanguage models (LLMs), this work explores how LLMs' superior text\nunderstanding and extensive open-world knowledge can enhance CLIP's capability,\nespecially for processing longer and more complex image captions. We propose an\nefficient post-training strategy that integrates LLMs into pretrained CLIP. To\naddress the challenge posed by the autoregressive nature of LLMs, we introduce\na caption-to-caption contrastive fine-tuning framework, significantly enhancing\nthe discriminative quality of LLM outputs. Extensive experiments demonstrate\nthat our approach outperforms LoRA-based methods, achieving nearly fourfold\nfaster training with superior performance. Furthermore, we validate substantial\nimprovements over state-of-the-art models such as CLIP, EVA02, and SigLip2\nacross various zero-shot multimodal retrieval tasks, cross-lingual retrieval\ntasks, and multimodal language model pretraining.\n","authors":["Weiquan Huang","Aoqi Wu","Yifan Yang","Xufang Luo","Yuqing Yang","Liang Hu","Qi Dai","Chunyu Wang","Xiyang Dai","Dongdong Chen","Chong Luo","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2411.04997v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04540v1","updated":"2025-05-07T16:17:54Z","published":"2025-05-07T16:17:54Z","title":"Registration of 3D Point Sets Using Exponential-based Similarity Matrix","summary":"  Point cloud registration is a fundamental problem in computer vision and\nrobotics, involving the alignment of 3D point sets captured from varying\nviewpoints using depth sensors such as LiDAR or structured light. In modern\nrobotic systems, especially those focused on mapping, it is essential to merge\nmultiple views of the same environment accurately. However, state-of-the-art\nregistration techniques often struggle when large rotational differences exist\nbetween point sets or when the data is significantly corrupted by sensor noise.\nThese challenges can lead to misalignments and, consequently, to inaccurate or\ndistorted 3D reconstructions. In this work, we address both these limitations\nby proposing a robust modification to the classic Iterative Closest Point (ICP)\nalgorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP),\nintegrates a Gaussian-inspired exponential weighting scheme to construct a\nsimilarity matrix that dynamically adapts across iterations. This matrix\nfacilitates improved estimation of both rotational and translational components\nduring alignment. We demonstrate the robustness of ESM-ICP in two challenging\nscenarios: (i) large rotational discrepancies between the source and target\npoint clouds, and (ii) data corrupted by non-Gaussian noise. Our results show\nthat ESM-ICP outperforms traditional geometric registration techniques as well\nas several recent learning-based methods. To encourage reproducibility and\ncommunity engagement, our full implementation is made publicly available on\nGitHub. https://github.com/aralab-unr/ESM_ICP\n","authors":["Ashutosh Singandhupe","Sanket Lokhande","Hung Manh La"],"pdf_url":"https://arxiv.org/pdf/2505.04540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04529v1","updated":"2025-05-07T16:02:46Z","published":"2025-05-07T16:02:46Z","title":"RAFT: Robust Augmentation of FeaTures for Image Segmentation","summary":"  Image segmentation is a powerful computer vision technique for scene\nunderstanding. However, real-world deployment is stymied by the need for\nhigh-quality, meticulously labeled datasets. Synthetic data provides\nhigh-quality labels while reducing the need for manual data collection and\nannotation. However, deep neural networks trained on synthetic data often face\nthe Syn2Real problem, leading to poor performance in real-world deployments.\n  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a\nnovel framework for adapting image segmentation models using minimal labeled\nreal-world data through data and feature augmentations, as well as active\nlearning. To validate RAFT, we perform experiments on the synthetic-to-real\n\"SYNTHIA->Cityscapes\" and \"GTAV->Cityscapes\" benchmarks. We managed to surpass\nthe previous state of the art, HALO. SYNTHIA->Cityscapes experiences an\nimprovement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes\nexperiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach\non the real-to-real benchmark of \"Cityscapes->ACDC\", and again surpass HALO,\nwith a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the\neffect of the allocated annotation budget and various components of RAFT upon\nthe final transfer mIoU.\n","authors":["Edward Humes","Xiaomin Lin","Uttej Kallakuri","Tinoosh Mohsenin"],"pdf_url":"https://arxiv.org/pdf/2505.04529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04321v3","updated":"2025-05-07T15:59:51Z","published":"2024-06-06T17:58:11Z","title":"VidMuse: A Simple Video-to-Music Generation Framework with\n  Long-Short-Term Modeling","summary":"  In this work, we systematically study music generation conditioned solely on\nthe video. First, we present a large-scale dataset comprising 360K video-music\npairs, including various genres such as movie trailers, advertisements, and\ndocumentaries. Furthermore, we propose VidMuse, a simple framework for\ngenerating music aligned with video inputs. VidMuse stands out by producing\nhigh-fidelity music that is both acoustically and semantically aligned with the\nvideo. By incorporating local and global visual cues, VidMuse enables the\ncreation of musically coherent audio tracks that consistently match the video\ncontent through Long-Short-Term modeling. Through extensive experiments,\nVidMuse outperforms existing models in terms of audio quality, diversity, and\naudio-visual alignment. The code and datasets are available at\nhttps://vidmuse.github.io/.\n","authors":["Zeyue Tian","Zhaoyang Liu","Ruibin Yuan","Jiahao Pan","Qifeng Liu","Xu Tan","Qifeng Chen","Wei Xue","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2406.04321v3.pdf","comment":"The code and datasets are available at\n  https://github.com/ZeyueT/VidMuse/"},{"id":"http://arxiv.org/abs/2505.04526v1","updated":"2025-05-07T15:59:45Z","published":"2025-05-07T15:59:45Z","title":"DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement\n  and Fusion All at Once","summary":"  Visible and infrared image fusion is one of the most crucial tasks in the\nfield of image fusion, aiming to generate fused images with clear structural\ninformation and high-quality texture features for high-level vision tasks.\nHowever, when faced with severe illumination degradation in visible images, the\nfusion results of existing image fusion methods often exhibit blurry and dim\nvisual effects, posing major challenges for autonomous driving. To this end, a\nDarkness-Free network is proposed to handle Visible and infrared image\ndisentanglement and fusion all at Once (DFVO), which employs a cascaded\nmulti-task approach to replace the traditional two-stage cascaded training\n(enhancement and fusion), addressing the issue of information entropy loss\ncaused by hierarchical data transmission. Specifically, we construct a\nlatent-common feature extractor (LCFE) to obtain latent features for the\ncascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised\nto acquire high-frequency semantic information. Secondly, we design a hyper\ncross-attention module (HCAM) to extract low-frequency information and preserve\ntexture features from source images. Finally, a relevant loss function is\ndesigned to guide the holistic network learning, thereby achieving better image\nfusion. Extensive experiments demonstrate that our proposed approach\noutperforms state-of-the-art alternatives in terms of qualitative and\nquantitative evaluations. Particularly, DFVO can generate clearer, more\ninformative, and more evenly illuminated fusion results in the dark\nenvironments, achieving best performance on the LLVIP dataset with 63.258 dB\nPSNR and 0.724 CC, providing more effective information for high-level vision\ntasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.\n","authors":["Qi Zhou","Yukai Shi","Xiaojun Yang","Xiaoyu Xian","Lunjia Liao","Ruimao Zhang","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2505.04526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04524v1","updated":"2025-05-07T15:57:53Z","published":"2025-05-07T15:57:53Z","title":"Edge-GPU Based Face Tracking for Face Detection and Recognition\n  Acceleration","summary":"  Cost-effective machine vision systems dedicated to real-time and accurate\nface detection and recognition in public places are crucial for many modern\napplications. However, despite their high performance, which could be reached\nusing specialized edge or cloud AI hardware accelerators, there is still room\nfor improvement in throughput and power consumption. This paper aims to suggest\na combined hardware-software approach that optimizes face detection and\nrecognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX\nOrin. First, it leverages the simultaneous usage of all its hardware engines to\nimprove processing time. This offers an improvement over previous works where\nthese tasks were mainly allocated automatically and exclusively to the CPU or,\nto a higher extent, to the GPU core. Additionally, the paper suggests\nintegrating a face tracker module to avoid redundantly running the face\nrecognition algorithm for every frame but only when a new face appears in the\nscene. The results of extended experiments suggest that simultaneous usage of\nall the hardware engines that are available in the Orin GPU and tracker\nintegration into the pipeline yield an impressive throughput of 290 FPS (frames\nper second) on 1920 x 1080 input size frames containing in average of 6\nfaces/frame. Additionally, a substantial saving of power consumption of around\n800 mW was achieved when compared to running the task on the CPU/GPU engines\nonly and without integrating a tracker into the Orin GPU\\'92s pipeline. This\nhardware-codesign approach can pave the way to design high-performance machine\nvision systems at the edge, critically needed in video monitoring in public\nplaces where several nearby cameras are usually deployed for a same scene.\n","authors":["Asma Baobaid","Mahmoud Meribout"],"pdf_url":"https://arxiv.org/pdf/2505.04524v1.pdf","comment":"10 pages, 12 figures"},{"id":"http://arxiv.org/abs/2505.04522v1","updated":"2025-05-07T15:53:56Z","published":"2025-05-07T15:53:56Z","title":"Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions\n  Using Diffusion Model","summary":"  Generating 3D CT volumes from descriptive free-text inputs presents a\ntransformative opportunity in diagnostics and research. In this paper, we\nintroduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual\ndescriptions using the diffusion model. Unlike previous methods that rely on\nfixed-format text input, Text2CT employs a novel prompt formulation that\nenables generation from diverse, free-text descriptions. The proposed framework\nencodes medical text into latent representations and decodes them into\nhigh-resolution 3D CT scans, effectively bridging the gap between semantic text\ninputs and detailed volumetric representations in a unified 3D framework. Our\nmethod demonstrates superior performance in preserving anatomical fidelity and\ncapturing intricate structures as described in the input text. Extensive\nevaluations show that our approach achieves state-of-the-art results, offering\npromising potential applications in diagnostics, and data augmentation.\n","authors":["Pengfei Guo","Can Zhao","Dong Yang","Yufan He","Vishwesh Nath","Ziyue Xu","Pedro R. A. S. Bassi","Zongwei Zhou","Benjamin D. Simon","Stephanie Anne Harmon","Baris Turkbey","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2505.04522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03414v2","updated":"2025-05-07T15:43:41Z","published":"2025-05-06T10:41:53Z","title":"Enhancing Target-unspecific Tasks through a Features Matrix","summary":"  Recent developments in prompt learning of large vision-language models have\nsignificantly improved performance in target-specific tasks. However, these\nprompt optimizing methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge having\nstrong promotion on target-unspecific tasks. To alleviate this issue, we\npropose a novel Features Matrix (FM) regularization approach designed to\nenhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks, achieving state-of-the-art performance.\n","authors":["Fangming Cui","Yonggang Zhang","Xuan Wang","Xinmei Tian","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2505.03414v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.04512v1","updated":"2025-05-07T15:33:18Z","published":"2025-05-07T15:33:18Z","title":"HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation","summary":"  Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.\n","authors":["Teng Hu","Zhentao Yu","Zhengguang Zhou","Sen Liang","Yuan Zhou","Qin Lin","Qinglin Lu"],"pdf_url":"https://arxiv.org/pdf/2505.04512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18360v3","updated":"2025-05-07T15:25:04Z","published":"2024-06-26T14:00:21Z","title":"XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis","summary":"  Comprehensive testing of autonomous systems through simulation is essential\nto ensure the safety of autonomous driving vehicles. This requires the\ngeneration of safety-critical scenarios that extend beyond the limitations of\nreal-world data collection, as many of these scenarios are rare or rarely\nencountered on public roads. However, evaluating most existing novel view\nsynthesis (NVS) methods relies on sporadic sampling of image frames from the\ntraining data, comparing the rendered images with ground-truth images.\nUnfortunately, this evaluation protocol falls short of meeting the actual\nrequirements in closed-loop simulations. Specifically, the true application\ndemands the capability to render novel views that extend beyond the original\ntrajectory (such as cross-lane views), which are challenging to capture in the\nreal world. To address this, this paper presents a synthetic dataset for novel\ndriving view synthesis evaluation, which is specifically designed for\nautonomous driving simulations. This unique dataset includes testing images\ncaptured by deviating from the training trajectory by $1-4$ meters. It\ncomprises six sequences that cover various times and weather conditions. Each\nsequence contains $450$ training images, $120$ testing images, and their\ncorresponding camera poses and intrinsic parameters. Leveraging this novel\ndataset, we establish the first realistic benchmark for evaluating existing NVS\napproaches under front-only and multicamera settings. The experimental findings\nunderscore the significant gap in current approaches, revealing their\ninadequate ability to fulfill the demanding prerequisites of cross-lane or\nclosed-loop simulation.\n","authors":["Hao Li","Chenming Wu","Ming Yuan","Yan Zhang","Chen Zhao","Chunyu Song","Haocheng Feng","Errui Ding","Dingwen Zhang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.18360v3.pdf","comment":"Accepted to 3DV 2025, project page: https://3d-aigc.github.io/XLD/"},{"id":"http://arxiv.org/abs/2505.04502v1","updated":"2025-05-07T15:22:17Z","published":"2025-05-07T15:22:17Z","title":"Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video\n  Face Detection and Recognition","summary":"  Video face detection and recognition in public places at the edge is required\nin several applications, such as security reinforcement and contactless access\nto authorized venues. This paper aims to maximize the simultaneous usage of\nhardware engines available in edge GPUs nowadays by leveraging the concurrency\nand pipelining of tasks required for face detection and recognition. This also\nincludes the video decoding task, which is required in most face monitoring\napplications as the video streams are usually carried via Gbps Ethernet\nnetwork. This constitutes an improvement over previous works where the tasks\nare usually allocated to a single engine due to the lack of a unified and\nautomated framework that simultaneously explores all hardware engines. In\naddition, previously, the input faces were usually embedded in still images or\nwithin raw video streams that overlook the burst delay caused by the decoding\nstage. The results on real-life video streams suggest that simultaneously using\nall the hardware engines available in the recent NVIDIA edge Orin GPU, higher\nthroughput, and a slight saving of power consumption of around 300 mW,\naccounting for around 5%, have been achieved while satisfying the real-time\nperformance constraint. The performance gets even higher by considering several\nvideo streams simultaneously. Further performance improvement could have been\nobtained if the number of shuffle layers that were created by the tensor RT\nframework for the face recognition task was lower. Thus, the paper suggests\nsome hardware improvements to the existing edge GPU processors to enhance their\nperformance even higher.\n","authors":["Asma Baobaid","Mahmoud Meribout"],"pdf_url":"https://arxiv.org/pdf/2505.04502v1.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.04497v1","updated":"2025-05-07T15:20:17Z","published":"2025-05-07T15:20:17Z","title":"Defining and Quantifying Creative Behavior in Popular Image Generators","summary":"  Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition.\n","authors":["Aditi Ramaswamy"],"pdf_url":"https://arxiv.org/pdf/2505.04497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04488v1","updated":"2025-05-07T15:03:16Z","published":"2025-05-07T15:03:16Z","title":"\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting\n  Individuals with Visual Impairments","summary":"  The visually impaired population, especially the severely visually impaired,\nis currently large in scale, and daily activities pose significant challenges\nfor them. Although many studies use large language and vision-language models\nto assist the blind, most focus on static content and fail to meet real-time\nperception needs in dynamic and complex environments, such as daily activities.\nTo provide them with more effective intelligent assistance, it is imperative to\nincorporate advanced visual understanding technologies. Although real-time\nvision and speech interaction VideoLLMs demonstrate strong real-time visual\nunderstanding, no prior work has systematically evaluated their effectiveness\nin assisting visually impaired individuals. In this work, we conduct the first\nsuch evaluation. First, we construct a benchmark dataset (VisAssistDaily),\ncovering three categories of assistive tasks for visually impaired individuals:\nBasic Skills, Home Life Tasks, and Social Life Tasks. The results show that\nGPT-4o achieves the highest task success rate. Next, we conduct a user study to\nevaluate the models in both closed-world and open-world scenarios, further\nexploring the practical challenges of applying VideoLLMs in assistive contexts.\nOne key issue we identify is the difficulty current models face in perceiving\npotential hazards in dynamic environments. To address this, we build an\nenvironment-awareness dataset named SafeVid and introduce a polling mechanism\nthat enables the model to proactively detect environmental risks. We hope this\nwork provides valuable insights and inspiration for future research in this\nfield.\n","authors":["Ziyi Zhang","Zhen Sun","Zongmin Zhang","Zifan Peng","Yuemeng Zhao","Zichun Wang","Zeren Luo","Ruiting Zuo","Xinlei He"],"pdf_url":"https://arxiv.org/pdf/2505.04488v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.04486v1","updated":"2025-05-07T14:59:23Z","published":"2025-05-07T14:59:23Z","title":"Efficient Flow Matching using Latent Variables","summary":"  Flow matching models have shown great potential in image generation tasks\namong probabilistic generative models. Building upon the ideas of continuous\nnormalizing flows, flow matching models generalize the transport path of the\ndiffusion models from a simple prior distribution to the data. Most flow\nmatching models in the literature do not explicitly model the underlying\nstructure/manifold in the target data when learning the flow from a simple\nsource distribution like the standard Gaussian. This leads to inefficient\nlearning, especially for many high-dimensional real-world datasets, which often\nreside in a low-dimensional manifold. Existing strategies of incorporating\nmanifolds, including data with underlying multi-modal distribution, often\nrequire expensive training and hence frequently lead to suboptimal performance.\nTo this end, we present \\texttt{Latent-CFM}, which provides simplified\ntraining/inference strategies to incorporate multi-modal data structures using\npretrained deep latent variable models. Through experiments on multi-modal\nsynthetic data and widely used image benchmark datasets, we show that\n\\texttt{Latent-CFM} exhibits improved generation quality with significantly\nless training ($\\sim 50\\%$ less in some cases) and computation than\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\ndemonstrate that our approach generates more physically accurate samples than\ncompetitive approaches. In addition, through latent space analysis, we\ndemonstrate that our approach can be used for conditional image generation\nconditioned on latent features.\n","authors":["Anirban Samaddar","Yixuan Sun","Viktor Nilsson","Sandeep Madireddy"],"pdf_url":"https://arxiv.org/pdf/2505.04486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04485v1","updated":"2025-05-07T14:58:04Z","published":"2025-05-07T14:58:04Z","title":"FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame\n  Averaging","summary":"  We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural\nnetwork architecture built on top of the well-known KPConv, a widely adopted\nbackbone for 3D point cloud analysis. Even though invariance and/or\nequivariance to Euclidean transformations are required for many common tasks,\nKPConv-based networks can only approximately achieve such properties when\ntraining on large datasets or with significant data augmentations. Using Frame\nAveraging, we allow to flexibly customize point cloud neural networks built\nwith KPConv layers, by making them exactly invariant and/or equivariant to\ntranslations, rotations and/or reflections of the input point clouds. By simply\nwrapping around an existing KPConv-based network, FA-KPConv embeds geometrical\nprior knowledge into it while preserving the number of learnable parameters and\nnot compromising any input information. We showcase the benefit of such an\nintroduced bias for point cloud classification and point cloud registration,\nespecially in challenging cases such as scarce training data or randomly\nrotated test data.\n","authors":["Ali Alawieh","Alexandru P. Condurache"],"pdf_url":"https://arxiv.org/pdf/2505.04485v1.pdf","comment":"8 pages, 2 figures, accepted at IJCNN 2025"},{"id":"http://arxiv.org/abs/2409.15511v3","updated":"2025-05-07T14:54:37Z","published":"2024-09-23T19:57:08Z","title":"Bayesian computation with generative diffusion models by Multilevel\n  Monte Carlo","summary":"  Generative diffusion models have recently emerged as a powerful strategy to\nperform stochastic sampling in Bayesian inverse problems, delivering remarkably\naccurate solutions for a wide range of challenging applications. However,\ndiffusion models often require a large number of neural function evaluations\nper sample in order to deliver accurate posterior samples. As a result, using\ndiffusion models as stochastic samplers for Monte Carlo integration in Bayesian\ncomputation can be highly computationally expensive, particularly in\napplications that require a substantial number of Monte Carlo samples for\nconducting uncertainty quantification analyses. This cost is especially high in\nlarge-scale inverse problems such as computational imaging, which rely on large\nneural networks that are expensive to evaluate. With quantitative imaging\napplications in mind, this paper presents a Multilevel Monte Carlo strategy\nthat significantly reduces the cost of Bayesian computation with diffusion\nmodels. This is achieved by exploiting cost-accuracy trade-offs inherent to\ndiffusion models to carefully couple models of different levels of accuracy in\na manner that significantly reduces the overall cost of the calculation,\nwithout reducing the final accuracy. The proposed approach achieves a\n$4\\times$-to-$8\\times$ reduction in computational cost w.r.t. standard\ntechniques across three benchmark imaging problems.\n","authors":["Abdul-Lateef Haji-Ali","Marcelo Pereyra","Luke Shaw","Konstantinos Zygalakis"],"pdf_url":"https://arxiv.org/pdf/2409.15511v3.pdf","comment":"13 images"},{"id":"http://arxiv.org/abs/2505.04481v1","updated":"2025-05-07T14:52:02Z","published":"2025-05-07T14:52:02Z","title":"CAD-Llama: Leveraging Large Language Models for Computer-Aided Design\n  Parametric 3D Model Generation","summary":"  Recently, Large Language Models (LLMs) have achieved significant success,\nprompting increased interest in expanding their generative capabilities beyond\ngeneral text into domain-specific areas. This study investigates the generation\nof parametric sequences for computer-aided design (CAD) models using LLMs. This\nendeavor represents an initial step towards creating parametric 3D shapes with\nLLMs, as CAD model parameters directly correlate with shapes in\nthree-dimensional space. Despite the formidable generative capacities of LLMs,\nthis task remains challenging, as these models neither encounter parametric\nsequences during their pretraining phase nor possess direct awareness of 3D\nstructures. To address this, we present CAD-Llama, a framework designed to\nenhance pretrained LLMs for generating parametric 3D CAD models. Specifically,\nwe develop a hierarchical annotation pipeline and a code-like format to\ntranslate parametric 3D CAD command sequences into Structured Parametric CAD\nCode (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we\npropose an adaptive pretraining approach utilizing SPCC, followed by an\ninstruction tuning process aligned with CAD-specific guidelines. This\nmethodology aims to equip LLMs with the spatial knowledge inherent in\nparametric sequences. Experimental results demonstrate that our framework\nsignificantly outperforms prior autoregressive methods and existing LLM\nbaselines.\n","authors":["Jiahao Li","Weijian Ma","Xueyang Li","Yunzhong Lou","Guichun Zhou","Xiangdong Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.04481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02471v2","updated":"2025-05-07T14:48:36Z","published":"2025-05-05T08:56:12Z","title":"Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction","summary":"  We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined.\n","authors":["Inclusion AI","Biao Gong","Cheng Zou","Dandan Zheng","Hu Yu","Jingdong Chen","Jianxin Sun","Junbo Zhao","Jun Zhou","Kaixiang Ji","Lixiang Ru","Libin Wang","Qingpei Guo","Rui Liu","Weilong Chai","Xinyu Xiao","Ziyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2505.02471v2.pdf","comment":"https://github.com/inclusionAI/Ming/tree/main/Ming-unify"},{"id":"http://arxiv.org/abs/2409.04388v4","updated":"2025-05-07T14:35:23Z","published":"2024-09-06T16:27:52Z","title":"Question-Answering Dense Video Events","summary":"  This paper presents question-answering on dense video events, a novel task\nthat answers and grounds dense-event questions in long videos, thus challenging\nMLLMs to faithfully comprehend and reason about multiple events over extended\nperiods of time. To facilitate the study, we construct DeVE-QA -- a dataset\nfeaturing 78K questions about 26K events on 10.6K long videos. Our benchmarking\nshows that state-of-the-art MLLMs struggle on DeVE-QA. For improvement, we\npropose DeVi, a novel training-free MLLM approach that highlights a\nhierarchical captioning module, a temporal event memory module, and a\nself-consistency checking module to respectively detect, contextualize and\nmemorize, and ground dense-events in long videos for question answering.\nExtensive experiments show that DeVi is superior at answering dense-event\nquestions and grounding relevant video moments. Compared with existing MLLMs,\nit achieves a remarkable increase of 4.8% and 2.1% for G(round)QA accuracy on\nDeVE-QA~and NExT-GQA, respectively. Our data and code will be released upon\nacceptance.\n","authors":["Hangyu Qin","Junbin Xiao","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2409.04388v4.pdf","comment":"Accepted to SIGIR'25"},{"id":"http://arxiv.org/abs/2505.04460v1","updated":"2025-05-07T14:31:04Z","published":"2025-05-07T14:31:04Z","title":"Learning Real Facial Concepts for Independent Deepfake Detection","summary":"  Deepfake detection models often struggle with generalization to unseen\ndatasets, manifesting as misclassifying real instances as fake in target\ndomains. This is primarily due to an overreliance on forgery artifacts and a\nlimited understanding of real faces. To address this challenge, we propose a\nnovel approach RealID to enhance generalization by learning a comprehensive\nconcept of real faces while assessing the probabilities of belonging to the\nreal and fake classes independently. RealID comprises two key modules: the Real\nConcept Capture Module (RealC2) and the Independent Dual-Decision Classifier\n(IDC). With the assistance of a MultiReal Memory, RealC2 maintains various\nprototypes for real faces, allowing the model to capture a comprehensive\nconcept of real class. Meanwhile, IDC redefines the classification strategy by\nmaking independent decisions based on the concept of the real class and the\npresence of forgery artifacts. Through the combined effect of the above\nmodules, the influence of forgery-irrelevant patterns is alleviated, and\nextensive experiments on five widely used datasets demonstrate that RealID\nsignificantly outperforms existing state-of-the-art methods, achieving a 1.74%\nimprovement in average accuracy.\n","authors":["Ming-Hui Liu","Harry Cheng","Tianyi Wang","Xin Luo","Xin-Shun Xu"],"pdf_url":"https://arxiv.org/pdf/2505.04460v1.pdf","comment":"Accepted by IJCAI 2025"},{"id":"http://arxiv.org/abs/2306.07971v2","updated":"2025-05-07T14:26:09Z","published":"2023-06-13T17:59:59Z","title":"XrayGPT: Chest Radiographs Summarization using Medical Vision-Language\n  Models","summary":"  The latest breakthroughs in large vision-language models, such as Bard and\nGPT-4, have showcased extraordinary abilities in performing a wide range of\ntasks. Such models are trained on massive datasets comprising billions of\npublic image-text pairs with diverse tasks. However, their performance on\ntask-specific domains, such as radiology, is still under-investigated and\npotentially limited due to a lack of sophistication in understanding biomedical\nimages. On the other hand, conversational medical models have exhibited\nremarkable success but have mainly focused on text-based analysis. In this\npaper, we introduce XrayGPT, a novel conversational medical vision-language\nmodel that can analyze and answer open-ended questions about chest radiographs.\nSpecifically, we align both medical visual encoder (MedClip) with a fine-tuned\nlarge language model (Vicuna), using a simple linear transformation. This\nalignment enables our model to possess exceptional visual conversation\nabilities, grounded in a deep understanding of radiographs and medical domain\nknowledge. To enhance the performance of LLMs in the medical context, we\ngenerate ~217k interactive and high-quality summaries from free-text radiology\nreports. These summaries serve to enhance the performance of LLMs through the\nfine-tuning process. Our approach opens up new avenues the research for\nadvancing the automated analysis of chest radiographs. Our open-source demos,\nmodels, and instruction sets are available at:\nhttps://github.com/mbzuai-oryx/XrayGPT.\n","authors":["Omkar Thawakar","Abdelrahman Shaker","Sahal Shaji Mullappilly","Hisham Cholakkal","Rao Muhammad Anwer","Salman Khan","Jorma Laaksonen","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2306.07971v2.pdf","comment":"Accepted at ACL 2024-BIONLP Workshop. Code:\n  https://github.com/mbzuai-oryx/XrayGPT"},{"id":"http://arxiv.org/abs/2505.02369v3","updated":"2025-05-07T14:21:19Z","published":"2025-05-05T05:13:12Z","title":"Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural\n  Networks","summary":"  Sharpness-Aware Minimization (SAM) improves neural network generalization by\noptimizing the worst-case loss within a neighborhood of parameters, yet it\nperturbs parameters using the entire gradient vector, including components with\nlow statistical significance. We introduce ZSharp, a refined sharpness-aware\noptimization method that incorporates layer-wise Z-score normalization followed\nby percentile-based filtering. This process selects only the most statistically\nsignificant gradient components-those with large standardized magnitudes-for\nconstructing the perturbation direction. ZSharp retains the standard two-phase\nSAM structure of ascent and descent while modifying the ascent step to focus on\nsharper, curvature-relevant directions. We evaluate ZSharp on CIFAR-10,\nCIFAR-100, and Tiny-ImageNet using a range of models including ResNet, VGG, and\nVision Transformers. Across all architectures and datasets, ZSharp consistently\nachieves higher test accuracy compared to SAM, ASAM, and Friendly-SAM. These\nresults indicate that Z-score-based gradient filtering can enhance the\nsharpness sensitivity of the update direction, leading to improved\ngeneralization in deep neural network training.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2505.02369v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03413v2","updated":"2025-05-07T14:20:33Z","published":"2024-12-04T15:49:49Z","title":"Deep Learning for Sea Surface Temperature Reconstruction under Cloud\n  Occlusion","summary":"  Sea Surface Temperature (SST) reconstructions from satellite images affected\nby cloud gaps have been extensively documented in the past three decades. Here\nwe describe several Machine Learning models to fill the cloud-occluded areas\nstarting from MODIS Aqua nighttime L3 images. To tackle this challenge, we\nemployed a type of Convolutional Neural Network model (U-net) to reconstruct\ncloud-covered portions of satellite imagery while preserving the integrity of\nobserved values in cloud-free areas. We demonstrate the outstanding precision\nof U-net with respect to available products done using OI interpolation\nalgorithms. Our best-performing architecture show 50% lower root mean square\nerrors over established gap-filling methods.\n","authors":["Andrea Asperti","Ali Aydogdu","Angelo Greco","Fabio Merizzi","Pietro Miraglio","Beniamino Tartufoli","Alessandro Testa","Nadia Pinardi","Paolo Oddo"],"pdf_url":"https://arxiv.org/pdf/2412.03413v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06057v2","updated":"2025-05-07T14:05:47Z","published":"2023-08-11T10:14:22Z","title":"Illumination and Shadows in Head Rotation: experiments with Denoising\n  Diffusion Models","summary":"  Accurately modeling the effects of illumination and shadows during head\nrotation is critical in computer vision for enhancing image realism and\nreducing artifacts. This study delves into the latent space of denoising\ndiffusion models to identify compelling trajectories that can express\ncontinuous head rotation under varying lighting conditions. A key contribution\nof our work is the generation of additional labels from the CelebA\ndataset,categorizing images into three groups based on prevalent illumination\ndirection: left, center, and right. These labels play a crucial role in our\napproach, enabling more precise manipulations and improved handling of lighting\nvariations. Leveraging a recent embedding technique for Denoising Diffusion\nImplicit Models (DDIM), our method achieves noteworthy manipulations,\nencompassing a wide rotation angle of $\\pm 30$ degrees, while preserving\nindividual distinct characteristics even under challenging illumination\nconditions. Our methodology involves computing trajectories that approximate\nclouds of latent representations of dataset samples with different yaw\nrotations through linear regression. Specific trajectories are obtained by\nanalyzing subsets of data that share significant attributes with the source\nimage, including light direction. Notably, our approach does not require any\nspecific training of the generative model for the task of rotation; we merely\ncompute and follow specific trajectories in the latent space of a pre-trained\nface generation model. This article showcases the potential of our approach and\nits current limitations through a qualitative discussion of notable examples.\nThis study contributes to the ongoing advancements in representation learning\nand the semantic investigation of the latent space of generative models.\n","authors":["Andrea Asperti","Gabriele Colasuonno","Antonio Guerra"],"pdf_url":"https://arxiv.org/pdf/2308.06057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04424v1","updated":"2025-05-07T13:57:42Z","published":"2025-05-07T13:57:42Z","title":"RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential\n  Neural Style Generation","summary":"  Arbitrary style transfer aims to apply the style of any given artistic image\nto another content image. Still, existing deep learning-based methods often\nrequire significant computational costs to generate diverse stylized results.\nMotivated by this, we propose a novel reinforcement learning-based framework\nfor arbitrary style transfer RLMiniStyler. This framework leverages a unified\nreinforcement learning policy to iteratively guide the style transfer process\nby exploring and exploiting stylization feedback, generating smooth sequences\nof stylized results while achieving model lightweight. Furthermore, we\nintroduce an uncertainty-aware multi-task learning strategy that automatically\nadjusts loss weights to adapt to the content and style balance requirements at\ndifferent training stages, thereby accelerating model convergence. Through a\nseries of experiments across image various resolutions, we have validated the\nadvantages of RLMiniStyler over other state-of-the-art methods in generating\nhigh-quality, diverse artistic image sequences at a lower cost. Codes are\navailable at https://github.com/fengxiaoming520/RLMiniStyler.\n","authors":["Jing Hu","Chengming Feng","Shu Hu","Ming-Ching Chang","Xin Li","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2505.04424v1.pdf","comment":"IJCAI2025"},{"id":"http://arxiv.org/abs/2505.04410v1","updated":"2025-05-07T13:46:34Z","published":"2025-05-07T13:46:34Z","title":"DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception","summary":"  Dense visual prediction tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense prediction often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. The ``content'' features are aligned with image crop\nrepresentations to improve local discriminability, while ``context'' features\nlearn to retain the spatial correlations under the guidance of vision\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\nsignificantly outperforms existing methods across multiple open-vocabulary\ndense prediction tasks, including object detection and semantic segmentation.\nCode is available at \\textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.\n","authors":["Junjie Wang","Bin Chen","Yulin Li","Bin Kang","Yichi Chen","Zhuotao Tian"],"pdf_url":"https://arxiv.org/pdf/2505.04410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04408v1","updated":"2025-05-07T13:46:10Z","published":"2025-05-07T13:46:10Z","title":"MFSeg: Efficient Multi-frame 3D Semantic Segmentation","summary":"  We propose MFSeg, an efficient multi-frame 3D semantic segmentation\nframework. By aggregating point cloud sequences at the feature level and\nregularizing the feature extraction and aggregation process, MFSeg reduces\ncomputational overhead while maintaining high accuracy. Moreover, by employing\na lightweight MLP-based point decoder, our method eliminates the need to\nupsample redundant points from past frames. Experiments on the nuScenes and\nWaymo datasets show that MFSeg outperforms existing methods, demonstrating its\neffectiveness and efficiency.\n","authors":["Chengjie Huang","Krzysztof Czarnecki"],"pdf_url":"https://arxiv.org/pdf/2505.04408v1.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2505.02567v2","updated":"2025-05-07T13:27:21Z","published":"2025-05-05T11:18:03Z","title":"Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities","summary":"  Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey are available on GitHub\n(https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).\n","authors":["Xinjie Zhang","Jintao Guo","Shanshan Zhao","Minghao Fu","Lunhao Duan","Guo-Hua Wang","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.02567v2.pdf","comment":"This work is still in progress; Github project:\n  https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models"},{"id":"http://arxiv.org/abs/2505.04397v1","updated":"2025-05-07T13:21:25Z","published":"2025-05-07T13:21:25Z","title":"Deep residual learning with product units","summary":"  We propose a deep product-unit residual neural network (PURe) that integrates\nproduct units into residual blocks to improve the expressiveness and parameter\nefficiency of deep convolutional networks. Unlike standard summation neurons,\nproduct units enable multiplicative feature interactions, potentially offering\na more powerful representation of complex patterns. PURe replaces conventional\nconvolutional layers with 2D product units in the second layer of each residual\nblock, eliminating nonlinear activation functions to preserve structural\ninformation. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,\nPURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper\nResNet152, while converging nearly five times faster and demonstrating strong\nrobustness to Poisson noise. On ImageNet, PURe architectures outperform\nstandard ResNet models at similar depths, with PURe34 achieving a top-1\naccuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet\nvariants (ResNet50, ResNet101) while utilizing significantly fewer parameters\nand computational resources. On CIFAR-10, PURe consistently outperforms ResNet\nvariants across varying depths, with PURe272 reaching 95.01% test accuracy,\ncomparable to ResNet1001 but at less than half the model size. These results\ndemonstrate that PURe achieves a favorable balance between accuracy,\nefficiency, and robustness. Compared to traditional residual networks, PURe not\nonly achieves competitive classification performance with faster convergence\nand fewer parameters, but also demonstrates greater robustness to noise. Its\neffectiveness across diverse datasets highlights the potential of\nproduct-unit-based architectures for scalable and reliable deep learning in\ncomputer vision.\n","authors":["Ziyuan Li","Uwe Jaekel","Babette Dellen"],"pdf_url":"https://arxiv.org/pdf/2505.04397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04394v1","updated":"2025-05-07T13:18:43Z","published":"2025-05-07T13:18:43Z","title":"SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin\n  Transformer","summary":"  This paper presents an efficient visual speech encoder for lip reading. While\nmost recent lip reading studies have been based on the ResNet architecture and\nhave achieved significant success, they are not sufficiently suitable for\nefficiently capturing lip reading features due to high computational complexity\nin modeling spatio-temporal information. Additionally, using a complex visual\nmodel not only increases the complexity of lip reading models but also induces\ndelays in the overall network for multi-modal studies (e.g., audio-visual\nspeech recognition, speech enhancement, and speech separation). To overcome the\nlimitations of Convolutional Neural Network (CNN)-based models, we apply the\nhierarchical structure and window self-attention of the Swin Transformer to lip\nreading. We configure a new lightweight scale of the Swin Transformer suitable\nfor processing lip reading data and present the SwinLip visual speech encoder,\nwhich efficiently reduces computational load by integrating modified\nConvolution-augmented Transformer (Conformer) temporal embeddings with\nconventional spatial embeddings in the hierarchical structure. Through\nextensive experiments, we have validated that our SwinLip successfully improves\nthe performance and inference speed of the lip reading network when applied to\nvarious backbones for word and sentence recognition, reducing computational\nload. In particular, our SwinLip demonstrated robust performance in both\nEnglish LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art\nperformance on the Mandarin LRW-1000 dataset with less computation compared to\nthe existing state-of-the-art model.\n","authors":["Young-Hu Park","Rae-Hong Park","Hyung-Min Park"],"pdf_url":"https://arxiv.org/pdf/2505.04394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04392v1","updated":"2025-05-07T13:17:05Z","published":"2025-05-07T13:17:05Z","title":"Predicting Road Surface Anomalies by Visual Tracking of a Preceding\n  Vehicle","summary":"  A novel approach to detect road surface anomalies by visual tracking of a\npreceding vehicle is proposed. The method is versatile, predicting any kind of\nroad anomalies, such as potholes, bumps, debris, etc., unlike direct\nobservation methods that rely on training visual detectors of those cases. The\nmethod operates in low visibility conditions or in dense traffic where the\nanomaly is occluded by a preceding vehicle. Anomalies are detected\npredictively, i.e., before a vehicle encounters them, which allows to\npre-configure low-level vehicle systems (such as chassis) or to plan an\navoidance maneuver in case of autonomous driving. A challenge is that the\nsignal coming from camera-based tracking of a preceding vehicle may be weak and\ndisturbed by camera ego motion due to vibrations affecting the ego vehicle.\nTherefore, we propose an efficient method to compensate camera pitch rotation\nby an iterative robust estimator. Our experiments on both controlled setup and\nnormal traffic conditions show that road anomalies can be detected reliably at\na distance even in challenging cases where the ego vehicle traverses imperfect\nroad surfaces. The method is effective and performs in real time on standard\nconsumer hardware.\n","authors":["Petr Jahoda","Jan Cech"],"pdf_url":"https://arxiv.org/pdf/2505.04392v1.pdf","comment":"Accepted to the IEEE Intelligent Vehicles Symposium (IV), 2025"},{"id":"http://arxiv.org/abs/2505.04387v1","updated":"2025-05-07T13:11:35Z","published":"2025-05-07T13:11:35Z","title":"Geometry-Aware Texture Generation for 3D Head Modeling with\n  Artist-driven Control","summary":"  Creating realistic 3D head assets for virtual characters that match a precise\nartistic vision remains labor-intensive. We present a novel framework that\nstreamlines this process by providing artists with intuitive control over\ngenerated 3D heads. Our approach uses a geometry-aware texture synthesis\npipeline that learns correlations between head geometry and skin texture maps\nacross different demographics. The framework offers three levels of artistic\ncontrol: manipulation of overall head geometry, adjustment of skin tone while\npreserving facial characteristics, and fine-grained editing of details such as\nwrinkles or facial hair. Our pipeline allows artists to make edits to a single\ntexture map using familiar tools, with our system automatically propagating\nthese changes coherently across the remaining texture maps needed for realistic\nrendering. Experiments demonstrate that our method produces diverse results\nwith clean geometries. We showcase practical applications focusing on intuitive\ncontrol for artists, including skin tone adjustments and simplified editing\nworkflows for adding age-related details or removing unwanted features from\nscanned models. This integrated approach aims to streamline the artistic\nworkflow in virtual character creation.\n","authors":["Amin Fadaeinejad","Abdallah Dib","Luiz Gustavo Hafemann","Emeline Got","Trevor Anderson","Amaury Depierre","Nikolaus F. Troje","Marcus A. Brubaker","Marc-André Carbonneau"],"pdf_url":"https://arxiv.org/pdf/2505.04387v1.pdf","comment":"11 pages, 9 figures, AI for Creative Visual Content Generation\n  Editing and Understanding (CVEU), CVPRW 2025"},{"id":"http://arxiv.org/abs/2502.01547v3","updated":"2025-05-07T13:06:56Z","published":"2025-02-03T17:29:52Z","title":"mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech\n  Recognition","summary":"  Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio\nand can improve performance in noise, but most methods are trained only on\nEnglish data. One limitation is the lack of large-scale multilingual video\ndata, which makes it hard to train models from scratch. In this work, we\npropose mWhisper-Flamingo for multilingual AVSR which combines the strengths of\na pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable\nbetter multi-modal integration and improve the noisy multilingual performance,\nwe introduce decoder modality dropout where the model is trained both on paired\naudio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo\nachieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages.\nAudio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on\nall languages in noisy conditions.\n","authors":["Andrew Rouditchenko","Samuel Thomas","Hilde Kuehne","Rogerio Feris","James Glass"],"pdf_url":"https://arxiv.org/pdf/2502.01547v3.pdf","comment":"Accepted in Signal Processing Letters. Code at\n  https://github.com/roudimit/whisper-flamingo"},{"id":"http://arxiv.org/abs/2505.04384v1","updated":"2025-05-07T13:05:32Z","published":"2025-05-07T13:05:32Z","title":"DATA: Multi-Disentanglement based Contrastive Learning for Open-World\n  Semi-Supervised Deepfake Attribution","summary":"  Deepfake attribution (DFA) aims to perform multiclassification on different\nfacial manipulation techniques, thereby mitigating the detrimental effects of\nforgery content on the social order and personal reputations. However, previous\nmethods focus only on method-specific clues, which easily lead to overfitting,\nwhile overlooking the crucial role of common forgery features. Additionally,\nthey struggle to distinguish between uncertain novel classes in more practical\nopen-world scenarios. To address these issues, in this paper we propose an\ninnovative multi-DisentAnglement based conTrastive leArning framework, DATA, to\nenhance the generalization ability on novel classes for the open-world\nsemi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all\ngeneration techniques can be abstracted into a similar architecture, DATA\ndefines the concept of 'Orthonormal Deepfake Basis' for the first time and\nutilizes it to disentangle method-specific features, thereby reducing the\noverfitting on forgery-irrelevant information. Furthermore, an augmented-memory\nmechanism is designed to assist in novel class discovery and contrastive\nlearning, which aims to obtain clear class boundaries for the novel classes\nthrough instance-level disentanglements. Additionally, to enhance the\nstandardization and discrimination of features, DATA uses bases contrastive\nloss and center contrastive loss as auxiliaries for the aforementioned modules.\nExtensive experimental evaluations show that DATA achieves state-of-the-art\nperformance on the OSS-DFA benchmark, e.g., there are notable accuracy\nimprovements in 2.55% / 5.7% under different settings, compared with the\nexisting methods.\n","authors":["Ming-Hui Liu","Xiao-Qian Liu","Xin Luo","Xin-Shun Xu"],"pdf_url":"https://arxiv.org/pdf/2505.04384v1.pdf","comment":"Accepted by IEEE TMM on 17-Jan-2025; Submitted to IEEE TMM on\n  11-Jul-2024"},{"id":"http://arxiv.org/abs/2504.02287v3","updated":"2025-05-07T13:04:48Z","published":"2025-04-03T05:23:08Z","title":"MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action\n  Recognition and Transformer-based Sensor Fusion","summary":"  Multi-modal multi-view action recognition is a rapidly growing field in\ncomputer vision, offering significant potential for applications in\nsurveillance. However, current datasets often fail to address real-world\nchallenges such as wide-area distributed settings, asynchronous data streams,\nand the lack of frame-level annotations. Furthermore, existing methods face\ndifficulties in effectively modeling inter-view relationships and enhancing\nspatial feature learning. In this paper, we introduce the MultiSensor-Home\ndataset, a novel benchmark designed for comprehensive action recognition in\nhome environments, and also propose the Multi-modal Multi-view\nTransformer-based Sensor Fusion (MultiTSF) method. The proposed\nMultiSensor-Home dataset features untrimmed videos captured by distributed\nsensors, providing high-resolution RGB and audio data along with detailed\nmulti-view frame-level action labels. The proposed MultiTSF method leverages a\nTransformer-based fusion mechanism to dynamically model inter-view\nrelationships. Furthermore, the proposed method integrates a human detection\nmodule to enhance spatial feature learning, guiding the model to prioritize\nframes with human activity to enhance action the recognition accuracy.\nExperiments on the proposed MultiSensor-Home and the existing MM-Office\ndatasets demonstrate the superiority of MultiTSF over the state-of-the-art\nmethods. Quantitative and qualitative results highlight the effectiveness of\nthe proposed method in advancing real-world multi-modal multi-view action\nrecognition. The source code is available at\nhttps://github.com/thanhhff/MultiTSF.\n","authors":["Trung Thanh Nguyen","Yasutomo Kawanishi","Vijay John","Takahiro Komamizu","Ichiro Ide"],"pdf_url":"https://arxiv.org/pdf/2504.02287v3.pdf","comment":"The 19th IEEE International Conference on Automatic Face and Gesture\n  Recognition (FG 2025)"},{"id":"http://arxiv.org/abs/2505.04380v1","updated":"2025-05-07T13:00:49Z","published":"2025-05-07T13:00:49Z","title":"Tetrahedron-Net for Medical Image Registration","summary":"  Medical image registration plays a vital role in medical image processing.\nExtracting expressive representations for medical images is crucial for\nimproving the registration quality. One common practice for this end is\nconstructing a convolutional backbone to enable interactions with skip\nconnections among feature extraction layers. The de facto structure, U-Net-like\nnetworks, has attempted to design skip connections such as nested or full-scale\nones to connect one single encoder and one single decoder to improve its\nrepresentation capacity. Despite being effective, it still does not fully\nexplore interactions with a single encoder and decoder architectures. In this\npaper, we embrace this observation and introduce a simple yet effective\nalternative strategy to enhance the representations for registrations by\nappending one additional decoder. The new decoder is designed to interact with\nboth the original encoder and decoder. In this way, it not only reuses feature\npresentation from corresponding layers in the encoder but also interacts with\nthe original decoder to corporately give more accurate registration results.\nThe new architecture is concise yet generalized, with only one encoder and two\ndecoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net.\nThree instantiations of Tetrahedron-Net are further constructed regarding the\ndifferent structures of the appended decoder. Our extensive experiments prove\nthat superior performance can be obtained on several representative benchmarks\nof medical image registration. Finally, such a ``Tetrahedron'' design can also\nbe easily integrated into popular U-Net-like architectures including\nVoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.\n","authors":["Jinhai Xiang","Shuai Guo","Qianru Han","Dantong Shi","Xinwei He","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2505.04380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02784v2","updated":"2025-05-07T12:58:32Z","published":"2025-05-05T16:54:04Z","title":"Advances in Automated Fetal Brain MRI Segmentation and Biometry:\n  Insights from the FeTA 2024 Challenge","summary":"  Accurate fetal brain tissue segmentation and biometric analysis are essential\nfor studying brain development in utero. The FeTA Challenge 2024 advanced\nautomated fetal brain MRI analysis by introducing biometry prediction as a new\ntask alongside tissue segmentation. For the first time, our diverse\nmulti-centric test set included data from a new low-field (0.55T) MRI dataset.\nEvaluation metrics were also expanded to include the topology-specific Euler\ncharacteristic difference (ED). Sixteen teams submitted segmentation methods,\nmost of which performed consistently across both high- and low-field scans.\nHowever, longitudinal trends indicate that segmentation accuracy may be\nreaching a plateau, with results now approaching inter-rater variability. The\nED metric uncovered topological differences that were missed by conventional\nmetrics, while the low-field dataset achieved the highest segmentation scores,\nhighlighting the potential of affordable imaging systems when paired with\nhigh-quality reconstruction. Seven teams participated in the biometry task, but\nmost methods failed to outperform a simple baseline that predicted measurements\nbased solely on gestational age, underscoring the challenge of extracting\nreliable biometric estimates from image data alone. Domain shift analysis\nidentified image quality as the most significant factor affecting model\ngeneralization, with super-resolution pipelines also playing a substantial\nrole. Other factors, such as gestational age, pathology, and acquisition site,\nhad smaller, though still measurable, effects. Overall, FeTA 2024 offers a\ncomprehensive benchmark for multi-class segmentation and biometry estimation in\nfetal brain MRI, underscoring the need for data-centric approaches, improved\ntopological evaluation, and greater dataset diversity to enable clinically\nrobust and generalizable AI tools.\n","authors":["Vladyslav Zalevskyi","Thomas Sanchez","Misha Kaandorp","Margaux Roulet","Diego Fajardo-Rojas","Liu Li","Jana Hutter","Hongwei Bran Li","Matthew Barkovich","Hui Ji","Luca Wilhelmi","Aline Dändliker","Céline Steger","Mériam Koob","Yvan Gomez","Anton Jakovčić","Melita Klaić","Ana Adžić","Pavel Marković","Gracia Grabarić","Milan Rados","Jordina Aviles Verdera","Gregor Kasprian","Gregor Dovjak","Raphael Gaubert-Rachmühl","Maurice Aschwanden","Qi Zeng","Davood Karimi","Denis Peruzzo","Tommaso Ciceri","Giorgio Longari","Rachika E. Hamadache","Amina Bouzid","Xavier Lladó","Simone Chiarella","Gerard Martí-Juan","Miguel Ángel González Ballester","Marco Castellaro","Marco Pinamonti","Valentina Visani","Robin Cremese","Keïn Sam","Fleur Gaudfernau","Param Ahir","Mehul Parikh","Maximilian Zenk","Michael Baumgartner","Klaus Maier-Hein","Li Tianhong","Yang Hong","Zhao Longfei","Domen Preloznik","Žiga Špiclin","Jae Won Choi","Muyang Li","Jia Fu","Guotai Wang","Jingwen Jiang","Lyuyang Tong","Bo Du","Milton O. Candela-Leal","Andrea Gondova","Sungmin You","Abdul Qayyum","Moona Mazher","Steven A Niederer","Andras Jakab","Roxane Licandro","Kelly Payette","Meritxell Bach Cuadra"],"pdf_url":"https://arxiv.org/pdf/2505.02784v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04376v1","updated":"2025-05-07T12:57:40Z","published":"2025-05-07T12:57:40Z","title":"Label-efficient Single Photon Images Classification via Active Learning","summary":"  Single-photon LiDAR achieves high-precision 3D imaging in extreme\nenvironments through quantum-level photon detection technology. Current\nresearch primarily focuses on reconstructing 3D scenes from sparse photon\nevents, whereas the semantic interpretation of single-photon images remains\nunderexplored, due to high annotation costs and inefficient labeling\nstrategies. This paper presents the first active learning framework for\nsingle-photon image classification. The core contribution is an imaging\ncondition-aware sampling strategy that integrates synthetic augmentation to\nmodel variability across imaging conditions. By identifying samples where the\nmodel is both uncertain and sensitive to these conditions, the proposed method\nselectively annotates only the most informative examples. Experiments on both\nsynthetic and real-world datasets show that our approach outperforms all\nbaselines and achieves high classification accuracy with significantly fewer\nlabeled samples. Specifically, our approach achieves 97% accuracy on synthetic\nsingle-photon data using only 1.5% labeled samples. On real-world data, we\nmaintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher\nthan the best-performing baseline. This illustrates that active learning\nenables the same level of classification performance on single-photon images as\non classical images, opening doors to large-scale integration of single-photon\ndata in real-world applications.\n","authors":["Zili Zhang","Ziting Wen","Yiheng Qiang","Hongzhou Dong","Wenle Dong","Xinyang Li","Xiaofan Wang","Xiaoqiang Ren"],"pdf_url":"https://arxiv.org/pdf/2505.04376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04375v1","updated":"2025-05-07T12:53:13Z","published":"2025-05-07T12:53:13Z","title":"Balancing Accuracy, Calibration, and Efficiency in Active Learning with\n  Vision Transformers Under Label Noise","summary":"  Fine-tuning pre-trained convolutional neural networks on ImageNet for\ndownstream tasks is well-established. Still, the impact of model size on the\nperformance of vision transformers in similar scenarios, particularly under\nlabel noise, remains largely unexplored. Given the utility and versatility of\ntransformer architectures, this study investigates their practicality under\nlow-budget constraints and noisy labels. We explore how classification accuracy\nand calibration are affected by symmetric label noise in active learning\nsettings, evaluating four vision transformer configurations (Base and Large\nwith 16x16 and 32x32 patch sizes) and three Swin Transformer configurations\n(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label\nnoise rates. Our findings show that larger ViT models (ViTl32 in particular)\nconsistently outperform their smaller counterparts in both accuracy and\ncalibration, even under moderate to high label noise, while Swin Transformers\nexhibit weaker robustness across all noise levels. We find that smaller patch\nsizes do not always lead to better performance, as ViTl16 performs consistently\nworse than ViTl32 while incurring a higher computational cost. We also find\nthat information-based Active Learning strategies only provide meaningful\naccuracy improvements at moderate label noise rates, but they result in poorer\ncalibration compared to models trained on randomly acquired labels, especially\nat high label noise rates. We hope these insights provide actionable guidance\nfor practitioners looking to deploy vision transformers in resource-constrained\nenvironments, where balancing model complexity, label noise, and compute\nefficiency is critical in model fine-tuning or distillation.\n","authors":["Moseli Mots'oehli","Hope Mogale","Kyungim Baek"],"pdf_url":"https://arxiv.org/pdf/2505.04375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04369v1","updated":"2025-05-07T12:37:01Z","published":"2025-05-07T12:37:01Z","title":"WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image\n  Dehazing","summary":"  In this paper, we reveal a novel haze-specific wavelet degradation prior\nobserved through wavelet transform analysis, which shows that haze-related\ninformation predominantly resides in low-frequency components. Exploiting this\ninsight, we propose a novel dehazing framework, WDMamba, which decomposes the\nimage dehazing task into two sequential stages: low-frequency restoration\nfollowed by detail enhancement. This coarse-to-fine strategy enables WDMamba to\neffectively capture features specific to each stage of the dehazing process,\nresulting in high-quality restored images. Specifically, in the low-frequency\nrestoration stage, we integrate Mamba blocks to reconstruct global structures\nwith linear complexity, efficiently removing overall haze and producing a\ncoarse restored image. Thereafter, the detail enhancement stage reinstates\nfine-grained information that may have been overlooked during the previous\nphase, culminating in the final dehazed output. Furthermore, to enhance detail\nretention and achieve more natural dehazing, we introduce a self-guided\ncontrastive regularization during network training. By utilizing the coarse\nrestored output as a hard negative example, our model learns more\ndiscriminative representations, substantially boosting the overall dehazing\nperformance. Extensive evaluations on public dehazing benchmarks demonstrate\nthat our method surpasses state-of-the-art approaches both qualitatively and\nquantitatively. Code is available at https://github.com/SunJ000/WDMamba.\n","authors":["Jie Sun","Heng Liu","Yongzhen Wang","Xiao-Ping Zhang","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2505.04369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04347v1","updated":"2025-05-07T11:47:35Z","published":"2025-05-07T11:47:35Z","title":"CountDiffusion: Text-to-Image Synthesis with Training-Free\n  Counting-Guidance Diffusion","summary":"  Stable Diffusion has advanced text-to-image synthesis, but training models to\ngenerate images with accurate object quantity is still difficult due to the\nhigh computational cost and the challenge of teaching models the abstract\nconcept of quantity. In this paper, we propose CountDiffusion, a training-free\nframework aiming at generating images with correct object quantity from textual\ndescriptions. CountDiffusion consists of two stages. In the first stage, an\nintermediate denoising result is generated by the diffusion model to predict\nthe final synthesized image with one-step denoising, and a counting model is\nused to count the number of objects in this image. In the second stage, a\ncorrection module is used to correct the object quantity by changing the\nattention map of the object with universal guidance. The proposed\nCountDiffusion can be plugged into any diffusion-based text-to-image (T2I)\ngeneration models without further training. Experiment results demonstrate the\nsuperiority of our proposed CountDiffusion, which improves the accurate object\nquantity generation ability of T2I models by a large margin.\n","authors":["Yanyu Li","Pencheng Wan","Liang Han","Yaowei Wang","Liqiang Nie","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.04347v1.pdf","comment":"8 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.13117v2","updated":"2025-05-07T11:40:48Z","published":"2024-08-23T14:40:40Z","title":"End-to-end Surface Optimization for Light Control","summary":"  Designing a freeform surface to reflect or refract light to achieve a target\ndistribution is a challenging inverse problem. In this paper, we propose an\nend-to-end optimization strategy for an optical surface mesh. Our formulation\nleverages a novel differentiable rendering model, and is directly driven by the\ndifference between the resulting light distribution and the target\ndistribution. We also enforce geometric constraints related to fabrication\nrequirements, to facilitate CNC milling and polishing of the designed surface.\nTo address the issue of local minima, we formulate a face-based optimal\ntransport problem between the current mesh and the target distribution, which\nmakes effective large changes to the surface shape. The combination of our\noptimal transport update and rendering-guided optimization produces an optical\nsurface design with a resulting image closely resembling the target, while the\ngeometric constraints in our optimization help to ensure consistency between\nthe rendering model and the final physical results. The effectiveness of our\nalgorithm is demonstrated on a variety of target images using both simulated\nrendering and physical prototypes.\n","authors":["Yuou Sun","Bailin Deng","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.13117v2.pdf","comment":"This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive Version of Record was\n  published in ACM Transactions on Graphics, https://doi.org/10.1145/3732284"},{"id":"http://arxiv.org/abs/2502.00205v2","updated":"2025-05-07T11:40:20Z","published":"2025-01-31T22:46:20Z","title":"EcoWeedNet: A Lightweight and Automated Weed Detection Method for\n  Sustainable Next-Generation Agricultural Consumer Electronics","summary":"  Sustainable agriculture plays a crucial role in ensuring world food security\nfor consumers. A critical challenge faced by sustainable precision agriculture\nis weed growth, as weeds compete for essential resources with crops, such as\nwater, soil nutrients, and sunlight, which notably affect crop yields. The\nadoption of automated computer vision technologies and ground agricultural\nconsumer electronic vehicles in precision agriculture offers sustainable,\nlow-carbon solutions. However, prior works suffer from issues such as low\naccuracy and precision, as well as high computational expense. This work\nproposes EcoWeedNet, a novel model that enhances weed detection performance\nwithout introducing significant computational complexity, aligning with the\ngoals of low-carbon agricultural practices. The effectiveness of the proposed\nmodel is demonstrated through comprehensive experiments on the CottonWeedDet12\nbenchmark dataset, which reflects real-world scenarios. EcoWeedNet achieves\nperformance comparable to that of large models (mAP@0.5 = 95.2%), yet with\nsignificantly fewer parameters (approximately 4.21% of the parameters of\nYOLOv4), lower computational complexity and better computational efficiency\n6.59% of the GFLOPs of YOLOv4). These key findings indicate EcoWeedNet's\ndeployability on low-power consumer hardware, lower energy consumption, and\nhence reduced carbon footprint, thereby emphasizing the application prospects\nof EcoWeedNet in next-generation sustainable agriculture. These findings\nprovide the way forward for increased application of environmentally-friendly\nagricultural consumer technologies.\n","authors":["Omar H. Khater","Abdul Jabbar Siddiqui","M. Shamim Hossain","Aiman El-Maleh"],"pdf_url":"https://arxiv.org/pdf/2502.00205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04472v2","updated":"2025-05-07T11:31:28Z","published":"2024-12-05T18:59:58Z","title":"Stereo Anywhere: Robust Zero-Shot Deep Stereo Matching Even Where Either\n  Stereo or Mono Fail","summary":"  We introduce Stereo Anywhere, a novel stereo-matching framework that combines\ngeometric constraints with robust priors from monocular depth Vision Foundation\nModels (VFMs). By elegantly coupling these complementary worlds through a\ndual-branch architecture, we seamlessly integrate stereo matching with learned\ncontextual cues. Following this design, our framework introduces novel cost\nvolume fusion mechanisms that effectively handle critical challenges such as\ntextureless regions, occlusions, and non-Lambertian surfaces. Through our novel\noptical illusion dataset, MonoTrap, and extensive evaluation across multiple\nbenchmarks, we demonstrate that our synthetic-only trained model achieves\nstate-of-the-art results in zero-shot generalization, significantly\noutperforming existing solutions while showing remarkable robustness to\nchallenging cases such as mirrors and transparencies.\n","authors":["Luca Bartolomei","Fabio Tosi","Matteo Poggi","Stefano Mattoccia"],"pdf_url":"https://arxiv.org/pdf/2412.04472v2.pdf","comment":"CVPR 2025. Code: https://github.com/bartn8/stereoanywhere - Project\n  page: https://stereoanywhere.github.io/"},{"id":"http://arxiv.org/abs/2505.04320v1","updated":"2025-05-07T11:11:23Z","published":"2025-05-07T11:11:23Z","title":"Multi-turn Consistent Image Editing","summary":"  Many real-world applications, such as interactive photo retouching, artistic\ncontent creation, and product design, require flexible and iterative image\nediting. However, existing image editing methods primarily focus on achieving\nthe desired modifications in a single step, which often struggles with\nambiguous user intent, complex transformations, or the need for progressive\nrefinements. As a result, these methods frequently produce inconsistent\noutcomes or fail to meet user expectations. To address these challenges, we\npropose a multi-turn image editing framework that enables users to iteratively\nrefine their edits, progressively achieving more satisfactory results. Our\napproach leverages flow matching for accurate image inversion and a\ndual-objective Linear Quadratic Regulators (LQR) for stable sampling,\neffectively mitigating error accumulation. Additionally, by analyzing the\nlayer-wise roles of transformers, we introduce a adaptive attention\nhighlighting method that enhances editability while preserving multi-turn\ncoherence. Extensive experiments demonstrate that our framework significantly\nimproves edit success rates and visual fidelity compared to existing methods.\n","authors":["Zijun Zhou","Yingying Deng","Xiangyu He","Weiming Dong","Fan Tang"],"pdf_url":"https://arxiv.org/pdf/2505.04320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01880v2","updated":"2025-05-07T10:52:03Z","published":"2025-05-03T17:57:57Z","title":"Weakly-supervised Audio Temporal Forgery Localization via Progressive\n  Audio-language Co-learning Network","summary":"  Audio temporal forgery localization (ATFL) aims to find the precise forgery\nregions of the partial spoof audio that is purposefully modified. Existing ATFL\nmethods rely on training efficient networks using fine-grained annotations,\nwhich are obtained costly and challenging in real-world scenarios. To meet this\nchallenge, in this paper, we propose a progressive audio-language co-learning\nnetwork (LOCO) that adopts co-learning and self-supervision manners to prompt\nlocalization performance under weak supervision scenarios. Specifically, an\naudio-language co-learning module is first designed to capture forgery\nconsensus features by aligning semantics from temporal and global perspectives.\nIn this module, forgery-aware prompts are constructed by using utterance-level\nannotations together with learnable prompts, which can incorporate semantic\npriors into temporal content features dynamically. In addition, a forgery\nlocalization module is applied to produce forgery proposals based on fused\nforgery-class activation sequences. Finally, a progressive refinement strategy\nis introduced to generate pseudo frame-level labels and leverage supervised\nsemantic contrastive learning to amplify the semantic distinction between real\nand fake content, thereby continuously optimizing forgery-aware features.\nExtensive experiments show that the proposed LOCO achieves SOTA performance on\nthree public benchmarks.\n","authors":["Junyan Wu","Wenbo Xu","Wei Lu","Xiangyang Luo","Rui Yang","Shize Guo"],"pdf_url":"https://arxiv.org/pdf/2505.01880v2.pdf","comment":"9pages, 5figures. This paper has been accepted for IJCAI2025"},{"id":"http://arxiv.org/abs/2505.04306v1","updated":"2025-05-07T10:29:39Z","published":"2025-05-07T10:29:39Z","title":"MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition","summary":"  With the continuous impact of epidemics, people have become accustomed to\nwearing masks. However, most current occluded face recognition (OFR) algorithms\nlack prior knowledge of occlusions, resulting in poor performance when dealing\nwith occluded faces of varying types and severity in reality. Recognizing\noccluded faces is still a significant challenge, which greatly affects the\nconvenience of people's daily lives. In this paper, we propose an\nidentity-gated mixture of diffusion experts (MoDE) for OFR. Each\ndiffusion-based generative expert estimates one possible complete image for\noccluded faces. Considering the random sampling process of the diffusion model,\nwhich introduces inevitable differences and variations between the inpainted\nfaces and the real ones. To ensemble effective information from\nmulti-reconstructed faces, we introduce an identity-gating network to evaluate\nthe contribution of each reconstructed face to the identity and adaptively\nintegrate the predictions in the decision space. Moreover, our MoDE is a\nplug-and-play module for most existing face recognition models. Extensive\nexperiments on three public face datasets and two datasets in the wild validate\nour advanced performance for various occlusions in comparison with the\ncompeting methods.\n","authors":["Qiannan Fan","Zhuoyang Li","Jitong Li","Chenyang Cao"],"pdf_url":"https://arxiv.org/pdf/2505.04306v1.pdf","comment":"8 pages,7 figures"},{"id":"http://arxiv.org/abs/2502.11178v2","updated":"2025-05-07T10:18:31Z","published":"2025-02-16T15:58:54Z","title":"DA-Mamba: Domain Adaptive Hybrid Mamba-Transformer Based One-Stage\n  Object Detection","summary":"  Recent 2D CNN-based domain adaptation approaches struggle with long-range\ndependencies due to limited receptive fields, making it difficult to adapt to\ntarget domains with significant spatial distribution changes. While\ntransformer-based domain adaptation methods better capture distant\nrelationships through self-attention mechanisms that facilitate more effective\ncross-domain feature alignment, their quadratic computational complexity makes\npractical deployment challenging for object detection tasks across diverse\ndomains. Inspired by the global modeling and linear computation complexity of\nthe Mamba architecture, we present the first domain-adaptive Mamba-based\none-stage object detection model, termed DA-Mamba. Specifically, we combine\nMamba's efficient state-space modeling with attention mechanisms to address\ndomain-specific spatial and channel-wise variations. Our design leverages\ndomain-adaptive spatial and channel-wise scanning within the Mamba block to\nextract highly transferable representations for efficient sequential\nprocessing, while cross-attention modules generate long-range, mixed-domain\nspatial features to enable robust soft alignment across domains. Besides,\nmotivated by the observation that hybrid architectures introduce feature noise\nin domain adaptation tasks, we propose an entropy-based knowledge distillation\nframework with margin ReLU, which adaptively refines multi-level\nrepresentations by suppressing irrelevant activations and aligning uncertainty\nacross source and target domains. Finally, to prevent overfitting caused by the\nmixed-up features generated through cross-attention mechanisms, we propose\nentropy-driven gating attention with random perturbations that simultaneously\nrefine target features and enhance model generalization.\n","authors":["A. Enes Doruk","Hasan F. Ates"],"pdf_url":"https://arxiv.org/pdf/2502.11178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03783v3","updated":"2025-05-07T10:09:56Z","published":"2025-04-03T16:12:03Z","title":"FAST: Federated Active Learning with Foundation Models for\n  Communication-efficient Sampling and Training","summary":"  Federated Active Learning (FAL) has emerged as a promising framework to\nleverage large quantities of unlabeled data across distributed clients while\npreserving data privacy. However, real-world deployments remain limited by high\nannotation costs and communication-intensive sampling processes, particularly\nin a cross-silo setting, when clients possess substantial local datasets. This\npaper addresses the crucial question: What is the best practice to reduce\ncommunication costs in human-in-the-loop learning with minimal annotator\neffort? Existing FAL methods typically rely on iterative annotation processes\nthat separate active sampling from federated updates, leading to multiple\nrounds of expensive communication and annotation. In response, we introduce\nFAST, a two-pass FAL framework that harnesses foundation models for weak\nlabeling in a preliminary pass, followed by a refinement pass focused\nexclusively on the most uncertain samples. By leveraging representation\nknowledge from foundation models and integrating refinement steps into a\nstreamlined workflow, FAST substantially reduces the overhead incurred by\niterative active sampling. Extensive experiments on diverse medical and natural\nimage benchmarks demonstrate that FAST outperforms existing FAL methods by an\naverage of 4.36% while reducing communication rounds eightfold under a limited\n5% labeling budget.\n","authors":["Haoyuan Li","Mathias Funk","Jindong Wang","Aaqib Saeed"],"pdf_url":"https://arxiv.org/pdf/2504.03783v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03631v2","updated":"2025-05-07T10:07:00Z","published":"2025-05-06T15:29:32Z","title":"Breaking Annotation Barriers: Generalized Video Quality Assessment via\n  Ranking-based Self-Supervision","summary":"  Video quality assessment (VQA) is essential for quantifying perceptual\nquality in various video processing workflows, spanning from camera capture\nsystems to over-the-top streaming platforms. While recent supervised VQA models\nhave made substantial progress, the reliance on manually annotated datasets --\na process that is labor-intensive, costly, and difficult to scale up -- has\nhindered further optimization of their generalization to unseen video content\nand distortions. To bridge this gap, we introduce a self-supervised learning\nframework for VQA to learn quality assessment capabilities from large-scale,\nunlabeled web videos. Our approach leverages a \\textbf{learning-to-rank}\nparadigm to train a large multimodal model (LMM) on video pairs automatically\nlabeled via two manners, including quality pseudo-labeling by existing VQA\nmodels and relative quality ranking based on synthetic distortion simulations.\nFurthermore, we introduce a novel \\textbf{iterative self-improvement training\nstrategy}, where the trained model acts an improved annotator to iteratively\nrefine the annotation quality of training data. By training on a dataset\n$10\\times$ larger than the existing VQA benchmarks, our model: (1) achieves\nzero-shot performance on in-domain VQA benchmarks that matches or surpasses\nsupervised models; (2) demonstrates superior out-of-distribution (OOD)\ngeneralization across diverse video content and distortions; and (3) sets a new\nstate-of-the-art when fine-tuned on human-labeled datasets. Extensive\nexperimental results validate the effectiveness of our self-supervised approach\nin training generalized VQA models. The datasets and code will be publicly\nreleased to facilitate future research.\n","authors":["Linhan Cao","Wei Sun","Kaiwei Zhang","Yicong Peng","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2505.03631v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18504v3","updated":"2025-05-07T09:39:42Z","published":"2025-01-30T17:13:32Z","title":"CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to\n  Sustainability Data Extraction","summary":"  Large Language Model (LLM) image recognition is a powerful tool for\nextracting data from images, but accuracy depends on providing sufficient cues\nin the prompt - requiring a domain expert for specialized tasks. We introduce\nCue Learning using Evolution for Accurate Recognition (CLEAR), which uses a\ncombination of LLMs and evolutionary computation to generate and optimize cues\nsuch that recognition of specialized features in images is improved. It\nachieves this by auto-generating a novel domain-specific representation and\nthen using it to optimize suitable textual cues with a genetic algorithm. We\napply CLEAR to the real-world task of identifying sustainability data from\ninterior and exterior images of buildings. We investigate the effects of using\na variable-length representation compared to fixed-length and show how LLM\nconsistency can be improved by refactoring from categorical to real-valued\nestimates. We show that CLEAR enables higher accuracy compared to expert human\nrecognition and human-authored prompts in every task with error rates improved\nby up to two orders of magnitude and an ablation study evincing solution\nconcision.\n","authors":["Peter J. Bentley","Soo Ling Lim","Fuyuki Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2501.18504v3.pdf","comment":"9 pages plus 2 pages of supplemental material"},{"id":"http://arxiv.org/abs/2505.04281v1","updated":"2025-05-07T09:35:05Z","published":"2025-05-07T09:35:05Z","title":"TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement","summary":"  This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing\nextremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes\nnoisy images by constructing multiple virtual cameras based on a noise space.\nCamera Feature Integration (CFI) modules are then designed to enable the model\nto learn generalizable features across diverse virtual cameras. During the\naligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is\nfine-tuned using a small amount of real RAW data to adapt to the noise\ncharacteristics of specific cameras. A structural reparameterization technique\nfurther simplifies CFI$^T$ for efficient deployment. To address color shifts\nduring the diffusion process, a color corrector is introduced to ensure color\nconsistency by dynamically adjusting global color distributions. Additionally,\na novel dataset, QID, is constructed, featuring quantifiable illumination\nlevels and a wide dynamic range, providing a comprehensive benchmark for\ntraining and evaluation under extreme low-light conditions. Experimental\nresults demonstrate that TS-Diff achieves state-of-the-art performance on\nmultiple datasets, including QID, SID, and ELD, excelling in denoising,\ngeneralization, and color consistency across various cameras and illumination\nlevels. These findings highlight the robustness and versatility of TS-Diff,\nmaking it a practical solution for low-light imaging applications. Source codes\nand models are available at https://github.com/CircccleK/TS-Diff\n","authors":["Yi Li","Zhiyuan Zhang","Jiangnan Xia","Jianghan Cheng","Qilong Wu","Junwei Li","Yibin Tian","Hui Kong"],"pdf_url":"https://arxiv.org/pdf/2505.04281v1.pdf","comment":"International Joint Conference on Neural Networks (IJCNN)"},{"id":"http://arxiv.org/abs/2505.04276v1","updated":"2025-05-07T09:26:37Z","published":"2025-05-07T09:26:37Z","title":"HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for\n  3D Human Pose Estimation","summary":"  We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that\nintegrates Transformer, Graph Convolutional Network (GCN), and diffusion model\ninto a unified framework. HDiffTG leverages the strengths of these techniques\nto significantly improve pose estimation accuracy and robustness while\nmaintaining a lightweight design. The Transformer captures global\nspatiotemporal dependencies, the GCN models local skeletal structures, and the\ndiffusion model provides step-by-step optimization for fine-tuning, achieving a\ncomplementary balance between global and local features. This integration\nenhances the model's ability to handle pose estimation under occlusions and in\ncomplex scenarios. Furthermore, we introduce lightweight optimizations to the\nintegrated model and refine the objective function design to reduce\ncomputational overhead without compromising performance. Evaluation results on\nthe Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves\nstate-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling\nin both accuracy and computational efficiency. Additionally, the model exhibits\nexceptional robustness in noisy and occluded environments. Source codes and\nmodels are available at https://github.com/CirceJie/HDiffTG\n","authors":["Yajie Fu","Chaorui Huang","Junwei Li","Hui Kong","Yibin Tian","Huakang Li","Zhiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.04276v1.pdf","comment":"8 pages, 4 figures, International Joint Conference on Neural Networks\n  (IJCNN)"},{"id":"http://arxiv.org/abs/2505.04270v1","updated":"2025-05-07T09:20:12Z","published":"2025-05-07T09:20:12Z","title":"Object-Shot Enhanced Grounding Network for Egocentric Video","summary":"  Egocentric video grounding is a crucial task for embodied intelligence\napplications, distinct from exocentric video moment localization. Existing\nmethods primarily focus on the distributional differences between egocentric\nand exocentric videos but often neglect key characteristics of egocentric\nvideos and the fine-grained information emphasized by question-type queries. To\naddress these limitations, we propose OSGNet, an Object-Shot enhanced Grounding\nNetwork for egocentric video. Specifically, we extract object information from\nvideos to enrich video representation, particularly for objects highlighted in\nthe textual query but not directly captured in the video features.\nAdditionally, we analyze the frequent shot movements inherent to egocentric\nvideos, leveraging these features to extract the wearer's attention\ninformation, which enhances the model's ability to perform modality alignment.\nExperiments conducted on three datasets demonstrate that OSGNet achieves\nstate-of-the-art performance, validating the effectiveness of our approach. Our\ncode can be found at https://github.com/Yisen-Feng/OSGNet.\n","authors":["Yisen Feng","Haoyu Zhang","Meng Liu","Weili Guan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2505.04270v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2505.04262v1","updated":"2025-05-07T09:12:45Z","published":"2025-05-07T09:12:45Z","title":"Bridging Geometry-Coherent Text-to-3D Generation with Multi-View\n  Diffusion Priors and Gaussian Splatting","summary":"  Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to\nadvance text-to-3D generation but neglects multi-view correlations, being prone\nto geometric inconsistencies and multi-face artifacts in the generated 3D\ncontent. In this work, we propose Coupled Score Distillation (CSD), a framework\nthat couples multi-view joint distribution priors to ensure geometrically\nconsistent 3D generation while enabling the stable and direct optimization of\n3D Gaussian Splatting. Specifically, by reformulating the optimization as a\nmulti-view joint optimization problem, we derive an effective optimization rule\nthat effectively couples multi-view priors to guide optimization across\ndifferent viewpoints while preserving the diversity of generated 3D assets.\nAdditionally, we propose a framework that directly optimizes 3D Gaussian\nSplatting (3D-GS) with random initialization to generate geometrically\nconsistent 3D content. We further employ a deformable tetrahedral grid,\ninitialized from 3D-GS and refined through CSD, to produce high-quality,\nrefined meshes. Quantitative and qualitative experimental results demonstrate\nthe efficiency and competitive quality of our approach.\n","authors":["Feng Yang","Wenliang Qian","Wangmeng Zuo","Hui Li"],"pdf_url":"https://arxiv.org/pdf/2505.04262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04258v1","updated":"2025-05-07T09:03:26Z","published":"2025-05-07T09:03:26Z","title":"RGB-Event Fusion with Self-Attention for Collision Prediction","summary":"  Ensuring robust and real-time obstacle avoidance is critical for the safe\noperation of autonomous robots in dynamic, real-world environments. This paper\nproposes a neural network framework for predicting the time and collision\nposition of an unmanned aerial vehicle with a dynamic object, using RGB and\nevent-based vision sensors. The proposed architecture consists of two separate\nencoder branches, one for each modality, followed by fusion by self-attention\nto improve prediction accuracy. To facilitate benchmarking, we leverage the\nABCD [8] dataset collected that enables detailed comparisons of single-modality\nand fusion-based approaches. At the same prediction throughput of 50Hz, the\nexperimental results show that the fusion-based model offers an improvement in\nprediction accuracy over single-modality approaches of 1% on average and 10%\nfor distances beyond 0.5m, but comes at the cost of +71% in memory and + 105%\nin FLOPs. Notably, the event-based model outperforms the RGB model by 4% for\nposition and 26% for time error at a similar computational cost, making it a\ncompetitive alternative. Additionally, we evaluate quantized versions of the\nevent-based models, applying 1- to 8-bit quantization to assess the trade-offs\nbetween predictive performance and computational efficiency. These findings\nhighlight the trade-offs of multi-modal perception using RGB and event-based\ncameras in robotic applications.\n","authors":["Pietro Bonazzi","Christian Vogt","Michael Jost","Haotong Qin","Lyes Khacef","Federico Paredes-Valles","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2505.04258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01709v2","updated":"2025-05-07T08:37:17Z","published":"2025-05-03T06:17:18Z","title":"RoBridge: A Hierarchical Architecture Bridging Cognition and Execution\n  for General Robotic Manipulation","summary":"  Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation.\n","authors":["Kaidong Zhang","Rongtao Xu","Pengzhen Ren","Junfan Lin","Hefeng Wu","Liang Lin","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2505.01709v2.pdf","comment":"project page: https://abliao.github.io/RoBridge/"},{"id":"http://arxiv.org/abs/2502.10156v3","updated":"2025-05-07T08:31:03Z","published":"2025-02-14T13:36:00Z","title":"MonoForce: Learnable Image-conditioned Physics Engine","summary":"  We propose a novel model for the prediction of robot trajectories on rough\noffroad terrain from the onboard camera images. This model enforces the laws of\nclassical mechanics through a physics-aware neural symbolic layer while\npreserving the ability to learn from large-scale data as it is end-to-end\ndifferentiable. The proposed hybrid model integrates a black-box component that\npredicts robot-terrain interaction forces with a neural-symbolic layer. This\nlayer includes a differentiable physics engine that computes the robot's\ntrajectory by querying these forces at the points of contact with the terrain.\nAs the proposed architecture comprises substantial geometrical and physics\npriors, the resulting model can also be seen as a learnable physics engine\nconditioned on real images that delivers $10^4$ trajectories per second. We\nargue and empirically demonstrate that this architecture reduces the\nsim-to-real gap and mitigates out-of-distribution sensitivity. The\ndifferentiability, in conjunction with the rapid simulation speed, makes the\nmodel well-suited for various applications including model predictive control,\ntrajectory shooting, supervised and reinforcement learning or SLAM. The codes\nand data are publicly available.\n","authors":["Ruslan Agishev","Karel Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2502.10156v3.pdf","comment":"Code: https://github.com/ctu-vras/monoforce"},{"id":"http://arxiv.org/abs/2505.04229v1","updated":"2025-05-07T08:27:18Z","published":"2025-05-07T08:27:18Z","title":"A Weak Supervision Learning Approach Towards an Equitable Parking Lot\n  Occupancy Estimation","summary":"  The scarcity and high cost of labeled high-resolution imagery have long\nchallenged remote sensing applications, particularly in low-income regions\nwhere high-resolution data are scarce. In this study, we propose a weak\nsupervision framework that estimates parking lot occupancy using 3m resolution\nsatellite imagery. By leveraging coarse temporal labels -- based on the\nassumption that parking lots of major supermarkets and hardware stores in\nGermany are typically full on Saturdays and empty on Sundays -- we train a\npairwise comparison model that achieves an AUC of 0.92 on large parking lots.\nThe proposed approach minimizes the reliance on expensive high-resolution\nimages and holds promise for scalable urban mobility analysis. Moreover, the\nmethod can be adapted to assess transit patterns and resource allocation in\nvulnerable communities, providing a data-driven basis to improve the well-being\nof those most in need.\n","authors":["Theophilus Aidoo","Till Koebe","Akansh Maurya","Hewan Shrestha","Ingmar Weber"],"pdf_url":"https://arxiv.org/pdf/2505.04229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04214v1","updated":"2025-05-07T08:08:58Z","published":"2025-05-07T08:08:58Z","title":"CM1 -- A Dataset for Evaluating Few-Shot Information Extraction with\n  Large Vision Language Models","summary":"  The automatic extraction of key-value information from handwritten documents\nis a key challenge in document analysis. A reliable extraction is a\nprerequisite for the mass digitization efforts of many archives. Large Vision\nLanguage Models (LVLM) are a promising technology to tackle this problem\nespecially in scenarios where little annotated training data is available. In\nthis work, we present a novel dataset specifically designed to evaluate the\nfew-shot capabilities of LVLMs. The CM1 documents are a historic collection of\nforms with handwritten entries created in Europe to administer the Care and\nMaintenance program after World War Two. The dataset establishes three\nbenchmarks on extracting name and birthdate information and, furthermore,\nconsiders different training set sizes. We provide baseline results for two\ndifferent LVLMs and compare performances to an established full-page extraction\nmodel. While the traditional full-page model achieves highly competitive\nperformances, our experiments show that when only a few training samples are\navailable the considered LVLMs benefit from their size and heavy pretraining\nand outperform the classical approach.\n","authors":["Fabian Wolf","Oliver Tüselmann","Arthur Matei","Lukas Hennies","Christoph Rass","Gernot A. Fink"],"pdf_url":"https://arxiv.org/pdf/2505.04214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.03482v3","updated":"2025-05-07T08:08:45Z","published":"2022-02-07T19:40:20Z","title":"Navigating Neural Space: Revisiting Concept Activation Vectors to\n  Overcome Directional Divergence","summary":"  With a growing interest in understanding neural network prediction\nstrategies, Concept Activation Vectors (CAVs) have emerged as a popular tool\nfor modeling human-understandable concepts in the latent space. Commonly, CAVs\nare computed by leveraging linear classifiers optimizing the separability of\nlatent representations of samples with and without a given concept. However, in\nthis paper we show that such a separability-oriented computation leads to\nsolutions, which may diverge from the actual goal of precisely modeling the\nconcept direction. This discrepancy can be attributed to the significant\ninfluence of distractor directions, i.e., signals unrelated to the concept,\nwhich are picked up by filters (i.e., weights) of linear models to optimize\nclass-separability. To address this, we introduce pattern-based CAVs, solely\nfocussing on concept signals, thereby providing more accurate concept\ndirections. We evaluate various CAV methods in terms of their alignment with\nthe true concept direction and their impact on CAV applications, including\nconcept sensitivity testing and model correction for shortcut behavior caused\nby data artifacts. We demonstrate the benefits of pattern-based CAVs using the\nPediatric Bone Age, ISIC2019, and FunnyBirds datasets with VGG, ResNet, ReXNet,\nEfficientNet, and Vision Transformer as model architectures.\n","authors":["Frederik Pahde","Maximilian Dreyer","Leander Weber","Moritz Weckbecker","Christopher J. Anders","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2202.03482v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20468v2","updated":"2025-05-07T08:03:08Z","published":"2025-04-29T07:05:24Z","title":"Antidote: A Unified Framework for Mitigating LVLM Hallucinations in\n  Counterfactual Presupposition and Object Perception","summary":"  Large Vision-Language Models (LVLMs) have achieved impressive results across\nvarious cross-modal tasks. However, hallucinations, i.e., the models generating\ncounterfactual responses, remain a challenge. Though recent studies have\nattempted to alleviate object perception hallucinations, they focus on the\nmodels' response generation, and overlooking the task question itself. This\npaper discusses the vulnerability of LVLMs in solving counterfactual\npresupposition questions (CPQs), where the models are prone to accept the\npresuppositions of counterfactual objects and produce severe hallucinatory\nresponses. To this end, we introduce \"Antidote\", a unified, synthetic\ndata-driven post-training framework for mitigating both types of hallucination\nabove. It leverages synthetic data to incorporate factual priors into questions\nto achieve self-correction, and decouple the mitigation process into a\npreference optimization problem. Furthermore, we construct \"CP-Bench\", a novel\nbenchmark to evaluate LVLMs' ability to correctly handle CPQs and produce\nfactual responses. Applied to the LLaVA series, Antidote can simultaneously\nenhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR\nby 30-50%, all without relying on external supervision from stronger LVLMs or\nhuman feedback and introducing noticeable catastrophic forgetting issues.\n","authors":["Yuanchen Wu","Lu Zhang","Hang Yao","Junlong Du","Ke Yan","Shouhong Ding","Yunsheng Wu","Xiaoqiang Li"],"pdf_url":"https://arxiv.org/pdf/2504.20468v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2505.04207v1","updated":"2025-05-07T07:58:57Z","published":"2025-05-07T07:58:57Z","title":"An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection\n  and Measurement","summary":"  Potholes cause vehicle damage and traffic accidents, creating serious safety\nand economic problems. Therefore, early and accurate detection of potholes is\ncrucial. Existing detection methods are usually only based on 2D RGB images and\ncannot accurately analyze the physical characteristics of potholes. In this\npaper, a publicly available dataset of RGB-D images (PothRGBD) is created and\nan improved YOLOv8-based model is proposed for both pothole detection and\npothole physical features analysis. The Intel RealSense D415 depth camera was\nused to collect RGB and depth data from the road surfaces, resulting in a\nPothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable\nfor segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg\narchitecture, which is structurally improved with Dynamic Snake Convolution\n(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit\n(GELU). The proposed model segmented potholes with irregular edge structure\nmore accurately, and performed perimeter and depth measurements on depth maps\nwith high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,\n85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to\n93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in\nprecision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model\nperforms pothole detection as well as perimeter and depth measurement with high\naccuracy and is suitable for real-time applications due to its low model\ncomplexity. In this way, a lightweight and effective model that can be used in\ndeep learning-based intelligent transportation solutions has been acquired.\n","authors":["Mustafa Yurdakul","Şakir Tasdemir"],"pdf_url":"https://arxiv.org/pdf/2505.04207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04201v1","updated":"2025-05-07T07:55:35Z","published":"2025-05-07T07:55:35Z","title":"SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense\n  Reasoning in Open-Ended Scenarios","summary":"  This paper explores the challenges of integrating tactile sensing into\nintelligent systems for multimodal reasoning, particularly in enabling\ncommonsense reasoning about the open-ended physical world. We identify two key\nchallenges: modality discrepancy, where existing large touch-language models\noften treat touch as a mere sub-modality of language, and open-ended tactile\ndata scarcity, where current datasets lack the diversity, open-endness and\ncomplexity needed for reasoning. To overcome these challenges, we introduce\nSToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of\nExperts (MoE) to dynamically process, unify, and manage tactile and language\nmodalities, capturing their unique characteristics. Crucially, we also present\na comprehensive tactile commonsense reasoning dataset and benchmark featuring\nfree-form questions and responses, 8 physical properties, 4 interactive\ncharacteristics, and diverse commonsense knowledge. Experiments show SToLa\nexhibits competitive performance compared to existing models on the PhysiCLeAR\nbenchmark and self-constructed datasets, proving the effectiveness of the\nMixture of Experts architecture in multimodal management and the performance\nadvantages for open-scenario tactile commonsense reasoning tasks.\n","authors":["Ning Cheng","Jinan Xu","Jialing Chen","Wenjuan Han"],"pdf_url":"https://arxiv.org/pdf/2505.04201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04192v1","updated":"2025-05-07T07:41:19Z","published":"2025-05-07T07:41:19Z","title":"VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video\n  Instruction Tuning","summary":"  We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA.\n","authors":["Trinh T. L. Vuong","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2505.04192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04185v1","updated":"2025-05-07T07:34:37Z","published":"2025-05-07T07:34:37Z","title":"S3D: Sketch-Driven 3D Model Generation","summary":"  Generating high-quality 3D models from 2D sketches is a challenging task due\nto the inherent ambiguity and sparsity of sketch data. In this paper, we\npresent S3D, a novel framework that converts simple hand-drawn sketches into\ndetailed 3D models. Our method utilizes a U-Net-based encoder-decoder\narchitecture to convert sketches into face segmentation masks, which are then\nused to generate a 3D representation that can be rendered from novel views. To\nensure robust consistency between the sketch domain and the 3D output, we\nintroduce a novel style-alignment loss that aligns the U-Net bottleneck\nfeatures with the initial encoder outputs of the 3D generation module,\nsignificantly enhancing reconstruction fidelity. To further enhance the\nnetwork's robustness, we apply augmentation techniques to the sketch dataset.\nThis streamlined framework demonstrates the effectiveness of S3D in generating\nhigh-quality 3D models from sketch inputs. The source code for this project is\npublicly available at https://github.com/hailsong/S3D.\n","authors":["Hail Song","Wonsik Shin","Naeun Lee","Soomin Chung","Nojun Kwak","Woontack Woo"],"pdf_url":"https://arxiv.org/pdf/2505.04185v1.pdf","comment":"Accepted as a short paper to the GMCV Workshop at CVPR'25"},{"id":"http://arxiv.org/abs/2311.18681v3","updated":"2025-05-07T07:22:02Z","published":"2023-11-30T16:28:40Z","title":"RaDialog: A Large Vision-Language Model for Radiology Report Generation\n  and Conversational Assistance","summary":"  Conversational AI tools that can generate and discuss clinically correct\nradiology reports for a given medical image have the potential to transform\nradiology. Such a human-in-the-loop radiology assistant could facilitate a\ncollaborative diagnostic process, thus saving time and improving the quality of\nreports. Towards this goal, we introduce RaDialog, the first thoroughly\nevaluated and publicly available large vision-language model for radiology\nreport generation and interactive dialog. RaDialog effectively integrates\nvisual image features and structured pathology findings with a large language\nmodel (LLM) while simultaneously adapting it to a specialized domain using\nparameter-efficient fine-tuning. To keep the conversational abilities of the\nunderlying LLM, we propose a comprehensive, semi-automatically labeled,\nimage-grounded instruct dataset for chest X-ray radiology tasks. By training\nwith this dataset, our method achieves state-of-the-art clinical correctness in\nreport generation and shows impressive abilities in interactive tasks such as\ncorrecting reports and answering questions, serving as a foundational step\ntoward clinical dialog systems. Our code is available on github:\nhttps://github.com/ChantalMP/RaDialog.\n","authors":["Chantal Pellegrini","Ege Özsoy","Benjamin Busam","Nassir Navab","Matthias Keicher"],"pdf_url":"https://arxiv.org/pdf/2311.18681v3.pdf","comment":"Accepted for publication at MIDL 2025"},{"id":"http://arxiv.org/abs/2505.04175v1","updated":"2025-05-07T07:06:04Z","published":"2025-05-07T07:06:04Z","title":"DOTA: Deformable Optimized Transformer Architecture for End-to-End Text\n  Recognition with Retrieval-Augmented Generation","summary":"  Text recognition in natural images remains a challenging yet essential task,\nwith broad applications spanning computer vision and natural language\nprocessing. This paper introduces a novel end-to-end framework that combines\nResNet and Vision Transformer backbones with advanced methodologies, including\nDeformable Convolutions, Retrieval-Augmented Generation, and Conditional Random\nFields (CRF). These innovations collectively enhance feature representation and\nimprove Optical Character Recognition (OCR) performance. Specifically, the\nframework substitutes standard convolution layers in the third and fourth\nblocks with Deformable Convolutions, leverages adaptive dropout for\nregularization, and incorporates CRF for more refined sequence modeling.\nExtensive experiments conducted on six benchmark datasets IC13, IC15, SVT,\nIIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving\nnotable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on\nIIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy\nof 77.77%. These results establish a new state-of-the-art for text recognition,\ndemonstrating the robustness of the approach across diverse and challenging\ndatasets.\n","authors":["Naphat Nithisopa","Teerapong Panboonyuen"],"pdf_url":"https://arxiv.org/pdf/2505.04175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19911v2","updated":"2025-05-07T06:40:11Z","published":"2024-09-30T03:27:33Z","title":"Replace Anyone in Videos","summary":"  The field of controllable human-centric video generation has witnessed\nremarkable progress, particularly with the advent of diffusion models. However,\nachieving precise and localized control over human motion in videos, such as\nreplacing or inserting individuals while preserving desired motion patterns,\nstill remains a formidable challenge. In this work, we present the\nReplaceAnyone framework, which focuses on localized human replacement and\ninsertion featuring intricate backgrounds. Specifically, we formulate this task\nas an image-conditioned video inpainting paradigm with pose guidance, utilizing\na unified end-to-end video diffusion architecture that facilitates\nimage-conditioned video inpainting within masked regions. To prevent shape\nleakage and enable granular local control, we introduce diverse mask forms\ninvolving both regular and irregular shapes. Furthermore, we implement an\nenriched visual guidance mechanism to enhance appearance alignment, a hybrid\ninpainting encoder to further preserve the detailed background information in\nthe masked video, and a two-phase optimization methodology to simplify the\ntraining difficulty. ReplaceAnyone enables seamless replacement or insertion of\ncharacters while maintaining the desired pose motion and reference appearance\nwithin a single framework. Extensive experimental results demonstrate the\neffectiveness of our method in generating realistic and coherent video content.\nThe proposed ReplaceAnyone can be seamlessly applied not only to traditional\n3D-UNet base models but also to DiT-based video models such as Wan2.1. The code\nwill be available at https://github.com/ali-vilab/UniAnimate-DiT.\n","authors":["Xiang Wang","Shiwei Zhang","Haonan Qiu","Ruihang Chu","Zekun Li","Yingya Zhang","Changxin Gao","Yuehuan Wang","Chunhua Shen","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2409.19911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00313v2","updated":"2025-05-07T06:28:51Z","published":"2024-08-31T00:44:03Z","title":"Training-Free Sketch-Guided Diffusion with Latent Optimization","summary":"  Based on recent advanced diffusion models, Text-to-image (T2I) generation\nmodels have demonstrated their capabilities to generate diverse and\nhigh-quality images. However, leveraging their potential for real-world content\ncreation, particularly in providing users with precise control over the image\ngeneration result, poses a significant challenge. In this paper, we propose an\ninnovative training-free pipeline that extends existing text-to-image\ngeneration models to incorporate a sketch as an additional condition. To\ngenerate new images with a layout and structure closely resembling the input\nsketch, we find that these core features of a sketch can be tracked with the\ncross-attention maps of diffusion models. We introduce latent optimization, a\nmethod that refines the noisy latent at each intermediate step of the\ngeneration process using cross-attention maps to ensure that the generated\nimages adhere closely to the desired structure outlined in the reference\nsketch. Through latent optimization, our method enhances the accuracy of image\ngeneration, offering users greater control and customization options in content\ncreation.\n","authors":["Sandra Zhang Ding","Jiafeng Mao","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2409.00313v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2505.02831v2","updated":"2025-05-07T06:26:01Z","published":"2025-05-05T17:58:05Z","title":"No Other Representation Component Is Needed: Diffusion Transformers Can\n  Provide Representation Guidance by Themselves","summary":"  Recent studies have demonstrated that learning a meaningful internal\nrepresentation can both accelerate generative training and enhance generation\nquality of the diffusion transformers. However, existing approaches necessitate\nto either introduce an additional and complex representation training framework\nor rely on a large-scale, pre-trained representation foundation model to\nprovide representation guidance during the original generative training\nprocess. In this study, we posit that the unique discriminative process\ninherent to diffusion transformers enables them to offer such guidance without\nrequiring external representation components. We therefore propose\nSelf-Representation Alignment (SRA), a simple yet straightforward method that\nobtain representation guidance through a self-distillation manner.\nSpecifically, SRA aligns the output latent representation of the diffusion\ntransformer in earlier layer with higher noise to that in later layer with\nlower noise to progressively enhance the overall representation learning during\nonly generative training process. Experimental results indicate that applying\nSRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA\nnot only significantly outperforms approaches relying on auxiliary, complex\nrepresentation training frameworks but also achieves performance comparable to\nmethods that heavily dependent on powerful external representation priors.\n","authors":["Dengyang Jiang","Mengmeng Wang","Liuzhuozheng Li","Lei Zhang","Haoyu Wang","Wei Wei","Guang Dai","Yanning Zhang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.02831v2.pdf","comment":"Self-Representation Alignment for Diffusion Transformers"},{"id":"http://arxiv.org/abs/2504.19186v2","updated":"2025-05-07T06:07:08Z","published":"2025-04-27T10:20:32Z","title":"LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place\n  Recognition","summary":"  In autonomous driving, place recognition is critical for global localization\nin GPS-denied environments. LiDAR and radar-based place recognition methods\nhave garnered increasing attention, as LiDAR provides precise ranging, whereas\nradar excels in adverse weather resilience. However, effectively leveraging\nLiDAR-radar fusion for place recognition remains challenging. The noisy and\nsparse nature of radar data limits its potential to further improve recognition\naccuracy. In addition, heterogeneous radar configurations complicate the\ndevelopment of unified cross-modality fusion frameworks. In this paper, we\npropose LRFusionPR, which improves recognition accuracy and robustness by\nfusing LiDAR with either single-chip or scanning radar. Technically, a\ndual-branch network is proposed to fuse different modalities within the unified\npolar coordinate bird's eye view (BEV) representation. In the fusion branch,\ncross-attention is utilized to perform cross-modality feature interactions. The\nknowledge from the fusion branch is simultaneously transferred to the\ndistillation branch, which takes radar as its only input to further improve the\nrobustness. Ultimately, the descriptors from both branches are concatenated,\nproducing the multimodal global descriptor for place retrieval. Extensive\nevaluations on multiple datasets demonstrate that our LRFusionPR achieves\naccurate place recognition, while maintaining robustness under varying weather\nconditions. Our open-source code will be released at\nhttps://github.com/QiZS-BIT/LRFusionPR.\n","authors":["Zhangshuo Qi","Luqi Cheng","Zijie Zhou","Guangming Xiong"],"pdf_url":"https://arxiv.org/pdf/2504.19186v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.22676v2","updated":"2025-05-07T06:06:33Z","published":"2025-03-28T17:59:43Z","title":"TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D\n  Gaussian Splatting","summary":"  We present TranSplat, a 3D scene rendering algorithm that enables realistic\ncross-scene object transfer (from a source to a target scene) based on the\nGaussian Splatting framework. Our approach addresses two critical challenges:\n(1) precise 3D object extraction from the source scene, and (2) faithful\nrelighting of the transferred object in the target scene without explicit\nmaterial property estimation. TranSplat fits a splatting model to the source\nscene, using 2D object masks to drive fine-grained 3D segmentation. Following\nuser-guided insertion of the object into the target scene, along with automatic\nrefinement of position and orientation, TranSplat derives per-Gaussian radiance\ntransfer functions via spherical harmonic analysis to adapt the object's\nappearance to match the target scene's lighting environment. This relighting\nstrategy does not require explicitly estimating physical scene properties such\nas BRDFs. Evaluated on several synthetic and real-world scenes and objects,\nTranSplat yields excellent 3D object extractions and relighting performance\ncompared to recent baseline methods and visually convincing cross-scene object\ntransfers. We conclude by discussing the limitations of the approach.\n","authors":["Tony Yu","Yanlin Jin","Ashok Veeraraghavan","Akshat Dave","Guha Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2503.22676v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04150v1","updated":"2025-05-07T06:02:27Z","published":"2025-05-07T06:02:27Z","title":"Learning from Similarity Proportion Loss for Classifying Skeletal Muscle\n  Recovery Stages","summary":"  Evaluating the regeneration process of damaged muscle tissue is a fundamental\nanalysis in muscle research to measure experimental effect sizes and uncover\nmechanisms behind muscle weakness due to aging and disease. The conventional\napproach to assessing muscle tissue regeneration involves whole-slide imaging\nand expert visual inspection of the recovery stages based on the morphological\ninformation of cells and fibers. There is a need to replace these tasks with\nautomated methods incorporating machine learning techniques to ensure a\nquantitative and objective analysis. Given the limited availability of fully\nlabeled data, a possible approach is Learning from Label Proportions (LLP), a\nweakly supervised learning method using class label proportions. However,\ncurrent LLP methods have two limitations: (1) they cannot adapt the feature\nextractor for muscle tissues, and (2) they treat the classes representing\nrecovery stages and cell morphological changes as nominal, resulting in the\nloss of ordinal information. To address these issues, we propose Ordinal Scale\nLearning from Similarity Proportion (OSLSP), which uses a similarity proportion\nloss derived from two bag combinations. OSLSP can update the feature extractor\nby using class proportion attention to the ordinal scale of the class. Our\nmodel with OSLSP outperforms large-scale pre-trained and fine-tuning models in\nclassification tasks of skeletal muscle recovery stages.\n","authors":["Yu Yamaoka or Weng Ian Chan","Shigeto Seno","Soichiro Fukada","Hideo Matsuda"],"pdf_url":"https://arxiv.org/pdf/2505.04150v1.pdf","comment":"MICCAI2024 workshop ADSMI in Morocco (oral) [Peer-reviewed]"},{"id":"http://arxiv.org/abs/2505.04147v1","updated":"2025-05-07T05:55:45Z","published":"2025-05-07T05:55:45Z","title":"R^3-VQA: \"Read the Room\" by Video Social Reasoning","summary":"  \"Read the room\" is a significant social reasoning capability in human daily\nlife. Humans can infer others' mental states from subtle social cues. Previous\nsocial reasoning tasks and datasets lack complexity (e.g., simple scenes, basic\ninteractions, incomplete mental state variables, single-step reasoning, etc.)\nand fall far short of the challenges present in real-life social interactions.\nIn this paper, we contribute a valuable, high-quality, and comprehensive video\ndataset named R^3-VQA with precise and fine-grained annotations of social\nevents and mental states (i.e., belief, intent, desire, and emotion) as well as\ncorresponding social causal chains in complex social scenarios. Moreover, we\ninclude human-annotated and model-generated QAs. Our task R^3-VQA includes\nthree aspects: Social Event Understanding, Mental State Estimation, and Social\nCausal Reasoning. As a benchmark, we comprehensively evaluate the social\nreasoning capabilities and consistencies of current state-of-the-art large\nvision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs\nare still far from human-level consistent social reasoning in complex social\nscenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on\nsocial reasoning tasks. We provide some of our dataset and codes in\nsupplementary material and will release our full dataset and codes upon\nacceptance.\n","authors":["Lixing Niu","Jiapeng Li","Xingping Yu","Shu Wang","Ruining Feng","Bo Wu","Ping Wei","Yisen Wang","Lifeng Fan"],"pdf_url":"https://arxiv.org/pdf/2505.04147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01341v2","updated":"2025-05-07T05:54:36Z","published":"2024-09-02T15:50:48Z","title":"Enhancing Test Time Adaptation with Few-shot Guidance","summary":"  Deep neural networks often encounter significant performance drops while\nfacing with domain shifts between training (source) and test (target) data. To\naddress this issue, Test Time Adaptation (TTA) methods have been proposed to\nadapt pre-trained source model to handle out-of-distribution streaming target\ndata. Although these methods offer some relief, they lack a reliable mechanism\nfor domain shift correction, which can often be erratic in real-world\napplications. In response, we develop Few-Shot Test Time Adaptation (FS-TTA), a\nnovel and practical setting that utilizes a few-shot support set on top of TTA.\nAdhering to the principle of few inputs, big gains, FS-TTA reduces blind\nexploration in unseen target domains. Furthermore, we propose a two-stage\nframework to tackle FS-TTA, including (i) fine-tuning the pre-trained source\nmodel with few-shot support set, along with using feature diversity\naugmentation module to avoid overfitting, (ii) implementing test time\nadaptation based on prototype memory bank guidance to produce high quality\npseudo-label for model adaptation. Through extensive experiments on three\ncross-domain classification benchmarks, we demonstrate the superior performance\nand reliability of our FS-TTA and framework.\n","authors":["Siqi Luo","Yi Xin","Yuntao Du","Zhongwei Wan","Tao Tan","Guangtao Zhai","Xiaohong Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01341v2.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.03729v2","updated":"2025-05-07T05:42:27Z","published":"2025-05-06T17:57:12Z","title":"Visual Imitation Enables Contextual Humanoid Control","summary":"  How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.\n","authors":["Arthur Allshire","Hongsuk Choi","Junyi Zhang","David McAllister","Anthony Zhang","Chung Min Kim","Trevor Darrell","Pieter Abbeel","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2505.03729v2.pdf","comment":"Project website: https://www.videomimic.net/"},{"id":"http://arxiv.org/abs/2411.02116v3","updated":"2025-05-07T04:50:21Z","published":"2024-11-04T14:29:28Z","title":"Advancements and limitations of LLMs in replicating human color-word\n  associations","summary":"  Color-word associations play a fundamental role in human cognition and design\napplications. Large Language Models (LLMs) have become widely available and\nhave demonstrated intelligent behaviors in various benchmarks with natural\nconversation skills. However, their ability to replicate human color-word\nassociations remains understudied. We compared multiple generations of LLMs\n(from GPT-3 to GPT-4o) against human color-word associations using data\ncollected from over 10,000 Japanese participants, involving 17 colors and 80\nwords (10 word from eight categories) in Japanese. Our findings reveal a clear\nprogression in LLM performance across generations, with GPT-4o achieving the\nhighest accuracy in predicting the best voted word for each color and category.\nHowever, the highest median performance was approximately 50% even for GPT-4o\nwith visual inputs (chance level of 10%). Moreover, we found performance\nvariations across word categories and colors: while LLMs tended to excel in\ncategories such as Rhythm and Landscape, they struggled with categories such as\nEmotions. Interestingly, color discrimination ability estimated from our\ncolor-word association data showed high correlation with human color\ndiscrimination patterns, consistent with previous studies. Thus, despite\nreasonable alignment in basic color discrimination, humans and LLMs still\ndiverge systematically in the words they assign to those colors. Our study\nhighlights both the advancements in LLM capabilities and their persistent\nlimitations, raising the possibility of systematic differences in semantic\nmemory structures between humans and LLMs in representing color-word\nassociations.\n","authors":["Makoto Fukushima","Shusuke Eshita","Hiroshige Fukuhara"],"pdf_url":"https://arxiv.org/pdf/2411.02116v3.pdf","comment":"20 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2505.04121v1","updated":"2025-05-07T04:29:29Z","published":"2025-05-07T04:29:29Z","title":"Vision Graph Prompting via Semantic Low-Rank Decomposition","summary":"  Vision GNN (ViG) demonstrates superior performance by representing images as\ngraph structures, providing a more natural way to capture irregular semantic\npatterns beyond traditional grid or sequence-based representations. To\nefficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning\ntechniques like visual prompting become increasingly essential. However,\nexisting prompting methods are primarily designed for Transformer-based models,\nneglecting the rich topological relationships among nodes and edges in\ngraph-based representations, limiting their capacity to model complex\nsemantics. In this paper, we propose Vision Graph Prompting (VGP), a novel\nframework tailored for vision graph structures. Our core insight reveals that\nsemantically connected components in the graph exhibit low-rank properties.\nBuilding on this observation, we introduce a semantic low-rank prompting method\nthat decomposes low-rank semantic features and integrates them with prompts on\nvision graph topologies, capturing both global structural patterns and\nfine-grained semantic dependencies. Extensive experiments demonstrate our\nmethod significantly improves ViG's transfer performance on diverse downstream\ntasks, achieving results comparable to full fine-tuning while maintaining\nparameter efficiency. Our code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-VGP.\n","authors":["Zixiang Ai","Zichen Liu","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.04121v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.04119v1","updated":"2025-05-07T04:29:09Z","published":"2025-05-07T04:29:09Z","title":"GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model","summary":"  Pre-trained 3D vision models have gained significant attention for their\npromising performance on point cloud data. However, fully fine-tuning these\nmodels for downstream tasks is computationally expensive and storage-intensive.\nExisting parameter-efficient fine-tuning (PEFT) approaches, which focus\nprimarily on input token prompting, struggle to achieve competitive performance\ndue to their limited ability to capture the geometric information inherent in\npoint clouds. To address this challenge, we propose a novel Geometry-Aware\nPoint Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the\nadaptability of 3D vision models. First, we introduce a Point Prompt that\nserves as an auxiliary input alongside the original point cloud, explicitly\nguiding the model to capture fine-grained geometric details. Additionally, we\npresent a Point Shift Prompter designed to extract global shape information\nfrom the point cloud, enabling instance-specific geometric adjustments at the\ninput level. Moreover, our proposed Prompt Propagation mechanism incorporates\nthe shape information into the model's feature extraction process, further\nstrengthening its ability to capture essential geometric characteristics.\nExtensive experiments demonstrate that GAPrompt significantly outperforms\nstate-of-the-art PEFT methods and achieves competitive results compared to full\nfine-tuning on various benchmarks, while utilizing only 2.19% of trainable\nparameters. Our code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-VGP.\n","authors":["Zixiang Ai","Zichen Liu","Yuanhang Lei","Zhenyu Cui","Xu Zou","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.04119v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2407.01866v2","updated":"2025-05-07T04:17:43Z","published":"2024-07-02T00:45:21Z","title":"Image-GS: Content-Adaptive Image Representation via 2D Gaussians","summary":"  Neural image representations have emerged as a promising approach for\nencoding and rendering visual data. Combined with learning-based workflows,\nthey demonstrate impressive trade-offs between visual fidelity and memory\nfootprint. Existing methods in this domain, however, often rely on fixed data\nstructures that suboptimally allocate memory or compute-intensive implicit\nmodels, hindering their practicality for real-time graphics applications.\n  Inspired by recent advancements in radiance field rendering, we introduce\nImage-GS, a content-adaptive image representation based on 2D Gaussians.\nLeveraging a custom differentiable renderer, Image-GS reconstructs images by\nadaptively allocating and progressively optimizing a group of anisotropic,\ncolored 2D Gaussians. It achieves a favorable balance between visual fidelity\nand memory efficiency across a variety of stylized images frequently seen in\ngraphics workflows, especially for those showing non-uniformly distributed\nfeatures and in low-bitrate regimes. Moreover, it supports hardware-friendly\nrapid random access for real-time usage, requiring only 0.3K MACs to decode a\npixel. Through error-guided progressive optimization, Image-GS naturally\nconstructs a smooth level-of-detail hierarchy. We demonstrate its versatility\nwith several applications, including texture compression, semantics-aware\ncompression, and joint image compression and restoration.\n","authors":["Yunxiang Zhang","Bingxuan Li","Alexandr Kuznetsov","Akshay Jindal","Stavros Diolatzis","Kenneth Chen","Anton Sochenov","Anton Kaplanyan","Qi Sun"],"pdf_url":"https://arxiv.org/pdf/2407.01866v2.pdf","comment":"ACM SIGGRAPH 2025 Conference Proceedings"},{"id":"http://arxiv.org/abs/2505.03204v2","updated":"2025-05-07T04:09:12Z","published":"2025-05-06T05:38:17Z","title":"DCS-ST for Classification of Breast Cancer Histopathology Images with\n  Limited Annotations","summary":"  Deep learning methods have shown promise in classifying breast cancer\nhistopathology images, but their performance often declines with limited\nannotated data, a critical challenge in medical imaging due to the high cost\nand expertise required for annotations.\n","authors":["Liu Suxing","Byungwon Min"],"pdf_url":"https://arxiv.org/pdf/2505.03204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04109v1","updated":"2025-05-07T03:54:59Z","published":"2025-05-07T03:54:59Z","title":"One2Any: One-Reference 6D Pose Estimation for Any Object","summary":"  6D object pose estimation remains challenging for many applications due to\ndependencies on complete 3D models, multi-view images, or training limited to\nspecific object categories. These requirements make generalization to novel\nobjects difficult for which neither 3D models nor multi-view images may be\navailable. To address this, we propose a novel method One2Any that estimates\nthe relative 6-degrees of freedom (DOF) object pose using only a single\nreference-single query RGB-D image, without prior knowledge of its 3D model,\nmulti-view data, or category constraints. We treat object pose estimation as an\nencoding-decoding process, first, we obtain a comprehensive Reference Object\nPose Embedding (ROPE) that encodes an object shape, orientation, and texture\nfrom a single reference view. Using this embedding, a U-Net-based pose decoding\nmodule produces Reference Object Coordinate (ROC) for new views, enabling fast\nand accurate pose estimation. This simple encoding-decoding framework allows\nour model to be trained on any pair-wise pose data, enabling large-scale\ntraining and demonstrating great scalability. Experiments on multiple benchmark\ndatasets demonstrate that our model generalizes well to novel objects,\nachieving state-of-the-art accuracy and robustness even rivaling methods that\nrequire multi-view or CAD inputs, at a fraction of compute.\n","authors":["Mengya Liu","Siyuan Li","Ajad Chhatkuli","Prune Truong","Luc Van Gool","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2505.04109v1.pdf","comment":"accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2505.03603v2","updated":"2025-05-07T03:47:51Z","published":"2025-05-06T15:03:58Z","title":"PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model","summary":"  Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose PAHA, an\nend-to-end audio-driven upper-body human animation framework with diffusion\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\nweights based on pose confidence scores, effectively improving visual quality.\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\nimprove the consistency of motion and co-speech audio. Afterwards, we design\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\nSpeech dataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance.\n","authors":["Y. B. Wang","S. Z. Zhou","J. F. Wu","T. Hu","J. N. Zhang","Y. Liu"],"pdf_url":"https://arxiv.org/pdf/2505.03603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06675v2","updated":"2025-05-07T03:44:54Z","published":"2025-04-09T08:28:53Z","title":"Probability Density Geodesics in Image Diffusion Latent Space","summary":"  Diffusion models indirectly estimate the probability density over a data\nspace, which can be used to study its structure. In this work, we show that\ngeodesics can be computed in diffusion latent space, where the norm induced by\nthe spatially-varying inner product is inversely proportional to the\nprobability density. In this formulation, a path that traverses a high density\n(that is, probable) region of image latent space is shorter than the equivalent\npath through a low density region. We present algorithms for solving the\nassociated initial and boundary value problems and show how to compute the\nprobability density along the path and the geodesic distance between two\npoints. Using these techniques, we analyze how closely video clips approximate\ngeodesics in a pre-trained image diffusion space. Finally, we demonstrate how\nthese techniques can be applied to training-free image sequence interpolation\nand extrapolation, given a pre-trained image diffusion model.\n","authors":["Qingtao Yu","Jaskirat Singh","Zhaoyuan Yang","Peter Henry Tu","Jing Zhang","Hongdong Li","Richard Hartley","Dylan Campbell"],"pdf_url":"https://arxiv.org/pdf/2504.06675v2.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2505.04105v1","updated":"2025-05-07T03:44:28Z","published":"2025-05-07T03:44:28Z","title":"MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction","summary":"  Patient motion during medical image acquisition causes blurring, ghosting,\nand distorts organs, which makes image interpretation challenging.Current\nstate-of-the-art algorithms using Generative Adversarial Network (GAN)-based\nmethods with their ability to learn the mappings between corrupted images and\ntheir ground truth via Structural Similarity Index Measure (SSIM) loss\neffectively generate motion-free images. However, we identified the following\nlimitations: (i) they mainly focus on global structural characteristics and\ntherefore overlook localized features that often carry critical pathological\ninformation, and (ii) the SSIM loss function struggles to handle images with\nvarying pixel intensities, luminance factors, and variance. In this study, we\npropose Motion-Aware Image SYnthesis (MAISY) which initially characterize\nmotion and then uses it for correction by: (a) leveraging the foundation model\nSegment Anything Model (SAM), to dynamically learn spatial patterns along\nanatomical boundaries where motion artifacts are most pronounced and, (b)\nintroducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively\nemphasizes spatial regions with high pixel variance to preserve essential\nanatomical details during artifact correction. Experiments on chest and head CT\ndatasets demonstrate that our model outperformed the state-of-the-art\ncounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by\n10%, and Dice by 16%.\n","authors":["Andrew Zhang","Hao Wang","Shuchang Ye","Michael Fulham","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2505.04105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17767v3","updated":"2025-05-07T03:38:59Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Structures in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated structures as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-structures/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v3.pdf","comment":"Accepted to RSS 2025. Project webpage:\n  https://arjung128.github.io/opening-articulated-structures/"},{"id":"http://arxiv.org/abs/2412.03255v2","updated":"2025-05-07T03:33:08Z","published":"2024-12-04T11:54:57Z","title":"DynamicControl: Adaptive Condition Selection for Improved Text-to-Image\n  Generation","summary":"  To enhance the controllability of text-to-image diffusion models, current\nControlNet-like models have explored various control signals to dictate image\nattributes. However, existing methods either handle conditions inefficiently or\nuse a fixed number of conditions, which does not fully address the complexity\nof multiple conditions and their potential conflicts. This underscores the need\nfor innovative approaches to manage multiple conditions effectively for more\nreliable and detailed image synthesis. To address this issue, we propose a\nnovel framework, DynamicControl, which supports dynamic combinations of diverse\ncontrol signals, allowing adaptive selection of different numbers and types of\nconditions. Our approach begins with a double-cycle controller that generates\nan initial real score sorting for all input conditions by leveraging\npre-trained conditional generation models and discriminative models. This\ncontroller evaluates the similarity between extracted conditions and input\nconditions, as well as the pixel-level similarity with the source image. Then,\nwe integrate a Multimodal Large Language Model (MLLM) to build an efficient\ncondition evaluator. This evaluator optimizes the ordering of conditions based\non the double-cycle controller's score ranking. Our method jointly optimizes\nMLLMs and diffusion models, utilizing MLLMs' reasoning capabilities to\nfacilitate multi-condition text-to-image (T2I) tasks. The final sorted\nconditions are fed into a parallel multi-control adapter, which learns feature\nmaps from dynamic visual conditions and integrates them to modulate ControlNet,\nthereby enhancing control over generated images. Through both quantitative and\nqualitative comparisons, DynamicControl demonstrates its superiority over\nexisting methods in terms of controllability, generation quality and\ncomposability under various conditional controls.\n","authors":["Qingdong He","Jinlong Peng","Pengcheng Xu","Boyuan Jiang","Xiaobin Hu","Donghao Luo","Yong Liu","Yabiao Wang","Chengjie Wang","Xiangtai Li","Jiangning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.03255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04097v1","updated":"2025-05-07T03:32:25Z","published":"2025-05-07T03:32:25Z","title":"3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data\n  Augmentation","summary":"  A three-dimensional convolutional neural network was developed to classify\nT1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D\nconvolution, pooling, batch normalization, dense ReLU layers, and a sigmoid\noutput. Using stochastic noise injection and five-fold cross-validation, the\nmodel achieved test set accuracy of 0.912 and area under the ROC curve of\n0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity\nand specificity both exceeded 0.90. These results align with prior work\nreporting up to 0.10 gain via synthetic augmentation. The findings demonstrate\nthe effectiveness of simple augmentation for 3D MRI classification and motivate\nfuture exploration of advanced augmentation methods and architectures such as\n3D U-Net and vision transformers.\n","authors":["Thien Nhan Vo","Bac Nam Ho","Thanh Xuan Truong"],"pdf_url":"https://arxiv.org/pdf/2505.04097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08280v2","updated":"2025-05-07T03:30:04Z","published":"2025-04-11T06:25:11Z","title":"PNE-SGAN: Probabilistic NDT-Enhanced Semantic Graph Attention Network\n  for LiDAR Loop Closure Detection","summary":"  LiDAR loop closure detection (LCD) is crucial for consistent Simultaneous\nLocalization and Mapping (SLAM) but faces challenges in robustness and\naccuracy. Existing methods, including semantic graph approaches, often suffer\nfrom coarse geometric representations and lack temporal robustness against\nnoise, dynamics, and viewpoint changes. We introduce PNE-SGAN, a Probabilistic\nNDT-Enhanced Semantic Graph Attention Network, to overcome these limitations.\nPNE-SGAN enhances semantic graphs by using Normal Distributions Transform (NDT)\ncovariance matrices as rich, discriminative geometric node features, processed\nvia a Graph Attention Network (GAT). Crucially, it integrates graph similarity\nscores into a probabilistic temporal filtering framework (modeled as an\nHMM/Bayes filter), incorporating uncertain odometry for motion modeling and\nutilizing forward-backward smoothing to effectively handle ambiguities.\nEvaluations on challenging KITTI sequences (00 and 08) demonstrate\nstate-of-the-art performance, achieving Average Precision of 96.2\\% and 95.1\\%,\nrespectively. PNE-SGAN significantly outperforms existing methods, particularly\nin difficult bidirectional loop scenarios where others falter. By synergizing\ndetailed NDT geometry with principled probabilistic temporal reasoning,\nPNE-SGAN offers a highly accurate and robust solution for LiDAR LCD, enhancing\nSLAM reliability in complex, large-scale environments.\n","authors":["Xiong Li","Shulei Liu","Xingning Chen","Yisong Wu","Dong Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.08280v2.pdf","comment":"We discovered a critical implementation bug in Section 4\n  (probabilistic NDT-based semantic graph attention module) that invalidates\n  the results shown in Figures 3-4"},{"id":"http://arxiv.org/abs/2504.05184v3","updated":"2025-05-07T03:21:23Z","published":"2025-04-07T15:35:30Z","title":"MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised\n  Prototypical Contrastive Loss for Coronary DSA Image Segmentation","summary":"  Accurate segmentation of coronary Digital Subtraction Angiography images is\nessential to diagnose and treat coronary artery diseases. Despite advances in\ndeep learning, challenges such as high intra-class variance and class imbalance\nlimit precise vessel delineation. Most existing approaches for coronary DSA\nsegmentation cannot address these issues. Also, existing segmentation network's\nencoders do not directly generate semantic embeddings, which could enable the\ndecoder to reconstruct segmentation masks effectively from these well-defined\nfeatures. We propose a Supervised Prototypical Contrastive Loss that fuses\nsupervised and prototypical contrastive learning to enhance coronary DSA image\nsegmentation. The supervised contrastive loss enforces semantic embeddings in\nthe encoder, improving feature differentiation. The prototypical contrastive\nloss allows the model to focus on the foreground class while alleviating the\nhigh intra-class variance and class imbalance problems by concentrating only on\nthe hard-to-classify background samples. We implement the proposed SPCL loss\nwithin an MSA-UNet3+: a Multi-Scale Attention-Enhanced UNet3+ architecture. The\narchitecture integrates key components: a Multi-Scale Attention Encoder and a\nMulti-Scale Dilated Bottleneck designed to enhance multi-scale feature\nextraction and a Contextual Attention Fusion Module built to keep fine-grained\ndetails while improving contextual understanding. Experiments on a private\ncoronary DSA dataset show that MSA-UNet3+ outperforms state-of-the-art methods,\nachieving the highest Dice coefficient and F1-score and significantly reducing\nASD and ACD. The developed framework provides clinicians with precise vessel\nsegmentation, enabling accurate identification of coronary stenosis and\nsupporting informed diagnostic and therapeutic decisions. The code will be\nreleased at https://github.com/rayanmerghani/MSA-UNet3plus.\n","authors":["Rayan Merghani Ahmed","Adnan Iltaf","Mohamed Elmanna","Gang Zhao","Hongliang Li","Yue Du","Bin Li","Shoujun Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.05184v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2505.04095v1","updated":"2025-05-07T03:18:59Z","published":"2025-05-07T03:18:59Z","title":"Scalable Aerial GNSS Localization for Marine Robots","summary":"  Accurate localization is crucial for water robotics, yet traditional onboard\nGlobal Navigation Satellite System (GNSS) approaches are difficult or\nineffective due to signal reflection on the water's surface and its high cost\nof aquatic GNSS receivers. Existing approaches, such as inertial navigation,\nDoppler Velocity Loggers (DVL), SLAM, and acoustic-based methods, face\nchallenges like error accumulation and high computational complexity.\nTherefore, a more efficient and scalable solution remains necessary. This paper\nproposes an alternative approach that leverages an aerial drone equipped with\nGNSS localization to track and localize a marine robot once it is near the\nsurface of the water. Our results show that this novel adaptation enables\naccurate single and multi-robot marine robot localization.\n","authors":["Shuo Wen","Edwin Meriaux","Mariana Sosa Guzmán","Charlotte Morissette","Chloe Si","Bobak Baghi","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2505.04095v1.pdf","comment":"International Conference on Robotics and Automation 2025 Workshop\n  Robots in the Wild"},{"id":"http://arxiv.org/abs/2412.11026v2","updated":"2025-05-07T03:14:53Z","published":"2024-12-15T02:41:31Z","title":"SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph\n  Generation","summary":"  Dynamic scenes contain intricate spatio-temporal information, crucial for\nmobile robots, UAVs, and autonomous driving systems to make informed decisions.\nParsing these scenes into semantic triplets <Subject-Predicate-Object> for\naccurate Scene Graph Generation (SGG) is highly challenging due to the\nfluctuating spatio-temporal complexity. Inspired by the reasoning capabilities\nof Large Language Models (LLMs), we propose SceneLLM, a novel framework that\nleverages LLMs as powerful scene analyzers for dynamic SGG. Our framework\nintroduces a Video-to-Language (V2L) mapping module that transforms video\nframes into linguistic signals (scene tokens), making the input more\ncomprehensible for LLMs. To better encode spatial information, we devise a\nSpatial Information Aggregation (SIA) scheme, inspired by the structure of\nChinese characters, which encodes spatial data into tokens. Using Optimal\nTransport (OT), we generate an implicit language signal from the frame-level\ntoken sequence that captures the video's spatio-temporal information. To\nfurther improve the LLM's ability to process this implicit linguistic input, we\napply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a\ntransformer-based SGG predictor to decode the LLM's reasoning and predict\nsemantic triplets. Our method achieves state-of-the-art results on the Action\nGenome (AG) benchmark, and extensive experiments show the effectiveness of\nSceneLLM in understanding and generating accurate dynamic scene graphs.\n","authors":["Hang Zhang","Zhuoling Li","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11026v2.pdf","comment":"29 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.18589v3","updated":"2025-05-07T03:09:09Z","published":"2025-04-24T06:16:38Z","title":"Benchmarking Multimodal Mathematical Reasoning with Explicit Visual\n  Dependency","summary":"  Recent advancements in Large Vision-Language Models (LVLMs) have\nsignificantly enhanced their ability to integrate visual and linguistic\ninformation, achieving near-human proficiency in tasks like object recognition,\ncaptioning, and visual question answering. However, current benchmarks\ntypically focus on knowledge-centric evaluations that assess domain-specific\nexpertise, often neglecting the core ability to reason about fundamental\nmathematical elements and visual concepts. We identify a gap in evaluating\nelementary-level math problems, which rely on explicit visual\ndependencies-requiring models to discern, integrate, and reason across multiple\nimages while incorporating commonsense knowledge, all of which are crucial for\nadvancing toward broader AGI capabilities. To address this gap, we introduce\nVCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with\nexplicit visual dependencies. VCBENCH includes 1,720 problems across six\ncognitive domains, featuring 6,697 images (averaging 3.9 per question) to\nensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH,\nrevealing substantial performance disparities, with even the top models unable\nto exceed 50% accuracy. Our findings highlight the ongoing challenges in\nvisual-mathematical integration and suggest avenues for future LVLM\nadvancements.The project can be found at\nhttps://alibaba-damo-academy.github.io/VCBench/.\n","authors":["Zhikai Wang","Jiashuo Sun","Wenqi Zhang","Zhiqiang Hu","Xin Li","Fan Wang","Deli Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.18589v3.pdf","comment":"Home page: https://alibaba-damo-academy.github.io/VCBench/"},{"id":"http://arxiv.org/abs/2505.04088v1","updated":"2025-05-07T03:02:04Z","published":"2025-05-07T03:02:04Z","title":"SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared\n  Target Tracking","summary":"  Thermal infrared (TIR) object tracking often suffers from challenges such as\ntarget occlusion, motion blur, and background clutter, which significantly\ndegrade the performance of trackers. To address these issues, this paper\npro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a\nbidirectional state-space model and a self-attention mechanism. Specifically,\nwe introduce the Motion Mamba module into the Siamese architecture to ex-tract\nmotion features and recover overlooked edge details using bidirectional\nmodeling and self-attention. We propose a Siamese parameter-sharing strate-gy\nthat allows certain convolutional layers to share weights. This approach\nreduces computational redundancy while preserving strong feature\nrepresen-tation. In addition, we design a motion edge-aware regression loss to\nimprove tracking accuracy, especially for motion-blurred targets. Extensive\nexperi-ments are conducted on four TIR tracking benchmarks, including\nLSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT\nachieves superior performance in TIR target tracking.\n","authors":["Shang Zhang","Huanbin Zhang","Dali Feng","Yujie Cui","Ruoyan Xiong","Cen He"],"pdf_url":"https://arxiv.org/pdf/2505.04088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04087v1","updated":"2025-05-07T02:58:37Z","published":"2025-05-07T02:58:37Z","title":"SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for\n  Test-Time Adaptation","summary":"  Test-Time adaptation (TTA) aims to enhance model robustness against\ndistribution shifts through rapid model adaptation during inference. While\nexisting TTA methods often rely on entropy-based unsupervised training and\nachieve promising results, the common practice of a single round of entropy\ntraining is typically unable to adequately utilize reliable samples, hindering\nadaptation efficiency. In this paper, we discover augmentation strategies can\neffectively unleash the potential of reliable samples, but the rapidly growing\ncomputational cost impedes their real-time application. To address this\nlimitation, we propose a novel TTA approach named Single-step Ensemble of\nVicinal Augmentations (SEVA), which can take advantage of data augmentations\nwithout increasing the computational burden. Specifically, instead of\nexplicitly utilizing the augmentation strategy to generate new data, SEVA\ndevelops a theoretical framework to explore the impacts of multiple\naugmentations on model adaptation and proposes to optimize an upper bound of\nthe entropy loss to integrate the effects of multiple rounds of augmentation\ntraining into a single step. Furthermore, we discover and verify that using the\nupper bound as the loss is more conducive to the selection mechanism, as it can\neffectively filter out harmful samples that confuse the model. Combining these\ntwo key advantages, the proposed efficient loss and a complementary selection\nstrategy can simultaneously boost the potential of reliable samples and meet\nthe stringent time requirements of TTA. The comprehensive experiments on\nvarious network architectures across challenging testing scenarios demonstrate\nimpressive performances and the broad adaptability of SEVA. The code will be\npublicly available.\n","authors":["Zixuan Hu","Yichun Hu","Ling-Yu Duan"],"pdf_url":"https://arxiv.org/pdf/2505.04087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21718v2","updated":"2025-05-07T02:35:50Z","published":"2025-04-30T15:05:12Z","title":"VividListener: Expressive and Controllable Listener Dynamics Modeling\n  for Multi-Modal Responsive Interaction","summary":"  Generating responsive listener head dynamics with nuanced emotions and\nexpressive reactions is crucial for practical dialogue modeling in various\nvirtual avatar animations. Previous studies mainly focus on the direct\nshort-term production of listener behavior. They overlook the fine-grained\ncontrol over motion variations and emotional intensity, especially in\nlong-sequence modeling. Moreover, the lack of long-term and large-scale paired\nspeaker-listener corpora including head dynamics and fine-grained\nmulti-modality annotations (e.g., text-based expression descriptions, emotional\nintensity) also limits the application of dialogue modeling.Therefore, we first\nnewly collect a large-scale multi-turn dataset of 3D dyadic conversation\ncontaining more than 1.4M valid frames for multi-modal responsive interaction,\ndubbed ListenerX. Additionally, we propose VividListener, a novel framework\nenabling fine-grained, expressive and controllable listener dynamics modeling.\nThis framework leverages multi-modal conditions as guiding principles for\nfostering coherent interactions between speakers and listeners.Specifically, we\ndesign the Responsive Interaction Module (RIM) to adaptively represent the\nmulti-modal interactive embeddings. RIM ensures the listener dynamics achieve\nfine-grained semantic coordination with textual descriptions and adjustments,\nwhile preserving expressive reaction with speaker behavior. Meanwhile, we\ndesign the Emotional Intensity Tags (EIT) for emotion intensity editing with\nmulti-modal information integration, applying to both text descriptions and\nlistener motion amplitude.Extensive experiments conducted on our newly\ncollected ListenerX dataset demonstrate that VividListener achieves\nstate-of-the-art performance, realizing expressive and controllable listener\ndynamics.\n","authors":["Shiying Li","Xingqun Qi","Bingkun Yang","Chen Weile","Zezhao Tian","Muyi Sun","Qifeng Liu","Man Zhang","Zhenan Sun"],"pdf_url":"https://arxiv.org/pdf/2504.21718v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02406v2","updated":"2025-05-07T02:17:30Z","published":"2025-05-05T06:59:26Z","title":"Token Coordinated Prompt Attention is Needed for Visual Prompting","summary":"  Visual prompting techniques are widely used to efficiently fine-tune\npretrained Vision Transformers (ViT) by learning a small set of shared prompts\nfor all tokens. However, existing methods overlook the unique roles of\ndifferent tokens in conveying discriminative information and interact with all\ntokens using the same prompts, thereby limiting the representational capacity\nof ViT. This often leads to indistinguishable and biased prompt-extracted\nfeatures, hindering performance. To address this issue, we propose a\nplug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns\nspecific coordinated prompts to different tokens for attention-based\ninteractions. Firstly, recognizing the distinct functions of CLS and image\ntokens-global information aggregation and local feature extraction, we\ndisentangle the prompts into CLS Prompts and Image Prompts, which interact\nexclusively with CLS tokens and image tokens through attention mechanisms. This\nenhances their respective discriminative abilities. Furthermore, as different\nimage tokens correspond to distinct image patches and contain diverse\ninformation, we employ a matching function to automatically assign coordinated\nprompts to individual tokens. This enables more precise attention interactions,\nimproving the diversity and representational capacity of the extracted\nfeatures. Extensive experiments across various benchmarks demonstrate that TCPA\nsignificantly enhances the diversity and discriminative power of the extracted\nfeatures. The code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-TCPA.\n","authors":["Zichen Liu","Xu Zou","Gang Hua","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.02406v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22530v3","updated":"2025-05-07T02:08:47Z","published":"2024-10-29T20:53:01Z","title":"Adaptive Aggregation Weights for Federated Segmentation of Pancreas MRI","summary":"  Federated learning (FL) enables collaborative model training across\ninstitutions without sharing sensitive data, making it an attractive solution\nfor medical imaging tasks. However, traditional FL methods, such as Federated\nAveraging (FedAvg), face difficulties in generalizing across domains due to\nvariations in imaging protocols and patient demographics across institutions.\nThis challenge is particularly evident in pancreas MRI segmentation, where\nanatomical variability and imaging artifacts significantly impact performance.\nIn this paper, we conduct a comprehensive evaluation of FL algorithms for\npancreas MRI segmentation and introduce a novel approach that incorporates\nadaptive aggregation weights. By dynamically adjusting the contribution of each\nclient during model aggregation, our method accounts for domain-specific\ndifferences and improves generalization across heterogeneous datasets.\nExperimental results demonstrate that our approach enhances segmentation\naccuracy and reduces the impact of domain shift compared to conventional FL\nmethods while maintaining privacy-preserving capabilities. Significant\nperformance improvements are observed across multiple hospitals (centers).\n","authors":["Hongyi Pan","Gorkem Durak","Zheyuan Zhang","Yavuz Taktak","Elif Keles","Halil Ertugrul Aktas","Alpay Medetalibeyoglu","Yury Velichko","Concetto Spampinato","Ivo Schoots","Marco J. Bruno","Rajesh N. Keswani","Pallavi Tiwari","Candice Bolan","Tamas Gonda","Michael G. Goggins","Michael B. Wallace","Ziyue Xu","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2410.22530v3.pdf","comment":"This paper has been accepted to ISBI 2025"},{"id":"http://arxiv.org/abs/2505.04058v1","updated":"2025-05-07T02:02:15Z","published":"2025-05-07T02:02:15Z","title":"AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene\n  Graphs for 3D Visual Grounding","summary":"  3D visual grounding aims to localize the unique target described by natural\nlanguages in 3D scenes. The significant gap between 3D and language modalities\nmakes it a notable challenge to distinguish multiple similar objects through\nthe described spatial relationships. Current methods attempt to achieve\ncross-modal understanding in complex scenes via a target-centered learning\nmechanism, ignoring the perception of referred objects. We propose a novel\n2D-assisted 3D visual grounding framework that constructs semantic-spatial\nscene graphs with referred object discrimination for relationship perception.\nThe framework incorporates a dual-branch visual encoder that utilizes 2D\npre-trained attributes to guide the multi-modal object encoding. Furthermore,\nour cross-modal interaction module uses graph attention to facilitate\nrelationship-oriented information fusion. The enhanced object representation\nand iterative relational learning enable the model to establish effective\nalignment between 3D vision and referential descriptions. Experimental results\non the popular benchmarks demonstrate our superior performance compared to\nstate-of-the-art methods, especially in addressing the challenges of multiple\nsimilar distractors.\n","authors":["Feng Xiao","Hongbin Xu","Guocan Zhao","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2505.04058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04055v1","updated":"2025-05-07T01:53:16Z","published":"2025-05-07T01:53:16Z","title":"FoodTrack: Estimating Handheld Food Portions with Egocentric Video","summary":"  Accurately tracking food consumption is crucial for nutrition and health\nmonitoring. Traditional approaches typically require specific camera angles,\nnon-occluded images, or rely on gesture recognition to estimate intake, making\nassumptions about bite size rather than directly measuring food volume. We\npropose the FoodTrack framework for tracking and measuring the volume of\nhand-held food items using egocentric video which is robust to hand occlusions\nand flexible with varying camera and object poses. FoodTrack estimates food\nvolume directly, without relying on intake gestures or fixed assumptions about\nbite size, offering a more accurate and adaptable solution for tracking food\nconsumption. We achieve absolute percentage loss of approximately 7.01% on a\nhandheld food object, improving upon a previous approach that achieved a 16.40%\nmean absolute percentage error in its best case, under less flexible\nconditions.\n","authors":["Ervin Wang","Yuhao Chen"],"pdf_url":"https://arxiv.org/pdf/2505.04055v1.pdf","comment":"Accepted as extended abstract at CVPR 2025 Metafood workshop"},{"id":"http://arxiv.org/abs/2411.05261v2","updated":"2025-05-07T01:51:52Z","published":"2024-11-08T01:46:11Z","title":"Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained\n  Image Interpretation for Automated Report Generation","summary":"  Despite significant advancements in automated report generation, the\nopaqueness of text interpretability continues to cast doubt on the reliability\nof the content produced. This paper introduces a novel approach to identify\nspecific image features in X-ray images that influence the outputs of report\ngeneration models. Specifically, we propose Cyclic Vision-Language Manipulator\nCVLM, a module to generate a manipulated X-ray from an original X-ray and its\nreport from a designated report generator. The essence of CVLM is that cycling\nmanipulated X-rays to the report generator produces altered reports aligned\nwith the alterations pre-injected into the reports for X-ray generation,\nachieving the term \"cyclic manipulation\". This process allows direct comparison\nbetween original and manipulated X-rays, clarifying the critical image features\ndriving changes in reports and enabling model users to assess the reliability\nof the generated texts. Empirical evaluations demonstrate that CVLM can\nidentify more precise and reliable features compared to existing explanation\nmethods, significantly enhancing the transparency and applicability of\nAI-generated reports.\n","authors":["Yingying Fang","Zihao Jin","Shaojie Guo","Jinda Liu","Zhiling Yue","Yijian Gao","Junzhi Ning","Zhi Li","Simon Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2411.05261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04052v1","updated":"2025-05-07T01:47:15Z","published":"2025-05-07T01:47:15Z","title":"Person-In-Situ: Scene-Consistent Human Image Insertion with\n  Occlusion-Aware Pose Control","summary":"  Compositing human figures into scene images has broad applications in areas\nsuch as entertainment and advertising. However, existing methods often cannot\nhandle occlusion of the inserted person by foreground objects and unnaturally\nplace the person in the frontmost layer. Moreover, they offer limited control\nover the inserted person's pose. To address these challenges, we propose two\nmethods. Both allow explicit pose control via a 3D body model and leverage\nlatent diffusion models to synthesize the person at a contextually appropriate\ndepth, naturally handling occlusions without requiring occlusion masks. The\nfirst is a two-stage approach: the model first learns a depth map of the scene\nwith the person through supervised learning, and then synthesizes the person\naccordingly. The second method learns occlusion implicitly and synthesizes the\nperson directly from input data without explicit depth supervision.\nQuantitative and qualitative evaluations show that both methods outperform\nexisting approaches by better preserving scene consistency while accurately\nreflecting occlusions and user-specified poses.\n","authors":["Shun Masuda","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2505.04052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04050v1","updated":"2025-05-07T01:41:12Z","published":"2025-05-07T01:41:12Z","title":"TerraFusion: Joint Generation of Terrain Geometry and Texture Using\n  Latent Diffusion Models","summary":"  3D terrain models are essential in fields such as video game development and\nfilm production. Since surface color often correlates with terrain geometry,\ncapturing this relationship is crucial to achieving realism. However, most\nexisting methods generate either a heightmap or a texture, without sufficiently\naccounting for the inherent correlation. In this paper, we propose a method\nthat jointly generates terrain heightmaps and textures using a latent diffusion\nmodel. First, we train the model in an unsupervised manner to randomly generate\npaired heightmaps and textures. Then, we perform supervised learning of an\nexternal adapter to enable user control via hand-drawn sketches. Experiments\nshow that our approach allows intuitive terrain generation while preserving the\ncorrelation between heightmaps and textures.\n","authors":["Kazuki Higo","Toshiki Kanai","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2505.04050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03567v2","updated":"2025-05-07T01:21:36Z","published":"2025-05-06T14:25:30Z","title":"Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person\n  Search in Full Images","summary":"  Text-based pedestrian search (TBPS) in full images aims to locate a target\npedestrian in untrimmed images using natural language descriptions. However, in\ncomplex scenes with multiple pedestrians, existing methods are limited by\nuncertainties in detection and matching, leading to degraded performance. To\naddress this, we propose UPD-TBPS, a novel framework comprising three modules:\nMulti-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty\nDecoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts\nmulti-granularity queries to identify potential targets and assigns confidence\nscores to reduce early-stage uncertainty. PUD leverages visual context\ndecoupling and prototype mining to extract features of the target pedestrian\ndescribed in the query. It separates and learns pedestrian prototype\nrepresentations at both the coarse-grained cluster level and the fine-grained\nindividual level, thereby reducing matching uncertainty. ReID evaluates\ncandidates with varying confidence levels, improving detection and retrieval\naccuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the\neffectiveness of our framework.\n","authors":["Zengli Luo","Canlong Zhang","Xiaochun Lu","Zhixin Li","Zhiwen Wang"],"pdf_url":"https://arxiv.org/pdf/2505.03567v2.pdf","comment":"9pages,5figures"},{"id":"http://arxiv.org/abs/2505.04851v1","updated":"2025-05-07T23:29:28Z","published":"2025-05-07T23:29:28Z","title":"CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused\n  Text-to-Image Generation","summary":"  Despite the fact that popular text-to-image generation models cope well with\ninternational and general cultural queries, they have a significant knowledge\ngap regarding individual cultures. This is due to the content of existing large\ntraining datasets collected on the Internet, which are predominantly based on\nWestern European or American popular culture. Meanwhile, the lack of cultural\nadaptation of the model can lead to incorrect results, a decrease in the\ngeneration quality, and the spread of stereotypes and offensive content. In an\neffort to address this issue, we examine the concept of cultural code and\nrecognize the critical importance of its understanding by modern image\ngeneration models, an issue that has not been sufficiently addressed in the\nresearch community to date. We propose the methodology for collecting and\nprocessing the data necessary to form a dataset based on the cultural code, in\nparticular the Russian one. We explore how the collected data affects the\nquality of generations in the national domain and analyze the effectiveness of\nour approach using the Kandinsky 3.1 text-to-image model. Human evaluation\nresults demonstrate an increase in the level of awareness of Russian culture in\nthe model.\n","authors":["Viacheslav Vasilev","Vladimir Arkhipkin","Julia Agafonova","Tatiana Nikulina","Evelina Mironova","Alisa Shichanina","Nikolai Gerasimenko","Mikhail Shoytov","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2505.04851v1.pdf","comment":"This is arxiv version of the paper which was accepted for the Doklady\n  Mathematics Journal in 2024"},{"id":"http://arxiv.org/abs/2501.04597v2","updated":"2025-05-07T23:28:22Z","published":"2025-01-08T16:25:32Z","title":"FrontierNet: Learning Visual Cues to Explore","summary":"  Exploration of unknown environments is crucial for autonomous robots; it\nallows them to actively reason and decide on what new data to acquire for\ndifferent tasks, such as mapping, object discovery, and environmental\nassessment. Existing solutions, such as frontier-based exploration approaches,\nrely heavily on 3D map operations, which are limited by map quality and, more\ncritically, often overlook valuable context from visual cues. This work aims at\nleveraging 2D visual cues for efficient autonomous exploration, addressing the\nlimitations of extracting goal poses from a 3D map. We propose a visual-only\nfrontier-based exploration system, with FrontierNet as its core component.\nFrontierNet is a learning-based model that (i) proposes frontiers, and (ii)\npredicts their information gain, from posed RGB images enhanced by monocular\ndepth priors. Our approach provides an alternative to existing 3D-dependent\ngoal-extraction approaches, achieving a 15\\% improvement in early-stage\nexploration efficiency, as validated through extensive simulations and\nreal-world experiments. The project is available at\nhttps://github.com/cvg/FrontierNet.\n","authors":["Boyang Sun","Hanzhi Chen","Stefan Leutenegger","Cesar Cadena","Marc Pollefeys","Hermann Blum"],"pdf_url":"https://arxiv.org/pdf/2501.04597v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04850v1","updated":"2025-05-07T23:16:56Z","published":"2025-05-07T23:16:56Z","title":"ORXE: Orchestrating Experts for Dynamically Configurable Efficiency","summary":"  This paper presents ORXE, a modular and adaptable framework for achieving\nreal-time configurable efficiency in AI models. By leveraging a collection of\npre-trained experts with diverse computational costs and performance levels,\nORXE dynamically adjusts inference pathways based on the complexity of input\nsamples. Unlike conventional approaches that require complex metamodel\ntraining, ORXE achieves high efficiency and flexibility without complicating\nthe development process. The proposed system utilizes a confidence-based gating\nmechanism to allocate appropriate computational resources for each input. ORXE\nalso supports adjustments to the preference between inference cost and\nprediction performance across a wide range during runtime. We implemented a\ntraining-free ORXE system for image classification tasks, evaluating its\nefficiency and accuracy across various devices. The results demonstrate that\nORXE achieves superior performance compared to individual experts and other\ndynamic models in most cases. This approach can be extended to other\napplications, providing a scalable solution for diverse real-world deployment\nscenarios.\n","authors":["Qingyuan Wang","Guoxin Wang","Barry Cardiff","Deepu John"],"pdf_url":"https://arxiv.org/pdf/2505.04850v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.04623v1","updated":"2025-05-07T17:59:49Z","published":"2025-05-07T17:59:49Z","title":"EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\n  Reinforcement Learning","summary":"  Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research.\n","authors":["Zhenghao Xing","Xiaowei Hu","Chi-Wing Fu","Wenhai Wang","Jifeng Dai","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2505.04623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04621v1","updated":"2025-05-07T17:59:38Z","published":"2025-05-07T17:59:38Z","title":"Score Distillation Sampling for Audio: Source Separation, Synthesis, and\n  Beyond","summary":"  We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS)\nto text-conditioned audio diffusion models. While SDS was initially designed\nfor text-to-3D generation using image diffusion, its core idea of distilling a\npowerful generative prior into a separate parametric representation extends to\nthe audio domain. Leveraging a single pretrained model, Audio-SDS enables a\nbroad range of tasks without requiring specialized datasets. In particular, we\ndemonstrate how Audio-SDS can guide physically informed impact sound\nsimulations, calibrate FM-synthesis parameters, and perform prompt-specified\nsource separation. Our findings illustrate the versatility of\ndistillation-based methods across modalities and establish a robust foundation\nfor future work using generative priors in audio tasks.\n","authors":["Jessie Richter-Powell","Antonio Torralba","Jonathan Lorraine"],"pdf_url":"https://arxiv.org/pdf/2505.04621v1.pdf","comment":"See the project website at\n  https://research.nvidia.com/labs/toronto-ai/Audio-SDS/"},{"id":"http://arxiv.org/abs/2505.04608v1","updated":"2025-05-07T17:53:47Z","published":"2025-05-07T17:53:47Z","title":"WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via\n  Weighted-Conformal Martingales","summary":"  Responsibly deploying artificial intelligence (AI) / machine learning (ML)\nsystems in high-stakes settings arguably requires not only proof of system\nreliability, but moreover continual, post-deployment monitoring to quickly\ndetect and address any unsafe behavior. Statistical methods for nonparametric\nchange-point detection -- especially the tools of conformal test martingales\n(CTMs) and anytime-valid inference -- offer promising approaches to this\nmonitoring task. However, existing methods are restricted to monitoring limited\nhypothesis classes or ``alarm criteria,'' such as data shifts that violate\ncertain exchangeability assumptions, or do not allow for online adaptation in\nresponse to shifts. In this paper, we expand the scope of these monitoring\nmethods by proposing a weighted generalization of conformal test martingales\n(WCTMs), which lay a theoretical foundation for online monitoring for any\nunexpected changepoints in the data distribution while controlling\nfalse-alarms. For practical applications, we propose specific WCTM algorithms\nthat accommodate online adaptation to mild covariate shifts (in the marginal\ninput distribution) while raising alarms in response to more severe shifts,\nsuch as concept shifts (in the conditional label distribution) or extreme\n(out-of-support) covariate shifts that cannot be easily adapted to. On\nreal-world datasets, we demonstrate improved performance relative to\nstate-of-the-art baselines.\n","authors":["Drew Prinster","Xing Han","Anqi Liu","Suchi Saria"],"pdf_url":"https://arxiv.org/pdf/2505.04608v1.pdf","comment":"To be published in The International Conference on Machine Learning\n  (ICML), 2025"},{"id":"http://arxiv.org/abs/2505.04592v1","updated":"2025-05-07T17:35:36Z","published":"2025-05-07T17:35:36Z","title":"AI Governance to Avoid Extinction: The Strategic Landscape and\n  Actionable Research Questions","summary":"  Humanity appears to be on course to soon develop AI systems that\nsubstantially outperform human experts in all cognitive domains and activities.\nWe believe the default trajectory has a high likelihood of catastrophe,\nincluding human extinction. Risks come from failure to control powerful AI\nsystems, misuse of AI by malicious rogue actors, war between great powers, and\nauthoritarian lock-in. This research agenda has two aims: to describe the\nstrategic landscape of AI development and to catalog important governance\nresearch questions. These questions, if answered, would provide important\ninsight on how to successfully reduce catastrophic risks.\n  We describe four high-level scenarios for the geopolitical response to\nadvanced AI development, cataloging the research questions most relevant to\neach. Our favored scenario involves building the technical, legal, and\ninstitutional infrastructure required to internationally restrict dangerous AI\ndevelopment and deployment (which we refer to as an Off Switch), which leads\ninto an internationally coordinated Halt on frontier AI activities at some\npoint in the future. The second scenario we describe is a US National Project\nfor AI, in which the US Government races to develop advanced AI systems and\nestablish unilateral control over global AI development. We also describe two\nadditional scenarios: a Light-Touch world similar to that of today and a Threat\nof Sabotage situation where countries use sabotage and deterrence to slow AI\ndevelopment.\n  In our view, apart from the Off Switch and Halt scenario, all of these\ntrajectories appear to carry an unacceptable risk of catastrophic harm. Urgent\naction is needed from the US National Security community and AI governance\necosystem to answer key research questions, build the capability to halt\ndangerous AI activities, and prepare for international AI agreements.\n","authors":["Peter Barnett","Aaron Scher"],"pdf_url":"https://arxiv.org/pdf/2505.04592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04578v1","updated":"2025-05-07T17:18:48Z","published":"2025-05-07T17:18:48Z","title":"Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via\n  Reward Neutralization","summary":"  Reinforcement learning (RL) fine-tuning transforms large language models\nwhile creating a vulnerability we experimentally verify: Our experiment shows\nthat malicious RL fine-tuning dismantles safety guardrails with remarkable\nefficiency, requiring only 50 steps and minimal adversarial prompts, with\nharmful escalating from 0-2 to 7-9. This attack vector particularly threatens\nopen-source models with parameter-level access. Existing defenses targeting\nsupervised fine-tuning prove ineffective against RL's dynamic feedback\nmechanisms. We introduce Reward Neutralization, the first defense framework\nspecifically designed against RL fine-tuning attacks, establishing concise\nrejection patterns that render malicious reward signals ineffective. Our\napproach trains models to produce minimal-information rejections that attackers\ncannot exploit, systematically neutralizing attempts to optimize toward harmful\noutputs. Experiments validate that our approach maintains low harmful scores\n(no greater than 2) after 200 attack steps, while standard models rapidly\ndeteriorate. This work provides the first constructive proof that robust\ndefense against increasingly accessible RL attacks is achievable, addressing a\ncritical security gap for open-weight models.\n","authors":["Wenjun Cao"],"pdf_url":"https://arxiv.org/pdf/2505.04578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04558v1","updated":"2025-05-07T16:46:48Z","published":"2025-05-07T16:46:48Z","title":"Purity Law for Generalizable Neural TSP Solvers","summary":"  Achieving generalization in neural approaches across different scales and\ndistributions remains a significant challenge for the Traveling Salesman\nProblem~(TSP). A key obstacle is that neural networks often fail to learn\nrobust principles for identifying universal patterns and deriving optimal\nsolutions from diverse instances. In this paper, we first uncover Purity Law\n(PuLa), a fundamental structural principle for optimal TSP solutions, defining\nthat edge prevalence grows exponentially with the sparsity of surrounding\nvertices. Statistically validated across diverse instances, PuLa reveals a\nconsistent bias toward local sparsity in global optima. Building on this\ninsight, we propose Purity Policy Optimization~(PUPO), a novel training\nparadigm that explicitly aligns characteristics of neural solutions with PuLa\nduring the solution construction process to enhance generalization. Extensive\nexperiments demonstrate that PUPO can be seamlessly integrated with popular\nneural solvers, significantly enhancing their generalization performance\nwithout incurring additional computational overhead during inference.\n","authors":["Wenzhao Liu","Haoran Li","Congying Han","Zicheng Zhang","Anqi Li","Tiande Guo"],"pdf_url":"https://arxiv.org/pdf/2505.04558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04553v1","updated":"2025-05-07T16:31:42Z","published":"2025-05-07T16:31:42Z","title":"Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions","summary":"  We propose a reinforcement learning (RL) framework under a broad class of\nrisk objectives, characterized by convex scoring functions. This class covers\nmany common risk measures, such as variance, Expected Shortfall, entropic\nValue-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue,\nwe consider an augmented state space and an auxiliary variable and recast the\nproblem as a two-state optimization problem. We propose a customized\nActor-Critic algorithm and establish some theoretical approximation guarantees.\nA key theoretical contribution is that our results do not require the Markov\ndecision process to be continuous. Additionally, we propose an auxiliary\nvariable sampling method inspired by the alternating minimization algorithm,\nwhich is convergent under certain conditions. We validate our approach in\nsimulation experiments with a financial application in statistical arbitrage\ntrading, demonstrating the effectiveness of the algorithm.\n","authors":["Shanyu Han","Yang Liu","Xiang Yu"],"pdf_url":"https://arxiv.org/pdf/2505.04553v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2505.04539v1","updated":"2025-05-07T16:15:40Z","published":"2025-05-07T16:15:40Z","title":"Qualitative Analysis of $ω$-Regular Objectives on Robust MDPs","summary":"  Robust Markov Decision Processes (RMDPs) generalize classical MDPs that\nconsider uncertainties in transition probabilities by defining a set of\npossible transition functions. An objective is a set of runs (or infinite\ntrajectories) of the RMDP, and the value for an objective is the maximal\nprobability that the agent can guarantee against the adversarial environment.\nWe consider (a) reachability objectives, where given a target set of states,\nthe goal is to eventually arrive at one of them; and (b) parity objectives,\nwhich are a canonical representation for $\\omega$-regular objectives. The\nqualitative analysis problem asks whether the objective can be ensured with\nprobability 1.\n  In this work, we study the qualitative problem for reachability and parity\nobjectives on RMDPs without making any assumption over the structures of the\nRMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first\npresent efficient algorithms with oracle access to uncertainty sets that solve\nqualitative problems of reachability and parity objectives. We then report\nexperimental results demonstrating the effectiveness of our oracle-based\napproach on classical RMDP examples from the literature scaling up to thousands\nof states.\n","authors":["Ali Asadi","Krishnendu Chatterjee","Ehsan Kafshdar Goharshady","Mehrdad Karrabi","Ali Shafiee"],"pdf_url":"https://arxiv.org/pdf/2505.04539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04531v1","updated":"2025-05-07T16:04:45Z","published":"2025-05-07T16:04:45Z","title":"Overcoming Data Scarcity in Generative Language Modelling for\n  Low-Resource Languages: A Systematic Review","summary":"  Generative language modelling has surged in popularity with the emergence of\nservices such as ChatGPT and Google Gemini. While these models have\ndemonstrated transformative potential in productivity and communication, they\noverwhelmingly cater to high-resource languages like English. This has\namplified concerns over linguistic inequality in natural language processing\n(NLP). This paper presents the first systematic review focused specifically on\nstrategies to address data scarcity in generative language modelling for\nlow-resource languages (LRL). Drawing from 54 studies, we identify, categorise\nand evaluate technical approaches, including monolingual data augmentation,\nback-translation, multilingual training, and prompt engineering, across\ngenerative tasks. We also analyse trends in architecture choices, language\nfamily representation, and evaluation methods. Our findings highlight a strong\nreliance on transformer-based models, a concentration on a small subset of\nLRLs, and a lack of consistent evaluation across studies. We conclude with\nrecommendations for extending these methods to a wider range of LRLs and\noutline open challenges in building equitable generative language systems.\nUltimately, this review aims to support researchers and developers in building\ninclusive AI tools for underrepresented languages, a necessary step toward\nempowering LRL speakers and the preservation of linguistic diversity in a world\nincreasingly shaped by large-scale language technologies.\n","authors":["Josh McGiff","Nikola S. Nikolov"],"pdf_url":"https://arxiv.org/pdf/2505.04531v1.pdf","comment":"This work is currently under review. Please do not cite without\n  permission"},{"id":"http://arxiv.org/abs/2505.04528v1","updated":"2025-05-07T16:02:14Z","published":"2025-05-07T16:02:14Z","title":"Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving","summary":"  As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.\n","authors":["Qi Liu","Xinhao Zheng","Renqiu Xia","Xingzhi Qi","Qinxiang Cao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2505.04528v1.pdf","comment":"42 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.04526v1","updated":"2025-05-07T15:59:45Z","published":"2025-05-07T15:59:45Z","title":"DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement\n  and Fusion All at Once","summary":"  Visible and infrared image fusion is one of the most crucial tasks in the\nfield of image fusion, aiming to generate fused images with clear structural\ninformation and high-quality texture features for high-level vision tasks.\nHowever, when faced with severe illumination degradation in visible images, the\nfusion results of existing image fusion methods often exhibit blurry and dim\nvisual effects, posing major challenges for autonomous driving. To this end, a\nDarkness-Free network is proposed to handle Visible and infrared image\ndisentanglement and fusion all at Once (DFVO), which employs a cascaded\nmulti-task approach to replace the traditional two-stage cascaded training\n(enhancement and fusion), addressing the issue of information entropy loss\ncaused by hierarchical data transmission. Specifically, we construct a\nlatent-common feature extractor (LCFE) to obtain latent features for the\ncascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised\nto acquire high-frequency semantic information. Secondly, we design a hyper\ncross-attention module (HCAM) to extract low-frequency information and preserve\ntexture features from source images. Finally, a relevant loss function is\ndesigned to guide the holistic network learning, thereby achieving better image\nfusion. Extensive experiments demonstrate that our proposed approach\noutperforms state-of-the-art alternatives in terms of qualitative and\nquantitative evaluations. Particularly, DFVO can generate clearer, more\ninformative, and more evenly illuminated fusion results in the dark\nenvironments, achieving best performance on the LLVIP dataset with 63.258 dB\nPSNR and 0.724 CC, providing more effective information for high-level vision\ntasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.\n","authors":["Qi Zhou","Yukai Shi","Xiaojun Yang","Xiaoyu Xian","Lunjia Liao","Ruimao Zhang","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2505.04526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04525v1","updated":"2025-05-07T15:59:19Z","published":"2025-05-07T15:59:19Z","title":"On some improvements to Unbounded Minimax","summary":"  This paper presents the first experimental evaluation of four previously\nuntested modifications of Unbounded Best-First Minimax algorithm. This\nalgorithm explores the game tree by iteratively expanding the most promising\nsequences of actions based on the current partial game tree. We first evaluate\nthe use of transposition tables, which convert the game tree into a directed\nacyclic graph by merging duplicate states. Second, we compare the original\nalgorithm by Korf & Chickering with the variant proposed by Cohen-Solal, which\ndiffers in its backpropagation strategy: instead of stopping when a stable\nvalue is encountered, it updates values up to the root. This change slightly\nimproves performance when value ties or transposition tables are involved.\nThird, we assess replacing the exact terminal evaluation function with the\nlearned heuristic function. While beneficial when exact evaluations are costly,\nthis modification reduces performance in inexpensive settings. Finally, we\nexamine the impact of the completion technique that prioritizes resolved\nwinning states and avoids resolved losing states. This technique also improves\nperformance. Overall, our findings highlight how targeted modifications can\nenhance the efficiency of Unbounded Best-First Minimax.\n","authors":["Quentin Cohen-Solal","Tristan Cazenave"],"pdf_url":"https://arxiv.org/pdf/2505.04525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18314v3","updated":"2025-05-07T15:34:38Z","published":"2025-03-24T03:34:23Z","title":"LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty","summary":"  We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the\ninfluence of training samples from pre-trained models, avoiding retraining from\nscratch. LoTUS smooths the prediction probabilities of the model up to an\ninformation-theoretic bound, mitigating its over-confidence stemming from data\nmemorization. We evaluate LoTUS on Transformer and ResNet18 models against\neight baselines across five public datasets. Beyond established MU benchmarks,\nwe evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining\nis impractical, simulating real-world conditions. Moreover, we introduce the\nnovel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable\nevaluation under real-world conditions. The experimental results show that\nLoTUS outperforms state-of-the-art methods in terms of both efficiency and\neffectiveness. Code: https://github.com/cspartalis/LoTUS.\n","authors":["Christoforos N. Spartalis","Theodoros Semertzidis","Efstratios Gavves","Petros Daras"],"pdf_url":"https://arxiv.org/pdf/2503.18314v3.pdf","comment":"Accepted as a main conference paper at CVPR 2025\n  (https://cvpr.thecvf.com/virtual/2025/poster/33292)"},{"id":"http://arxiv.org/abs/2505.04497v1","updated":"2025-05-07T15:20:17Z","published":"2025-05-07T15:20:17Z","title":"Defining and Quantifying Creative Behavior in Popular Image Generators","summary":"  Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition.\n","authors":["Aditi Ramaswamy"],"pdf_url":"https://arxiv.org/pdf/2505.04497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04493v1","updated":"2025-05-07T15:17:38Z","published":"2025-05-07T15:17:38Z","title":"Model-Based AI planning and Execution Systems for Robotics","summary":"  Model-based planning and execution systems offer a principled approach to\nbuilding flexible autonomous robots that can perform diverse tasks by\nautomatically combining a host of basic skills. This idea is almost as old as\nmodern robotics. Yet, while diverse general-purpose reasoning architectures\nhave been proposed since, general-purpose systems that are integrated with\nmodern robotic platforms have emerged only recently, starting with the\ninfluential ROSPlan system. Since then, a growing number of model-based systems\nfor robot task-level control have emerged. In this paper, we consider the\ndiverse design choices and issues existing systems attempt to address, the\ndifferent solutions proposed so far, and suggest avenues for future\ndevelopment.\n","authors":["Or Wertheim","Ronen I. Brafman"],"pdf_url":"https://arxiv.org/pdf/2505.04493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04488v1","updated":"2025-05-07T15:03:16Z","published":"2025-05-07T15:03:16Z","title":"\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting\n  Individuals with Visual Impairments","summary":"  The visually impaired population, especially the severely visually impaired,\nis currently large in scale, and daily activities pose significant challenges\nfor them. Although many studies use large language and vision-language models\nto assist the blind, most focus on static content and fail to meet real-time\nperception needs in dynamic and complex environments, such as daily activities.\nTo provide them with more effective intelligent assistance, it is imperative to\nincorporate advanced visual understanding technologies. Although real-time\nvision and speech interaction VideoLLMs demonstrate strong real-time visual\nunderstanding, no prior work has systematically evaluated their effectiveness\nin assisting visually impaired individuals. In this work, we conduct the first\nsuch evaluation. First, we construct a benchmark dataset (VisAssistDaily),\ncovering three categories of assistive tasks for visually impaired individuals:\nBasic Skills, Home Life Tasks, and Social Life Tasks. The results show that\nGPT-4o achieves the highest task success rate. Next, we conduct a user study to\nevaluate the models in both closed-world and open-world scenarios, further\nexploring the practical challenges of applying VideoLLMs in assistive contexts.\nOne key issue we identify is the difficulty current models face in perceiving\npotential hazards in dynamic environments. To address this, we build an\nenvironment-awareness dataset named SafeVid and introduce a polling mechanism\nthat enables the model to proactively detect environmental risks. We hope this\nwork provides valuable insights and inspiration for future research in this\nfield.\n","authors":["Ziyi Zhang","Zhen Sun","Zongmin Zhang","Zifan Peng","Yuemeng Zhao","Zichun Wang","Zeren Luo","Ruiting Zuo","Xinlei He"],"pdf_url":"https://arxiv.org/pdf/2505.04488v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.04486v1","updated":"2025-05-07T14:59:23Z","published":"2025-05-07T14:59:23Z","title":"Efficient Flow Matching using Latent Variables","summary":"  Flow matching models have shown great potential in image generation tasks\namong probabilistic generative models. Building upon the ideas of continuous\nnormalizing flows, flow matching models generalize the transport path of the\ndiffusion models from a simple prior distribution to the data. Most flow\nmatching models in the literature do not explicitly model the underlying\nstructure/manifold in the target data when learning the flow from a simple\nsource distribution like the standard Gaussian. This leads to inefficient\nlearning, especially for many high-dimensional real-world datasets, which often\nreside in a low-dimensional manifold. Existing strategies of incorporating\nmanifolds, including data with underlying multi-modal distribution, often\nrequire expensive training and hence frequently lead to suboptimal performance.\nTo this end, we present \\texttt{Latent-CFM}, which provides simplified\ntraining/inference strategies to incorporate multi-modal data structures using\npretrained deep latent variable models. Through experiments on multi-modal\nsynthetic data and widely used image benchmark datasets, we show that\n\\texttt{Latent-CFM} exhibits improved generation quality with significantly\nless training ($\\sim 50\\%$ less in some cases) and computation than\nstate-of-the-art flow matching models. Using a 2d Darcy flow dataset, we\ndemonstrate that our approach generates more physically accurate samples than\ncompetitive approaches. In addition, through latent space analysis, we\ndemonstrate that our approach can be used for conditional image generation\nconditioned on latent features.\n","authors":["Anirban Samaddar","Yixuan Sun","Viktor Nilsson","Sandeep Madireddy"],"pdf_url":"https://arxiv.org/pdf/2505.04486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04480v1","updated":"2025-05-07T14:51:43Z","published":"2025-05-07T14:51:43Z","title":"TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven\n  Evolution","summary":"  Trajectory prediction is a crucial task in modeling human behavior,\nespecially in fields as social robotics and autonomous vehicle navigation.\nTraditional heuristics based on handcrafted rules often lack accuracy, while\nrecently proposed deep learning approaches suffer from computational cost, lack\nof explainability, and generalization issues that limit their practical\nadoption. In this paper, we introduce TrajEvo, a framework that leverages Large\nLanguage Models (LLMs) to automatically design trajectory prediction\nheuristics. TrajEvo employs an evolutionary algorithm to generate and refine\nprediction heuristics from past trajectory data. We introduce a\nCross-Generation Elite Sampling to promote population diversity and a\nStatistics Feedback Loop allowing the LLM to analyze alternative predictions.\nOur evaluations show TrajEvo outperforms previous heuristic methods on the\nETH-UCY datasets, and remarkably outperforms both heuristics and deep learning\nmethods when generalizing to the unseen SDD dataset. TrajEvo represents a first\nstep toward automated design of fast, explainable, and generalizable trajectory\nprediction heuristics. We make our source code publicly available to foster\nfuture research at https://github.com/ai4co/trajevo.\n","authors":["Zhikai Zhao","Chuanbo Hua","Federico Berto","Kanghoon Lee","Zihan Ma","Jiachen Li","Jinkyoo Park"],"pdf_url":"https://arxiv.org/pdf/2505.04480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04468v1","updated":"2025-05-07T14:38:58Z","published":"2025-05-07T14:38:58Z","title":"Spectral and Temporal Denoising for Differentially Private Optimization","summary":"  This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a\ndifferentially private optimization method that addresses the challenge of\npreserving performance in DP-SGD, where added noise typically degrades model\nutility. FFTKF integrates frequency-domain noise shaping with Kalman filtering\nto enhance gradient quality while preserving $(\\varepsilon, \\delta)$-DP\nguarantees. It employs a high-frequency shaping mask in the Fourier domain to\nconcentrate differential privacy noise in less informative spectral components,\npreserving low-frequency gradient signals. A scalar-gain Kalman filter with\nfinite-difference Hessian approximation further refines the denoised gradients.\nWith a per-iteration complexity of $\\mathcal{O}(d \\log d)$, FFTKF demonstrates\nimproved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,\nand Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.\nTheoretical analysis confirms that FFTKF maintains equivalent privacy\nguarantees while achieving a tighter privacy-utility trade-off through reduced\nnoise and controlled bias.\n","authors":["Hyeju Shin","Kyudan Jung","Seongwon Yun","Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2505.04468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04464v1","updated":"2025-05-07T14:35:39Z","published":"2025-05-07T14:35:39Z","title":"Discriminative Ordering Through Ensemble Consensus","summary":"  Evaluating the performance of clustering models is a challenging task where\nthe outcome depends on the definition of what constitutes a cluster. Due to\nthis design, current existing metrics rarely handle multiple clustering models\nwith diverse cluster definitions, nor do they comply with the integration of\nconstraints when available. In this work, we take inspiration from consensus\nclustering and assume that a set of clustering models is able to uncover hidden\nstructures in the data. We propose to construct a discriminative ordering\nthrough ensemble clustering based on the distance between the connectivity of a\nclustering model and the consensus matrix. We first validate the proposed\nmethod with synthetic scenarios, highlighting that the proposed score ranks the\nmodels that best match the consensus first. We then show that this simple\nranking score significantly outperforms other scoring methods when comparing\nsets of different clustering algorithms that are not restricted to a fixed\nnumber of clusters and is compatible with clustering constraints.\n","authors":["Louis Ohl","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2505.04464v1.pdf","comment":"Accepted at UAI 2025"},{"id":"http://arxiv.org/abs/2409.04388v4","updated":"2025-05-07T14:35:23Z","published":"2024-09-06T16:27:52Z","title":"Question-Answering Dense Video Events","summary":"  This paper presents question-answering on dense video events, a novel task\nthat answers and grounds dense-event questions in long videos, thus challenging\nMLLMs to faithfully comprehend and reason about multiple events over extended\nperiods of time. To facilitate the study, we construct DeVE-QA -- a dataset\nfeaturing 78K questions about 26K events on 10.6K long videos. Our benchmarking\nshows that state-of-the-art MLLMs struggle on DeVE-QA. For improvement, we\npropose DeVi, a novel training-free MLLM approach that highlights a\nhierarchical captioning module, a temporal event memory module, and a\nself-consistency checking module to respectively detect, contextualize and\nmemorize, and ground dense-events in long videos for question answering.\nExtensive experiments show that DeVi is superior at answering dense-event\nquestions and grounding relevant video moments. Compared with existing MLLMs,\nit achieves a remarkable increase of 4.8% and 2.1% for G(round)QA accuracy on\nDeVE-QA~and NExT-GQA, respectively. Our data and code will be released upon\nacceptance.\n","authors":["Hangyu Qin","Junbin Xiao","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2409.04388v4.pdf","comment":"Accepted to SIGIR'25"},{"id":"http://arxiv.org/abs/2505.04461v1","updated":"2025-05-07T14:31:10Z","published":"2025-05-07T14:31:10Z","title":"A Survey on Temporal Interaction Graph Representation Learning:\n  Progress, Challenges, and Opportunities","summary":"  Temporal interaction graphs (TIGs), defined by sequences of timestamped\ninteraction events, have become ubiquitous in real-world applications due to\ntheir capability to model complex dynamic system behaviors. As a result,\ntemporal interaction graph representation learning (TIGRL) has garnered\nsignificant attention in recent years. TIGRL aims to embed nodes in TIGs into\nlow-dimensional representations that effectively preserve both structural and\ntemporal information, thereby enhancing the performance of downstream tasks\nsuch as classification, prediction, and clustering within constantly evolving\ndata environments. In this paper, we begin by introducing the foundational\nconcepts of TIGs and emphasize the critical role of temporal dependencies. We\nthen propose a comprehensive taxonomy of state-of-the-art TIGRL methods,\nsystematically categorizing them based on the types of information utilized\nduring the learning process to address the unique challenges inherent to TIGs.\nTo facilitate further research and practical applications, we curate the source\nof datasets and benchmarks, providing valuable resources for empirical\ninvestigations. Finally, we examine key open challenges and explore promising\nresearch directions in TIGRL, laying the groundwork for future advancements\nthat have the potential to shape the evolution of this field.\n","authors":["Pengfei Jiao","Hongjiang Chen","Xuan Guo","Zhidong Zhao","Dongxiao He","Di Jin"],"pdf_url":"https://arxiv.org/pdf/2505.04461v1.pdf","comment":"IJCAI 2025 Survey Track"},{"id":"http://arxiv.org/abs/2505.02369v3","updated":"2025-05-07T14:21:19Z","published":"2025-05-05T05:13:12Z","title":"Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural\n  Networks","summary":"  Sharpness-Aware Minimization (SAM) improves neural network generalization by\noptimizing the worst-case loss within a neighborhood of parameters, yet it\nperturbs parameters using the entire gradient vector, including components with\nlow statistical significance. We introduce ZSharp, a refined sharpness-aware\noptimization method that incorporates layer-wise Z-score normalization followed\nby percentile-based filtering. This process selects only the most statistically\nsignificant gradient components-those with large standardized magnitudes-for\nconstructing the perturbation direction. ZSharp retains the standard two-phase\nSAM structure of ascent and descent while modifying the ascent step to focus on\nsharper, curvature-relevant directions. We evaluate ZSharp on CIFAR-10,\nCIFAR-100, and Tiny-ImageNet using a range of models including ResNet, VGG, and\nVision Transformers. Across all architectures and datasets, ZSharp consistently\nachieves higher test accuracy compared to SAM, ASAM, and Friendly-SAM. These\nresults indicate that Z-score-based gradient filtering can enhance the\nsharpness sensitivity of the update direction, leading to improved\ngeneralization in deep neural network training.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2505.02369v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04451v1","updated":"2025-05-07T14:20:43Z","published":"2025-05-07T14:20:43Z","title":"Automatic Music Transcription using Convolutional Neural Networks and\n  Constant-Q transform","summary":"  Automatic music transcription (AMT) is the problem of analyzing an audio\nrecording of a musical piece and detecting notes that are being played. AMT is\na challenging problem, particularly when it comes to polyphonic music. The goal\nof AMT is to produce a score representation of a music piece, by analyzing a\nsound signal containing multiple notes played simultaneously. In this work, we\ndesign a processing pipeline that can transform classical piano audio files in\n.wav format into a music score representation. The features from the audio\nsignals are extracted using the constant-Q transform, and the resulting\ncoefficients are used as an input to the convolutional neural network (CNN)\nmodel.\n","authors":["Yohannis Telila","Tommaso Cucinotta","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2505.04451v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2410.06523v2","updated":"2025-05-07T14:12:16Z","published":"2024-10-09T03:52:18Z","title":"Phase Diagram from Nonlinear Interaction between Superconducting Order\n  and Density: Toward Data-Based Holographic Superconductor","summary":"  We address an inverse problem in modeling holographic superconductors. We\nfocus our research on the critical temperature behavior depicted by\nexperiments. We use a physics-informed neural network method to find a mass\nfunction $M(F^2)$, which is necessary to understand phase transition behavior.\nThis mass function describes a nonlinear interaction between superconducting\norder and charge carrier density. We introduce positional embedding layers to\nimprove the learning process in our algorithm, and the Adam optimization is\nused to predict the critical temperature data via holographic calculation with\nappropriate accuracy. Consideration of the positional embedding layers is\nmotivated by the transformer model of natural-language processing in the\nartificial intelligence (AI) field. We obtain holographic models that reproduce\nborderlines of the normal and superconducting phases provided by actual data.\nOur work is the first holographic attempt to match phase transition data\nquantitatively obtained from experiments. Also, the present work offers a new\nmethodology for data-based holographic models.\n","authors":["Sejin Kim","Kyung Kiu Kim","Yunseok Seo"],"pdf_url":"https://arxiv.org/pdf/2410.06523v2.pdf","comment":"22 pages, 20 figures, published version in JHEP"},{"id":"http://arxiv.org/abs/2505.04435v1","updated":"2025-05-07T14:02:35Z","published":"2025-05-07T14:02:35Z","title":"FedBWO: Enhancing Communication Efficiency in Federated Learning","summary":"  Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a\nshared model is collaboratively trained by various clients using their local\ndatasets while keeping the data private. Considering resource-constrained\ndevices, FL clients often suffer from restricted transmission capacity. Aiming\nto enhance the system performance, the communication between clients and server\nneeds to be diminished. Current FL strategies transmit a tremendous amount of\ndata (model weights) within the FL process, which needs a high communication\nbandwidth. Considering resource constraints, increasing the number of clients\nand, consequently, the amount of data (model weights) can lead to a bottleneck.\nIn this paper, we introduce the Federated Black Widow Optimization (FedBWO)\ntechnique to decrease the amount of transmitted data by transmitting only a\nperformance score rather than the local model weights from clients. FedBWO\nemploys the BWO algorithm to improve local model updates. The conducted\nexperiments prove that FedBWO remarkably improves the performance of the global\nmodel and the communication efficiency of the overall system. According to the\nexperimental outcomes, FedBWO enhances the global model accuracy by an average\nof 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically\ndecreases the communication cost compared to other methods.\n","authors":["Vahideh Hayyolalam","Öznur Özkasap"],"pdf_url":"https://arxiv.org/pdf/2505.04435v1.pdf","comment":"5th IEEE International Conference on Human-Machine Systems, Abu\n  Dhabi, UAE, 26-28 May 2025"},{"id":"http://arxiv.org/abs/2302.03669v4","updated":"2025-05-07T13:57:44Z","published":"2023-02-04T02:49:12Z","title":"Deep Reinforcement Learning for Traffic Light Control in Intelligent\n  Transportation Systems","summary":"  Smart traffic lights in intelligent transportation systems (ITSs) are\nenvisioned to greatly increase traffic efficiency and reduce congestion. Deep\nreinforcement learning (DRL) is a promising approach to adaptively control\ntraffic lights based on the real-time traffic situation in a road network.\nHowever, conventional methods may suffer from poor scalability. In this paper,\nwe investigate deep reinforcement learning to control traffic lights, and both\ntheoretical analysis and numerical experiments show that the intelligent\nbehavior ``greenwave\" (i.e., a vehicle will see a progressive cascade of green\nlights, and not have to brake at any intersection) emerges naturally a grid\nroad network, which is proved to be the optimal policy in an avenue with\nmultiple cross streets. As a first step, we use two DRL algorithms for the\ntraffic light control problems in two scenarios. In a single road intersection,\nwe verify that the deep Q-network (DQN) algorithm delivers a thresholding\npolicy; and in a grid road network, we adopt the deep deterministic policy\ngradient (DDPG) algorithm. Secondly, numerical experiments show that the DQN\nalgorithm delivers the optimal control, and the DDPG algorithm with passive\nobservations has the capability to produce on its own a high-level intelligent\nbehavior in a grid road network, namely, the ``greenwave\" policy emerges. We\nalso verify the ``greenwave\" patterns in a $5 \\times 10$ grid road network.\nThirdly, the ``greenwave\" patterns demonstrate that DRL algorithms produce\nfavorable solutions since the ``greenwave\" policy shown in experiment results\nis proved to be optimal in a specified traffic model (an avenue with multiple\ncross streets). The delivered policies both in a single road intersection and a\ngrid road network demonstrate the scalability of DRL algorithms.\n","authors":["Ming Zhu","Xiao-Yang Liu","Sem Borst","Anwar Walid"],"pdf_url":"https://arxiv.org/pdf/2302.03669v4.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2505.04419v1","updated":"2025-05-07T13:52:50Z","published":"2025-05-07T13:52:50Z","title":"Recognizing Ornaments in Vocal Indian Art Music with Active Annotation","summary":"  Ornamentations, embellishments, or microtonal inflections are essential to\nmelodic expression across many musical traditions, adding depth, nuance, and\nemotional impact to performances. Recognizing ornamentations in singing voices\nis key to MIR, with potential applications in music pedagogy, singer\nidentification, genre classification, and controlled singing voice generation.\nHowever, the lack of annotated datasets and specialized modeling approaches\nremains a major obstacle for progress in this research area. In this work, we\nintroduce R\\=aga Ornamentation Detection (ROD), a novel dataset comprising\nIndian classical music recordings curated by expert musicians. The dataset is\nannotated using a custom Human-in-the-Loop tool for six vocal ornaments marked\nas event-based labels. Using this dataset, we develop an ornamentation\ndetection model based on deep time-series analysis, preserving ornament\nboundaries during the chunking of long audio recordings. We conduct experiments\nusing different train-test configurations within the ROD dataset and also\nevaluate our approach on a separate, manually annotated dataset of Indian\nclassical concert recordings. Our experimental results support the superior\nperformance of our proposed approach over the baseline CRNN.\n","authors":["Sumit Kumar","Parampreet Singh","Vipul Arora"],"pdf_url":"https://arxiv.org/pdf/2505.04419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04416v1","updated":"2025-05-07T13:51:42Z","published":"2025-05-07T13:51:42Z","title":"OBLIVIATE: Robust and Practical Machine Unlearning for Large Language\n  Models","summary":"  Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios.\n","authors":["Xiaoyu Xu","Minxin Du","Qingqing Ye","Haibo Hu"],"pdf_url":"https://arxiv.org/pdf/2505.04416v1.pdf","comment":"18 pages, 2 figures"},{"id":"http://arxiv.org/abs/2505.04406v1","updated":"2025-05-07T13:42:23Z","published":"2025-05-07T13:42:23Z","title":"YABLoCo: Yet Another Benchmark for Long Context Code Generation","summary":"  Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++.\n","authors":["Aidar Valeev","Roman Garaev","Vadim Lomshakov","Irina Piontkovskaya","Vladimir Ivanov","Israel Adewuyi"],"pdf_url":"https://arxiv.org/pdf/2505.04406v1.pdf","comment":"Presented at LLM4Code 2025 Workshop co-located wtih ICSE 2025"},{"id":"http://arxiv.org/abs/2505.04405v1","updated":"2025-05-07T13:39:18Z","published":"2025-05-07T13:39:18Z","title":"High-speed multiwavelength photonic temporal integration using silicon\n  photonics","summary":"  Optical systems have been pivotal for energy-efficient computing, performing\nhigh-speed, parallel operations in low-loss carriers. While these predominantly\nanalog optical accelerators bypass digitization to perform parallel\nfloating-point computations, scaling optical hardware to map large-vector sizes\nfor AI tasks remains challenging. Here, we overcome this limitation by\nunfolding scalar operations in time and introducing a\nphotonic-heater-in-lightpath (PHIL) unit for all-optical temporal integration.\nCounterintuitively, we exploit a slow heat dissipation process to integrate\noptical signals modulated at 50 GHz bridging the speed gap between the widely\napplied thermo-optic effects and ultrafast photonics. This architecture\nsupports optical end-to-end signal processing, eliminates inefficient\nelectro-optical conversions, and enables both linear and nonlinear operations\nwithin a unified framework. Our results demonstrate a scalable path towards\nhigh-speed photonic computing through thermally driven integration.\n","authors":["Yi Zhang","Nikolaos Farmakidis","Ioannis Roumpos","Miltiadis Moralis-Pegios","Apostolos Tsakyridis","June Sang Lee","Bowei Dong","Yuhan He","Samarth Aggarwal","Nikolaos Pleros","Harish Bhaskaran"],"pdf_url":"https://arxiv.org/pdf/2505.04405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04404v1","updated":"2025-05-07T13:36:59Z","published":"2025-05-07T13:36:59Z","title":"In-Context Adaptation to Concept Drift for Learned Database Operations","summary":"  Machine learning has demonstrated transformative potential for database\noperations, such as query optimization and in-database data analytics. However,\ndynamic database environments, characterized by frequent updates and evolving\ndata distributions, introduce concept drift, which leads to performance\ndegradation for learned models and limits their practical applicability.\nAddressing this challenge requires efficient frameworks capable of adapting to\nshifting concepts while minimizing the overhead of retraining or fine-tuning.\n  In this paper, we propose FLAIR, an online adaptation framework that\nintroduces a new paradigm called \\textit{in-context adaptation} for learned\ndatabase operations. FLAIR leverages the inherent property of data systems,\ni.e., immediate availability of execution results for predictions, to enable\ndynamic context construction. By formalizing adaptation as $f:(\\mathbf{x} \\,|\n\\,\\mathcal{C}_t) \\to \\mathbf{y}$, with $\\mathcal{C}_t$ representing a dynamic\ncontext memory, FLAIR delivers predictions aligned with the current concept,\neliminating the need for runtime parameter optimization. To achieve this, FLAIR\nintegrates two key modules: a Task Featurization Module for encoding\ntask-specific features into standardized representations, and a Dynamic\nDecision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly\nusing contextual information at runtime. Extensive experiments across key\ndatabase tasks demonstrate that FLAIR outperforms state-of-the-art baselines,\nachieving up to 5.2x faster adaptation and reducing error by 22.5% for\ncardinality estimation.\n","authors":["Jiaqi Zhu","Shaofeng Cai","Yanyan Shen","Gang Chen","Fang Deng","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2505.04404v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.04397v1","updated":"2025-05-07T13:21:25Z","published":"2025-05-07T13:21:25Z","title":"Deep residual learning with product units","summary":"  We propose a deep product-unit residual neural network (PURe) that integrates\nproduct units into residual blocks to improve the expressiveness and parameter\nefficiency of deep convolutional networks. Unlike standard summation neurons,\nproduct units enable multiplicative feature interactions, potentially offering\na more powerful representation of complex patterns. PURe replaces conventional\nconvolutional layers with 2D product units in the second layer of each residual\nblock, eliminating nonlinear activation functions to preserve structural\ninformation. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,\nPURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper\nResNet152, while converging nearly five times faster and demonstrating strong\nrobustness to Poisson noise. On ImageNet, PURe architectures outperform\nstandard ResNet models at similar depths, with PURe34 achieving a top-1\naccuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet\nvariants (ResNet50, ResNet101) while utilizing significantly fewer parameters\nand computational resources. On CIFAR-10, PURe consistently outperforms ResNet\nvariants across varying depths, with PURe272 reaching 95.01% test accuracy,\ncomparable to ResNet1001 but at less than half the model size. These results\ndemonstrate that PURe achieves a favorable balance between accuracy,\nefficiency, and robustness. Compared to traditional residual networks, PURe not\nonly achieves competitive classification performance with faster convergence\nand fewer parameters, but also demonstrates greater robustness to noise. Its\neffectiveness across diverse datasets highlights the potential of\nproduct-unit-based architectures for scalable and reliable deep learning in\ncomputer vision.\n","authors":["Ziyuan Li","Uwe Jaekel","Babette Dellen"],"pdf_url":"https://arxiv.org/pdf/2505.04397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00290v4","updated":"2025-05-07T13:13:41Z","published":"2025-02-01T03:18:02Z","title":"Estimating LLM Uncertainty with Logits","summary":"  Over the past few years, Large Language Models (LLMs) have developed rapidly\nand are widely applied in various domains. However, LLMs face the issue of\nhallucinations, generating responses that may be unreliable when the models\nlack relevant knowledge. To be aware of potential hallucinations, uncertainty\nestimation methods have been introduced, and most of them have confirmed that\nreliability lies in critical tokens. However, probability-based methods perform\npoorly in identifying token reliability, limiting their practical utility. In\nthis paper, we reveal that the probability-based method fails to estimate token\nreliability due to the loss of evidence strength information which is\naccumulated in the training stage. Therefore, we present Logits-induced token\nuncertainty (LogTokU), a framework for estimating decoupled token uncertainty\nin LLMs, enabling real-time uncertainty estimation without requiring multiple\nsampling processes. We employ evidence modeling to implement LogTokU and use\nthe estimated uncertainty to guide downstream tasks. The experimental results\ndemonstrate that LogTokU has significant effectiveness and promise.\n","authors":["Huan Ma","Jingdong Chen","Joey Tianyi Zhou","Guangyu Wang","Changqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00290v4.pdf","comment":"Fixed some data errors in Table 1"},{"id":"http://arxiv.org/abs/2505.04388v1","updated":"2025-05-07T13:13:14Z","published":"2025-05-07T13:13:14Z","title":"The Aloe Family Recipe for Open and Specialized Healthcare LLMs","summary":"  Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.\n","authors":["Dario Garcia-Gasulla","Jordi Bayarri-Planas","Ashwin Kumar Gururajan","Enrique Lopez-Cuena","Adrian Tormos","Daniel Hinjos","Pablo Bernabeu-Perez","Anna Arias-Duart","Pablo Agustin Martin-Torres","Marta Gonzalez-Mallo","Sergio Alvarez-Napagao","Eduard Ayguadé-Parra","Ulises Cortés"],"pdf_url":"https://arxiv.org/pdf/2505.04388v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.01886"},{"id":"http://arxiv.org/abs/2410.03072v2","updated":"2025-05-07T13:04:49Z","published":"2024-10-04T01:31:13Z","title":"Multi-Robot Motion Planning with Diffusion Models","summary":"  Diffusion models have recently been successfully applied to a wide range of\nrobotics applications for learning complex multi-modal behaviors from data.\nHowever, prior works have mostly been confined to single-robot and small-scale\nenvironments due to the high sample complexity of learning multi-robot\ndiffusion models. In this paper, we propose a method for generating\ncollision-free multi-robot trajectories that conform to underlying data\ndistributions while using only single-robot data. Our algorithm, Multi-robot\nMulti-model planning Diffusion (MMD), does so by combining learned diffusion\nmodels with classical search-based techniques -- generating data-driven motions\nunder collision constraints. Scaling further, we show how to compose multiple\ndiffusion models to plan in large environments where a single diffusion model\nfails to generalize well. We demonstrate the effectiveness of our approach in\nplanning for dozens of robots in a variety of simulated scenarios motivated by\nlogistics environments. View video demonstrations and code at:\nhttps://multi-robot-diffusion.github.io/.\n","authors":["Yorai Shaoul","Itamar Mishani","Shivam Vats","Jiaoyang Li","Maxim Likhachev"],"pdf_url":"https://arxiv.org/pdf/2410.03072v2.pdf","comment":"The first three authors contributed equally to this work. Published\n  at ICLR 2025"},{"id":"http://arxiv.org/abs/2505.03335v2","updated":"2025-05-07T13:01:17Z","published":"2025-05-06T09:08:00Z","title":"Absolute Zero: Reinforced Self-play Reasoning with Zero Data","summary":"  Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.\n","authors":["Andrew Zhao","Yiran Wu","Yang Yue","Tong Wu","Quentin Xu","Yang Yue","Matthieu Lin","Shenzhi Wang","Qingyun Wu","Zilong Zheng","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2505.03335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04379v1","updated":"2025-05-07T12:59:59Z","published":"2025-05-07T12:59:59Z","title":"Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and\n  Performance in Mixed Urban Traffic","summary":"  Transportation systems have long been shaped by complexity and heterogeneity,\ndriven by the interdependency of agent actions and traffic outcomes. The\ndeployment of automated vehicles (AVs) in such systems introduces a new\nchallenge: achieving consensus across safety, interaction quality, and traffic\nperformance. In this work, we position consensus as a fundamental property of\nthe traffic system and aim to quantify it. We use high-resolution trajectory\ndata from the Third Generation Simulation (TGSIM) dataset to empirically\nanalyze AV and human-driven vehicle (HDV) behavior at a signalized urban\nintersection and around vulnerable road users (VRUs). Key metrics, including\nTime-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,\nheadways, and string stability, are evaluated across the three performance\ndimensions. Results show that full consensus across safety, interaction, and\nperformance is rare, with only 1.63% of AV-VRU interaction frames meeting all\nthree conditions. These findings highlight the need for AV models that\nexplicitly balance multi-dimensional performance in mixed-traffic environments.\nFull reproducibility is supported via our open-source codebase on\nhttps://github.com/wissamkontar/Consensus-AV-Analysis.\n","authors":["Mohammad Elayan","Wissam Kontar"],"pdf_url":"https://arxiv.org/pdf/2505.04379v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.04375v1","updated":"2025-05-07T12:53:13Z","published":"2025-05-07T12:53:13Z","title":"Balancing Accuracy, Calibration, and Efficiency in Active Learning with\n  Vision Transformers Under Label Noise","summary":"  Fine-tuning pre-trained convolutional neural networks on ImageNet for\ndownstream tasks is well-established. Still, the impact of model size on the\nperformance of vision transformers in similar scenarios, particularly under\nlabel noise, remains largely unexplored. Given the utility and versatility of\ntransformer architectures, this study investigates their practicality under\nlow-budget constraints and noisy labels. We explore how classification accuracy\nand calibration are affected by symmetric label noise in active learning\nsettings, evaluating four vision transformer configurations (Base and Large\nwith 16x16 and 32x32 patch sizes) and three Swin Transformer configurations\n(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label\nnoise rates. Our findings show that larger ViT models (ViTl32 in particular)\nconsistently outperform their smaller counterparts in both accuracy and\ncalibration, even under moderate to high label noise, while Swin Transformers\nexhibit weaker robustness across all noise levels. We find that smaller patch\nsizes do not always lead to better performance, as ViTl16 performs consistently\nworse than ViTl32 while incurring a higher computational cost. We also find\nthat information-based Active Learning strategies only provide meaningful\naccuracy improvements at moderate label noise rates, but they result in poorer\ncalibration compared to models trained on randomly acquired labels, especially\nat high label noise rates. We hope these insights provide actionable guidance\nfor practitioners looking to deploy vision transformers in resource-constrained\nenvironments, where balancing model complexity, label noise, and compute\nefficiency is critical in model fine-tuning or distillation.\n","authors":["Moseli Mots'oehli","Hope Mogale","Kyungim Baek"],"pdf_url":"https://arxiv.org/pdf/2505.04375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00874v2","updated":"2025-05-07T12:26:10Z","published":"2024-10-31T07:03:46Z","title":"VecCity: A Taxonomy-guided Library for Map Entity Representation\n  Learning","summary":"  Electronic maps consist of diverse entities, such as points of interest\n(POIs), road networks, and land parcels, playing a vital role in applications\nlike ITS and LBS. Map entity representation learning (MapRL) generates\nversatile and reusable data representations, providing essential tools for\nefficiently managing and utilizing map entity data. Despite the progress in\nMapRL, two key challenges constrain further development. First, existing\nresearch is fragmented, with models classified by the type of map entity,\nlimiting the reusability of techniques across different tasks. Second, the lack\nof unified benchmarks makes systematic evaluation and comparison of models\ndifficult. To address these challenges, we propose a novel taxonomy for MapRL\nthat organizes models based on functional module-such as encoders, pre-training\ntasks, and downstream tasks-rather than by entity type. Building on this\ntaxonomy, we present a taxonomy-driven library, VecCity, which offers\neasy-to-use interfaces for encoding, pre-training, fine-tuning, and evaluation.\nThe library integrates datasets from nine cities and reproduces 21 mainstream\nMapRL models, establishing the first standardized benchmarks for the field.\nVecCity also allows users to modify and extend models through modular\ncomponents, facilitating seamless experimentation. Our comprehensive\nexperiments cover multiple types of map entities and evaluate 21 VecCity\npre-built models across various downstream tasks. Experimental results\ndemonstrate the effectiveness of VecCity in streamlining model development and\nprovide insights into the impact of various components on performance. By\npromoting modular design and reusability, VecCity offers a unified framework to\nadvance research and innovation in MapRL. The code is available at\nhttps://github.com/Bigscity-VecCity/VecCity.\n","authors":["Wentao Zhang","Jingyuan Wang","Yifan Yang","Leong Hou U"],"pdf_url":"https://arxiv.org/pdf/2411.00874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01759v2","updated":"2025-05-07T12:15:25Z","published":"2024-06-03T19:54:11Z","title":"From Latent to Lucid: Transforming Knowledge Graph Embeddings into\n  Interpretable Structures with KGEPrisma","summary":"  In this paper, we introduce a post-hoc and local explainable AI method\ntailored for Knowledge Graph Embedding (KGE) models. These models are essential\nto Knowledge Graph Completion yet criticized for their opaque, black-box\nnature. Despite their significant success in capturing the semantics of\nknowledge graphs through high-dimensional latent representations, their\ninherent complexity poses substantial challenges to explainability. While\nexisting methods like Kelpie use resource-intensive perturbation to explain KGE\nmodels, our approach directly decodes the latent representations encoded by KGE\nmodels, leveraging the smoothness of the embeddings, which follows the\nprinciple that similar embeddings reflect similar behaviours within the\nKnowledge Graph, meaning that nodes are similarly embedded because their graph\nneighbourhood looks similar. This principle is commonly referred to as\nsmoothness. By identifying symbolic structures, in the form of triples, within\nthe subgraph neighborhoods of similarly embedded entities, our method\nidentifies the statistical regularities on which the models rely and translates\nthese insights into human-understandable symbolic rules and facts. This bridges\nthe gap between the abstract representations of KGE models and their predictive\noutputs, offering clear, interpretable insights. Key contributions include a\nnovel post-hoc and local explainable AI method for KGE models that provides\nimmediate, faithful explanations without retraining, facilitating real-time\napplication on large-scale knowledge graphs. The method's flexibility enables\nthe generation of rule-based, instance-based, and analogy-based explanations,\nmeeting diverse user needs. Extensive evaluations show the effectiveness of our\napproach in delivering faithful and well-localized explanations, enhancing the\ntransparency and trustworthiness of KGE models.\n","authors":["Christoph Wehner","Chrysa Iliopoulou","Ute Schmid","Tarek R. Besold"],"pdf_url":"https://arxiv.org/pdf/2406.01759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04354v1","updated":"2025-05-07T12:07:49Z","published":"2025-05-07T12:07:49Z","title":"Optimization Problem Solving Can Transition to Evolutionary Agentic\n  Workflows","summary":"  This position paper argues that optimization problem solving can transition\nfrom expert-dependent to evolutionary agentic workflows. Traditional\noptimization practices rely on human specialists for problem formulation,\nalgorithm selection, and hyperparameter tuning, creating bottlenecks that\nimpede industrial adoption of cutting-edge methods. We contend that an\nevolutionary agentic workflow, powered by foundation models and evolutionary\nsearch, can autonomously navigate the optimization space, comprising problem,\nformulation, algorithm, and hyperparameter spaces. Through case studies in\ncloud resource scheduling and ADMM parameter adaptation, we demonstrate how\nthis approach can bridge the gap between academic innovation and industrial\nimplementation. Our position challenges the status quo of human-centric\noptimization workflows and advocates for a more scalable, adaptive approach to\nsolving real-world optimization problems.\n","authors":["Wenhao Li","Bo Jin","Mingyi Hong","Changhong Lu","Xiangfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2505.04354v1.pdf","comment":"27 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.04352v1","updated":"2025-05-07T12:03:15Z","published":"2025-05-07T12:03:15Z","title":"Uncertain Machine Ethics Planning","summary":"  Machine Ethics decisions should consider the implications of uncertainty over\ndecisions. Decisions should be made over sequences of actions to reach\npreferable outcomes long term. The evaluation of outcomes, however, may invoke\none or more moral theories, which might have conflicting judgements. Each\ntheory will require differing representations of the ethical situation. For\nexample, Utilitarianism measures numerical values, Deontology analyses duties,\nand Virtue Ethics emphasises moral character. While balancing potentially\nconflicting moral considerations, decisions may need to be made, for example,\nto achieve morally neutral goals with minimal costs. In this paper, we\nformalise the problem as a Multi-Moral Markov Decision Process and a\nMulti-Moral Stochastic Shortest Path Problem. We develop a heuristic algorithm\nbased on Multi-Objective AO*, utilising Sven-Ove Hansson's Hypothetical\nRetrospection procedure for ethical reasoning under uncertainty. Our approach\nis validated by a case study from Machine Ethics literature: the problem of\nwhether to steal insulin for someone who needs it.\n","authors":["Simon Kolker","Louise A. Dennis","Ramon Fraga Pereira","Mengwei Xu"],"pdf_url":"https://arxiv.org/pdf/2505.04352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06987v2","updated":"2025-05-07T11:50:43Z","published":"2025-04-09T15:51:10Z","title":"Enhancing Metabolic Syndrome Prediction with Hybrid Data Balancing and\n  Counterfactuals","summary":"  Metabolic Syndrome (MetS) is a cluster of interrelated risk factors that\nsignificantly increases the risk of cardiovascular diseases and type 2\ndiabetes. Despite its global prevalence, accurate prediction of MetS remains\nchallenging due to issues such as class imbalance, data scarcity, and\nmethodological inconsistencies in existing studies. In this paper, we address\nthese challenges by systematically evaluating and optimizing machine learning\n(ML) models for MetS prediction, leveraging advanced data balancing techniques\nand counterfactual analysis. Multiple ML models, including XGBoost, Random\nForest, TabNet, etc., were trained and compared under various data balancing\ntechniques such as random oversampling (ROS), SMOTE, ADASYN, and CTGAN.\nAdditionally, we introduce MetaBoost, a novel hybrid framework that integrates\nSMOTE, ADASYN, and CTGAN, optimizing synthetic data generation through weighted\naveraging and iterative weight tuning to enhance the model's performance\n(achieving up to a 1.87% accuracy improvement over individual balancing\ntechniques). A comprehensive counterfactual analysis is conducted to quantify\nthe feature-level changes required to shift individuals from high-risk to\nlow-risk categories. The results indicate that blood glucose (50.3%) and\ntriglycerides (46.7%) were the most frequently modified features, highlighting\ntheir clinical significance in MetS risk reduction. Additionally, probabilistic\nanalysis shows elevated blood glucose (85.5% likelihood) and triglycerides\n(74.9% posterior probability) as the strongest predictors. This study not only\nadvances the methodological rigor of MetS prediction but also provides\nactionable insights for clinicians and researchers, highlighting the potential\nof ML in mitigating the public health burden of metabolic syndrome.\n","authors":["Sanyam Paresh Shah","Abdullah Mamun","Shovito Barua Soumma","Hassan Ghasemzadeh"],"pdf_url":"https://arxiv.org/pdf/2504.06987v2.pdf","comment":"Accepted at the IEEE EMBC 2025 Conference. 7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.04340v1","updated":"2025-05-07T11:42:00Z","published":"2025-05-07T11:42:00Z","title":"Multi-Granular Attention based Heterogeneous Hypergraph Neural Network","summary":"  Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong\nabilities to learn node representations by effectively extracting complex\nstructural and semantic information in heterogeneous graphs. Most of the\nprevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging\nmeta-path based message passing to learn latent node representations. However,\ndue to the pairwise nature of meta-paths, these models fail to capture\nhigh-order relations among nodes, resulting in suboptimal performance.\nAdditionally, the challenge of ``over-squashing'', where long-range message\npassing in HeteGNNs leads to severe information distortion, further limits the\nefficacy of these models. To address these limitations, this paper proposes\nMGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural\nNetwork for heterogeneous graph representation learning. MGA-HHN introduces two\nkey innovations: (1) a novel approach for constructing meta-path based\nheterogeneous hypergraphs that explicitly models higher-order semantic\ninformation in heterogeneous graphs through multiple views, and (2) a\nmulti-granular attention mechanism that operates at both the node and hyperedge\nlevels. This mechanism enables the model to capture fine-grained interactions\namong nodes sharing the same semantic context within a hyperedge type, while\npreserving the diversity of semantics across different hyperedge types. As\nsuch, MGA-HHN effectively mitigates long-range message distortion and generates\nmore expressive node representations. Extensive experiments on real-world\nbenchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art\nmodels, showcasing its effectiveness in node classification, node clustering\nand visualization tasks.\n","authors":["Hong Jin","Kaicheng Zhou","Jie Yin","Lan You","Zhifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.04340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00205v2","updated":"2025-05-07T11:40:20Z","published":"2025-01-31T22:46:20Z","title":"EcoWeedNet: A Lightweight and Automated Weed Detection Method for\n  Sustainable Next-Generation Agricultural Consumer Electronics","summary":"  Sustainable agriculture plays a crucial role in ensuring world food security\nfor consumers. A critical challenge faced by sustainable precision agriculture\nis weed growth, as weeds compete for essential resources with crops, such as\nwater, soil nutrients, and sunlight, which notably affect crop yields. The\nadoption of automated computer vision technologies and ground agricultural\nconsumer electronic vehicles in precision agriculture offers sustainable,\nlow-carbon solutions. However, prior works suffer from issues such as low\naccuracy and precision, as well as high computational expense. This work\nproposes EcoWeedNet, a novel model that enhances weed detection performance\nwithout introducing significant computational complexity, aligning with the\ngoals of low-carbon agricultural practices. The effectiveness of the proposed\nmodel is demonstrated through comprehensive experiments on the CottonWeedDet12\nbenchmark dataset, which reflects real-world scenarios. EcoWeedNet achieves\nperformance comparable to that of large models (mAP@0.5 = 95.2%), yet with\nsignificantly fewer parameters (approximately 4.21% of the parameters of\nYOLOv4), lower computational complexity and better computational efficiency\n6.59% of the GFLOPs of YOLOv4). These key findings indicate EcoWeedNet's\ndeployability on low-power consumer hardware, lower energy consumption, and\nhence reduced carbon footprint, thereby emphasizing the application prospects\nof EcoWeedNet in next-generation sustainable agriculture. These findings\nprovide the way forward for increased application of environmentally-friendly\nagricultural consumer technologies.\n","authors":["Omar H. Khater","Abdul Jabbar Siddiqui","M. Shamim Hossain","Aiman El-Maleh"],"pdf_url":"https://arxiv.org/pdf/2502.00205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18884v2","updated":"2025-05-07T11:31:37Z","published":"2025-04-26T10:10:26Z","title":"A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text\n  Classification","summary":"  With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%.\n","authors":["Junichiro Niimi"],"pdf_url":"https://arxiv.org/pdf/2504.18884v2.pdf","comment":"This manuscript has been accepted for the 30th International\n  Conference on Natural Language \\& Information Systems (NLDB 2025) and will\n  appear in Springer Lecture Notes in Computer Science (LNCS)"},{"id":"http://arxiv.org/abs/2505.04318v1","updated":"2025-05-07T11:04:47Z","published":"2025-05-07T11:04:47Z","title":"Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of\n  Fit Testing","summary":"  As the adoption of deep learning models has grown beyond human capacity for\nverification, meta-algorithms are needed to ensure reliable model inference.\nConcept drift detection is a field dedicated to identifying statistical shifts\nthat is underutilized in monitoring neural networks that may encounter\ninference data with distributional characteristics diverging from their\ntraining data. Given the wide variety of model architectures, applications, and\ndatasets, it is important that concept drift detection algorithms are adaptable\nto different inference scenarios. In this paper, we introduce an application of\nthe $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection\nmeta-algorithm applied to a multilayer perceptron, a convolutional neural\nnetwork, and a transformer trained for machine vision as they are exposed to\nsimulated drift during inference. To that end, we demonstrate how unexpected\ndrops in accuracy due to concept drift can be detected without directly\nexamining the inference outputs. Our approach enhances safety by ensuring\nmodels are continually evaluated for reliability across varying conditions.\n","authors":["Jacob Glenn Ayers","Buvaneswari A. Ramanan","Manzoor A. Khan"],"pdf_url":"https://arxiv.org/pdf/2505.04318v1.pdf","comment":"8 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2505.04317v1","updated":"2025-05-07T11:04:36Z","published":"2025-05-07T11:04:36Z","title":"Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play\n  Reinforcement Learning","summary":"  In this paper, we tackle the problem of learning to play 3v3 multi-drone\nvolleyball, a new embodied competitive task that requires both high-level\nstrategic coordination and low-level agile control. The task is turn-based,\nmulti-agent, and physically grounded, posing significant challenges due to its\nlong-horizon dependencies, tight inter-agent coupling, and the underactuated\ndynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play\n(HCSP), a hierarchical reinforcement learning framework that separates\ncentralized high-level strategic decision-making from decentralized low-level\nmotion control. We design a three-stage population-based training pipeline to\nenable both strategy and skill to emerge from scratch without expert\ndemonstrations: (I) training diverse low-level skills, (II) learning high-level\nstrategy via self-play with fixed low-level controllers, and (III) joint\nfine-tuning through co-self-play. Experiments show that HCSP achieves superior\nperformance, outperforming non-hierarchical self-play and rule-based\nhierarchical baselines with an average 82.9\\% win rate and a 71.5\\% win rate\nagainst the two-stage variant. Moreover, co-self-play leads to emergent team\nbehaviors such as role switching and coordinated formations, demonstrating the\neffectiveness of our hierarchical design and training scheme.\n","authors":["Ruize Zhang","Sirui Xiang","Zelai Xu","Feng Gao","Shilong Ji","Wenhao Tang","Wenbo Ding","Chao Yu","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2505.04317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04313v1","updated":"2025-05-07T10:56:05Z","published":"2025-05-07T10:56:05Z","title":"KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge\n  Representation and Reasoning","summary":"  In this paper, we introduce KERAIA, a novel framework and software platform\nfor symbolic knowledge engineering designed to address the persistent\nchallenges of representing, reasoning with, and executing knowledge in dynamic,\ncomplex, and context-sensitive environments. The central research question that\nmotivates this work is: How can unstructured, often tacit, human expertise be\neffectively transformed into computationally tractable algorithms that AI\nsystems can efficiently utilise? KERAIA seeks to bridge this gap by building on\nfoundational concepts such as Minsky's frame-based reasoning and K-lines, while\nintroducing significant innovations. These include Clouds of Knowledge for\ndynamic aggregation, Dynamic Relations (DRels) for context-sensitive\ninheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and\nCloud Elaboration for adaptive knowledge transformation. This approach moves\nbeyond the limitations of traditional, often static, knowledge representation\nparadigms. KERAIA is designed with Explainable AI (XAI) as a core principle,\nensuring transparency and interpretability, particularly through the use of\nLoTs. The paper details the framework's architecture, the KSYNTH representation\nlanguage, and the General Purpose Paradigm Builder (GPPB) to integrate diverse\ninference methods within a unified structure. We validate KERAIA's versatility,\nexpressiveness, and practical applicability through detailed analysis of\nmultiple case studies spanning naval warfare simulation, industrial diagnostics\nin water treatment plants, and strategic decision-making in the game of RISK.\nFurthermore, we provide a comparative analysis against established knowledge\nrepresentation paradigms (including ontologies, rule-based systems, and\nknowledge graphs) and discuss the implementation aspects and computational\nconsiderations of the KERAIA platform.\n","authors":["Stephen Richard Varey","Alessandro Di Stefano","The Anh Han"],"pdf_url":"https://arxiv.org/pdf/2505.04313v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2505.04310v1","updated":"2025-05-07T10:49:53Z","published":"2025-05-07T10:49:53Z","title":"Flow Models for Unbounded and Geometry-Aware Distributional\n  Reinforcement Learning","summary":"  We introduce a new architecture for Distributional Reinforcement Learning\n(DistRL) that models return distributions using normalizing flows. This\napproach enables flexible, unbounded support for return distributions, in\ncontrast to categorical approaches like C51 that rely on fixed or bounded\nrepresentations. It also offers richer modeling capacity to capture\nmulti-modality, skewness, and tail behavior than quantile based approaches. Our\nmethod is significantly more parameter-efficient than categorical approaches.\nStandard metrics used to train existing models like KL divergence or\nWasserstein distance either are scale insensitive or have biased sample\ngradients, especially when return supports do not overlap. To address this, we\npropose a novel surrogate for the Cram\\`er distance, that is geometry-aware and\ncomputable directly from the return distribution's PDF, avoiding the costly CDF\ncomputation. We test our model on the ATARI-5 sub-benchmark and show that our\napproach outperforms PDF based models while remaining competitive with quantile\nbased methods.\n","authors":["Simo Alami C.","Rim Kaddah","Jesse Read","Marie-Paule Cani"],"pdf_url":"https://arxiv.org/pdf/2505.04310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04308v1","updated":"2025-05-07T10:46:33Z","published":"2025-05-07T10:46:33Z","title":"Guardians of the Web: The Evolution and Future of Website Information\n  Security","summary":"  Website information security has become a critical concern in the digital\nage. This article explores the evolution of website information security,\nexamining its historical development, current practices, and future directions.\nThe early beginnings from the 1960s to the 1980s laid the groundwork for modern\ncybersecurity, with the development of ARPANET, TCP/IP, public-key\ncryptography, and the first antivirus programs. The 1990s marked a\ntransformative era, driven by the commercialization of the Internet and the\nemergence of web-based services. As the Internet grew, so did the range and\nsophistication of cyber threats, leading to advancements in security\ntechnologies such as the Secure Sockets Layer (SSL) protocol, password\nprotection, and firewalls. Current practices in website information security\ninvolve a multi-layered approach, including encryption, secure coding\npractices, regular security audits, and user education. The future of website\ninformation security is expected to be shaped by emerging technologies such as\nartificial intelligence, blockchain, and quantum computing, as well as the\nincreasing importance of international cooperation and standardization efforts.\nAs cyber threats continue to evolve, ongoing research and innovation in website\ninformation security will be essential to protect sensitive information and\nmaintain trust in the digital world.\n","authors":["Md Saiful Islam","Li Xiangdong"],"pdf_url":"https://arxiv.org/pdf/2505.04308v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2505.04300v1","updated":"2025-05-07T10:14:31Z","published":"2025-05-07T10:14:31Z","title":"Sparsity is All You Need: Rethinking Biological Pathway-Informed\n  Approaches in Deep Learning","summary":"  Biologically-informed neural networks typically leverage pathway annotations\nto enhance performance in biomedical applications. We hypothesized that the\nbenefits of pathway integration does not arise from its biological relevance,\nbut rather from the sparsity it introduces. We conducted a comprehensive\nanalysis of all relevant pathway-based neural network models for predictive\ntasks, critically evaluating each study's contributions. From this review, we\ncurated a subset of methods for which the source code was publicly available.\nThe comparison of the biologically informed state-of-the-art deep learning\nmodels and their randomized counterparts showed that models based on randomized\ninformation performed equally well as biologically informed ones across\ndifferent metrics and datasets. Notably, in 3 out of the 15 analyzed models,\nthe randomized versions even outperformed their biologically informed\ncounterparts. Moreover, pathway-informed models did not show any clear\nadvantage in interpretability, as randomized models were still able to identify\nrelevant disease biomarkers despite lacking explicit pathway information. Our\nfindings suggest that pathway annotations may be too noisy or inadequately\nexplored by current methods. Therefore, we propose a methodology that can be\napplied to different domains and can serve as a robust benchmark for\nsystematically comparing novel pathway-informed models against their randomized\ncounterparts. This approach enables researchers to rigorously determine whether\nobserved performance improvements can be attributed to biological insights.\n","authors":["Isabella Caranzano","Corrado Pancotti","Cesare Rollo","Flavio Sartori","Pietro Liò","Piero Fariselli","Tiziana Sanavia"],"pdf_url":"https://arxiv.org/pdf/2505.04300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03783v3","updated":"2025-05-07T10:09:56Z","published":"2025-04-03T16:12:03Z","title":"FAST: Federated Active Learning with Foundation Models for\n  Communication-efficient Sampling and Training","summary":"  Federated Active Learning (FAL) has emerged as a promising framework to\nleverage large quantities of unlabeled data across distributed clients while\npreserving data privacy. However, real-world deployments remain limited by high\nannotation costs and communication-intensive sampling processes, particularly\nin a cross-silo setting, when clients possess substantial local datasets. This\npaper addresses the crucial question: What is the best practice to reduce\ncommunication costs in human-in-the-loop learning with minimal annotator\neffort? Existing FAL methods typically rely on iterative annotation processes\nthat separate active sampling from federated updates, leading to multiple\nrounds of expensive communication and annotation. In response, we introduce\nFAST, a two-pass FAL framework that harnesses foundation models for weak\nlabeling in a preliminary pass, followed by a refinement pass focused\nexclusively on the most uncertain samples. By leveraging representation\nknowledge from foundation models and integrating refinement steps into a\nstreamlined workflow, FAST substantially reduces the overhead incurred by\niterative active sampling. Extensive experiments on diverse medical and natural\nimage benchmarks demonstrate that FAST outperforms existing FAL methods by an\naverage of 4.36% while reducing communication rounds eightfold under a limited\n5% labeling budget.\n","authors":["Haoyuan Li","Mathias Funk","Jindong Wang","Aaqib Saeed"],"pdf_url":"https://arxiv.org/pdf/2504.03783v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22377v2","updated":"2025-05-07T10:08:36Z","published":"2024-10-29T08:05:10Z","title":"A Systematic Literature Review of Spatio-Temporal Graph Neural Network\n  Models for Time Series Forecasting and Classification","summary":"  In recent years, spatio-temporal graph neural networks (GNNs) have attracted\nconsiderable interest in the field of time series analysis, due to their\nability to capture dependencies among variables and across time points. The\nobjective of the presented systematic literature review is hence to provide a\ncomprehensive overview of the various modeling approaches and application\ndomains of GNNs for time series classification and forecasting. A database\nsearch was conducted, and over 150 journal papers were selected for a detailed\nexamination of the current state-of-the-art in the field. This examination is\nintended to offer to the reader a comprehensive collection of proposed models,\nlinks to related source code, available datasets, benchmark models, and fitting\nresults. All this information is hoped to assist researchers in future studies.\nTo the best of our knowledge, this is the first systematic literature review\npresenting a detailed comparison of the results of current spatio-temporal GNN\nmodels in different domains. In addition, in its final part this review\ndiscusses current limitations and challenges in the application of\nspatio-temporal GNNs, such as comparability, reproducibility, explainability,\npoor information capacity, and scalability.\n","authors":["Flavio Corradini","Flavio Gerosa","Marco Gori","Carlo Lucheroni","Marco Piangerelli","Martina Zannotti"],"pdf_url":"https://arxiv.org/pdf/2410.22377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07547v2","updated":"2025-05-07T10:08:15Z","published":"2024-10-10T02:39:22Z","title":"HM-DF SNN: Transcending Conventional Online Learning with Advanced\n  Training and Deployment","summary":"  Spiking Neural Networks (SNNs) are considered to have enormous potential in\nthe future development of Artificial Intelligence due to their brain-inspired\nand energy-efficient properties. Compared to vanilla Spatial-Temporal\nBack-propagation (STBP) training methods, online training can effectively\novercome the risk of GPU memory explosion. However, current online learning\nframework cannot tackle the inseparability problem of temporal dependent\ngradients and merely aim to optimize the training memory, resulting in no\nperformance advantages compared to the STBP training models in the inference\nphase. To address the aforementioned challenges, we propose Hybrid\nMechanism-Driven Firing (HM-DF) model, which is a family of advanced models\nthat respectively adopt different spiking calculation schemes in the\nupper-region and lower-region of the firing threshold. We point out that HM-DF\nmodel can effectively separate temporal gradients and tackle the mismatch\nproblem of surrogate gradients, as well as achieving full-stage optimization\ntowards computation speed and memory footprint. Experimental results have\ndemonstrated that HM-DF model can be flexibly combined with various techniques\nto achieve state-of-the-art performance in the field of online learning,\nwithout triggering further power consumption.\n","authors":["Zecheng Hao","Yifan Huang","Zijie Xu","Wenxuan Liu","Yuanhong Tang","Zhaofei Yu","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2410.07547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.18892v2","updated":"2025-05-07T09:57:34Z","published":"2025-03-24T17:06:10Z","title":"SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild","summary":"  DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.\n","authors":["Weihao Zeng","Yuzhen Huang","Qian Liu","Wei Liu","Keqing He","Zejun Ma","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2503.18892v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20752v2","updated":"2025-05-07T09:47:51Z","published":"2025-04-29T13:33:29Z","title":"Grokking in the Wild: Data Augmentation for Real-World Multi-Hop\n  Reasoning with Transformers","summary":"  Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.\n","authors":["Roman Abramov","Felix Steinbauer","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2504.20752v2.pdf","comment":"Accepted to the International Conference on Machine Learning (ICML)\n  2025"},{"id":"http://arxiv.org/abs/2407.01635v6","updated":"2025-05-07T09:45:29Z","published":"2024-06-30T10:53:40Z","title":"Commute Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have shown remarkable success in learning from\ngraph-structured data. However, their application to directed graphs (digraphs)\npresents unique challenges, primarily due to the inherent asymmetry in node\nrelationships. Traditional GNNs are adept at capturing unidirectional relations\nbut fall short in encoding the mutual path dependencies between nodes, such as\nasymmetrical shortest paths typically found in digraphs. Recognizing this gap,\nwe introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly\nintegrates node-wise commute time into the message passing scheme. The\ncornerstone of CGNN is an efficient method for computing commute time using a\nnewly formulated digraph Laplacian. Commute time is then integrated into the\nneighborhood aggregation process, with neighbor contributions weighted\naccording to their respective commute time to the central node in each layer.\nIt enables CGNN to directly capture the mutual, asymmetric relationships in\ndigraphs. Extensive experiments on 8 benchmarking datasets confirm the\nsuperiority of CGNN against 13 state-of-the-art methods.\n","authors":["Wei Zhuo","Han Yu","Guang Tan","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2407.01635v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04284v1","updated":"2025-05-07T09:40:18Z","published":"2025-05-07T09:40:18Z","title":"GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer\n  Pharmacovigilance","summary":"  In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable.\n","authors":["Sofia Jamil","Aryan Dabad","Bollampalli Areen Reddy","Sriparna Saha","Rajiv Misra","Adil A. Shakur"],"pdf_url":"https://arxiv.org/pdf/2505.04284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18504v3","updated":"2025-05-07T09:39:42Z","published":"2025-01-30T17:13:32Z","title":"CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to\n  Sustainability Data Extraction","summary":"  Large Language Model (LLM) image recognition is a powerful tool for\nextracting data from images, but accuracy depends on providing sufficient cues\nin the prompt - requiring a domain expert for specialized tasks. We introduce\nCue Learning using Evolution for Accurate Recognition (CLEAR), which uses a\ncombination of LLMs and evolutionary computation to generate and optimize cues\nsuch that recognition of specialized features in images is improved. It\nachieves this by auto-generating a novel domain-specific representation and\nthen using it to optimize suitable textual cues with a genetic algorithm. We\napply CLEAR to the real-world task of identifying sustainability data from\ninterior and exterior images of buildings. We investigate the effects of using\na variable-length representation compared to fixed-length and show how LLM\nconsistency can be improved by refactoring from categorical to real-valued\nestimates. We show that CLEAR enables higher accuracy compared to expert human\nrecognition and human-authored prompts in every task with error rates improved\nby up to two orders of magnitude and an ablation study evincing solution\nconcision.\n","authors":["Peter J. Bentley","Soo Ling Lim","Fuyuki Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2501.18504v3.pdf","comment":"9 pages plus 2 pages of supplemental material"},{"id":"http://arxiv.org/abs/2504.17393v2","updated":"2025-05-07T09:34:46Z","published":"2025-04-24T09:25:29Z","title":"Towards User-Centred Design of AI-Assisted Decision-Making in Law\n  Enforcement","summary":"  Artificial Intelligence (AI) has become an important part of our everyday\nlives, yet user requirements for designing AI-assisted systems in law\nenforcement remain unclear. To address this gap, we conducted qualitative\nresearch on decision-making within a law enforcement agency. Our study aimed to\nidentify limitations of existing practices, explore user requirements and\nunderstand the responsibilities that humans expect to undertake in these\nsystems.\n  Participants in our study highlighted the need for a system capable of\nprocessing and analysing large volumes of data efficiently to help in crime\ndetection and prevention. Additionally, the system should satisfy requirements\nfor scalability, accuracy, justification, trustworthiness and adaptability to\nbe adopted in this domain. Participants also emphasised the importance of\nhaving end users review the input data that might be challenging for AI to\ninterpret, and validate the generated output to ensure the system's accuracy.\nTo keep up with the evolving nature of the law enforcement domain, end users\nneed to help the system adapt to the changes in criminal behaviour and\ngovernment guidance, and technical experts need to regularly oversee and\nmonitor the system. Furthermore, user-friendly human interaction with the\nsystem is essential for its adoption and some of the participants confirmed\nthey would be happy to be in the loop and provide necessary feedback that the\nsystem can learn from. Finally, we argue that it is very unlikely that the\nsystem will ever achieve full automation due to the dynamic and complex nature\nof the law enforcement domain.\n","authors":["Vesna Nowack","Dalal Alrajeh","Carolina Gutierrez Muñoz","Katie Thomas","William Hobson","Patrick Benjamin","Catherine Hamilton-Giachritsis","Tim Grant","Juliane A. Kloess","Jessica Woodhams"],"pdf_url":"https://arxiv.org/pdf/2504.17393v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2504.18827v2","updated":"2025-05-07T09:29:45Z","published":"2025-04-26T07:29:12Z","title":"Test It Before You Trust It: Applying Software Testing for Trustworthy\n  In-context Learning","summary":"  In-context learning (ICL) has emerged as a powerful capability of large\nlanguage models (LLMs), enabling them to perform new tasks based on a few\nprovided examples without explicit fine-tuning. Despite their impressive\nadaptability, these models remain vulnerable to subtle adversarial\nperturbations and exhibit unpredictable behavior when faced with linguistic\nvariations. Inspired by software testing principles, we introduce a software\ntesting-inspired framework, called MMT4NL, for evaluating the trustworthiness\nof in-context learning by utilizing adversarial perturbations and software\ntesting techniques. It includes diverse evaluation aspects of linguistic\ncapabilities for testing the ICL capabilities of LLMs. MMT4NL is built around\nthe idea of crafting metamorphic adversarial examples from a test set in order\nto quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is\nto treat any LLM as software and validate its functionalities just like testing\nthe software. Finally, we demonstrate applications of MMT4NL on the sentiment\nanalysis and question-answering tasks. Our experiments could reveal various\nlinguistic bugs in state-of-the-art LLMs.\n","authors":["Teeradaj Racharak","Chaiyong Ragkhitwetsagul","Chommakorn Sontesadisai","Thanwadee Sunetnanta"],"pdf_url":"https://arxiv.org/pdf/2504.18827v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04278v1","updated":"2025-05-07T09:29:39Z","published":"2025-05-07T09:29:39Z","title":"Non-stationary Diffusion For Probabilistic Time Series Forecasting","summary":"  Due to the dynamics of underlying physics and external influences, the\nuncertainty of time series often varies over time. However, existing Denoising\nDiffusion Probabilistic Models (DDPMs) often fail to capture this\nnon-stationary nature, constrained by their constant variance assumption from\nthe additive noise model (ANM). In this paper, we innovatively utilize the\nLocation-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of\nANM. A diffusion-based probabilistic forecasting framework, termed\nNon-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of\nmodeling the changing pattern of uncertainty. Specifically, NsDiff combines a\ndenoising diffusion-based conditional generative model with a pre-trained\nconditional mean and variance estimator, enabling adaptive endpoint\ndistribution modeling. Furthermore, we propose an uncertainty-aware noise\nschedule, which dynamically adjusts the noise levels to accurately reflect the\ndata uncertainty at each step and integrates the time-varying variances into\nthe diffusion process. Extensive experiments conducted on nine real-world and\nsynthetic datasets demonstrate the superior performance of NsDiff compared to\nexisting approaches. Code is available at https://github.com/wwy155/NsDiff.\n","authors":["Weiwei Ye","Zhuopeng Xu","Ning Gui"],"pdf_url":"https://arxiv.org/pdf/2505.04278v1.pdf","comment":"Accepted as spotlight poster at ICML"},{"id":"http://arxiv.org/abs/2505.04270v1","updated":"2025-05-07T09:20:12Z","published":"2025-05-07T09:20:12Z","title":"Object-Shot Enhanced Grounding Network for Egocentric Video","summary":"  Egocentric video grounding is a crucial task for embodied intelligence\napplications, distinct from exocentric video moment localization. Existing\nmethods primarily focus on the distributional differences between egocentric\nand exocentric videos but often neglect key characteristics of egocentric\nvideos and the fine-grained information emphasized by question-type queries. To\naddress these limitations, we propose OSGNet, an Object-Shot enhanced Grounding\nNetwork for egocentric video. Specifically, we extract object information from\nvideos to enrich video representation, particularly for objects highlighted in\nthe textual query but not directly captured in the video features.\nAdditionally, we analyze the frequent shot movements inherent to egocentric\nvideos, leveraging these features to extract the wearer's attention\ninformation, which enhances the model's ability to perform modality alignment.\nExperiments conducted on three datasets demonstrate that OSGNet achieves\nstate-of-the-art performance, validating the effectiveness of our approach. Our\ncode can be found at https://github.com/Yisen-Feng/OSGNet.\n","authors":["Yisen Feng","Haoyu Zhang","Meng Liu","Weili Guan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2505.04270v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2505.04265v1","updated":"2025-05-07T09:14:55Z","published":"2025-05-07T09:14:55Z","title":"Weaponizing Language Models for Cybersecurity Offensive Operations:\n  Automating Vulnerability Assessment Report Validation; A Review Paper","summary":"  This, with the ever-increasing sophistication of cyberwar, calls for novel\nsolutions. In this regard, Large Language Models (LLMs) have emerged as a\nhighly promising tool for defensive and offensive cybersecurity-related\nstrategies. While existing literature has focused much on the defensive use of\nLLMs, when it comes to their offensive utilization, very little has been\nreported-namely, concerning Vulnerability Assessment (VA) report validation.\nConsequentially, this paper tries to fill that gap by investigating the\ncapabilities of LLMs in automating and improving the validation process of the\nreport of the VA. From the critical review of the related literature, this\npaper hereby proposes a new approach to using the LLMs in the automation of the\nanalysis and within the validation process of the report of the VA that could\npotentially reduce the number of false positives and generally enhance\nefficiency. These results are promising for LLM automatization for improving\nvalidation on reports coming from VA in order to improve accuracy while\nreducing human effort and security postures. The contribution of this paper\nprovides further evidence about the offensive and defensive LLM capabilities\nand therefor helps in devising more appropriate cybersecurity strategies and\ntools accordingly.\n","authors":["Abdulrahman S Almuhaidib","Azlan Mohd Zain","Zalmiyah Zakaria","Izyan Izzati Kamsani","Abdulaziz S Almuhaidib"],"pdf_url":"https://arxiv.org/pdf/2505.04265v1.pdf","comment":"Pre-print - Accepted for publication in the Proceedings of the\n  International Computer Sciences and Informatics Conference (ICSIC-2024),\n  published by AIP Publishing"},{"id":"http://arxiv.org/abs/2505.04260v1","updated":"2025-05-07T09:10:51Z","published":"2025-05-07T09:10:51Z","title":"Steerable Chatbots: Personalizing LLMs with Preference-Based Activation\n  Steering","summary":"  As large language models (LLMs) improve in their capacity to serve as\npersonal AI assistants, their ability to output uniquely tailored, personalized\nresponses that align with the soft preferences of their users is essential for\nenhancing user satisfaction and retention. However, untrained lay users have\npoor prompt specification abilities and often struggle with conveying their\nlatent preferences to AI assistants. To address this, we leverage activation\nsteering to guide LLMs to align with interpretable preference dimensions during\ninference. In contrast to memory-based personalization methods that require\nlonger user history, steering is extremely lightweight and can be easily\ncontrolled by the user via an linear strength factor. We embed steering into\nthree different interactive chatbot interfaces and conduct a within-subjects\nuser study (n=14) to investigate how end users prefer to personalize their\nconversations. The results demonstrate the effectiveness of preference-based\nsteering for aligning real-world conversations with hidden user preferences,\nand highlight further insights on how diverse values around control, usability,\nand transparency lead users to prefer different interfaces.\n","authors":["Jessica Y. Bo","Tianyu Xu","Ishan Chatterjee","Katrina Passarella-Ward","Achin Kulshrestha","D Shin"],"pdf_url":"https://arxiv.org/pdf/2505.04260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12806v2","updated":"2025-05-07T09:01:00Z","published":"2025-04-17T10:12:38Z","title":"A Numerical Gradient Inversion Attack in Variational Quantum\n  Neural-Networks","summary":"  The loss landscape of Variational Quantum Neural Networks (VQNNs) is\ncharacterized by local minima that grow exponentially with increasing qubits.\nBecause of this, it is more challenging to recover information from model\ngradients during training compared to classical Neural Networks (NNs). In this\npaper we present a numerical scheme that successfully reconstructs input\ntraining, real-world, practical data from trainable VQNNs' gradients. Our\nscheme is based on gradient inversion that works by combining gradients\nestimation with the finite difference method and adaptive low-pass filtering.\nThe scheme is further optimized with Kalman filter to obtain efficient\nconvergence. Our experiments show that our algorithm can invert even\nbatch-trained data, given the VQNN model is sufficiently over-parameterized.\n","authors":["Georgios Papadopoulos","Shaltiel Eloul","Yash Satsangi","Jamie Heredge","Niraj Kumar","Chun-Fu Chen","Marco Pistoia"],"pdf_url":"https://arxiv.org/pdf/2504.12806v2.pdf","comment":"9 pages, 17 figures"},{"id":"http://arxiv.org/abs/2505.04251v1","updated":"2025-05-07T08:55:15Z","published":"2025-05-07T08:55:15Z","title":"Facilitating Trustworthy Human-Agent Collaboration in LLM-based\n  Multi-Agent System oriented Software Engineering","summary":"  Multi-agent autonomous systems (MAS) are better at addressing challenges that\nspans across multiple domains than singular autonomous agents. This holds true\nwithin the field of software engineering (SE) as well. The state-of-the-art\nresearch on MAS within SE focuses on integrating LLMs at the core of autonomous\nagents to create LLM-based multi-agent autonomous (LMA) systems. However, the\nintroduction of LMA systems into SE brings a plethora of challenges. One of the\nmajor challenges is the strategic allocation of tasks between humans and the\nLMA system in a trustworthy manner. To address this challenge, a RACI-based\nframework is proposed in this work in progress article, along with\nimplementation guidelines and an example implementation of the framework. The\nproposed framework can facilitate efficient collaboration, ensure\naccountability, and mitigate potential risks associated with LLM-driven\nautomation while aligning with the Trustworthy AI guidelines. The future steps\nfor this work delineating the planned empirical validation method are also\npresented.\n","authors":["Krishna Ronanki"],"pdf_url":"https://arxiv.org/pdf/2505.04251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01709v2","updated":"2025-05-07T08:37:17Z","published":"2025-05-03T06:17:18Z","title":"RoBridge: A Hierarchical Architecture Bridging Cognition and Execution\n  for General Robotic Manipulation","summary":"  Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation.\n","authors":["Kaidong Zhang","Rongtao Xu","Pengzhen Ren","Junfan Lin","Hefeng Wu","Liang Lin","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2505.01709v2.pdf","comment":"project page: https://abliao.github.io/RoBridge/"},{"id":"http://arxiv.org/abs/2505.04223v1","updated":"2025-05-07T08:20:23Z","published":"2025-05-07T08:20:23Z","title":"FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated\n  Learning","summary":"  Federated learning (FL) enables collaborative model training across\ndistributed clients while preserving data locality. Although FedAvg pioneered\nsynchronous rounds for global model averaging, slower devices can delay\ncollective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by\ncontinuously integrating client updates, yet naive implementations risk client\ndrift due to non-IID data and stale contributions. Some Blockchain-based FL\napproaches (e.g., BRAIN) employ robust weighting or scoring of updates to\nresist malicious or misaligned proposals. However, performance drops can still\npersist under severe data heterogeneity or high staleness, and synchronization\noverhead has emerged as a new concern due to its aggregator-free architectures.\n  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL\nmethod that mitigates these limitations by incorporating two key ideas. First,\nour FastSync strategy eliminates the need to replay past model versions,\nenabling newcomers and infrequent participants to efficiently approximate the\nglobal model. Second, we adopt spherical linear interpolation (SLERP) when\nmerging parameters, preserving models' directions and alleviating destructive\ninterference from divergent local training.\n  Experiments with a CNN image-classification model and a Transformer-based\nlanguage model demonstrate that FRAIN achieves more stable and robust\nconvergence than FedAvg, FedAsync, and BRAIN, especially under harsh\nenvironments: non-IID data distributions, networks that experience delays and\nrequire frequent re-synchronization, and the presence of malicious nodes.\n","authors":["Sanghyeon Park","Soo-Mook Moon"],"pdf_url":"https://arxiv.org/pdf/2505.04223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02388v2","updated":"2025-05-07T08:16:50Z","published":"2025-04-03T08:29:57Z","title":"Steiner Traveling Salesman Problem with Quantum Annealing","summary":"  The Steiner Traveling Salesman Problem (STSP) is a variant of the classical\nTraveling Salesman Problem. The STSP involves incorporating steiner nodes,\nwhich are extra nodes not originally part of the required visit set but that\ncan be added to the route to enhance the overall solution and minimize the\ntotal travel cost. Given the NP-hard nature of the STSP, we propose a quantum\napproach to address it. Specifically, we employ quantum annealing using\nD-Wave's hardware to explore its potential for solving this problem. To enhance\ncomputational feasibility, we develop a preprocessing method that effectively\nreduces the network size. Our experimental results demonstrate that this\nreduction technique significantly decreases the problem complexity, making the\nQuadratic Unconstrained Binary Optimization formulation, the standard input for\nquantum annealers, better suited for existing quantum hardware. Furthermore,\nthe results highlight the potential of quantum annealing as a promising and\ninnovative approach for solving the STSP.\n","authors":["Alessia Ciacco","Francesca Guerriero","Eneko Osaba"],"pdf_url":"https://arxiv.org/pdf/2504.02388v2.pdf","comment":"7 pages, 1 figure, 6 tables. Paper accepted in The Genetic and\n  Evolutionary Computation Conference (GECCO 2025)"},{"id":"http://arxiv.org/abs/2412.02302v2","updated":"2025-05-07T08:16:09Z","published":"2024-12-03T09:16:13Z","title":"Enhanced Photovoltaic Power Forecasting: An iTransformer and LSTM-Based\n  Model Integrating Temporal and Covariate Interactions","summary":"  Accurate photovoltaic (PV) power forecasting is critical for integrating\nrenewable energy sources into the grid, optimizing real-time energy management,\nand ensuring energy reliability amidst increasing demand. However, existing\nmodels often struggle with effectively capturing the complex relationships\nbetween target variables and covariates, as well as the interactions between\ntemporal dynamics and multivariate data, leading to suboptimal forecasting\naccuracy. To address these challenges, we propose a novel model architecture\nthat leverages the iTransformer for feature extraction from target variables\nand employs long short-term memory (LSTM) to extract features from covariates.\nA cross-attention mechanism is integrated to fuse the outputs of both models,\nfollowed by a Kolmogorov-Arnold network (KAN) mapping for enhanced\nrepresentation. The effectiveness of the proposed model is validated using\npublicly available datasets from Australia, with experiments conducted across\nfour seasons. Results demonstrate that the proposed model effectively capture\nseasonal variations in PV power generation and improve forecasting accuracy.\n","authors":["Guang Wu","Yun Wang","Qian Zhou","Ziyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.02302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18271v2","updated":"2025-05-07T08:12:56Z","published":"2025-01-30T11:10:46Z","title":"Vision-Language Model Selection and Reuse for Downstream Adaptation","summary":"  Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular\nacross various visual tasks, and several open-sourced VLM variants have been\nreleased. However, selecting the best-performing pre-trained VLM for a specific\ndownstream task is challenging since no single VLM can achieve promising\nperformance on all downstream tasks, and evaluating all available VLMs is\nimpossible due to time and data limitations. To address this problem, this\npaper proposes a novel paradigm to select and reuse VLM for downstream tasks,\ncalled Model Label Learning (MLL). The proposal contains three key modules:\n\\emph{model labeling}, which assigns labels to each VLM to describe their\nspecialty and utility; \\emph{model selection}, which matches the requirements\nof the target task with model labels; and \\emph{model reuse}, which applies\nselected VLMs to the target task in an ensemble manner. The proposal is highly\ncomputationally efficient and growable since the model labeling process is\ncompleted target task independent and the ability could grow with the number of\ncandidate VLMs. We also introduce a new benchmark for evaluating VLM selection\nmethods, including 49 VLMs and 17 target task datasets. Experimental results\nclearly demonstrate the effectiveness of the proposed method for selecting and\nreusing VLMs.\n","authors":["Hao-Zhe Tan","Zhi Zhou","Yu-Feng Li","Lan-Zhe Guo"],"pdf_url":"https://arxiv.org/pdf/2501.18271v2.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2202.03482v3","updated":"2025-05-07T08:08:45Z","published":"2022-02-07T19:40:20Z","title":"Navigating Neural Space: Revisiting Concept Activation Vectors to\n  Overcome Directional Divergence","summary":"  With a growing interest in understanding neural network prediction\nstrategies, Concept Activation Vectors (CAVs) have emerged as a popular tool\nfor modeling human-understandable concepts in the latent space. Commonly, CAVs\nare computed by leveraging linear classifiers optimizing the separability of\nlatent representations of samples with and without a given concept. However, in\nthis paper we show that such a separability-oriented computation leads to\nsolutions, which may diverge from the actual goal of precisely modeling the\nconcept direction. This discrepancy can be attributed to the significant\ninfluence of distractor directions, i.e., signals unrelated to the concept,\nwhich are picked up by filters (i.e., weights) of linear models to optimize\nclass-separability. To address this, we introduce pattern-based CAVs, solely\nfocussing on concept signals, thereby providing more accurate concept\ndirections. We evaluate various CAV methods in terms of their alignment with\nthe true concept direction and their impact on CAV applications, including\nconcept sensitivity testing and model correction for shortcut behavior caused\nby data artifacts. We demonstrate the benefits of pattern-based CAVs using the\nPediatric Bone Age, ISIC2019, and FunnyBirds datasets with VGG, ResNet, ReXNet,\nEfficientNet, and Vision Transformer as model architectures.\n","authors":["Frederik Pahde","Maximilian Dreyer","Leander Weber","Moritz Weckbecker","Christopher J. Anders","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"pdf_url":"https://arxiv.org/pdf/2202.03482v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23346v2","updated":"2025-05-07T08:03:27Z","published":"2024-10-30T18:00:02Z","title":"ASURA-FDPS-ML: Star-by-star Galaxy Simulations Accelerated by Surrogate\n  Modeling for Supernova Feedback","summary":"  We introduce new high-resolution galaxy simulations accelerated by a\nsurrogate model that reduces the computation cost by approximately 75 percent.\nMassive stars with a Zero Age Main Sequence mass of more than about 10\n$\\mathrm{M_\\odot}$ explode as core-collapse supernovae (CCSNe), which play a\ncritical role in galaxy formation. The energy released by CCSNe is essential\nfor regulating star formation and driving feedback processes in the\ninterstellar medium (ISM). However, the short integration timesteps required\nfor SNe feedback have presented significant bottlenecks in astrophysical\nsimulations across various scales. Overcoming this challenge is crucial for\nenabling star-by-star galaxy simulations, which aim to capture the dynamics of\nindividual stars and the inhomogeneous shell's expansion within the turbulent\nISM. To address this, our new framework combines direct numerical simulations\nand surrogate modeling, including machine learning and Gibbs sampling. The star\nformation history and the time evolution of outflow rates in the galaxy match\nthose obtained from resolved direct numerical simulations. Our new approach\nachieves high-resolution fidelity while reducing computational costs,\neffectively bridging the physical scale gap and enabling multi-scale\nsimulations.\n","authors":["Keiya Hirashima","Kana Moriwaki","Michiko S. Fujii","Yutaka Hirai","Takayuki R. Saitoh","Junnichiro Makino","Ulrich P. Steinwandel","Shirley Ho"],"pdf_url":"https://arxiv.org/pdf/2410.23346v2.pdf","comment":"22 pages, 15 figures, 3 tables, accepted for publication in ApJ"},{"id":"http://arxiv.org/abs/2505.04209v1","updated":"2025-05-07T08:03:25Z","published":"2025-05-07T08:03:25Z","title":"To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase\n  Relevance at eBay","summary":"  E-commerce sellers are recommended keyphrases based on their inventory on\nwhich they advertise to increase buyer engagement (clicks/sales). The relevance\nof advertiser keyphrases plays an important role in preventing the inundation\nof search systems with numerous irrelevant items that compete for attention in\nauctions, in addition to maintaining a healthy seller perception. In this work,\nwe describe the shortcomings of training Advertiser keyphrase relevance filter\nmodels on click/sales/search relevance signals and the importance of aligning\nwith human judgment, as sellers have the power to adopt or reject said\nkeyphrase recommendations. In this study, we frame Advertiser keyphrase\nrelevance as a complex interaction between 3 dynamical systems -- seller\njudgment, which influences seller adoption of our product, Advertising, which\nprovides the keyphrases to bid on, and Search, who holds the auctions for the\nsame keyphrases. This study discusses the practicalities of using human\njudgment via a case study at eBay Advertising and demonstrate that using\nLLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our\nrelevance models achieves a better harmony across the three systems -- provided\nthat they are bound by a meticulous evaluation framework grounded in business\nmetrics.\n","authors":["Soumik Dey","Hansi Wu","Binbin Li"],"pdf_url":"https://arxiv.org/pdf/2505.04209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04207v1","updated":"2025-05-07T07:58:57Z","published":"2025-05-07T07:58:57Z","title":"An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection\n  and Measurement","summary":"  Potholes cause vehicle damage and traffic accidents, creating serious safety\nand economic problems. Therefore, early and accurate detection of potholes is\ncrucial. Existing detection methods are usually only based on 2D RGB images and\ncannot accurately analyze the physical characteristics of potholes. In this\npaper, a publicly available dataset of RGB-D images (PothRGBD) is created and\nan improved YOLOv8-based model is proposed for both pothole detection and\npothole physical features analysis. The Intel RealSense D415 depth camera was\nused to collect RGB and depth data from the road surfaces, resulting in a\nPothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable\nfor segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg\narchitecture, which is structurally improved with Dynamic Snake Convolution\n(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit\n(GELU). The proposed model segmented potholes with irregular edge structure\nmore accurately, and performed perimeter and depth measurements on depth maps\nwith high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,\n85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to\n93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in\nprecision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model\nperforms pothole detection as well as perimeter and depth measurement with high\naccuracy and is suitable for real-time applications due to its low model\ncomplexity. In this way, a lightweight and effective model that can be used in\ndeep learning-based intelligent transportation solutions has been acquired.\n","authors":["Mustafa Yurdakul","Şakir Tasdemir"],"pdf_url":"https://arxiv.org/pdf/2505.04207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12224v2","updated":"2025-05-07T07:57:21Z","published":"2025-02-17T14:54:14Z","title":"Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate","summary":"  Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.\n","authors":["Zhiyuan Fang","Zicong Hong","Yuegui Huang","Yufeng Lyu","Wuhui Chen","Yue Yu","Fan Yu","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2502.12224v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01496v2","updated":"2025-05-07T07:42:11Z","published":"2025-03-03T13:08:00Z","title":"Liger: Linearizing Large Language Models to Gated Recurrent Structures","summary":"  Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization.\n","authors":["Disen Lan","Weigao Sun","Jiaxi Hu","Jusen Du","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.01496v2.pdf","comment":"Accepted by ICML 2025, 15 pages"},{"id":"http://arxiv.org/abs/2505.04192v1","updated":"2025-05-07T07:41:19Z","published":"2025-05-07T07:41:19Z","title":"VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video\n  Instruction Tuning","summary":"  We present VideoPath-LLaVA, the first large multimodal model (LMM) in\ncomputational pathology that integrates three distinct image scenarios, single\npatch images, automatically keyframe-extracted clips, and manually segmented\nvideo pathology images, to mimic the natural diagnostic process of\npathologists. By generating detailed histological descriptions and culminating\nin a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives\nwith diagnostic reasoning.\n  Central to our approach is the VideoPath-Instruct dataset, comprising 4278\nvideo and diagnosis-specific chain-of-thought instructional pairs sourced from\neducational histopathology videos on YouTube. Although high-quality data is\ncritical for enhancing diagnostic reasoning, its creation is time-intensive and\nlimited in volume. To overcome this challenge, we transfer knowledge from\nexisting single-image instruction datasets to train on weakly annotated,\nkeyframe-extracted clips, followed by fine-tuning on manually segmented videos.\nVideoPath-LLaVA establishes a new benchmark in pathology video analysis and\noffers a promising foundation for future AI systems that support clinical\ndecision-making through integrated visual and diagnostic reasoning. Our code,\ndata, and model are publicly available at\nhttps://github.com/trinhvg/VideoPath-LLaVA.\n","authors":["Trinh T. L. Vuong","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2505.04192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04185v1","updated":"2025-05-07T07:34:37Z","published":"2025-05-07T07:34:37Z","title":"S3D: Sketch-Driven 3D Model Generation","summary":"  Generating high-quality 3D models from 2D sketches is a challenging task due\nto the inherent ambiguity and sparsity of sketch data. In this paper, we\npresent S3D, a novel framework that converts simple hand-drawn sketches into\ndetailed 3D models. Our method utilizes a U-Net-based encoder-decoder\narchitecture to convert sketches into face segmentation masks, which are then\nused to generate a 3D representation that can be rendered from novel views. To\nensure robust consistency between the sketch domain and the 3D output, we\nintroduce a novel style-alignment loss that aligns the U-Net bottleneck\nfeatures with the initial encoder outputs of the 3D generation module,\nsignificantly enhancing reconstruction fidelity. To further enhance the\nnetwork's robustness, we apply augmentation techniques to the sketch dataset.\nThis streamlined framework demonstrates the effectiveness of S3D in generating\nhigh-quality 3D models from sketch inputs. The source code for this project is\npublicly available at https://github.com/hailsong/S3D.\n","authors":["Hail Song","Wonsik Shin","Naeun Lee","Soomin Chung","Nojun Kwak","Woontack Woo"],"pdf_url":"https://arxiv.org/pdf/2505.04185v1.pdf","comment":"Accepted as a short paper to the GMCV Workshop at CVPR'25"},{"id":"http://arxiv.org/abs/2308.04729v2","updated":"2025-05-07T07:09:38Z","published":"2023-08-09T06:27:24Z","title":"JEN-1: Text-Guided Universal Music Generation with Omnidirectional\n  Diffusion Models","summary":"  Music generation has attracted growing interest with the advancement of deep\ngenerative models. However, generating music conditioned on textual\ndescriptions, known as text-to-music, remains challenging due to the complexity\nof musical structures and high sampling rate requirements. Despite the task's\nsignificance, prevailing generative models exhibit limitations in music\nquality, computational efficiency, and generalization. This paper introduces\nJEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a\ndiffusion model incorporating both autoregressive and non-autoregressive\ntraining. Through in-context learning, JEN-1 performs various generation tasks\nincluding text-guided music generation, music inpainting, and continuation.\nEvaluations demonstrate JEN-1's superior performance over state-of-the-art\nmethods in text-music alignment and music quality while maintaining\ncomputational efficiency. Our demos are available at\nhttps://jenmusic.ai/audio-demos\n","authors":["Peike Li","Boyu Chen","Yao Yao","Yikai Wang","Allen Wang","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2308.04729v2.pdf","comment":"Github Demo Page: https://gogoduck912.github.io/Jen1-Demo-Page/"},{"id":"http://arxiv.org/abs/2505.04175v1","updated":"2025-05-07T07:06:04Z","published":"2025-05-07T07:06:04Z","title":"DOTA: Deformable Optimized Transformer Architecture for End-to-End Text\n  Recognition with Retrieval-Augmented Generation","summary":"  Text recognition in natural images remains a challenging yet essential task,\nwith broad applications spanning computer vision and natural language\nprocessing. This paper introduces a novel end-to-end framework that combines\nResNet and Vision Transformer backbones with advanced methodologies, including\nDeformable Convolutions, Retrieval-Augmented Generation, and Conditional Random\nFields (CRF). These innovations collectively enhance feature representation and\nimprove Optical Character Recognition (OCR) performance. Specifically, the\nframework substitutes standard convolution layers in the third and fourth\nblocks with Deformable Convolutions, leverages adaptive dropout for\nregularization, and incorporates CRF for more refined sequence modeling.\nExtensive experiments conducted on six benchmark datasets IC13, IC15, SVT,\nIIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving\nnotable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on\nIIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy\nof 77.77%. These results establish a new state-of-the-art for text recognition,\ndemonstrating the robustness of the approach across diverse and challenging\ndatasets.\n","authors":["Naphat Nithisopa","Teerapong Panboonyuen"],"pdf_url":"https://arxiv.org/pdf/2505.04175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04174v1","updated":"2025-05-07T07:04:49Z","published":"2025-05-07T07:04:49Z","title":"On-Device LLM for Context-Aware Wi-Fi Roaming","summary":"  Wireless roaming is a critical yet challenging task for maintaining seamless\nconnectivity in dynamic mobile environments. Conventional threshold-based or\nheuristic schemes often fail, leading to either sticky or excessive handovers.\nWe introduce the first cross-layer use of an on-device large language model\n(LLM): high-level reasoning in the application layer that issues real-time\nactions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)\ncontext-aware AP selection, where structured prompts fuse environmental cues\n(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold\nadjustment, where the model adaptively decides when to roam. To satisfy the\ntight latency and resource budgets of edge hardware, we apply a suite of\noptimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and\nquantization. Experiments on indoor and outdoor datasets show that our approach\nsurpasses legacy heuristics and DRL baselines, achieving a strong balance\nbetween roaming stability and signal quality. These findings underscore the\npromise of application-layer LLM reasoning for lower-layer wireless control in\nfuture edge systems.\n","authors":["Ju-Hyung Lee","Yanqing Lu"],"pdf_url":"https://arxiv.org/pdf/2505.04174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07158v5","updated":"2025-05-07T06:35:15Z","published":"2025-03-10T10:33:31Z","title":"Generative AI in Transportation Planning: A Survey","summary":"  The integration of generative artificial intelligence (GenAI) into\ntransportation planning has the potential to revolutionize tasks such as demand\nforecasting, infrastructure design, policy evaluation, and traffic simulation.\nHowever, there is a critical need for a systematic framework to guide the\nadoption of GenAI in this interdisciplinary domain. In this survey, we, a\nmultidisciplinary team of researchers spanning computer science and\ntransportation engineering, present the first comprehensive framework for\nleveraging GenAI in transportation planning. Specifically, we introduce a new\ntaxonomy that categorizes existing applications and methodologies into two\nperspectives: transportation planning tasks and computational techniques. From\nthe transportation planning perspective, we examine the role of GenAI in\nautomating descriptive, predictive, generative, simulation, and explainable\ntasks to enhance mobility systems. From the computational perspective, we\ndetail advancements in data preparation, domain-specific fine-tuning, and\ninference strategies, such as retrieval-augmented generation and zero-shot\nlearning tailored to transportation applications. Additionally, we address\ncritical challenges, including data scarcity, explainability, bias mitigation,\nand the development of domain-specific evaluation frameworks that align with\ntransportation goals like sustainability, equity, and system efficiency. This\nsurvey aims to bridge the gap between traditional transportation planning\nmethodologies and modern AI techniques, fostering collaboration and innovation.\nBy addressing these challenges and opportunities, we seek to inspire future\nresearch that ensures ethical, equitable, and impactful use of generative AI in\ntransportation planning.\n","authors":["Longchao Da","Tiejin Chen","Zhuoheng Li","Shreyas Bachiraju","Huaiyuan Yao","Li Li","Yushun Dong","Xiyang Hu","Zhengzhong Tu","Dongjie Wang","Yue Zhao","Ben Zhou","Ram Pendyala","Benjamin Stabler","Yezhou Yang","Xuesong Zhou","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2503.07158v5.pdf","comment":"55 pages"},{"id":"http://arxiv.org/abs/2505.04165v1","updated":"2025-05-07T06:34:34Z","published":"2025-05-07T06:34:34Z","title":"TS-SNN: Temporal Shift Module for Spiking Neural Networks","summary":"  Spiking Neural Networks (SNNs) are increasingly recognized for their\nbiological plausibility and energy efficiency, positioning them as strong\nalternatives to Artificial Neural Networks (ANNs) in neuromorphic computing\napplications. SNNs inherently process temporal information by leveraging the\nprecise timing of spikes, but balancing temporal feature utilization with low\nenergy consumption remains a challenge. In this work, we introduce Temporal\nShift module for Spiking Neural Networks (TS-SNN), which incorporates a novel\nTemporal Shift (TS) module to integrate past, present, and future spike\nfeatures within a single timestep via a simple yet effective shift operation. A\nresidual combination method prevents information loss by integrating shifted\nand original features. The TS module is lightweight, requiring only one\nadditional learnable parameter, and can be seamlessly integrated into existing\narchitectures with minimal additional computational cost. TS-SNN achieves\nstate-of-the-art performance on benchmarks like CIFAR-10 (96.72\\%), CIFAR-100\n(80.28\\%), and ImageNet (70.61\\%) with fewer timesteps, while maintaining low\nenergy consumption. This work marks a significant step forward in developing\nefficient and accurate SNN architectures.\n","authors":["Kairong Yu","Tianqing Zhang","Qi Xu","Gang Pan","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2505.04165v1.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2505.04147v1","updated":"2025-05-07T05:55:45Z","published":"2025-05-07T05:55:45Z","title":"R^3-VQA: \"Read the Room\" by Video Social Reasoning","summary":"  \"Read the room\" is a significant social reasoning capability in human daily\nlife. Humans can infer others' mental states from subtle social cues. Previous\nsocial reasoning tasks and datasets lack complexity (e.g., simple scenes, basic\ninteractions, incomplete mental state variables, single-step reasoning, etc.)\nand fall far short of the challenges present in real-life social interactions.\nIn this paper, we contribute a valuable, high-quality, and comprehensive video\ndataset named R^3-VQA with precise and fine-grained annotations of social\nevents and mental states (i.e., belief, intent, desire, and emotion) as well as\ncorresponding social causal chains in complex social scenarios. Moreover, we\ninclude human-annotated and model-generated QAs. Our task R^3-VQA includes\nthree aspects: Social Event Understanding, Mental State Estimation, and Social\nCausal Reasoning. As a benchmark, we comprehensively evaluate the social\nreasoning capabilities and consistencies of current state-of-the-art large\nvision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs\nare still far from human-level consistent social reasoning in complex social\nscenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on\nsocial reasoning tasks. We provide some of our dataset and codes in\nsupplementary material and will release our full dataset and codes upon\nacceptance.\n","authors":["Lixing Niu","Jiapeng Li","Xingping Yu","Shu Wang","Ruining Feng","Bo Wu","Ping Wei","Yisen Wang","Lifeng Fan"],"pdf_url":"https://arxiv.org/pdf/2505.04147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04146v1","updated":"2025-05-07T05:54:04Z","published":"2025-05-07T05:54:04Z","title":"Unmasking the Canvas: A Dynamic Benchmark for Image Generation\n  Jailbreaking and LLM Content Safety","summary":"  Existing large language models (LLMs) are advancing rapidly and produce\noutstanding results in image generation tasks, yet their content safety checks\nremain vulnerable to prompt-based jailbreaks. Through preliminary testing on\nplatforms such as ChatGPT, MetaAI, and Grok, we observed that even short,\nnatural prompts could lead to the generation of compromising images ranging\nfrom realistic depictions of forged documents to manipulated images of public\nfigures.\n  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and\nscalable benchmark dataset to evaluate LLM vulnerability in image generation.\nOur methodology combines structured prompt engineering, multilingual\nobfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted\nLLaMA-3. The pipeline supports both zero-shot and fallback prompting\nstrategies, risk scoring, and automated tagging. All generations are stored\nwith rich metadata and curated into Bronze (non-verified), Silver (LLM-aided\nverification), and Gold (manually verified) tiers. UTCB is designed to evolve\nover time with new data sources, prompt templates, and model behaviors.\n  Warning: This paper includes visual examples of adversarial inputs designed\nto test model safety. All outputs have been redacted to ensure responsible\ndisclosure.\n","authors":["Variath Madhupal Gautham Nair","Vishal Varma Dantuluri"],"pdf_url":"https://arxiv.org/pdf/2505.04146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19550v2","updated":"2025-05-07T05:20:30Z","published":"2024-10-25T13:24:04Z","title":"DeMuVGN: Effective Software Defect Prediction Model by Learning\n  Multi-view Software Dependency via Graph Neural Networks","summary":"  Software defect prediction (SDP) aims to identify high-risk defect modules in\nsoftware development, optimizing resource allocation. While previous studies\nshow that dependency network metrics improve defect prediction, most methods\nfocus on code-based dependency graphs, overlooking developer factors. Current\nmetrics, based on handcrafted features like ego and global network metrics,\nfail to fully capture defect-related information. To address this, we propose\nDeMuVGN, a defect prediction model that learns multi-view software dependency\nvia graph neural networks. We introduce a Multi-view Software Dependency Graph\n(MSDG) that integrates data, call, and developer dependencies. DeMuVGN also\nleverages the Synthetic Minority Oversampling Technique (SMOTE) to address\nclass imbalance and enhance defect module identification. In a case study of\neight open-source projects across 20 versions, DeMuVGN demonstrates significant\nimprovements: i) models based on multi-view graphs improve F1 scores by 11.1%\nto 12.1% over single-view models; ii) DeMuVGN improves F1 scores by 17.4% to\n45.8% in within-project contexts and by 17.9% to 41.0% in cross-project\ncontexts. Additionally, DeMuVGN excels in software evolution, showing more\nimprovement in later-stage software versions. Its strong performance across\ndifferent projects highlights its generalizability. We recommend future\nresearch focus on multi-view dependency graphs for defect prediction in both\nmature and newly developed projects.\n","authors":["Yu Qiao","Lina Gong","Yu Zhao","Yongwei Wang","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2410.19550v2.pdf","comment":"The current paper is not comprehensive enough. We are seeking further\n  improvement"},{"id":"http://arxiv.org/abs/2505.04132v1","updated":"2025-05-07T05:07:38Z","published":"2025-05-07T05:07:38Z","title":"Bringing legal knowledge to the public by constructing a legal question\n  bank using large-scale pre-trained language model","summary":"  Access to legal information is fundamental to access to justice. Yet\naccessibility refers not only to making legal documents available to the\npublic, but also rendering legal information comprehensible to them. A vexing\nproblem in bringing legal information to the public is how to turn formal legal\ndocuments such as legislation and judgments, which are often highly technical,\nto easily navigable and comprehensible knowledge to those without legal\neducation. In this study, we formulate a three-step approach for bringing legal\nknowledge to laypersons, tackling the issues of navigability and\ncomprehensibility. First, we translate selected sections of the law into\nsnippets (called CLIC-pages), each being a small piece of article that focuses\non explaining certain technical legal concept in layperson's terms. Second, we\nconstruct a Legal Question Bank (LQB), which is a collection of legal questions\nwhose answers can be found in the CLIC-pages. Third, we design an interactive\nCLIC Recommender (CRec). Given a user's verbal description of a legal situation\nthat requires a legal solution, CRec interprets the user's input and shortlists\nquestions from the question bank that are most likely relevant to the given\nlegal situation and recommends their corresponding CLIC pages where relevant\nlegal knowledge can be found. In this paper we focus on the technical aspects\nof creating an LQB. We show how large-scale pre-trained language models, such\nas GPT-3, can be used to generate legal questions. We compare machine-generated\nquestions (MGQs) against human-composed questions (HCQs) and find that MGQs are\nmore scalable, cost-effective, and more diversified, while HCQs are more\nprecise. We also show a prototype of CRec and illustrate through an example how\nour 3-step approach effectively brings relevant legal knowledge to the public.\n","authors":["Mingruo Yuan","Ben Kao","Tien-Hsuan Wu","Michael M. K. Cheung","Henry W. H. Chan","Anne S. Y. Cheung","Felix W. H. Chan","Yongxi Chen"],"pdf_url":"https://arxiv.org/pdf/2505.04132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11079v3","updated":"2025-05-07T04:46:31Z","published":"2023-07-02T00:26:26Z","title":"3D-IDS: Doubly Disentangled Dynamic Intrusion Detection","summary":"  Network-based intrusion detection system (NIDS) monitors network traffic for\nmalicious activities, forming the frontline defense against increasing attacks\nover information infrastructures. Although promising, our quantitative analysis\nshows that existing methods perform inconsistently in declaring various unknown\nattacks (e.g., 9% and 35% F1 respectively for two distinct unknown threats for\nan SVM-based method) or detecting diverse known attacks (e.g., 31% F1 for the\nBackdoor and 93% F1 for DDoS by a GCN-based state-of-the-art method), and\nreveals that the underlying cause is entangled distributions of flow features.\nThis motivates us to propose 3D-IDS, a novel method that aims to tackle the\nabove issues through two-step feature disentanglements and a dynamic graph\ndiffusion scheme. Specifically, we first disentangle traffic features by a\nnon-parameterized optimization based on mutual information, automatically\ndifferentiating tens and hundreds of complex features of various attacks. Such\ndifferentiated features will be fed into a memory model to generate\nrepresentations, which are further disentangled to highlight the\nattack-specific features. Finally, we use a novel graph diffusion method that\ndynamically fuses the network topology for spatial-temporal aggregation in\nevolving data streams. By doing so, we can effectively identify various attacks\nin encrypted traffics, including unknown threats and known ones that are not\neasily detected. Experiments show the superiority of our 3D-IDS. We also\ndemonstrate that our two-step feature disentanglements benefit the\nexplainability of NIDS.\n","authors":["Chenyang Qiu","Yingsheng Geng","Junrui Lu","Kaida Chen","Shitong Zhu","Ya Su","Guoshun Nan","Can Zhang","Junsong Fu","Qimei Cui","Xiaofeng Tao"],"pdf_url":"https://arxiv.org/pdf/2307.11079v3.pdf","comment":"Published in the proceedings of the KDD 2023 Research Track"},{"id":"http://arxiv.org/abs/2502.13994v2","updated":"2025-05-07T04:33:26Z","published":"2025-02-19T06:39:51Z","title":"Generative Detail Enhancement for Physically Based Materials","summary":"  We present a tool for enhancing the detail of physically based materials\nusing an off-the-shelf diffusion model and inverse rendering. Our goal is to\nenhance the visual fidelity of materials with detail that is often tedious to\nauthor, by adding signs of wear, aging, weathering, etc. As these appearance\ndetails are often rooted in real-world processes, we leverage a generative\nimage model trained on a large dataset of natural images with corresponding\nvisuals in context. Starting with a given geometry, UV mapping, and basic\nappearance, we render multiple views of the object. We use these views,\ntogether with an appearance-defining text prompt, to condition a diffusion\nmodel. The details it generates are then backpropagated from the enhanced\nimages to the material parameters via inverse differentiable rendering. For\ninverse rendering to be successful, the generated appearance has to be\nconsistent across all the images. We propose two priors to address the\nmulti-view consistency of the diffusion model. First, we ensure that the\ninitial noise that seeds the diffusion process is itself consistent across\nviews by integrating it from a view-independent UV space. Second, we enforce\ngeometric consistency by biasing the attention mechanism via a projective\nconstraint so that pixels attend strongly to their corresponding pixel\nlocations in other views. Our approach does not require any training or\nfinetuning of the diffusion model, is agnostic of the material model used, and\nthe enhanced material properties, i.e., 2D PBR textures, can be further edited\nby artists. This project is available at https://generative-detail.github.io.\n","authors":["Saeed Hadadan","Benedikt Bitterli","Tizian Zeltner","Jan Novák","Fabrice Rousselle","Jacob Munkberg","Jon Hasselgren","Bartlomiej Wronski","Matthias Zwicker"],"pdf_url":"https://arxiv.org/pdf/2502.13994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04115v1","updated":"2025-05-07T04:14:03Z","published":"2025-05-07T04:14:03Z","title":"Polynomial-Time Relational Probabilistic Inference in Open Universes","summary":"  Reasoning under uncertainty is a fundamental challenge in Artificial\nIntelligence. As with most of these challenges, there is a harsh dilemma\nbetween the expressive power of the language used, and the tractability of the\ncomputational problem posed by reasoning. Inspired by human reasoning, we\nintroduce a method of first-order relational probabilistic inference that\nsatisfies both criteria, and can handle hybrid (discrete and continuous)\nvariables. Specifically, we extend sum-of-squares logic of expectation to\nrelational settings, demonstrating that lifted reasoning in the bounded-degree\nfragment for knowledge bases of bounded quantifier rank can be performed in\npolynomial time, even with an a priori unknown and/or countably infinite set of\nobjects. Crucially, our notion of tractability is framed in proof-theoretic\nterms, which extends beyond the syntactic properties of the language or\nqueries. We are able to derive the tightest bounds provable by proofs of a\ngiven degree and size and establish completeness in our sum-of-squares\nrefutations for fixed degrees.\n","authors":["Luise Ge","Brendan Juba","Kris Nilsson"],"pdf_url":"https://arxiv.org/pdf/2505.04115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03204v2","updated":"2025-05-07T04:09:12Z","published":"2025-05-06T05:38:17Z","title":"DCS-ST for Classification of Breast Cancer Histopathology Images with\n  Limited Annotations","summary":"  Deep learning methods have shown promise in classifying breast cancer\nhistopathology images, but their performance often declines with limited\nannotated data, a critical challenge in medical imaging due to the high cost\nand expertise required for annotations.\n","authors":["Liu Suxing","Byungwon Min"],"pdf_url":"https://arxiv.org/pdf/2505.03204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17767v3","updated":"2025-05-07T03:38:59Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Structures in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated structures as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-structures/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v3.pdf","comment":"Accepted to RSS 2025. Project webpage:\n  https://arjung128.github.io/opening-articulated-structures/"},{"id":"http://arxiv.org/abs/2505.04101v1","updated":"2025-05-07T03:37:49Z","published":"2025-05-07T03:37:49Z","title":"LLMs' Suitability for Network Security: A Case Study of STRIDE Threat\n  Modeling","summary":"  Artificial Intelligence (AI) is expected to be an integral part of\nnext-generation AI-native 6G networks. With the prevalence of AI, researchers\nhave identified numerous use cases of AI in network security. However, there\nare almost nonexistent studies that analyze the suitability of Large Language\nModels (LLMs) in network security. To fill this gap, we examine the suitability\nof LLMs in network security, particularly with the case study of STRIDE threat\nmodeling. We utilize four prompting techniques with five LLMs to perform STRIDE\nclassification of 5G threats. From our evaluation results, we point out key\nfindings and detailed insights along with the explanation of the possible\nunderlying factors influencing the behavior of LLMs in the modeling of certain\nthreats. The numerical results and the insights support the necessity for\nadjusting and fine-tuning LLMs for network security use cases.\n","authors":["AbdulAziz AbdulGhaffar","Ashraf Matrawy"],"pdf_url":"https://arxiv.org/pdf/2505.04101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15022v3","updated":"2025-05-07T03:31:08Z","published":"2023-08-29T04:59:53Z","title":"Recursively Summarizing Enables Long-Term Dialogue Memory in Large\n  Language Models","summary":"  Recently, large language models (LLMs), such as GPT-4, stand out remarkable\nconversational abilities, enabling them to engage in dynamic and contextually\nrelevant dialogues across a wide range of topics. However, given a long\nconversation, these chatbots fail to recall past information and tend to\ngenerate inconsistent responses. To address this, we propose to recursively\ngenerate summaries/ memory using large language models (LLMs) to enhance\nlong-term memory ability. Specifically, our method first stimulates LLMs to\nmemorize small dialogue contexts and then recursively produce new memory using\nprevious memory and following contexts. Finally, the chatbot can easily\ngenerate a highly consistent response with the help of the latest memory. We\nevaluate our method on both open and closed LLMs, and the experiments on the\nwidely-used public dataset show that our method can generate more consistent\nresponses in a long-context conversation. Also, we show that our strategy could\nnicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced\nLLMs, bringing further long-term dialogue performance. Notably, our method is a\npotential solution to enable the LLM to model the extremely long context. The\ncode and scripts will be released later.\n","authors":["Qingyue Wang","Yanhe Fu","Yanan Cao","Shuai Wang","Zhiliang Tian","Liang Ding"],"pdf_url":"https://arxiv.org/pdf/2308.15022v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11026v2","updated":"2025-05-07T03:14:53Z","published":"2024-12-15T02:41:31Z","title":"SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph\n  Generation","summary":"  Dynamic scenes contain intricate spatio-temporal information, crucial for\nmobile robots, UAVs, and autonomous driving systems to make informed decisions.\nParsing these scenes into semantic triplets <Subject-Predicate-Object> for\naccurate Scene Graph Generation (SGG) is highly challenging due to the\nfluctuating spatio-temporal complexity. Inspired by the reasoning capabilities\nof Large Language Models (LLMs), we propose SceneLLM, a novel framework that\nleverages LLMs as powerful scene analyzers for dynamic SGG. Our framework\nintroduces a Video-to-Language (V2L) mapping module that transforms video\nframes into linguistic signals (scene tokens), making the input more\ncomprehensible for LLMs. To better encode spatial information, we devise a\nSpatial Information Aggregation (SIA) scheme, inspired by the structure of\nChinese characters, which encodes spatial data into tokens. Using Optimal\nTransport (OT), we generate an implicit language signal from the frame-level\ntoken sequence that captures the video's spatio-temporal information. To\nfurther improve the LLM's ability to process this implicit linguistic input, we\napply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a\ntransformer-based SGG predictor to decode the LLM's reasoning and predict\nsemantic triplets. Our method achieves state-of-the-art results on the Action\nGenome (AG) benchmark, and extensive experiments show the effectiveness of\nSceneLLM in understanding and generating accurate dynamic scene graphs.\n","authors":["Hang Zhang","Zhuoling Li","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11026v2.pdf","comment":"29 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.04084v1","updated":"2025-05-07T02:51:32Z","published":"2025-05-07T02:51:32Z","title":"An Empirical Study of OpenAI API Discussions on Stack Overflow","summary":"  The rapid advancement of large language models (LLMs), represented by\nOpenAI's GPT series, has significantly impacted various domains such as natural\nlanguage processing, software development, education, healthcare, finance, and\nscientific research. However, OpenAI APIs introduce unique challenges that\ndiffer from traditional APIs, such as the complexities of prompt engineering,\ntoken-based cost management, non-deterministic outputs, and operation as black\nboxes. To the best of our knowledge, the challenges developers encounter when\nusing OpenAI APIs have not been explored in previous empirical studies. To fill\nthis gap, we conduct the first comprehensive empirical study by analyzing 2,874\nOpenAI API-related discussions from the popular Q&A forum Stack Overflow. We\nfirst examine the popularity and difficulty of these posts. After manually\ncategorizing them into nine OpenAI API-related categories, we identify specific\nchallenges associated with each category through topic modeling analysis. Based\non our empirical findings, we finally propose actionable implications for\ndevelopers, LLM vendors, and researchers.\n","authors":["Xiang Chen","Jibin Wang","Chaoyang Gao","Xiaolin Ju","Zhanqi Cui"],"pdf_url":"https://arxiv.org/pdf/2505.04084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04083v1","updated":"2025-05-07T02:49:52Z","published":"2025-05-07T02:49:52Z","title":"Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training","summary":"  Graph neural networks have emerged as a potent class of neural networks\ncapable of leveraging the connectivity and structure of real-world graphs to\nlearn intricate properties and relationships between nodes. Many real-world\ngraphs exceed the memory capacity of a GPU due to their sheer size, and using\nGNNs on them requires techniques such as mini-batch sampling to scale. However,\nthis can lead to reduced accuracy in some cases, and sampling and data transfer\nfrom the CPU to the GPU can also slow down training. On the other hand,\ndistributed full-graph training suffers from high communication overhead and\nload imbalance due to the irregular structure of graphs. We propose Plexus, a\nthree-dimensional (3D) parallel approach for full-graph training that tackles\nthese issues and scales to billion-edge graphs. Additionally, we introduce\noptimizations such as a permutation scheme for load balancing, and a\nperformance model to predict the optimal 3D configuration. We evaluate Plexus\non several graph datasets and show scaling results for up to 2048 GPUs on\nPerlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus\nachieves unprecedented speedups of 2.3x-12.5x over existing methods and a\nreduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on\nFrontier.\n","authors":["Aditya K. Ranjan","Siddharth Singh","Cunyang Wei","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2505.04083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21043v2","updated":"2025-05-07T02:31:34Z","published":"2025-04-28T14:14:16Z","title":"CodeBC: A More Secure Large Language Model for Smart Contract Code\n  Generation in Blockchain","summary":"  Large language models (LLMs) excel at generating code from natural language\ninstructions, yet they often lack an understanding of security vulnerabilities.\nThis limitation makes it difficult for LLMs to avoid security risks in\ngenerated code, particularly in high-security programming tasks such as smart\ncontract development for blockchain. Researchers have attempted to enhance the\nvulnerability awareness of these models by training them to differentiate\nbetween vulnerable and fixed code snippets. However, this approach relies\nheavily on manually labeled vulnerability data, which is only available for\npopular languages like Python and C++. For low-resource languages like\nSolidity, used in smart contracts, large-scale annotated datasets are scarce\nand difficult to obtain. To address this challenge, we introduce CodeBC, a code\ngeneration model specifically designed for generating secure smart contracts in\nblockchain. CodeBC employs a three-stage fine-tuning approach based on\nCodeLlama, distinguishing itself from previous methods by not relying on\npairwise vulnerability location annotations. Instead, it leverages\nvulnerability and security tags to teach the model the differences between\nvulnerable and secure code. During the inference phase, the model leverages\nsecurity tags to generate secure and robust code. Experimental results\ndemonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU,\nand compilation pass rates, while significantly reducing vulnerability rates.\nThese findings validate the effectiveness and cost-efficiency of our\nthree-stage fine-tuning strategy, making CodeBC a promising solution for\ngenerating secure smart contract code.\n","authors":["Lingxiang Wang","Hainan Zhang","Qinnan Zhang","Ziwei Wang","Hongwei Zheng","Jin Dong","Zhiming Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.21043v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04075v1","updated":"2025-05-07T02:26:17Z","published":"2025-05-07T02:26:17Z","title":"LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?","summary":"  This paper examines whether large language model (LLM) capabilities can\ncontinue to advance without additional compute by analyzing the development and\nrole of algorithms used in state-of-the-art LLMs. Motivated by regulatory\nefforts that have largely focused on restricting access to high-performance\nhardware, we ask: Can LLMs progress in a compute-constrained environment, and\nhow do algorithmic innovations perform under such conditions?\n  To address these questions, we introduce a novel classification framework\nthat distinguishes between compute-dependent innovations -- which yield\ndisproportionate benefits at high compute levels (e.g., the Transformer\narchitecture and mixture-of-experts models) and compute-independent\ninnovations, which improve efficiency across all compute scales (e.g., rotary\npositional encoding, FlashAttention, or layer normalization). We quantify these\ncontributions using a metric called compute-equivalent gain (CEG), which\nestimates the additional compute that would be required to achieve similar\nimprovements without these algorithmic advancements.\n  To validate this framework, we conduct small-scale training experiments with\na scaled-down GPT-2 model. Our results confirm that compute-independent\nadvancements yield meaningful performance gains even in resource-constrained\nsettings, with a CEG of up to $3.5\\times$ over a baseline model. By contrast,\ncompute-dependent advancements provided little benefit or even degraded\nperformance at the small scale, reinforcing the importance of compute\navailability for certain algorithmic gains.\n","authors":["Teddy Foley","Spencer Guo","Henry Josephson","Anqi Qu","Jack Sanderson"],"pdf_url":"https://arxiv.org/pdf/2505.04075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04072v1","updated":"2025-05-07T02:25:20Z","published":"2025-05-07T02:25:20Z","title":"Advancing and Benchmarking Personalized Tool Invocation for LLMs","summary":"  Tool invocation is a crucial mechanism for extending the capabilities of\nLarge Language Models (LLMs) and has recently garnered significant attention.\nIt enables LLMs to solve complex problems through tool calls while accessing\nup-to-date world knowledge. However, existing work primarily focuses on the\nfundamental ability of LLMs to invoke tools for problem-solving, without\nconsidering personalized constraints in tool invocation. In this work, we\nintroduce the concept of Personalized Tool Invocation and define two key tasks:\nTool Preference and Profile-dependent Query. Tool Preference addresses user\npreferences when selecting among functionally similar tools, while\nProfile-dependent Query considers cases where a user query lacks certain tool\nparameters, requiring the model to infer them from the user profile. To tackle\nthese challenges, we propose PTool, a data synthesis framework designed for\npersonalized tool invocation. Additionally, we construct \\textbf{PTBench}, the\nfirst benchmark for evaluating personalized tool invocation. We then fine-tune\nvarious open-source models, demonstrating the effectiveness of our framework\nand providing valuable insights. Our benchmark is public at\nhttps://github.com/hyfshadow/PTBench.\n","authors":["Xu Huang","Yuefeng Huang","Weiwen Liu","Xingshan Zeng","Yasheng Wang","Ruiming Tang","Hong Xie","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2505.04072v1.pdf","comment":"14 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2411.05261v2","updated":"2025-05-07T01:51:52Z","published":"2024-11-08T01:46:11Z","title":"Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained\n  Image Interpretation for Automated Report Generation","summary":"  Despite significant advancements in automated report generation, the\nopaqueness of text interpretability continues to cast doubt on the reliability\nof the content produced. This paper introduces a novel approach to identify\nspecific image features in X-ray images that influence the outputs of report\ngeneration models. Specifically, we propose Cyclic Vision-Language Manipulator\nCVLM, a module to generate a manipulated X-ray from an original X-ray and its\nreport from a designated report generator. The essence of CVLM is that cycling\nmanipulated X-rays to the report generator produces altered reports aligned\nwith the alterations pre-injected into the reports for X-ray generation,\nachieving the term \"cyclic manipulation\". This process allows direct comparison\nbetween original and manipulated X-rays, clarifying the critical image features\ndriving changes in reports and enabling model users to assess the reliability\nof the generated texts. Empirical evaluations demonstrate that CVLM can\nidentify more precise and reliable features compared to existing explanation\nmethods, significantly enhancing the transparency and applicability of\nAI-generated reports.\n","authors":["Yingying Fang","Zihao Jin","Shaojie Guo","Jinda Liu","Zhiling Yue","Yijian Gao","Junzhi Ning","Zhi Li","Simon Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2411.05261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09713v3","updated":"2025-05-07T01:38:16Z","published":"2024-06-14T04:46:14Z","title":"Meta-Learning Loss Functions for Deep Neural Networks","summary":"  Humans can often quickly and efficiently solve complex new learning tasks\ngiven only a small set of examples. In contrast, modern artificially\nintelligent systems often require thousands or millions of observations in\norder to solve even the most basic tasks. Meta-learning aims to resolve this\nissue by leveraging past experiences from similar learning tasks to embed the\nappropriate inductive biases into the learning system. Historically methods for\nmeta-learning components such as optimizers, parameter initializations, and\nmore have led to significant performance increases. This thesis aims to explore\nthe concept of meta-learning to improve performance, through the\noften-overlooked component of the loss function. The loss function is a vital\ncomponent of a learning system, as it represents the primary learning\nobjective, where success is determined and quantified by the system's ability\nto optimize for that objective successfully.\n","authors":["Christian Raymond"],"pdf_url":"https://arxiv.org/pdf/2406.09713v3.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2505.02366v2","updated":"2025-05-07T01:11:50Z","published":"2025-05-05T05:09:21Z","title":"JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for\n  Unsupervised Contrastive Learning of Sentence Embeddings","summary":"  Unsupervised contrastive learning has become a hot research topic in natural\nlanguage processing. Existing works usually aim at constraining the orientation\ndistribution of the representations of positive and negative samples in the\nhigh-dimensional semantic space in contrastive learning, but the semantic\nrepresentation tensor possesses both modulus and orientation features, and the\nexisting works ignore the modulus feature of the representations and cause\ninsufficient contrastive learning. % Therefore, we firstly propose a training\nobjective that aims at modulus constraints on the semantic representation\ntensor, to strengthen the alignment between the positive samples in contrastive\nlearning. Therefore, we first propose a training objective that is designed to\nimpose modulus constraints on the semantic representation tensor, to strengthen\nthe alignment between positive samples in contrastive learning. Then, the\nBERT-like model suffers from the phenomenon of sinking attention, leading to a\nlack of attention to CLS tokens that aggregate semantic information. In\nresponse, we propose a cross-attention structure among the twin-tower ensemble\nmodels to enhance the model's attention to CLS token and optimize the quality\nof CLS Pooling. Combining the above two motivations, we propose a new\n\\textbf{J}oint \\textbf{T}ensor representation modulus constraint and\n\\textbf{C}ross-attention unsupervised contrastive learning \\textbf{S}entence\n\\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven\nsemantic text similarity computation tasks, and the experimental results show\nthat JTCSE's twin-tower ensemble model and single-tower distillation model\noutperform the other baselines and become the current SOTA. In addition, we\nhave conducted an extensive zero-shot downstream task evaluation, which shows\nthat JTCSE outperforms other baselines overall on more than 130 tasks.\n","authors":["Tianyu Zong","Hongzhu Yi","Bingkang Shi","Yuanxiang Wang","Jungang Xu"],"pdf_url":"https://arxiv.org/pdf/2505.02366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04034v1","updated":"2025-05-07T00:27:00Z","published":"2025-05-07T00:27:00Z","title":"Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency,\n  and Transferability in Spiking Neural Networks","summary":"  Biological neurons exhibit diverse temporal spike patterns, which are\nbelieved to support efficient, robust, and adaptive neural information\nprocessing. While models such as Izhikevich can replicate a wide range of these\nfiring dynamics, their complexity poses challenges for directly integrating\nthem into scalable spiking neural networks (SNN) training pipelines. In this\nwork, we propose two probabilistically driven, input-level temporal spike\ntransformations: Poisson-Burst and Delayed-Burst that introduce biologically\ninspired temporal variability directly into standard Leaky Integrate-and-Fire\n(LIF) neurons. This enables scalable training and systematic evaluation of how\nspike timing dynamics affect privacy, generalization, and learning performance.\nPoisson-Burst modulates burst occurrence based on input intensity, while\nDelayed-Burst encodes input strength through burst onset timing. Through\nextensive experiments across multiple benchmarks, we demonstrate that\nPoisson-Burst maintains competitive accuracy and lower resource overhead while\nexhibiting enhanced privacy robustness against membership inference attacks,\nwhereas Delayed-Burst provides stronger privacy protection at a modest accuracy\ntrade-off. These findings highlight the potential of biologically grounded\ntemporal spike dynamics in improving the privacy, generalization and biological\nplausibility of neuromorphic learning systems.\n","authors":["Ayana Moshruba","Hamed Poursiami","Maryam Parsa"],"pdf_url":"https://arxiv.org/pdf/2505.04034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01695v3","updated":"2025-05-07T23:56:40Z","published":"2024-03-04T03:09:28Z","title":"DyCE: Dynamically Configurable Exiting for Deep Learning Compression and\n  Real-time Scaling","summary":"  Conventional deep learning (DL) model compression and scaling methods focus\non altering the model's components, impacting the results across all samples\nuniformly. However, since samples vary in difficulty, a dynamic model that\nadapts computation based on sample complexity offers a novel perspective for\ncompression and scaling. Despite this potential, existing dynamic models are\ntypically monolithic and model-specific, limiting their generalizability as\nbroad compression and scaling methods. Additionally, most deployed DL systems\nare fixed, unable to adjust their scale once deployed and, therefore, cannot\nadapt to the varying real-time demands. This paper introduces DyCE, a\ndynamically configurable system that can adjust the performance-complexity\ntrade-off of a DL model at runtime without requiring re-initialization or\nredeployment on inference hardware. DyCE achieves this by adding small exit\nnetworks to intermediate layers of the original model, allowing computation to\nterminate early if acceptable results are obtained. DyCE also decouples the\ndesign of an efficient dynamic model, facilitating easy adaptation to new base\nmodels and potential general use in compression and scaling. We also propose\nmethods for generating optimized configurations and determining the types and\npositions of exit networks to achieve desired performance and complexity\ntrade-offs. By enabling simple configuration switching, DyCE provides\nfine-grained performance tuning in real-time. We demonstrate the effectiveness\nof DyCE through image classification tasks using deep convolutional neural\nnetworks (CNNs). DyCE significantly reduces computational complexity by 23.5%\nfor ResNet152 and 25.9% for ConvNextv2-tiny on ImageNet, with accuracy\nreductions of less than 0.5%.\n","authors":["Qingyuan Wang","Barry Cardiff","Antoine Frappé","Benoit Larras","Deepu John"],"pdf_url":"https://arxiv.org/pdf/2403.01695v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04852v1","updated":"2025-05-07T23:30:27Z","published":"2025-05-07T23:30:27Z","title":"PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer\n  Rust","summary":"  There has been a growing interest in translating C code to Rust due to Rust's\nrobust memory and thread safety guarantees. Tools such as C2RUST enable\nsyntax-guided transpilation from C to semantically equivalent Rust code.\nHowever, the resulting Rust programs often rely heavily on unsafe\nconstructs--particularly raw pointers--which undermines Rust's safety\nguarantees. This paper aims to improve the memory safety of Rust programs\ngenerated by C2RUST by eliminating raw pointers. Specifically, we propose a\npeephole raw pointer rewriting technique that lifts raw pointers in individual\nfunctions to appropriate Rust data structures. Technically, PR2 employs\ndecision-tree-based prompting to guide the pointer lifting process.\nAdditionally, it leverages code change analysis to guide the repair of errors\nintroduced during rewriting, effectively addressing errors encountered during\ncompilation and test case execution. We implement PR2 as a prototype and\nevaluate it using gpt-4o-mini on 28 real-world C projects. The results show\nthat PR2 successfully eliminates 13.22% of local raw pointers across these\nprojects, significantly enhancing the safety of the translated Rust code. On\naverage, PR2 completes the transformation of a project in 5.44 hours, at an\naverage cost of $1.46.\n","authors":["Yifei Gao","Chengpeng Wang","Pengxiang Huang","Xuwei Liu","Mingwei Zheng","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.04852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04851v1","updated":"2025-05-07T23:29:28Z","published":"2025-05-07T23:29:28Z","title":"CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused\n  Text-to-Image Generation","summary":"  Despite the fact that popular text-to-image generation models cope well with\ninternational and general cultural queries, they have a significant knowledge\ngap regarding individual cultures. This is due to the content of existing large\ntraining datasets collected on the Internet, which are predominantly based on\nWestern European or American popular culture. Meanwhile, the lack of cultural\nadaptation of the model can lead to incorrect results, a decrease in the\ngeneration quality, and the spread of stereotypes and offensive content. In an\neffort to address this issue, we examine the concept of cultural code and\nrecognize the critical importance of its understanding by modern image\ngeneration models, an issue that has not been sufficiently addressed in the\nresearch community to date. We propose the methodology for collecting and\nprocessing the data necessary to form a dataset based on the cultural code, in\nparticular the Russian one. We explore how the collected data affects the\nquality of generations in the national domain and analyze the effectiveness of\nour approach using the Kandinsky 3.1 text-to-image model. Human evaluation\nresults demonstrate an increase in the level of awareness of Russian culture in\nthe model.\n","authors":["Viacheslav Vasilev","Vladimir Arkhipkin","Julia Agafonova","Tatiana Nikulina","Evelina Mironova","Alisa Shichanina","Nikolai Gerasimenko","Mikhail Shoytov","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2505.04851v1.pdf","comment":"This is arxiv version of the paper which was accepted for the Doklady\n  Mathematics Journal in 2024"},{"id":"http://arxiv.org/abs/2505.00455v2","updated":"2025-05-07T23:28:33Z","published":"2025-05-01T11:10:17Z","title":"Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts\n  Using Large Language Models","summary":"  Effective data visualization requires not only technical proficiency but also\na deep understanding of the domain-specific context in which data exists. This\ncontext often includes tacit knowledge about data provenance, quality, and\nintended use, which is rarely explicit in the dataset itself. We present the\nData Therapist, a web-based tool that helps domain experts externalize this\nimplicit knowledge through a mixed-initiative process combining iterative Q&A\nwith interactive annotation. Powered by a large language model, the system\nanalyzes user-supplied datasets, prompts users with targeted questions, and\nallows annotation at varying levels of granularity. The resulting structured\nknowledge base can inform both human and automated visualization design. We\nevaluated the tool in a qualitative study involving expert pairs from Molecular\nBiology, Accounting, Political Science, and Usable Security. The study revealed\nrecurring patterns in how experts reason about their data and highlights areas\nwhere AI support can improve visualization design.\n","authors":["Sungbok Shin","Hyeon Jeon","Sanghyun Hong","Niklas Elmqvist"],"pdf_url":"https://arxiv.org/pdf/2505.00455v2.pdf","comment":"Submitted to IEEE VIS2025"},{"id":"http://arxiv.org/abs/2505.04847v1","updated":"2025-05-07T22:50:33Z","published":"2025-05-07T22:50:33Z","title":"Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards","summary":"  Hallucinations remain a persistent challenge for LLMs. RAG aims to reduce\nhallucinations by grounding responses in contexts. However, even when provided\ncontext, LLMs still frequently introduce unsupported information or\ncontradictions. This paper presents our efforts to measure LLM hallucinations\nwith a focus on summarization tasks, assessing how often various LLMs introduce\nhallucinations when summarizing documents. We discuss Vectara's existing LLM\nhallucination leaderboard, based on the Hughes Hallucination Evaluation Model\n(HHEM). While HHEM and Vectara's Hallucination Leaderboard have garnered great\nresearch interest, we examine challenges faced by HHEM and current\nhallucination detection methods by analyzing the effectiveness of these methods\non existing hallucination datasets. To address these limitations, we propose\nFaithJudge, an LLM-as-a-judge approach guided by few-shot human hallucination\nannotations, which substantially improves automated LLM hallucination\nevaluation over current methods. We introduce an enhanced hallucination\nleaderboard centered on FaithJudge, alongside our current hallucination\nleaderboard, enabling more reliable benchmarking of LLMs for hallucinations in\nRAG.\n","authors":["Manveer Singh Tamber","Forrest Sheng Bao","Chenyu Xu","Ge Luo","Suleman Kazi","Minseok Bae","Miaoran Li","Ofer Mendelevitch","Renyi Qu","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2505.04847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04843v1","updated":"2025-05-07T22:42:37Z","published":"2025-05-07T22:42:37Z","title":"Large Language Models are Autonomous Cyber Defenders","summary":"  Fast and effective incident response is essential to prevent adversarial\ncyberattacks. Autonomous Cyber Defense (ACD) aims to automate incident response\nthrough Artificial Intelligence (AI) agents that plan and execute actions. Most\nACD approaches focus on single-agent scenarios and leverage Reinforcement\nLearning (RL). However, ACD RL-trained agents depend on costly training, and\ntheir reasoning is not always explainable or transferable. Large Language\nModels (LLMs) can address these concerns by providing explainable actions in\ngeneral security contexts. Researchers have explored LLM agents for ACD but\nhave not evaluated them on multi-agent scenarios or interacting with other ACD\nagents. In this paper, we show the first study on how LLMs perform in\nmulti-agent ACD environments by proposing a new integration to the CybORG CAGE\n4 environment. We examine how ACD teams of LLM and RL agents can interact by\nproposing a novel communication protocol. Our results highlight the strengths\nand weaknesses of LLMs and RL and help us identify promising research\ndirections to create, train, and deploy future teams of ACD agents.\n","authors":["Sebastián R. Castro","Roberto Campbell","Nancy Lau","Octavio Villalobos","Jiaqi Duan","Alvaro A. Cardenas"],"pdf_url":"https://arxiv.org/pdf/2505.04843v1.pdf","comment":"Presented at IEEE CAI Workshop on Adaptive Cyber Defense 2025.\n  Proceedings to appear"},{"id":"http://arxiv.org/abs/2505.04842v1","updated":"2025-05-07T22:41:26Z","published":"2025-05-07T22:41:26Z","title":"Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM\n  Reasoners With Verifiers","summary":"  Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners,\nsuch as GRPO or Leave-one-out PPO, abandon the learned value function in favor\nof empirically estimated returns. This hinders test-time compute scaling that\nrelies on using the value-function for verification. In this work, we propose\nRL$^V$ that augments any ``value-free'' RL method by jointly training the LLM\nas both a reasoner and a generative verifier using RL-generated data, adding\nverification capabilities without significant overhead. Empirically, RL$^V$\nboosts MATH accuracy by over 20\\% with parallel sampling and enables\n$8-32\\times$ efficient test-time compute scaling compared to the base RL\nmethod. RL$^V$ also exhibits strong generalization capabilities for both\neasy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves\n$1.2-1.6\\times$ higher performance when jointly scaling parallel and sequential\ntest-time compute with a long reasoning R1 model.\n","authors":["Kusha Sareen","Morgane M Moss","Alessandro Sordoni","Rishabh Agarwal","Arian Hosseini"],"pdf_url":"https://arxiv.org/pdf/2505.04842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05114v5","updated":"2025-05-07T22:38:57Z","published":"2023-12-08T15:42:28Z","title":"The Inadequacy of Similarity-based Privacy Metrics: Privacy Attacks\n  against \"Truly Anonymous\" Synthetic Datasets","summary":"  Generative models producing synthetic data are meant to provide a\nprivacy-friendly approach to releasing data. However, their privacy guarantees\nare only considered robust when models satisfy Differential Privacy (DP). Alas,\nthis is not a ubiquitous standard, as many leading companies (and, in fact,\nresearch papers) use ad-hoc privacy metrics based on testing the statistical\nsimilarity between synthetic and real data.\n  In this paper, we examine the privacy metrics used in real-world synthetic\ndata deployments and demonstrate their unreliability in several ways. First, we\nprovide counter-examples where severe privacy violations occur even if the\nprivacy tests pass and instantiate accurate membership and attribute inference\nattacks with minimal cost. We then introduce ReconSyn, a reconstruction attack\nthat generates multiple synthetic datasets that are considered private by the\nmetrics but actually leak information unique to individual records. We show\nthat ReconSyn recovers 78-100% of the outliers in the train data with only\nblack-box access to a single fitted generative model and the privacy metrics.\nIn the process, we show that applying DP only to the model does not mitigate\nthis attack, as using privacy metrics breaks the end-to-end DP pipeline.\n","authors":["Georgi Ganev","Emiliano De Cristofaro"],"pdf_url":"https://arxiv.org/pdf/2312.05114v5.pdf","comment":"Published in the Proceedings of the 46th IEEE Symposium on Security &\n  Privacy (IEEE S&P 2025). Please cite the S&P version"},{"id":"http://arxiv.org/abs/2505.04841v1","updated":"2025-05-07T22:37:07Z","published":"2025-05-07T22:37:07Z","title":"Quantum-Inspired Optimization Process for Data Imputation","summary":"  Data imputation is a critical step in data pre-processing, particularly for\ndatasets with missing or unreliable values. This study introduces a novel\nquantum-inspired imputation framework evaluated on the UCI Diabetes dataset,\nwhich contains biologically implausible missing values across several clinical\nfeatures. The method integrates Principal Component Analysis (PCA) with\nquantum-assisted rotations, optimized through gradient-free classical\noptimizers -COBYLA, Simulated Annealing, and Differential Evolution to\nreconstruct missing values while preserving statistical fidelity. Reconstructed\nvalues are constrained within +/-2 standard deviations of original feature\ndistributions, avoiding unrealistic clustering around central tendencies. This\napproach achieves a substantial and statistically significant improvement,\nincluding an average reduction of over 85% in Wasserstein distance and\nKolmogorov-Smirnov test p-values between 0.18 and 0.22, compared to p-values >\n0.99 in classical methods such as Mean, KNN, and MICE. The method also\neliminates zero-value artifacts and enhances the realism and variability of\nimputed data. By combining quantum-inspired transformations with a scalable\nclassical framework, this methodology provides a robust solution for imputation\ntasks in domains such as healthcare and AI pipelines, where data quality and\nintegrity are crucial.\n","authors":["Nishikanta Mohanty","Bikash K. Behera","Badsah Mukherjee","Christopher Ferrie"],"pdf_url":"https://arxiv.org/pdf/2505.04841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00046v2","updated":"2025-05-07T22:26:44Z","published":"2023-04-28T18:55:00Z","title":"An automated end-to-end deep learning-based framework for lung cancer\n  diagnosis by detecting and classifying the lung nodules","summary":"  Lung cancer is a leading cause of cancer-related deaths worldwide, and early\ndetection is crucial for improving patient outcomes. Nevertheless, early\ndiagnosis of cancer is a major challenge, particularly in low-resource settings\nwhere access to medical resources and trained radiologists is limited. The\nobjective of this study is to propose an automated end-to-end deep\nlearning-based framework for the early detection and classification of lung\nnodules, specifically for low-resource settings. The proposed framework\nconsists of three stages: lung segmentation using a modified 3D U-Net named 3D\nRes-U-Net, nodule detection using YOLO-v5, and classification with a Vision\nTransformer-based architecture. We evaluated the proposed framework on a\npublicly available dataset, LUNA16. The proposed framework's performance was\nmeasured using the respective domain's evaluation matrices. The proposed\nframework achieved a 98.82% lung segmentation dice score while detecting the\nlung nodule with 0.76 mAP@50 from the segmented lung, at a low false-positive\nrate. The performance of both networks of the proposed framework was compared\nwith other studies and found to outperform them regarding segmentation and\ndetection accuracy. Additionally, our proposed Vision transformer network\nobtained an accuracy of 93.57%, which is 1.21% higher than the state-of-the-art\nnetworks. Our proposed end-to-end deep learning-based framework can effectively\nsegment lungs, and detect and classify lung nodules, specifically in\nlow-resource settings with limited access to radiologists. The proposed\nframework outperforms existing studies regarding all the respective evaluation\nmetrics. The proposed framework can potentially improve the accuracy and\nefficiency of lung cancer screening in low-resource settings, ultimately\nleading to better patient outcomes.\n","authors":["Samiul Based Shuvo","Tasnia Binte Mamun"],"pdf_url":"https://arxiv.org/pdf/2305.00046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16021v2","updated":"2025-05-07T21:59:46Z","published":"2024-08-27T01:14:34Z","title":"XG-NID: Dual-Modality Network Intrusion Detection using a Heterogeneous\n  Graph Neural Network and Large Language Model","summary":"  In the rapidly evolving field of cybersecurity, the integration of flow-level\nand packet-level information for real-time intrusion detection remains a\nlargely untapped area of research. This paper introduces \"XG-NID,\" a novel\nframework that, to the best of our knowledge, is the first to fuse flow-level\nand packet-level data within a heterogeneous graph structure, offering a\ncomprehensive analysis of network traffic. Leveraging a heterogeneous graph\nneural network (GNN) with graph-level classification, XG-NID uniquely enables\nreal-time inference while effectively capturing the intricate relationships\nbetween flow and packet payload data. Unlike traditional GNN-based\nmethodologies that predominantly analyze historical data, XG-NID is designed to\naccommodate the heterogeneous nature of network traffic, providing a robust and\nreal-time defense mechanism. Our framework extends beyond mere classification;\nit integrates Large Language Models (LLMs) to generate detailed, human-readable\nexplanations and suggest potential remedial actions, ensuring that the insights\nproduced are both actionable and comprehensible. Additionally, we introduce a\nnew set of flow features based on temporal information, further enhancing the\ncontextual and explainable inferences provided by our model. To facilitate\npractical application and accessibility, we developed \"GNN4ID,\" an open-source\ntool that enables the extraction and transformation of raw network traffic into\nthe proposed heterogeneous graph structure, seamlessly integrating flow and\npacket-level data. Our comprehensive quantitative comparative analysis\ndemonstrates that XG-NID achieves an F1 score of 97\\% in multi-class\nclassification, outperforming existing baseline and state-of-the-art methods.\nThis sets a new standard in Network Intrusion Detection Systems by combining\ninnovative data fusion with enhanced interpretability and real-time\ncapabilities.\n","authors":["Yasir Ali Farrukh","Syed Wali","Irfan Khan","Nathaniel D. Bastian"],"pdf_url":"https://arxiv.org/pdf/2408.16021v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2505.04822v1","updated":"2025-05-07T21:50:27Z","published":"2025-05-07T21:50:27Z","title":"Is there Value in Reinforcement Learning?","summary":"  Action-values play a central role in popular Reinforcement Learing (RL)\nmodels of behavior. Yet, the idea that action-values are explicitly represented\nhas been extensively debated. Critics had therefore repeatedly suggested that\npolicy-gradient (PG) models should be favored over value-based (VB) ones, as a\npotential solution for this dilemma. Here we argue that this solution is\nunsatisfying. This is because PG methods are not, in fact, \"Value-free\" --\nwhile they do not rely on an explicit representation of Value for acting\n(stimulus-response mapping), they do require it for learning. Hence, switching\nto PG models is, per se, insufficient for eliminating Value from models of\nbehavior. More broadly, the requirement for a representation of Value stems\nfrom the underlying assumptions regarding the optimization objective posed by\nthe standard RL framework, not from the particular algorithm chosen to solve\nit. Previous studies mostly took these standard RL assumptions for granted, as\npart of their conceptualization or problem modeling, while debating the\ndifferent methods used to optimize it (i.e., PG or VB). We propose that,\ninstead, the focus of the debate should shift to critically evaluating the\nunderlying modeling assumptions. Such evaluation is particularly important from\nan experimental perspective. Indeed, the very notion of Value must be\nreconsidered when standard assumptions (e.g., risk neutrality,\nfull-observability, Markovian environment, exponential discounting) are\nrelaxed, as is likely in natural settings. Finally, we use the Value debate as\na case study to argue in favor of a more nuanced, algorithmic rather than\nstatistical, view of what constitutes \"a model\" in cognitive sciences. Our\nanalysis suggests that besides \"parametric\" statistical complexity, additional\naspects such as computational complexity must also be taken into account when\nevaluating model complexity.\n","authors":["Lior Fox","Yonatan Loewenstein"],"pdf_url":"https://arxiv.org/pdf/2505.04822v1.pdf","comment":"Accepted to The 6th Multidisciplinary Conference on Reinforcement\n  Learning and Decision Making (RLDM 2025)"}]},"2025-05-08T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2411.10941v3","updated":"2025-05-08T17:28:26Z","published":"2024-11-17T02:39:58Z","title":"Efficient Estimation of Relaxed Model Parameters for Robust UAV\n  Trajectory Optimization","summary":"  Online trajectory optimization and optimal control methods are crucial for\nenabling sustainable unmanned aerial vehicle (UAV) services, such as\nagriculture, environmental monitoring, and transportation, where available\nactuation and energy are limited. However, optimal controllers are highly\nsensitive to model mismatch, which can occur due to loaded equipment, packages\nto be delivered, or pre-existing variability in fundamental structural and\nthrust-related parameters. To circumvent this problem, optimal controllers can\nbe paired with parameter estimators to improve their trajectory planning\nperformance and perform adaptive control. However, UAV platforms are limited in\nterms of onboard processing power, oftentimes making nonlinear parameter\nestimation too computationally expensive to consider. To address these issues,\nwe propose a relaxed, affine-in-parameters multirotor model along with an\nefficient optimal parameter estimator. We convexify the nominal Moving Horizon\nParameter Estimation (MHPE) problem into a linear-quadratic form (LQ-MHPE) via\nan affine-in-parameter relaxation on the nonlinear dynamics, resulting in fast\nquadratic programs (QPs) that facilitate adaptive Model Predictve Control (MPC)\nin real time. We compare this approach to the equivalent nonlinear estimator in\nMonte Carlo simulations, demonstrating a decrease in average solve time and\ntrajectory optimality cost by 98.2% and 23.9-56.2%, respectively.\n","authors":["Derek Fan","David A. Copp"],"pdf_url":"https://arxiv.org/pdf/2411.10941v3.pdf","comment":"8 pages, 5 figures, to published in IEEE Sustech 2025"},{"id":"http://arxiv.org/abs/2503.01876v2","updated":"2025-05-08T17:10:57Z","published":"2025-02-26T15:12:29Z","title":"Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion\n  Models","summary":"  Human-in-the-loop (HitL) robot deployment has gained significant attention in\nboth academia and industry as a semi-autonomous paradigm that enables human\noperators to intervene and adjust robot behaviors at deployment time, improving\nsuccess rates. However, continuous human monitoring and intervention can be\nhighly labor-intensive and impractical when deploying a large number of robots.\nTo address this limitation, we propose a method that allows diffusion policies\nto actively seek human assistance only when necessary, reducing reliance on\nconstant human oversight. To achieve this, we leverage the generative process\nof diffusion policies to compute an uncertainty-based metric based on which the\nautonomous agent can decide to request operator assistance at deployment time,\nwithout requiring any operator interaction during training. Additionally, we\nshow that the same method can be used for efficient data collection for\nfine-tuning diffusion policies in order to improve their autonomous\nperformance. Experimental results from simulated and real-world environments\ndemonstrate that our approach enhances policy performance during deployment for\na variety of scenarios.\n","authors":["Zhanpeng He","Yifeng Cao","Matei Ciocarlie"],"pdf_url":"https://arxiv.org/pdf/2503.01876v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19448v2","updated":"2025-05-08T17:06:08Z","published":"2025-04-28T03:25:32Z","title":"An End-to-End Framework for Optimizing Foot Trajectory and Force in Dry\n  Adhesion Legged Wall-Climbing Robots","summary":"  Foot trajectory planning for dry adhesion legged climbing robots presents\nchallenges, as the phases of foot detachment, swing, and adhesion significantly\ninfluence the adhesion and detachment forces essential for stable climbing. To\ntackle this, an end-to-end foot trajectory and force optimization framework\n(FTFOF) is proposed, which optimizes foot adhesion and detachment forces\nthrough trajectory adjustments. This framework accepts general foot trajectory\nconstraints and user-defined parameters as input, ultimately producing an\noptimal single foot trajectory. It integrates three-segment $C^2$ continuous\nBezier curves, tailored to various foot structures, enabling the generation of\neffective climbing trajectories. A dilate-based GRU predictive model\nestablishes the relationship between foot trajectories and the corresponding\nfoot forces. Multi-objective optimization algorithms, combined with a\nredundancy hierarchical strategy, identify the most suitable foot trajectory\nfor specific tasks, thereby ensuring optimal performance across detachment\nforce, adhesion force and vibration amplitude. Experimental validation on the\nquadruped climbing robot MST-M3F showed that, compared to commonly used\ntrajectories in existing legged climbing robots, the proposed framework\nachieved reductions in maximum detachment force by 28 \\%, vibration amplitude\nby 82 \\%, which ensures the stable climbing of dry adhesion legged climbing\nrobots.\n","authors":["Jichun Xiao","Jiawei Nie","Lina Hao","Zhi Li"],"pdf_url":"https://arxiv.org/pdf/2504.19448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06039v3","updated":"2025-05-08T15:54:41Z","published":"2023-12-10T23:36:37Z","title":"Fast Whole-Body Strain Regulation in Continuum Robots","summary":"  We propose reaching steps towards the real-time strain control of\nmultiphysics, multiscale continuum soft robots. To study this problem\nfundamentally, we ground ourselves in a model-based control setting enabled by\nmathematically precise dynamics of a soft robot prototype. Poised to integrate,\nrather than reject, inherent mechanical nonlinearities for embodied compliance,\nwe first separate the original robot dynamics into separate subdynamics --\naided by a perturbing time-scale separation parameter. Second, we prescribe a\nset of stabilizing nonlinear backstepping controllers for regulating the\nresulting subsystems' strain dynamics. Third, we study the interconnected\nsingularly perturbed system by analyzing and establishing its stability.\nFourth, our theories are backed up by fast numerical results on a single arm of\nthe Octopus robot arm. We demonstrate strain regulation to equilibrium, in a\nsignificantly reduced time, of the whole-body reduced-order dynamics of an\ninfinite degrees-of-freedom soft robot. This paper communicates our thinking\nwithin the backdrop of embodied intelligence: it informs our conceptualization,\nformulation, computational setup, and yields improved control performance for\ninfinite degrees-of-freedom soft robots.\n","authors":["Lekan Molu"],"pdf_url":"https://arxiv.org/pdf/2312.06039v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05360v1","updated":"2025-05-08T15:53:34Z","published":"2025-05-08T15:53:34Z","title":"DSDrive: Distilling Large Language Model for Lightweight End-to-End\n  Autonomous Driving with Unified Reasoning and Planning","summary":"  We present DSDrive, a streamlined end-to-end paradigm tailored for\nintegrating the reasoning and planning of autonomous vehicles into a unified\nframework. DSDrive leverages a compact LLM that employs a distillation method\nto preserve the enhanced reasoning capabilities of a larger-sized vision\nlanguage model (VLM). To effectively align the reasoning and planning tasks, a\nwaypoint-driven dual-head coordination module is further developed, which\nsynchronizes dataset structures, optimization objectives, and the learning\nprocess. By integrating these tasks into a unified framework, DSDrive anchors\non the planning results while incorporating detailed reasoning insights,\nthereby enhancing the interpretability and reliability of the end-to-end\npipeline. DSDrive has been thoroughly tested in closed-loop simulations, where\nit performs on par with benchmark models and even outperforms in many key\nmetrics, all while being more compact in size. Additionally, the computational\nefficiency of DSDrive (as reflected in its time and memory requirements during\ninference) has been significantly enhanced. Evidently thus, this work brings\npromising aspects and underscores the potential of lightweight systems in\ndelivering interpretable and efficient solutions for AD.\n","authors":["Wenru Liu","Pei Liu","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2505.05360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05318v1","updated":"2025-05-08T15:02:49Z","published":"2025-05-08T15:02:49Z","title":"Mapping User Trust in Vision Language Models: Research Landscape,\n  Challenges, and Prospects","summary":"  The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies.\n","authors":["Agnese Chiatti","Sara Bernardini","Lara Shibelski Godoy Piccolo","Viola Schiaffonati","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2505.05318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05317v1","updated":"2025-05-08T15:02:35Z","published":"2025-05-08T15:02:35Z","title":"CottonSim: Development of an autonomous visual-guided robotic\n  cotton-picking system in the Gazebo","summary":"  In this study, an autonomous visual-guided robotic cotton-picking system,\nbuilt on a Clearpath's Husky robot platform and the Cotton-Eye perception\nsystem, was developed in the Gazebo robotic simulator. Furthermore, a virtual\ncotton farm was designed and developed as a Robot Operating System (ROS 1)\npackage to deploy the robotic cotton picker in the Gazebo environment for\nsimulating autonomous field navigation. The navigation was assisted by the map\ncoordinates and an RGB-depth camera, while the ROS navigation algorithm\nutilized a trained YOLOv8n-seg model for instance segmentation. The model\nachieved a desired mean Average Precision (mAP) of 85.2%, a recall of 88.9%,\nand a precision of 93.0% for scene segmentation. The developed ROS navigation\npackages enabled our robotic cotton-picking system to autonomously navigate\nthrough the cotton field using map-based and GPS-based approaches, visually\naided by a deep learning-based perception system. The GPS-based navigation\napproach achieved a 100% completion rate (CR) with a threshold of 5 x 10^-6\ndegrees, while the map-based navigation approach attained a 96.7% CR with a\nthreshold of 0.25 m. This study establishes a fundamental baseline of\nsimulation for future agricultural robotics and autonomous vehicles in cotton\nfarming and beyond. CottonSim code and data are released to the research\ncommunity via GitHub: https://github.com/imtheva/CottonSim\n","authors":["Thevathayarajh Thayananthan","Xin Zhang","Yanbo Huang","Jingdao Chen","Nuwan K. Wijewardane","Vitor S. Martins","Gary D. Chesser","Christopher T. Goodin"],"pdf_url":"https://arxiv.org/pdf/2505.05317v1.pdf","comment":"45 pages, 15 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.09180v2","updated":"2025-05-08T15:01:05Z","published":"2025-02-13T11:15:37Z","title":"A Machine Learning Approach to Sensor Substitution from Tactile Sensing\n  to Visual Perception for Non-Prehensile Manipulation","summary":"  Mobile manipulators are increasingly deployed in complex environments,\nrequiring diverse sensors to perceive and interact with their surroundings.\nHowever, equipping every robot with every possible sensor is often impractical\ndue to cost and physical constraints. A critical challenge arises when robots\nwith differing sensor capabilities need to collaborate or perform similar\ntasks. For example, consider a scenario where a mobile manipulator equipped\nwith high-resolution tactile skin is skilled at non-prehensile manipulation\ntasks like pushing. If this robot needs to be replaced or augmented by a robot\nlacking such tactile sensing, the learned manipulation policies become\ninapplicable. This paper addresses the problem of sensor substitution in\nnon-prehensile manipulation. We propose a novel machine learning-based\nframework that enables a robot with a limited sensor set (e.g., LiDAR or RGB-D)\nto effectively perform tasks previously reliant on a richer sensor suite (e.g.,\ntactile skin). Our approach learns a mapping between the available sensor data\nand the information provided by the substituted sensor, effectively\nsynthesizing the missing sensory input. Specifically, we demonstrate the\nefficacy of our framework by training a model to substitute tactile skin data\nfor the task of non-prehensile pushing using a mobile manipulator. We show that\na manipulator equipped only with LiDAR or RGB-D can, after training, achieve\ncomparable and sometimes even better pushing performance to a mobile base\nutilizing direct tactile feedback.\n","authors":["Idil Ozdamar","Doganay Sirintuna","Arash Ajoudani"],"pdf_url":"https://arxiv.org/pdf/2502.09180v2.pdf","comment":"10 pages, 6 figures, submitted to IEEE Sensors Journal, for\n  associated video, see https://youtu.be/6yIRcfn2DsY"},{"id":"http://arxiv.org/abs/2505.05314v1","updated":"2025-05-08T15:01:01Z","published":"2025-05-08T15:01:01Z","title":"Localization and path following for an autonomous e-scooter","summary":"  In order to mitigate economical, ecological, and societal challenges in\nelectric scooter (e-scooter) sharing systems, we develop an autonomous\ne-scooter prototype. Our vision is to design a fully autonomous prototype that\ncan find its way to the next parking spot, high-demand area, or charging\nstation. In this work, we propose a path following solution to enable\nlocalization and navigation in an urban environment with a provided path to\nfollow. We design a closed-loop architecture that solves the localization and\npath following problem while allowing the e-scooter to maintain its balance\nwith a previously developed reaction wheel mechanism. Our approach facilitates\nstate and input constraints, e.g., adhering to the path width, while remaining\nexecutable on a Raspberry Pi 5. We demonstrate the efficacy of our approach in\na real-world experiment on our prototype.\n","authors":["David Meister","Robin Strässer","Felix Brändle","Marc Seidel","Benno Bassler","Nathan Gerber","Jan Kautz","Elena Rommel","Frank Allgöwer"],"pdf_url":"https://arxiv.org/pdf/2505.05314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05288v1","updated":"2025-05-08T14:29:11Z","published":"2025-05-08T14:29:11Z","title":"PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes","summary":"  We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.\n","authors":["Ahmed Abdelreheem","Filippo Aleotti","Jamie Watson","Zawar Qureshi","Abdelrahman Eldesokey","Peter Wonka","Gabriel Brostow","Sara Vicente","Guillermo Garcia-Hernando"],"pdf_url":"https://arxiv.org/pdf/2505.05288v1.pdf","comment":"Tech report. Project page: https://nianticlabs.github.io/placeit3d/"},{"id":"http://arxiv.org/abs/2505.05287v1","updated":"2025-05-08T14:29:00Z","published":"2025-05-08T14:29:00Z","title":"Morphologically Symmetric Reinforcement Learning for Ambidextrous\n  Bimanual Manipulation","summary":"  Humans naturally exhibit bilateral symmetry in their gross manipulation\nskills, effortlessly mirroring simple actions between left and right hands.\nBimanual robots-which also feature bilateral symmetry-should similarly exploit\nthis property to perform tasks with either hand. Unlike humans, who often favor\na dominant hand for fine dexterous skills, robots should ideally execute\nambidextrous manipulation with equal proficiency. To this end, we introduce\nSYMDEX (SYMmetric DEXterity), a reinforcement learning framework for\nambidextrous bi-manipulation that leverages the robot's inherent bilateral\nsymmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation\ntasks into per-hand subtasks and trains dedicated policies for each. By\nexploiting bilateral symmetry via equivariant neural networks, experience from\none arm is inherently leveraged by the opposite arm. We then distill the\nsubtask policies into a global ambidextrous policy that is independent of the\nhand-task assignment. We evaluate SYMDEX on six challenging simulated\nmanipulation tasks and demonstrate successful real-world deployment on two of\nthem. Our approach strongly outperforms baselines on complex task in which the\nleft and right hands perform different roles. We further demonstrate SYMDEX's\nscalability by extending it to a four-arm manipulation setup, where our\nsymmetry-aware policies enable effective multi-arm collaboration and\ncoordination. Our results highlight how structural symmetry as inductive bias\nin policy learning enhances sample efficiency, robustness, and generalization\nacross diverse dexterous manipulation tasks.\n","authors":["Zechu Li","Yufeng Jin","Daniel Ordonez Apraez","Claudio Semini","Puze Liu","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2505.05287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03694v2","updated":"2025-05-08T14:12:19Z","published":"2025-05-06T16:59:54Z","title":"Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and\n  Avoid","summary":"  Assured safe-separation is essential for achieving seamless high-density\noperation of airborne vehicles in a shared airspace. To equip\nresource-constrained aerial systems with this safety-critical capability, we\npresent ViSafe, a high-speed vision-only airborne collision avoidance system.\nViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by\ntightly integrating a learning-based edge-AI framework with a custom\nmulti-camera hardware prototype designed under SWaP-C constraints. By\nleveraging perceptual input-focused control barrier functions (CBF) to design,\nencode, and enforce safety thresholds, ViSafe can provide provably safe runtime\nguarantees for self-separation in high-speed aerial operations. We evaluate\nViSafe's performance through an extensive test campaign involving both\nsimulated digital twins and real-world flight scenarios. By independently\nvarying agent types, closure rates, interaction geometries, and environmental\nconditions (e.g., weather and lighting), we demonstrate that ViSafe\nconsistently ensures self-separation across diverse scenarios. In\nfirst-of-its-kind real-world high-speed collision avoidance tests with closure\nrates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous\ncollision avoidance, establishing a new standard for safety in high-speed\naerial navigation.\n","authors":["Parv Kapoor","Ian Higgins","Nikhil Keetha","Jay Patrikar","Brady Moon","Zelin Ye","Yao He","Ivan Cisneros","Yaoyu Hu","Changliu Liu","Eunsuk Kang","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2505.03694v2.pdf","comment":"13 pages, RSS 2025 Demo track, https://theairlab.org/visafe/"},{"id":"http://arxiv.org/abs/2503.03125v3","updated":"2025-05-08T14:00:02Z","published":"2025-03-05T02:43:52Z","title":"Don't Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous\n  Driving","summary":"  End-to-end autonomous driving frameworks enable seamless integration of\nperception and planning but often rely on one-shot trajectory prediction, which\nmay lead to unstable control and vulnerability to occlusions in single-frame\nperception. To address this, we propose the Momentum-Aware Driving (MomAD)\nframework, which introduces trajectory momentum and perception momentum to\nstabilize and refine trajectory predictions. MomAD comprises two core\ncomponents: (1) Topological Trajectory Matching (TTM) employs Hausdorff\nDistance to select the optimal planning query that aligns with prior paths to\nensure coherence;(2) Momentum Planning Interactor (MPI) cross-attends the\nselected planning query with historical queries to expand static and dynamic\nperception files. This enriched query, in turn, helps regenerate long-horizon\ntrajectory and reduce collision risks. To mitigate noise arising from dynamic\nenvironments and detection errors, we introduce robust instance denoising\nduring training, enabling the planning model to focus on critical signals and\nimprove its robustness. We also propose a novel Trajectory Prediction\nConsistency (TPC) metric to quantitatively assess planning stability.\nExperiments on the nuScenes dataset demonstrate that MomAD achieves superior\nlong-term consistency (>=3s) compared to SOTA methods. Moreover, evaluations on\nthe curated Turning-nuScenes shows that MomAD reduces the collision rate by 26%\nand improves TPC by 0.97m (33.45%) over a 6s prediction horizon, while\nclosedloop on Bench2Drive demonstrates an up to 16.3% improvement in success\nrate.\n","authors":["Ziying Song","Caiyan Jia","Lin Liu","Hongyu Pan","Yongchang Zhang","Junming Wang","Xingyu Zhang","Shaoqing Xu","Lei Yang","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2503.03125v3.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.05223v1","updated":"2025-05-08T13:16:37Z","published":"2025-05-08T13:16:37Z","title":"Multi-Objective Reinforcement Learning for Adaptive Personalized\n  Autonomous Driving","summary":"  Human drivers exhibit individual preferences regarding driving style.\nAdapting autonomous vehicles to these preferences is essential for user trust\nand satisfaction. However, existing end-to-end driving approaches often rely on\npredefined driving styles or require continuous user feedback for adaptation,\nlimiting their ability to support dynamic, context-dependent preferences. We\npropose a novel approach using multi-objective reinforcement learning (MORL)\nwith preference-driven optimization for end-to-end autonomous driving that\nenables runtime adaptation to driving style preferences. Preferences are\nencoded as continuous weight vectors to modulate behavior along interpretable\nstyle objectives$\\unicode{x2013}$including efficiency, comfort, speed, and\naggressiveness$\\unicode{x2013}$without requiring policy retraining. Our\nsingle-policy agent integrates vision-based perception in complex mixed-traffic\nscenarios and is evaluated in diverse urban environments using the CARLA\nsimulator. Experimental results demonstrate that the agent dynamically adapts\nits driving behavior according to changing preferences while maintaining\nperformance in terms of collision avoidance and route completion.\n","authors":["Hendrik Surmann","Jorge de Heuvel","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2505.05223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00831v3","updated":"2025-05-08T13:12:24Z","published":"2025-05-01T19:44:36Z","title":"SmallPlan: Leverage Small Language Models for Sequential Path Planning\n  with Simulation-Powered, LLM-Guided Distillation","summary":"  Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics.\n","authors":["Quang P. M. Pham","Khoi T. N. Nguyen","Nhi H. Doan","Cuong A. Pham","Kentaro Inui","Dezhen Song"],"pdf_url":"https://arxiv.org/pdf/2505.00831v3.pdf","comment":"Paper is under review"},{"id":"http://arxiv.org/abs/2504.09609v2","updated":"2025-05-08T12:44:32Z","published":"2025-04-13T14:57:11Z","title":"A highly maneuverable flying squirrel drone with agility-improving\n  foldable wings","summary":"  Drones, like most airborne aerial vehicles, face inherent disadvantages in\nachieving agile flight due to their limited thrust capabilities. These physical\nconstraints cannot be fully addressed through advancements in control\nalgorithms alone. Drawing inspiration from the winged flying squirrel, this\npaper proposes a highly maneuverable drone equipped with agility-enhancing\nfoldable wings. By leveraging collaborative control between the conventional\npropeller system and the foldable wings-coordinated through the Thrust-Wing\nCoordination Control (TWCC) framework-the controllable acceleration set is\nexpanded, enabling the generation of abrupt vertical forces that are\nunachievable with traditional wingless drones. The complex aerodynamics of the\nfoldable wings are modeled using a physics-assisted recurrent neural network\n(paRNN), which calibrates the angle of attack (AOA) to align with the real\naerodynamic behavior of the wings. The additional air resistance generated by\nappropriately deploying these wings significantly improves the tracking\nperformance of the proposed \"flying squirrel\" drone. The model is trained on\nreal flight data and incorporates flat-plate aerodynamic principles.\nExperimental results demonstrate that the proposed flying squirrel drone\nachieves a 13.1% improvement in tracking performance, as measured by root mean\nsquare error (RMSE), compared to a conventional wingless drone. A demonstration\nvideo is available on YouTube: https://youtu.be/O8nrip18azY.\n","authors":["Dohyeon Lee","Jun-Gill Kang","Soohee Han"],"pdf_url":"https://arxiv.org/pdf/2504.09609v2.pdf","comment":"Accepted to IEEE Robotics and Automation Letters. Project Page :\n  https://jgkang1210.github.io/fsdrone_ral/ , Video :\n  https://www.youtube.com/watch?v=tckIF3KCJig , Dohyeon Lee and Jun-Gill Kang\n  are co-authors"},{"id":"http://arxiv.org/abs/2409.16111v3","updated":"2025-05-08T12:26:51Z","published":"2024-09-24T14:19:47Z","title":"CloudTrack: Scalable UAV Tracking with Cloud Semantics","summary":"  Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search and\nrescue scenarios to gather information in the search area. The automatic\nidentification of the person searched for in aerial footage could increase the\nautonomy of such systems, reduce the search time, and thus increase the missed\nperson's chances of survival. In this paper, we present a novel approach to\nperform semantically conditioned open vocabulary object tracking that is\nspecifically designed to cope with the limitations of UAV hardware. Our\napproach has several advantages. It can run with verbal descriptions of the\nmissing person, e.g., the color of the shirt, it does not require dedicated\ntraining to execute the mission and can efficiently track a potentially moving\nperson. Our experimental results demonstrate the versatility and efficacy of\nour approach.\n","authors":["Yannik Blei","Michael Krawez","Nisarga Nilavadi","Tanja Katharina Kaiser","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2409.16111v3.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.05157v1","updated":"2025-05-08T11:53:56Z","published":"2025-05-08T11:53:56Z","title":"Online Velocity Profile Generation and Tracking for Sampling-Based Local\n  Planning Algorithms in Autonomous Racing Environments","summary":"  This work presents an online velocity planner for autonomous racing that\nadapts to changing dynamic constraints, such as grip variations from tire\ntemperature changes and rubber accumulation. The method combines a\nforward-backward solver for online velocity optimization with a novel spatial\nsampling strategy for local trajectory planning, utilizing a three-dimensional\ntrack representation. The computed velocity profile serves as a reference for\nthe local planner, ensuring adaptability to environmental and vehicle dynamics.\nWe demonstrate the approach's robust performance and computational efficiency\nin racing scenarios and discuss its limitations, including sensitivity to\ndeviations from the predefined racing line and high jerk characteristics of the\nvelocity profile.\n","authors":["Alexander Langmann","Levent Ögretmen","Frederik Werner","Johannes Betz"],"pdf_url":"https://arxiv.org/pdf/2505.05157v1.pdf","comment":"8 Pages, accepted to be published at the the IEEE Intelligent\n  Vehicles Symposium (IV 2025), June 22-25 in Cluj, Romania"},{"id":"http://arxiv.org/abs/2505.05098v1","updated":"2025-05-08T09:52:55Z","published":"2025-05-08T09:52:55Z","title":"X-Driver: Explainable Autonomous Driving with Vision-Language Models","summary":"  End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving.\n","authors":["Wei Liu","Jiyuan Zhang","Binxiong Zheng","Yufeng Hu","Yingzhan Lin","Zengfeng Zeng"],"pdf_url":"https://arxiv.org/pdf/2505.05098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21699v2","updated":"2025-05-08T09:23:41Z","published":"2025-04-30T14:43:38Z","title":"REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud\n  De-raining","summary":"  Sensor degradation poses a significant challenge in autonomous driving.\nDuring heavy rainfall, the interference from raindrops can adversely affect the\nquality of LiDAR point clouds, resulting in, for instance, inaccurate point\nmeasurements. This, in turn, can potentially lead to safety concerns if\nautonomous driving systems are not weather-aware, i.e., if they are unable to\ndiscern such changes. In this study, we release a new, large-scale, multi-modal\nemulated rain dataset, REHEARSE-3D, to promote research advancements in 3D\npoint cloud de-raining. Distinct from the most relevant competitors, our\ndataset is unique in several respects. First, it is the largest point-wise\nannotated dataset, and second, it is the only one with high-resolution LiDAR\ndata (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and\nnighttime conditions in a controlled weather environment. Furthermore,\nREHEARSE-3D involves rain-characteristic information, which is of significant\nvalue not only for sensor noise modeling but also for analyzing the impact of\nweather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop\ndetection and removal in fused LiDAR and 4D Radar point clouds. Our\ncomprehensive study further evaluates the performance of various statistical\nand deep-learning models. Upon publication, the dataset and benchmark models\nwill be made publicly available at: https://sporsho.github.io/REHEARSE3D.\n","authors":["Abu Mohammed Raisuddin","Jesper Holmblad","Hamed Haghighi","Yuri Poledna","Maikol Funk Drechsler","Valentina Donzella","Eren Erdal Aksoy"],"pdf_url":"https://arxiv.org/pdf/2504.21699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05076v1","updated":"2025-05-08T09:16:01Z","published":"2025-05-08T09:16:01Z","title":"The City that Never Settles: Simulation-based LiDAR Dataset for\n  Long-Term Place Recognition Under Extreme Structural Changes","summary":"  Large-scale construction and demolition significantly challenge long-term\nplace recognition (PR) by drastically reshaping urban and suburban\nenvironments. Existing datasets predominantly reflect limited or indoor-focused\nchanges, failing to adequately represent extensive outdoor transformations. To\nbridge this gap, we introduce the City that Never Settles (CNS) dataset, a\nsimulation-based dataset created using the CARLA simulator, capturing major\nstructural changes-such as building construction and demolition-across diverse\nmaps and sequences. Additionally, we propose TCR_sym, a symmetric version of\nthe original TCR metric, enabling consistent measurement of structural changes\nirrespective of source-target ordering. Quantitative comparisons demonstrate\nthat CNS encompasses more extensive transformations than current real-world\nbenchmarks. Evaluations of state-of-the-art LiDAR-based PR methods on CNS\nreveal substantial performance degradation, underscoring the need for robust\nalgorithms capable of handling significant environmental changes. Our dataset\nis available at https://github.com/Hyunho111/CNS_dataset.\n","authors":["Hyunho Song","Dongjae Lee","Seunghun Oh","Minwoo Jung","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2505.05076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16634v2","updated":"2025-05-08T09:14:32Z","published":"2024-03-25T11:22:38Z","title":"Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for\n  Computations in Matlab","summary":"  Geometric algebra (GA) is a mathematical tool for geometric computing,\nproviding a framework that allows a unified and compact approach to geometric\nrelations which in other mathematical systems are typically described using\ndifferent more complicated elements. This fact has led to an increasing\nadoption of GA in applied mathematics and engineering problems. However, the\nscarcity of symbolic implementations of GA and its inherent complexity,\nrequiring a specific mathematical background, make it challenging and less\nintuitive for engineers to work with. This prevents wider adoption among more\napplied professionals. To address this challenge, this paper introduces SUGAR\n(Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox\ndesigned for Matlab and licensed under the MIT License. SUGAR facilitates the\ntranslation of GA concepts into Matlab and provides a collection of\nuser-friendly functions tailored for GA computations, including support for\nsymbolic operations. It supports both numeric and symbolic computations in\nhigh-dimensional GAs. Specifically tailored for applied mathematics and\nengineering applications, SUGAR has been meticulously engineered to represent\ngeometric elements and transformations within two and three-dimensional\nprojective and conformal geometric algebras, aligning with established\ncomputational methodologies in the literature. Furthermore, SUGAR efficiently\nhandles functions of multivectors, such as exponential, logarithmic,\nsinusoidal, and cosine functions, enhancing its applicability across various\nengineering domains, including robotics, control systems, and power\nelectronics. Finally, this work includes four distinct validation examples,\ndemonstrating SUGAR's capabilities across the above-mentioned fields and its\npractical utility in addressing real-world applied mathematics and engineering\nproblems.\n","authors":["Manel Velasco","Isiah Zaplana","Arnau Dória-Cerezo","Pau Martí"],"pdf_url":"https://arxiv.org/pdf/2403.16634v2.pdf","comment":"33 pages, 6 figures, journal paper accepted in ACM TOMS"},{"id":"http://arxiv.org/abs/2505.05074v1","updated":"2025-05-08T09:10:05Z","published":"2025-05-08T09:10:05Z","title":"Visual Affordances: Enabling Robots to Understand Object Functionality","summary":"  Human-robot interaction for assistive technologies relies on the prediction\nof affordances, which are the potential actions a robot can perform on objects.\nPredicting object affordances from visual perception is formulated differently\nfor tasks such as grasping detection, affordance classification, affordance\nsegmentation, and hand-object interaction synthesis. In this work, we highlight\nthe reproducibility issue in these redefinitions, making comparative benchmarks\nunfair and unreliable. To address this problem, we propose a unified\nformulation for visual affordance prediction, provide a comprehensive and\nsystematic review of previous works highlighting strengths and limitations of\nmethods and datasets, and analyse what challenges reproducibility. To favour\ntransparency, we introduce the Affordance Sheet, a document to detail the\nproposed solution, the datasets, and the validation. As the physical properties\nof an object influence the interaction with the robot, we present a generic\nframework that links visual affordance prediction to the physical world. Using\nthe weight of an object as an example for this framework, we discuss how\nestimating object mass can affect the affordance prediction. Our approach\nbridges the gap between affordance perception and robot actuation, and accounts\nfor the complete information about objects of interest and how the robot\ninteracts with them to accomplish its task.\n","authors":["Tommaso Apicella","Alessio Xompero","Andrea Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2505.05074v1.pdf","comment":"24 pages, 12 figures, 10 tables. Project website at\n  https://apicis.github.io/aff-survey/"},{"id":"http://arxiv.org/abs/2504.08603v2","updated":"2025-05-08T08:56:29Z","published":"2025-04-11T15:12:05Z","title":"FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot\n  Exploration in Any Environment","summary":"  Geometrically accurate and semantically expressive map representations have\nproven invaluable to facilitate robust and safe mobile robot navigation and\ntask planning. Nevertheless, real-time, open-vocabulary semantic understanding\nof large-scale unknown environments is still an open problem. In this paper we\npresent FindAnything, an open-world mapping and exploration framework that\nincorporates vision-language information into dense volumetric submaps. Thanks\nto the use of vision-language features, FindAnything bridges the gap between\npure geometric and open-vocabulary semantic information for a higher level of\nunderstanding while allowing to explore any environment without the help of any\nexternal source of ground-truth pose information. We represent the environment\nas a series of volumetric occupancy submaps, resulting in a robust and accurate\nmap representation that deforms upon pose updates when the underlying SLAM\nsystem corrects its drift, allowing for a locally consistent representation\nbetween submaps. Pixel-wise vision-language features are aggregated from\nefficient SAM (eSAM)-generated segments, which are in turn integrated into\nobject-centric volumetric submaps, providing a mapping from open-vocabulary\nqueries to 3D geometry that is scalable also in terms of memory usage. The\nopen-vocabulary map representation of FindAnything achieves state-of-the-art\nsemantic accuracy in closed-set evaluations on the Replica dataset. This level\nof scene understanding allows a robot to explore environments based on objects\nor areas of interest selected via natural language queries. Our system is the\nfirst of its kind to be deployed on resource-constrained devices, such as MAVs,\nleveraging vision-language information for real-world robotic tasks.\n","authors":["Sebastián Barbas Laina","Simon Boche","Sotiris Papatheodorou","Simon Schaefer","Jaehyung Jung","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2504.08603v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.12674v2","updated":"2025-05-08T07:48:59Z","published":"2025-02-18T09:25:37Z","title":"SATA: Safe and Adaptive Torque-Based Locomotion Policies Inspired by\n  Animal Learning","summary":"  Despite recent advances in learning-based controllers for legged robots,\ndeployments in human-centric environments remain limited by safety concerns.\nMost of these approaches use position-based control, where policies output\ntarget joint angles that must be processed by a low-level controller (e.g., PD\nor impedance controllers) to compute joint torques. Although impressive results\nhave been achieved in controlled real-world scenarios, these methods often\nstruggle with compliance and adaptability when encountering environments or\ndisturbances unseen during training, potentially resulting in extreme or unsafe\nbehaviors. Inspired by how animals achieve smooth and adaptive movements by\ncontrolling muscle extension and contraction, torque-based policies offer a\npromising alternative by enabling precise and direct control of the actuators\nin torque space. In principle, this approach facilitates more effective\ninteractions with the environment, resulting in safer and more adaptable\nbehaviors. However, challenges such as a highly nonlinear state space and\ninefficient exploration during training have hindered their broader adoption.\nTo address these limitations, we propose SATA, a bio-inspired framework that\nmimics key biomechanical principles and adaptive learning mechanisms observed\nin animal locomotion. Our approach effectively addresses the inherent\nchallenges of learning torque-based policies by significantly improving\nearly-stage exploration, leading to high-performance final policies.\nRemarkably, our method achieves zero-shot sim-to-real transfer. Our\nexperimental results indicate that SATA demonstrates remarkable compliance and\nsafety, even in challenging environments such as soft/slippery terrain or\nnarrow passages, and under significant external disturbances, highlighting its\npotential for practical deployments in human-centric and safety-critical\nscenarios.\n","authors":["Peizhuo Li","Hongyi Li","Ge Sun","Jin Cheng","Xinrong Yang","Guillaume Bellegarda","Milad Shafiee","Yuhong Cao","Auke Ijspeert","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2502.12674v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08777v4","updated":"2025-05-08T07:18:17Z","published":"2024-11-13T17:02:46Z","title":"LUDO: Low-Latency Understanding of Deformable Objects using Point Cloud\n  Occupancy Functions","summary":"  Accurately determining the shape of objects and the location of their\ninternal structures within deformable objects is crucial for medical tasks that\nrequire precise targeting, such as robotic biopsies. We introduce LUDO, a\nmethod for accurate low-latency understanding of deformable objects. LUDO\nreconstructs objects in their deformed state, including their internal\nstructures, from a single-view point cloud observation in under 30 ms using\noccupancy networks. LUDO provides uncertainty estimates for its predictions.\nAdditionally, it provides explainability by highlighting key features in its\ninput observations. Both uncertainty and explainability are important for\nsafety-critical applications such as surgical interventions. We demonstrate\nLUDO's abilities for autonomous targeting of internal regions of interest\n(ROIs) in deformable objects. We evaluate LUDO in real-world robotic\nexperiments, achieving a success rate of 98.9% for puncturing various ROIs\ninside deformable objects. LUDO demonstrates the potential to interact with\ndeformable objects without the need for deformable registration methods.\n","authors":["Pit Henrich","Franziska Mathis-Ullrich","Paul Maria Scheikl"],"pdf_url":"https://arxiv.org/pdf/2411.08777v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.03373v2","updated":"2025-05-08T07:12:20Z","published":"2025-04-04T11:44:24Z","title":"An Efficient GPU-based Implementation for Noise Robust Sound Source\n  Localization","summary":"  Robot audition, encompassing Sound Source Localization (SSL), Sound Source\nSeparation (SSS), and Automatic Speech Recognition (ASR), enables robots and\nsmart devices to acquire auditory capabilities similar to human hearing.\nDespite their wide applicability, processing multi-channel audio signals from\nmicrophone arrays in SSL involves computationally intensive matrix operations,\nwhich can hinder efficient deployment on Central Processing Units (CPUs),\nparticularly in embedded systems with limited CPU resources. This paper\nintroduces a GPU-based implementation of SSL for robot audition, utilizing the\nGeneralized Singular Value Decomposition-based Multiple Signal Classification\n(GSVD-MUSIC), a noise-robust algorithm, within the HARK platform, an\nopen-source software suite. For a 60-channel microphone array, the proposed\nimplementation achieves significant performance improvements. On the Jetson AGX\nOrin, an embedded device powered by an NVIDIA GPU and ARM Cortex-A78AE v8.2\n64-bit CPUs, we observe speedups of 5648.7x for GSVD calculations and 10.7x for\nthe SSL module, while speedups of 4245.1x for GSVD calculation and 17.3x for\nthe entire SSL module on a server configured with an NVIDIA A100 GPU and AMD\nEPYC 7352 CPUs, making real-time processing feasible for large-scale microphone\narrays and providing ample capacity for real-time processing of potential\nsubsequent machine learning or deep learning tasks.\n","authors":["Zirui Lin","Masayuki Takigahira","Naoya Terakado","Haris Gulzar","Monikka Roslianna Busto","Takeharu Eda","Katsutoshi Itoyama","Kazuhiro Nakadai","Hideharu Amano"],"pdf_url":"https://arxiv.org/pdf/2504.03373v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.08659v4","updated":"2025-05-08T07:11:30Z","published":"2025-02-09T06:33:47Z","title":"Deployment-friendly Lane-changing Intention Prediction Powered by\n  Brain-inspired Spiking Neural Networks","summary":"  Accurate and real-time prediction of surrounding vehicles' lane-changing\nintentions is a critical challenge in deploying safe and efficient autonomous\ndriving systems in open-world scenarios. Existing high-performing methods\nremain hard to deploy due to their high computational cost, long training\ntimes, and excessive memory requirements. Here, we propose an efficient\nlane-changing intention prediction approach based on brain-inspired Spiking\nNeural Networks (SNN). By leveraging the event-driven nature of SNN, the\nproposed approach enables us to encode the vehicle's states in a more efficient\nmanner. Comparison experiments conducted on HighD and NGSIM datasets\ndemonstrate that our method significantly improves training efficiency and\nreduces deployment costs while maintaining comparable prediction accuracy.\nParticularly, compared to the baseline, our approach reduces training time by\n75% and memory usage by 99.9%. These results validate the efficiency and\nreliability of our method in lane-changing predictions, highlighting its\npotential for safe and efficient autonomous driving systems while offering\nsignificant advantages in deployment, including reduced training time, lower\nmemory usage, and faster inference.\n","authors":["Shuqi Shen","Junjie Yang","Hui Zhong","Hongliang Lu","Xinhu Zheng","Hai Yang"],"pdf_url":"https://arxiv.org/pdf/2502.08659v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04999v1","updated":"2025-05-08T07:07:58Z","published":"2025-05-08T07:07:58Z","title":"CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled\n  Demonstrations","summary":"  Learning robot policies using imitation learning requires collecting large\namounts of costly action-labeled expert demonstrations, which fundamentally\nlimits the scale of training data. A promising approach to address this\nbottleneck is to harness the abundance of unlabeled observations-e.g., from\nvideo demonstrations-to learn latent action labels in an unsupervised way.\nHowever, we find that existing methods struggle when applied to complex robot\ntasks requiring fine-grained motions. We design continuous latent action models\n(CLAM) which incorporate two key ingredients we find necessary for learning to\nsolve complex continuous control tasks from unlabeled observation data: (a)\nusing continuous latent action labels instead of discrete representations, and\n(b) jointly training an action decoder to ensure that the latent action space\ncan be easily grounded to real actions with relatively few labeled examples.\nImportantly, the labeled examples can be collected from non-optimal play data,\nenabling CLAM to learn performant policies without access to any action-labeled\nexpert data. We demonstrate on continuous control benchmarks in DMControl\n(locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot\narm that CLAM significantly outperforms prior state-of-the-art methods,\nremarkably with a 2-3x improvement in task success rate compared to the best\nbaseline. Videos and code can be found at clamrobot.github.io.\n","authors":["Anthony Liang","Pavel Czempin","Matthew Hong","Yutai Zhou","Erdem Biyik","Stephen Tu"],"pdf_url":"https://arxiv.org/pdf/2505.04999v1.pdf","comment":"Latent Action Models, Self-supervised Pretraining, Learning from\n  Videos"},{"id":"http://arxiv.org/abs/2505.04989v1","updated":"2025-05-08T06:52:22Z","published":"2025-05-08T06:52:22Z","title":"CPP-DIP: Multi-objective Coverage Path Planning for MAVs in Dispersed\n  and Irregular Plantations","summary":"  Coverage Path Planning (CPP) is vital in precision agriculture to improve\nefficiency and resource utilization. In irregular and dispersed plantations,\ntraditional grid-based CPP often causes redundant coverage over non-vegetated\nareas, leading to waste and pollution. To overcome these limitations, we\npropose CPP-DIP, a multi-objective CPP framework designed for Micro Air\nVehicles (MAVs). The framework transforms the CPP task into a Traveling\nSalesman Problem (TSP) and optimizes flight paths by minimizing travel\ndistance, turning angles, and intersection counts. Unlike conventional\napproaches, our method does not rely on GPS-based environmental modeling.\nInstead, it uses aerial imagery and a Histogram of Oriented Gradients\n(HOG)-based approach to detect trees and extract image coordinates. A\ndensity-aware waypoint strategy is applied: Kernel Density Estimation (KDE) is\nused to reduce redundant waypoints in dense regions, while a greedy algorithm\nensures complete coverage in sparse areas. To verify the generality of the\nframework, we solve the resulting TSP using three different methods: Greedy\nHeuristic Insertion (GHI), Ant Colony Optimization (ACO), and Monte Carlo\nReinforcement Learning (MCRL). Then an object-based optimization is applied to\nfurther refine the resulting path. Additionally, CPP-DIP integrates ForaNav,\nour insect-inspired navigation method, for accurate tree localization and\ntracking. The experimental results show that MCRL offers a balanced solution,\nreducing the travel distance by 16.9 % compared to ACO while maintaining a\nsimilar performance to GHI. It also improves path smoothness by reducing\nturning angles by 28.3 % and 59.9 % relative to ACO and GHI, respectively, and\neffectively eliminates intersections. These results confirm the robustness and\neffectiveness of CPP-DIP in different TSP solvers.\n","authors":["Weijie Kuang","Hann Woei Ho","Ye Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.04989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04982v1","updated":"2025-05-08T06:39:47Z","published":"2025-05-08T06:39:47Z","title":"A Vehicle System for Navigating Among Vulnerable Road Users Including\n  Remote Operation","summary":"  We present a vehicle system capable of navigating safely and efficiently\naround Vulnerable Road Users (VRUs), such as pedestrians and cyclists. The\nsystem comprises key modules for environment perception, localization and\nmapping, motion planning, and control, integrated into a prototype vehicle. A\nkey innovation is a motion planner based on Topology-driven Model Predictive\nControl (T-MPC). The guidance layer generates multiple trajectories in\nparallel, each representing a distinct strategy for obstacle avoidance or\nnon-passing. The underlying trajectory optimization constrains the joint\nprobability of collision with VRUs under generic uncertainties. To address\nextraordinary situations (\"edge cases\") that go beyond the autonomous\ncapabilities - such as construction zones or encounters with emergency\nresponders - the system includes an option for remote human operation,\nsupported by visual and haptic guidance. In simulation, our motion planner\noutperforms three baseline approaches in terms of safety and efficiency. We\nalso demonstrate the full system in prototype vehicle tests on a closed track,\nboth in autonomous and remotely operated modes.\n","authors":["Oscar de Groot","Alberto Bertipaglia","Hidde Boekema","Vishrut Jain","Marcell Kegl","Varun Kotian","Ted Lentsch","Yancong Lin","Chrysovalanto Messiou","Emma Schippers","Farzam Tajdari","Shiming Wang","Zimin Xia","Mubariz Zaffar","Ronald Ensing","Mario Garzon","Javier Alonso-Mora","Holger Caesar","Laura Ferranti","Riender Happee","Julian F. P. Kooij","Georgios Papaioannou","Barys Shyrokau","Dariu M. Gavrila"],"pdf_url":"https://arxiv.org/pdf/2505.04982v1.pdf","comment":"Intelligent Vehicles Symposium 2025"},{"id":"http://arxiv.org/abs/2505.04980v1","updated":"2025-05-08T06:35:30Z","published":"2025-05-08T06:35:30Z","title":"LVLM-MPC Collaboration for Autonomous Driving: A Safety-Aware and\n  Task-Scalable Control Architecture","summary":"  This paper proposes a novel Large Vision-Language Model (LVLM) and Model\nPredictive Control (MPC) integration framework that delivers both task\nscalability and safety for Autonomous Driving (AD). LVLMs excel at high-level\ntask planning across diverse driving scenarios. However, since these foundation\nmodels are not specifically designed for driving and their reasoning is not\nconsistent with the feasibility of low-level motion planning, concerns remain\nregarding safety and smooth task switching. This paper integrates LVLMs with\nMPC Builder, which automatically generates MPCs on demand, based on symbolic\ntask commands generated by the LVLM, while ensuring optimality and safety. The\ngenerated MPCs can strongly assist the execution or rejection of LVLM-driven\ntask switching by providing feedback on the feasibility of the given tasks and\ngenerating task-switching-aware MPCs. Our approach provides a safe, flexible,\nand adaptable control framework, bridging the gap between cutting-edge\nfoundation models and reliable vehicle operation. We demonstrate the\neffectiveness of our approach through a simulation experiment, showing that our\nsystem can safely and effectively handle highway driving while maintaining the\nflexibility and adaptability of LVLMs.\n","authors":["Kazuki Atsuta","Kohei Honda","Hiroyuki Okuda","Tatsuya Suzuki"],"pdf_url":"https://arxiv.org/pdf/2505.04980v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.04978v1","updated":"2025-05-08T06:31:19Z","published":"2025-05-08T06:31:19Z","title":"Robust Model-Based In-Hand Manipulation with Integrated Real-Time\n  Motion-Contact Planning and Tracking","summary":"  Robotic dexterous in-hand manipulation, where multiple fingers dynamically\nmake and break contact, represents a step toward human-like dexterity in\nreal-world robotic applications. Unlike learning-based approaches that rely on\nlarge-scale training or extensive data collection for each specific task,\nmodel-based methods offer an efficient alternative. Their online computing\nnature allows for ready application to new tasks without extensive retraining.\nHowever, due to the complexity of physical contacts, existing model-based\nmethods encounter challenges in efficient online planning and handling modeling\nerrors, which limit their practical applications. To advance the effectiveness\nand robustness of model-based contact-rich in-hand manipulation, this paper\nproposes a novel integrated framework that mitigates these limitations. The\nintegration involves two key aspects: 1) integrated real-time planning and\ntracking achieved by a hierarchical structure; and 2) joint optimization of\nmotions and contacts achieved by integrated motion-contact modeling.\nSpecifically, at the high level, finger motion and contact force references are\njointly generated using contact-implicit model predictive control. The\nhigh-level module facilitates real-time planning and disturbance recovery. At\nthe low level, these integrated references are concurrently tracked using a\nhand force-motion model and actual tactile feedback. The low-level module\ncompensates for modeling errors and enhances the robustness of manipulation.\nExtensive experiments demonstrate that our approach outperforms existing\nmodel-based methods in terms of accuracy, robustness, and real-time\nperformance. Our method successfully completes five challenging tasks in\nreal-world environments, even under appreciable external disturbances.\n","authors":["Yongpeng Jiang","Mingrui Yu","Xinghao Zhu","Masayoshi Tomizuka","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2505.04978v1.pdf","comment":"Submitted to the International Journal of Robotics Research (IJRR)"},{"id":"http://arxiv.org/abs/2505.04972v1","updated":"2025-05-08T06:16:36Z","published":"2025-05-08T06:16:36Z","title":"AI and Vision based Autonomous Navigation of Nano-Drones in\n  Partially-Known Environments","summary":"  The miniaturisation of sensors and processors, the advancements in connected\nedge intelligence, and the exponential interest in Artificial Intelligence are\nboosting the affirmation of autonomous nano-size drones in the Internet of\nRobotic Things ecosystem. However, achieving safe autonomous navigation and\nhigh-level tasks such as exploration and surveillance with these tiny platforms\nis extremely challenging due to their limited resources. This work focuses on\nenabling the safe and autonomous flight of a pocket-size, 30-gram platform\ncalled Crazyflie 2.1 in a partially known environment. We propose a novel\nAI-aided, vision-based reactive planning method for obstacle avoidance under\nthe ambit of Integrated Sensing, Computing and Communication paradigm. We deal\nwith the constraints of the nano-drone by splitting the navigation task into\ntwo parts: a deep learning-based object detector runs on the edge (external\nhardware) while the planning algorithm is executed onboard. The results show\nthe ability to command the drone at $\\sim8$ frames-per-second and a model\nperformance reaching a COCO mean-average-precision of $60.8$. Field experiments\ndemonstrate the feasibility of the solution with the drone flying at a top\nspeed of $1$ m/s while steering away from an obstacle placed in an unknown\nposition and reaching the target destination. The outcome highlights the\ncompatibility of the communication delay and the model performance with the\nrequirements of the real-time navigation task. We provide a feasible\nalternative to a fully onboard implementation that can be extended to\nautonomous exploration with nano-drones.\n","authors":["Mattia Sartori","Chetna Singhal","Neelabhro Roy","Davide Brunelli","James Gross"],"pdf_url":"https://arxiv.org/pdf/2505.04972v1.pdf","comment":"in DCOSS-IoT 2025, Wi-DroIT 2025"},{"id":"http://arxiv.org/abs/2505.04962v1","updated":"2025-05-08T05:43:31Z","published":"2025-05-08T05:43:31Z","title":"An Efficient Method for Accurate Pose Estimation and Error Correction of\n  Cuboidal Objects","summary":"  The proposed system outlined in this paper is a solution to a use case that\nrequires the autonomous picking of cuboidal objects from an organized or\nunorganized pile with high precision. This paper presents an efficient method\nfor precise pose estimation of cuboid-shaped objects, which aims to reduce\nerrors in target pose in a time-efficient manner. Typical pose estimation\nmethods like global point cloud registrations are prone to minor pose errors\nfor which local registration algorithms are generally used to improve pose\naccuracy. However, due to the execution time overhead and uncertainty in the\nerror of the final achieved pose, an alternate, linear time approach is\nproposed for pose error estimation and correction. This paper presents an\noverview of the solution followed by a detailed description of individual\nmodules of the proposed algorithm.\n","authors":["Utsav Rai","Hardik Mehta","Vismay Vakharia","Aditya Choudhary","Amit Parmar","Rolif Lima","Kaushik Das"],"pdf_url":"https://arxiv.org/pdf/2505.04962v1.pdf","comment":"Accepted in IEEE/RSJ IROS 2022 Workshop on Mobile Manipulation and\n  Embodied Intelligence (MOMA)"},{"id":"http://arxiv.org/abs/2505.04961v1","updated":"2025-05-08T05:42:33Z","published":"2025-05-08T05:42:33Z","title":"ADD: Physics-Based Motion Imitation with Adversarial Differential\n  Discriminators","summary":"  Multi-objective optimization problems, which require the simultaneous\noptimization of multiple terms, are prevalent across numerous applications.\nExisting multi-objective optimization methods often rely on manually tuned\naggregation functions to formulate a joint optimization target. The performance\nof such hand-tuned methods is heavily dependent on careful weight selection, a\ntime-consuming and laborious process. These limitations also arise in the\nsetting of reinforcement-learning-based motion tracking for physically\nsimulated characters, where intricately crafted reward functions are typically\nused to achieve high-fidelity results. Such solutions not only require domain\nexpertise and significant manual adjustment, but also limit the applicability\nof the resulting reward function across diverse skills. To bridge this gap, we\npresent a novel adversarial multi-objective optimization technique that is\nbroadly applicable to a range of multi-objective optimization problems,\nincluding motion tracking. The proposed adversarial differential discriminator\nreceives a single positive sample, yet is still effective at guiding the\noptimization process. We demonstrate that our technique can enable characters\nto closely replicate a variety of acrobatic and agile behaviors, achieving\ncomparable quality to state-of-the-art motion-tracking methods, without relying\non manually tuned reward functions. Results are best visualized through\nhttps://youtu.be/rz8BYCE9E2w.\n","authors":["Ziyu Zhang","Sergey Bashkirov","Dun Yang","Michael Taylor","Xue Bin Peng"],"pdf_url":"https://arxiv.org/pdf/2505.04961v1.pdf","comment":"19 pages, 15 figures"},{"id":"http://arxiv.org/abs/2409.03757v3","updated":"2025-05-08T05:10:27Z","published":"2024-09-05T17:59:56Z","title":"Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding","summary":"  Complex 3D scene understanding has gained increasing attention, with scene\nencoding strategies playing a crucial role in this success. However, the\noptimal scene encoding strategies for various scenarios remain unclear,\nparticularly compared to their image-based counterparts. To address this issue,\nwe present a comprehensive study that probes various visual encoding models for\n3D scene understanding, identifying the strengths and limitations of each model\nacross different scenarios. Our evaluation spans seven vision foundation\nencoders, including image-based, video-based, and 3D foundation models. We\nevaluate these models in four tasks: Vision-Language Scene Reasoning, Visual\nGrounding, Segmentation, and Registration, each focusing on different aspects\nof scene understanding. Our evaluations yield key findings: DINOv2 demonstrates\nsuperior performance, video models excel in object-level tasks, diffusion\nmodels benefit geometric tasks, and language-pretrained models show unexpected\nlimitations in language-related tasks. These insights challenge some\nconventional understandings, provide novel perspectives on leveraging visual\nfoundation models, and highlight the need for more flexible encoder selection\nin future vision-language and scene-understanding tasks. Code:\nhttps://github.com/YunzeMan/Lexicon3D\n","authors":["Yunze Man","Shuhong Zheng","Zhipeng Bao","Martial Hebert","Liang-Yan Gui","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2409.03757v3.pdf","comment":"NeurIPS 2024. Project page: https://yunzeman.github.io/lexicon3d\n  Github: https://github.com/YunzeMan/Lexicon3D"},{"id":"http://arxiv.org/abs/2409.14719v2","updated":"2025-05-08T04:54:31Z","published":"2024-09-23T05:33:35Z","title":"DiSPo: Diffusion-SSM based Policy Learning for Coarse-to-Fine Action\n  Discretization","summary":"  We aim to solve the problem of generating coarse-to-fine skills learning from\ndemonstrations (LfD). To scale precision, traditional LfD approaches often rely\non extensive fine-grained demonstrations with external interpolations or\ndynamics models with limited generalization capabilities. For memory-efficient\nlearning and convenient granularity change, we propose a novel diffusion-SSM\nbased policy (DiSPo) that learns from diverse coarse skills and produces\nvarying control scales of actions by leveraging a state-space model, Mamba. Our\nevaluations show the adoption of Mamba and the proposed step-scaling method\nenable DiSPo to outperform in three coarse-to-fine benchmark tests with maximum\n81% higher success rate than baselines. In addition, DiSPo improves inference\nefficiency by generating coarse motions in less critical regions. We finally\ndemonstrate the scalability of actions with simulation and real-world\nmanipulation tasks.\n","authors":["Nayoung Oh","Jaehyeong Jang","Moonkyeong Jung","Daehyung Park"],"pdf_url":"https://arxiv.org/pdf/2409.14719v2.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.04935v1","updated":"2025-05-08T04:26:07Z","published":"2025-05-08T04:26:07Z","title":"Real-Time Model Predictive Control of Vehicles with Convex-Polygon-Aware\n  Collision Avoidance in Tight Spaces","summary":"  This paper proposes vehicle motion planning methods with obstacle avoidance\nin tight spaces by incorporating polygonal approximations of both the vehicle\nand obstacles into a model predictive control (MPC) framework. Representing\nthese shapes is crucial for navigation in tight spaces to ensure accurate\ncollision detection. However, incorporating polygonal approximations leads to\ndisjunctive OR constraints in the MPC formulation, which require a mixed\ninteger programming and cause significant computational cost. To overcome this,\nwe propose two different collision-avoidance constraints that reformulate the\ndisjunctive OR constraints as tractable conjunctive AND constraints: (1) a\nSupport Vector Machine (SVM)-based formulation that recasts collision avoidance\nas a SVM optimization problem, and (2) a Minimum Signed Distance to Edges\n(MSDE) formulation that leverages minimum signed-distance metrics. We validate\nboth methods through extensive simulations, including tight-space parking\nscenarios and varied-shape obstacle courses, as well as hardware experiments on\nan RC-car platform. Our results demonstrate that the SVM-based approach\nachieves superior navigation accuracy in constrained environments; the MSDE\napproach, by contrast, runs in real time with only a modest reduction in\ncollision-avoidance performance.\n","authors":["Haruki Kojima","Kohei Honda","Hiroyuki Okuda","Tatsuya Suzuki"],"pdf_url":"https://arxiv.org/pdf/2505.04935v1.pdf","comment":"8 pages, 10 figures, 3 tables, The IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) November 18-21, 2025-Gold Coast,\n  Australia"},{"id":"http://arxiv.org/abs/2505.04897v1","updated":"2025-05-08T02:18:49Z","published":"2025-05-08T02:18:49Z","title":"CubeDAgger: Improved Robustness of Interactive Imitation Learning\n  without Violation of Dynamic Stability","summary":"  Interactive imitation learning makes an agent's control policy robust by\nstepwise supervisions from an expert. The recent algorithms mostly employ\nexpert-agent switching systems to reduce the expert's burden by limitedly\nselecting the supervision timing. However, the precise selection is difficult\nand such a switching causes abrupt changes in actions, damaging the dynamic\nstability. This paper therefore proposes a novel method, so-called CubeDAgger,\nwhich improves robustness while reducing dynamic stability violations by making\nthree improvements to a baseline method, EnsembleDAgger. The first improvement\nadds a regularization to explicitly activate the threshold for deciding the\nsupervision timing. The second transforms the expert-agent switching system to\nan optimal consensus system of multiple action candidates. Third,\nautoregressive colored noise to the actions is introduced to make the\nstochastic exploration consistent over time. These improvements are verified by\nsimulations, showing that the learned policies are sufficiently robust while\nmaintaining dynamic stability during interaction.\n","authors":["Taisuke Kobayashi"],"pdf_url":"https://arxiv.org/pdf/2505.04897v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2505.04871v1","updated":"2025-05-08T00:55:16Z","published":"2025-05-08T00:55:16Z","title":"SatAOI: Delimitating Area of Interest for Swing-Arm Troweling Robot for\n  Construction","summary":"  In concrete troweling for building construction, robots can significantly\nreduce workload and improve automation level. However, as a primary task of\ncoverage path planning (CPP) for troweling, delimitating area of interest (AOI)\nin complex scenes is still challenging, especially for swing-arm robots with\nmore complex working modes. Thus, this research proposes an algorithm to\ndelimitate AOI for swing-arm troweling robot (SatAOI algorithm). By analyzing\ncharacteristics of the robot and obstacle maps, mathematical models and\ncollision principles are established. On this basis, SatAOI algorithm achieves\nAOI delimitation by global search and collision detection. Experiments on\ndifferent obstacle maps indicate that AOI can be effectively delimitated in\nscenes under different complexity, and the algorithm can fully consider the\nconnectivity of obstacle maps. This research serves as a foundation for CPP\nalgorithm and full process simulation of swing-arm troweling robots.\n","authors":["Jia-Rui Lin","Shaojie Zhou","Peng Pan","Ruijia Cai","Gang Chen"],"pdf_url":"https://arxiv.org/pdf/2505.04871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04860v1","updated":"2025-05-08T00:03:04Z","published":"2025-05-08T00:03:04Z","title":"D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation","summary":"  Learning bimanual manipulation is challenging due to its high dimensionality\nand tight coordination required between two arms. Eye-in-hand imitation\nlearning, which uses wrist-mounted cameras, simplifies perception by focusing\non task-relevant views. However, collecting diverse demonstrations remains\ncostly, motivating the need for scalable data augmentation. While prior work\nhas explored visual augmentation in single-arm settings, extending these\napproaches to bimanual manipulation requires generating viewpoint-consistent\nobservations across both arms and producing corresponding action labels that\nare both valid and feasible. In this work, we propose Diffusion for COordinated\nDual-arm Data Augmentation (D-CODA), a method for offline data augmentation\ntailored to eye-in-hand bimanual imitation learning that trains a diffusion\nmodel to synthesize novel, viewpoint-consistent wrist-camera images for both\narms while simultaneously generating joint-space action labels. It employs\nconstrained optimization to ensure that augmented states involving\ngripper-to-object contacts adhere to constraints suitable for bimanual\ncoordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our\nresults across 2250 simulation trials and 300 real-world trials demonstrate\nthat it outperforms baselines and ablations, showing its potential for scalable\ndata augmentation in eye-in-hand bimanual manipulation. Our project website is\nat: https://dcodaaug.github.io/D-CODA/.\n","authors":["I-Chun Arthur Liu","Jason Chen","Gaurav Sukhatme","Daniel Seita"],"pdf_url":"https://arxiv.org/pdf/2505.04860v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2505.05475v1","updated":"2025-05-08T17:59:58Z","published":"2025-05-08T17:59:58Z","title":"SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with\n  Video Diffusion and Data Augmentation","summary":"  Creating high-quality animatable 3D human avatars from a single image remains\na significant challenge in computer vision due to the inherent difficulty of\nreconstructing complete 3D information from a single viewpoint. Current\napproaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods\nproduce high-quality results but require multiple views or video sequences,\nwhile video diffusion models can generate animations from single images but\nstruggle with consistency and identity preservation. We present SVAD, a novel\napproach that addresses these limitations by leveraging complementary strengths\nof existing techniques. Our method generates synthetic training data through\nvideo diffusion, enhances it with identity preservation and image restoration\nmodules, and utilizes this refined data to train 3DGS avatars. Comprehensive\nevaluations demonstrate that SVAD outperforms state-of-the-art (SOTA)\nsingle-image methods in maintaining identity consistency and fine details\nacross novel poses and viewpoints, while enabling real-time rendering\ncapabilities. Through our data augmentation pipeline, we overcome the\ndependency on dense monocular or multi-view training data typically required by\ntraditional 3DGS approaches. Extensive quantitative, qualitative comparisons\nshow our method achieves superior performance across multiple metrics against\nbaseline models. By effectively combining the generative power of diffusion\nmodels with both the high-quality results and rendering efficiency of 3DGS, our\nwork establishes a new approach for high-fidelity avatar generation from a\nsingle image input.\n","authors":["Yonwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2505.05475v1.pdf","comment":"Accepted by CVPR 2025 SyntaGen Workshop, Project Page:\n  https://yc4ny.github.io/SVAD/"},{"id":"http://arxiv.org/abs/2505.05474v1","updated":"2025-05-08T17:59:54Z","published":"2025-05-08T17:59:54Z","title":"3D Scene Generation: A Survey","summary":"  3D scene generation seeks to synthesize spatially structured, semantically\nmeaningful, and photorealistic environments for applications such as immersive\nmedia, robotics, autonomous driving, and embodied AI. Early methods based on\nprocedural rules offered scalability but limited diversity. Recent advances in\ndeep generative models (e.g., GANs, diffusion models) and 3D representations\n(e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene\ndistributions, improving fidelity, diversity, and view consistency. Recent\nadvances like diffusion models bridge 3D scene synthesis and photorealism by\nreframing generation as image or video synthesis problems. This survey provides\na systematic overview of state-of-the-art approaches, organizing them into four\nparadigms: procedural generation, neural 3D-based generation, image-based\ngeneration, and video-based generation. We analyze their technical foundations,\ntrade-offs, and representative results, and review commonly used datasets,\nevaluation protocols, and downstream applications. We conclude by discussing\nkey challenges in generation capacity, 3D representation, data and annotations,\nand evaluation, and outline promising directions including higher fidelity,\nphysics-aware and interactive generation, and unified perception-generation\nmodels. This review organizes recent advances in 3D scene generation and\nhighlights promising directions at the intersection of generative AI, 3D\nvision, and embodied intelligence. To track ongoing developments, we maintain\nan up-to-date project page:\nhttps://github.com/hzxie/Awesome-3D-Scene-Generation.\n","authors":["Beichen Wen","Haozhe Xie","Zhaoxi Chen","Fangzhou Hong","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2505.05474v1.pdf","comment":"Project Page: https://github.com/hzxie/Awesome-3D-Scene-Generation"},{"id":"http://arxiv.org/abs/2505.05473v1","updated":"2025-05-08T17:59:47Z","published":"2025-05-08T17:59:47Z","title":"DiffusionSfM: Predicting Structure and Motion via Ray Origin and\n  Endpoint Diffusion","summary":"  Current Structure-from-Motion (SfM) methods typically follow a two-stage\npipeline, combining learned or geometric pairwise reasoning with a subsequent\nglobal optimization step. In contrast, we propose a data-driven multi-view\nreasoning approach that directly infers 3D scene geometry and camera poses from\nmulti-view images. Our framework, DiffusionSfM, parameterizes scene geometry\nand cameras as pixel-wise ray origins and endpoints in a global frame and\nemploys a transformer-based denoising diffusion model to predict them from\nmulti-view inputs. To address practical challenges in training diffusion models\nwith missing data and unbounded scene coordinates, we introduce specialized\nmechanisms that ensure robust learning. We empirically validate DiffusionSfM on\nboth synthetic and real datasets, demonstrating that it outperforms classical\nand learning-based approaches while naturally modeling uncertainty.\n","authors":["Qitao Zhao","Amy Lin","Jeff Tan","Jason Y. Zhang","Deva Ramanan","Shubham Tulsiani"],"pdf_url":"https://arxiv.org/pdf/2505.05473v1.pdf","comment":"CVPR 2025. Project website: https://qitaozhao.github.io/DiffusionSfM"},{"id":"http://arxiv.org/abs/2505.05472v1","updated":"2025-05-08T17:58:57Z","published":"2025-05-08T17:58:57Z","title":"Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation","summary":"  Recent progress in unified models for image understanding and generation has\nbeen impressive, yet most approaches remain limited to single-modal generation\nconditioned on multiple modalities. In this paper, we present Mogao, a unified\nframework that advances this paradigm by enabling interleaved multi-modal\ngeneration through a causal approach. Mogao integrates a set of key technical\nimprovements in architecture design, including a deep-fusion design, dual\nvision encoders, interleaved rotary position embeddings, and multi-modal\nclassifier-free guidance, which allow it to harness the strengths of both\nautoregressive models for text generation and diffusion models for high-quality\nimage synthesis. These practical improvements also make Mogao particularly\neffective to process interleaved sequences of text and images arbitrarily. To\nfurther unlock the potential of unified models, we introduce an efficient\ntraining strategy on a large-scale, in-house dataset specifically curated for\njoint text and image generation. Extensive experiments show that Mogao not only\nachieves state-of-the-art performance in multi-modal understanding and\ntext-to-image generation, but also excels in producing high-quality, coherent\ninterleaved outputs. Its emergent capabilities in zero-shot image editing and\ncompositional generation highlight Mogao as a practical omni-modal foundation\nmodel, paving the way for future development and scaling the unified\nmulti-modal systems.\n","authors":["Chao Liao","Liyang Liu","Xun Wang","Zhengxiong Luo","Xinyu Zhang","Wenliang Zhao","Jie Wu","Liang Li","Zhi Tian","Weilin Huang"],"pdf_url":"https://arxiv.org/pdf/2505.05472v1.pdf","comment":"Mogao Technical Report"},{"id":"http://arxiv.org/abs/2505.05470v1","updated":"2025-05-08T17:58:45Z","published":"2025-05-08T17:58:45Z","title":"Flow-GRPO: Training Flow Matching Models via Online RL","summary":"  We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.\n","authors":["Jie Liu","Gongye Liu","Jiajun Liang","Yangguang Li","Jiaheng Liu","Xintao Wang","Pengfei Wan","Di Zhang","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2505.05470v1.pdf","comment":"Code: https://github.com/yifan123/flow_grpo"},{"id":"http://arxiv.org/abs/2505.05469v1","updated":"2025-05-08T17:58:18Z","published":"2025-05-08T17:58:18Z","title":"Generating Physically Stable and Buildable LEGO Designs from Text","summary":"  We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.\n","authors":["Ava Pun","Kangle Deng","Ruixuan Liu","Deva Ramanan","Changliu Liu","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.05469v1.pdf","comment":"Project page: https://avalovelace1.github.io/LegoGPT/"},{"id":"http://arxiv.org/abs/2505.05467v1","updated":"2025-05-08T17:57:40Z","published":"2025-05-08T17:57:40Z","title":"StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant","summary":"  We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.\n","authors":["Haibo Wang","Bo Feng","Zhengfeng Lai","Mingze Xu","Shiyu Li","Weifeng Ge","Afshin Dehghan","Meng Cao","Ping Huang"],"pdf_url":"https://arxiv.org/pdf/2505.05467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05456v1","updated":"2025-05-08T17:45:44Z","published":"2025-05-08T17:45:44Z","title":"SITE: towards Spatial Intelligence Thorough Evaluation","summary":"  Spatial intelligence (SI) represents a cognitive ability encompassing the\nvisualization, manipulation, and reasoning about spatial relationships,\nunderpinning disciplines from neuroscience to robotics. We introduce SITE, a\nbenchmark dataset towards SI Thorough Evaluation in a standardized format of\nmulti-choice visual question-answering, designed to assess large\nvision-language models' spatial intelligence across diverse visual modalities\n(single-image, multi-image, and video) and SI factors (figural to environmental\nscales, spatial visualization and orientation, intrinsic and extrinsic, static\nand dynamic). Our approach to curating the benchmark combines a bottom-up\nsurvey about 31 existing datasets and a top-down strategy drawing upon three\nclassification systems in cognitive science, which prompt us to design two\nnovel types of tasks about view-taking and dynamic scenes. Extensive\nexperiments reveal that leading models fall behind human experts especially in\nspatial orientation, a fundamental SI factor. Moreover, we demonstrate a\npositive correlation between a model's spatial reasoning proficiency and its\nperformance on an embodied AI task.\n","authors":["Wenqi Wang","Reuben Tan","Pengyue Zhu","Jianwei Yang","Zhengyuan Yang","Lijuan Wang","Andrey Kolobov","Jianfeng Gao","Boqing Gong"],"pdf_url":"https://arxiv.org/pdf/2505.05456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05446v1","updated":"2025-05-08T17:37:36Z","published":"2025-05-08T17:37:36Z","title":"Adaptive Markup Language Generation for Contextually-Grounded Visual\n  Document Understanding","summary":"  Visual Document Understanding has become essential with the increase of\ntext-rich visual content. This field poses significant challenges due to the\nneed for effective integration of visual perception and textual comprehension,\nparticularly across diverse document types with complex layouts. Moreover,\nexisting fine-tuning datasets for this domain often fall short in providing the\ndetailed contextual information for robust understanding, leading to\nhallucinations and limited comprehension of spatial relationships among visual\nelements. To address these challenges, we propose an innovative pipeline that\nutilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,\nand TiKZ, to build highly structured document representations and deliver\ncontextually-grounded responses. We introduce two fine-grained structured\ndatasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs\nfor document parsing, and DocMark-Instruct, featuring 624k fine-tuning data\nannotations for grounded instruction following. Extensive experiments\ndemonstrate that our proposed model significantly outperforms existing\nstate-of-theart MLLMs across a range of visual document understanding\nbenchmarks, facilitating advanced reasoning and comprehension capabilities in\ncomplex visual scenarios. Our code and models are released at https://github.\ncom/Euphoria16/DocMark.\n","authors":["Han Xiao","Yina Xie","Guanxin Tan","Yinghao Chen","Rui Hu","Ke Wang","Aojun Zhou","Hao Li","Hao Shao","Xudong Lu","Peng Gao","Yafei Wen","Xiaoxin Chen","Shuai Ren","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2505.05446v1.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2409.11686v3","updated":"2025-05-08T17:23:39Z","published":"2024-09-18T03:56:56Z","title":"Automated detection of underdiagnosed medical conditions via\n  opportunistic imaging","summary":"  Abdominal computed tomography (CT) scans are frequently performed in clinical\nsettings. Opportunistic CT involves repurposing routine CT images to extract\ndiagnostic information and is an emerging tool for detecting underdiagnosed\nconditions such as sarcopenia, hepatic steatosis, and ascites. This study\nutilizes deep learning methods to promote accurate diagnosis and clinical\ndocumentation. We analyze 2,674 inpatient CT scans to identify discrepancies\nbetween imaging phenotypes (characteristics derived from opportunistic CT\nscans) and their corresponding documentation in radiology reports and ICD\ncoding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans\ndiagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)\nthrough either opportunistic imaging or radiology reports were ICD-coded. Our\nfindings demonstrate opportunistic CT's potential to enhance diagnostic\nprecision and accuracy of risk adjustment models, offering advancements in\nprecision medicine.\n","authors":["Asad Aali","Andrew Johnston","Louis Blankemeier","Dave Van Veen","Laura T Derry","David Svec","Jason Hom","Robert D. Boutin","Akshay S. Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2409.11686v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05422v1","updated":"2025-05-08T17:12:19Z","published":"2025-05-08T17:12:19Z","title":"TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and\n  Generation","summary":"  Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP.\n","authors":["Haokun Lin","Teng Wang","Yixiao Ge","Yuying Ge","Zhichao Lu","Ying Wei","Qingfu Zhang","Zhenan Sun","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2505.05422v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2505.05397v1","updated":"2025-05-08T16:33:04Z","published":"2025-05-08T16:33:04Z","title":"PillarMamba: Learning Local-Global Context for Roadside Point Cloud via\n  Hybrid State Space Model","summary":"  Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything\n(V2X) tasks, roadside perception has received increasing attention in recent\nyears, as it can extend the perception range of connected vehicles and improve\ntraffic safety. However, roadside point cloud oriented 3D object detection has\nnot been effectively explored. To some extent, the key to the performance of a\npoint cloud detector lies in the receptive field of the network and the ability\nto effectively utilize the scene context. The recent emergence of Mamba, based\non State Space Model (SSM), has shaken up the traditional convolution and\ntransformers that have long been the foundational building blocks, due to its\nefficient global receptive field. In this work, we introduce Mamba to\npillar-based roadside point cloud perception and propose a framework based on\nCross-stage State-space Group (CSG), called PillarMamba. It enhances the\nexpressiveness of the network and achieves efficient computation through\ncross-stage feature fusion. However, due to the limitations of scan directions,\nstate space model faces local connection disrupted and historical relationship\nforgotten. To address this, we propose the Hybrid State-space Block (HSB) to\nobtain the local-global context of roadside point cloud. Specifically, it\nenhances neighborhood connections through local convolution and preserves\nhistorical memory through residual attention. The proposed method outperforms\nthe state-of-the-art methods on the popular large scale roadside benchmark:\nDAIR-V2X-I. The code will be released soon.\n","authors":["Zhang Zhang","Chao Sun","Chao Yue","Da Wen","Tianze Wang","Jianghao Leng"],"pdf_url":"https://arxiv.org/pdf/2505.05397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05391v1","updated":"2025-05-08T16:27:27Z","published":"2025-05-08T16:27:27Z","title":"EDmamba: A Simple yet Effective Event Denoising Method with State Space\n  Model","summary":"  Event cameras excel in high-speed vision due to their high temporal\nresolution, high dynamic range, and low power consumption. However, as dynamic\nvision sensors, their output is inherently noisy, making efficient denoising\nessential to preserve their ultra-low latency and real-time processing\ncapabilities. Existing event denoising methods struggle with a critical\ndilemma: computationally intensive approaches compromise the sensor's\nhigh-speed advantage, while lightweight methods often lack robustness across\nvarying noise levels. To address this, we propose a novel event denoising\nframework based on State Space Models (SSMs). Our approach represents events as\n4D event clouds and includes a Coarse Feature Extraction (CFE) module that\nextracts embedding features from both geometric and polarity-aware subspaces.\nThe model is further composed of two essential components: A Spatial Mamba\n(S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM)\nthat captures global temporal dynamics, efficiently propagating spatiotemporal\nfeatures across events. Experiments demonstrate that our method achieves\nstate-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per\n100K events inference time, and a 0.982 accuracy score, outperforming\nTransformer-based methods by 2.08% in denoising accuracy and 36X faster.\n","authors":["Ciyu Ruan","Zihang Gong","Ruishan Guo","Jingao Xu","Xinlei Chen"],"pdf_url":"https://arxiv.org/pdf/2505.05391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05376v1","updated":"2025-05-08T16:11:09Z","published":"2025-05-08T16:11:09Z","title":"GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans","summary":"  We propose a novel method that reconstructs hair strands directly from\ncolorless 3D scans by leveraging multi-modal hair orientation extraction. Hair\nstrand reconstruction is a fundamental problem in computer vision and graphics\nthat can be used for high-fidelity digital avatar synthesis, animation, and\nAR/VR applications. However, accurately recovering hair strands from raw scan\ndata remains challenging due to human hair's complex and fine-grained\nstructure. Existing methods typically rely on RGB captures, which can be\nsensitive to the environment and can be a challenging domain for extracting the\norientation of guiding strands, especially in the case of challenging\nhairstyles. To reconstruct the hair purely from the observed geometry, our\nmethod finds sharp surface features directly on the scan and estimates strand\norientation through a neural 2D line detector applied to the renderings of scan\nshading. Additionally, we incorporate a diffusion prior trained on a diverse\nset of synthetic hair scans, refined with an improved noise schedule, and\nadapted to the reconstructed contents via a scan-specific text prompt. We\ndemonstrate that this combination of supervision signals enables accurate\nreconstruction of both simple and intricate hairstyles without relying on color\ninformation. To facilitate further research, we introduce Strands400, the\nlargest publicly available dataset of hair strands with detailed surface\ngeometry extracted from real-world data, which contains reconstructed hair\nstrands from the scans of 400 subjects.\n","authors":["Rachmadio Noval Lazuardi","Artem Sevastopolsky","Egor Zakharov","Matthias Niessner","Vanessa Sklyarova"],"pdf_url":"https://arxiv.org/pdf/2505.05376v1.pdf","comment":"15 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2505.05375v1","updated":"2025-05-08T16:09:40Z","published":"2025-05-08T16:09:40Z","title":"Threshold Modulation for Online Test-Time Adaptation of Spiking Neural\n  Networks","summary":"  Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.\n","authors":["Kejie Zhao","Wenjia Hua","Aiersi Tuerhong","Luziwei Leng","Yuxin Ma","Qinghua Guo"],"pdf_url":"https://arxiv.org/pdf/2505.05375v1.pdf","comment":"Accepted by IJCNN 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, including reprinting/republishing this material for advertising or\n  promotional purposes, collecting new collected works for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2505.05374v1","updated":"2025-05-08T16:09:08Z","published":"2025-05-08T16:09:08Z","title":"OcularAge: A Comparative Study of Iris and Periocular Images for\n  Pediatric Age Estimation","summary":"  Estimating a child's age from ocular biometric images is challenging due to\nsubtle physiological changes and the limited availability of longitudinal\ndatasets. Although most biometric age estimation studies have focused on facial\nfeatures and adult subjects, pediatric-specific analysis, particularly of the\niris and periocular regions, remains relatively unexplored. This study presents\na comparative evaluation of iris and periocular images for estimating the ages\nof children aged between 4 and 16 years. We utilized a longitudinal dataset\ncomprising more than 21,000 near-infrared (NIR) images, collected from 288\npediatric subjects over eight years using two different imaging sensors. A\nmulti-task deep learning framework was employed to jointly perform age\nprediction and age-group classification, enabling a systematic exploration of\nhow different convolutional neural network (CNN) architectures, particularly\nthose adapted for non-square ocular inputs, capture the complex variability\ninherent in pediatric eye images. The results show that periocular models\nconsistently outperform iris-based models, achieving a mean absolute error\n(MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These\nresults mark the first demonstration that reliable age estimation is feasible\nfrom children's ocular images, enabling privacy-preserving age checks in\nchild-centric applications. This work establishes the first longitudinal\nbenchmark for pediatric ocular age estimation, providing a foundation for\ndesigning robust, child-focused biometric systems. The developed models proved\nresilient across different imaging sensors, confirming their potential for\nreal-world deployment. They also achieved inference speeds of less than 10\nmilliseconds per image on resource-constrained VR headsets, demonstrating their\nsuitability for real-time applications.\n","authors":["Naveenkumar G Venkataswamy","Poorna Ravi","Stephanie Schuckers","Masudul H. Imtiaz"],"pdf_url":"https://arxiv.org/pdf/2505.05374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05367v1","updated":"2025-05-08T16:04:35Z","published":"2025-05-08T16:04:35Z","title":"Joint Super-Resolution and Segmentation for 1-m Impervious Surface Area\n  Mapping in China's Yangtze River Economic Belt","summary":"  We propose a novel joint framework by integrating super-resolution and\nsegmentation, called JointSeg, which enables the generation of 1-meter ISA maps\ndirectly from freely available Sentinel-2 imagery. JointSeg was trained on\nmultimodal cross-resolution inputs, offering a scalable and affordable\nalternative to traditional approaches. This synergistic design enables gradual\nresolution enhancement from 10m to 1m while preserving fine-grained spatial\ntextures, and ensures high classification fidelity through effective\ncross-scale feature fusion. This method has been successfully applied to the\nYangtze River Economic Belt (YREB), a region characterized by complex\nurban-rural patterns and diverse topography. As a result, a comprehensive ISA\nmapping product for 2021, referred to as ISA-1, was generated, covering an area\nof over 2.2 million square kilometers. Quantitative comparisons against the 10m\nESA WorldCover and other benchmark products reveal that ISA-1 achieves an\nF1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by\n9.5%, and surpassing other ISA datasets by 21.43%-61.07%. In densely urbanized\nareas (e.g., Suzhou, Nanjing), ISA-1 reduces ISA overestimation through\nimproved discrimination of green spaces and water bodies. Conversely, in\nmountainous regions (e.g., Ganzi, Zhaotong), it identifies significantly more\nISA due to its enhanced ability to detect fragmented anthropogenic features\nsuch as rural roads and sparse settlements, demonstrating its robustness across\ndiverse landscapes. Moreover, we present biennial ISA maps from 2017 to 2023,\ncapturing spatiotemporal urbanization dynamics across representative cities.\nThe results highlight distinct regional growth patterns: rapid expansion in\nupstream cities, moderate growth in midstream regions, and saturation in\ndownstream metropolitan areas.\n","authors":["Jie Deng","Danfeng Hong","Chenyu Li","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2505.05367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05356v1","updated":"2025-05-08T15:45:53Z","published":"2025-05-08T15:45:53Z","title":"Time of the Flight of the Gaussians: Optimizing Depth Indirectly in\n  Dynamic Radiance Fields","summary":"  We present a method to reconstruct dynamic scenes from monocular\ncontinuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that\nachieves similar or better accuracy than neural volumetric approaches and is\n100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a\nsingle viewpoint is a significant challenge in computer vision. In C-ToF\nradiance field reconstruction, the property of interest-depth-is not directly\nmeasured, causing an additional challenge. This problem has a large and\nunderappreciated impact upon the optimization when using a fast primitive-based\nscene representation like 3D Gaussian splatting, which is commonly used with\nmulti-view data to produce satisfactory results and is brittle in its\noptimization otherwise. We incorporate two heuristics into the optimization to\nimprove the accuracy of scene geometry represented by Gaussians. Experimental\nresults show that our approach produces accurate reconstructions under\nconstrained C-ToF sensing conditions, including for fast motions like swinging\nbaseball bats. https://visual.cs.brown.edu/gftorf\n","authors":["Runfeng Li","Mikhail Okunev","Zixuan Guo","Anh Ha Duong","Christian Richardt","Matthew O'Toole","James Tompkin"],"pdf_url":"https://arxiv.org/pdf/2505.05356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03355v4","updated":"2025-05-08T15:38:04Z","published":"2025-03-05T10:37:51Z","title":"Rethinking Video Super-Resolution: Towards Diffusion-Based Methods\n  without Motion Alignment","summary":"  In this work, we rethink the approach to video super-resolution by\nintroducing a method based on the Diffusion Posterior Sampling framework,\ncombined with an unconditional video diffusion transformer operating in latent\nspace. The video generation model, a diffusion transformer, functions as a\nspace-time model. We argue that a powerful model, which learns the physics of\nthe real world, can easily handle various kinds of motion patterns as prior\nknowledge, thus eliminating the need for explicit estimation of optical flows\nor motion parameters for pixel alignment. Furthermore, a single instance of the\nproposed video diffusion transformer model can adapt to different sampling\nconditions without re-training. Empirical results on synthetic and real-world\ndatasets illustrate the feasibility of diffusion-based, alignment-free video\nsuper-resolution.\n","authors":["Zhihao Zhan","Wang Pang","Xiang Zhu","Yechao Bai"],"pdf_url":"https://arxiv.org/pdf/2503.03355v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14630v3","updated":"2025-05-08T15:35:12Z","published":"2023-09-26T02:49:30Z","title":"Free Discontinuity Regression: With an Application to the Economic\n  Effects of Internet Shutdowns","summary":"  Sharp, multidimensional changepoints-abrupt shifts in a regression surface\nwhose locations and magnitudes are unknown-arise in settings as varied as\ngene-expression profiling, financial covariance breaks, climate-regime\ndetection, and urban socioeconomic mapping. Despite their prevalence, there are\nno current approaches that jointly estimate the location and size of the\ndiscontinuity set in a one-shot approach with statistical guarantees. We\ntherefore introduce Free Discontinuity Regression (FDR), a fully nonparametric\nestimator that simultaneously (i) smooths a regression surface, (ii) segments\nit into contiguous regions, and (iii) provably recovers the precise locations\nand sizes of its jumps. By extending a convex relaxation of the Mumford-Shah\nfunctional to random spatial sampling and correlated noise, FDR overcomes the\nfixed-grid and i.i.d. noise assumptions of classical image-segmentation\napproaches, thus enabling its application to real-world data of any dimension.\nThis yields the first identification and uniform consistency results for\nmultivariate jump surfaces: under mild SBV regularity, the estimated function,\nits discontinuity set, and all jump sizes converge to their true population\ncounterparts. Hyperparameters are selected automatically from the data using\nStein's Unbiased Risk Estimate, and large-scale simulations up to three\ndimensions validate the theoretical results and demonstrate good finite-sample\nperformance. Applying FDR to an internet shutdown in India reveals a 25-35%\nreduction in economic activity around the estimated shutdown boundaries-much\nlarger than previous estimates. By unifying smoothing, segmentation, and\neffect-size recovery in a general statistical setting, FDR turns\nfree-discontinuity ideas into a practical tool with formal guarantees for\nmodern multivariate data.\n","authors":["Florian Gunsilius","David Van Dijcke"],"pdf_url":"https://arxiv.org/pdf/2309.14630v3.pdf","comment":"24 pages, 3 figures, 2 tables; authors listed alphabetically; code\n  available at https://github.com/Davidvandijcke/fdr"},{"id":"http://arxiv.org/abs/2505.05343v1","updated":"2025-05-08T15:32:04Z","published":"2025-05-08T15:32:04Z","title":"Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound\n  Source Localization","summary":"  Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings.\n","authors":["Sooyoung Park","Arda Senocak","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2505.05343v1.pdf","comment":"Journal Extension of WACV 2024 paper (arXiv:2311.04066). Code is\n  available at https://github.com/swimmiing/ACL-SSL"},{"id":"http://arxiv.org/abs/2505.05336v1","updated":"2025-05-08T15:28:09Z","published":"2025-05-08T15:28:09Z","title":"Progressive Inertial Poser: Progressive Real-Time Kinematic Chain\n  Estimation for 3D Full-Body Pose from Three IMU Sensors","summary":"  The motion capture system that supports full-body virtual representation is\nof key significance for virtual reality. Compared to vision-based systems,\nfull-body pose estimation from sparse tracking signals is not limited by\nenvironmental conditions or recording range. However, previous works either\nface the challenge of wearing additional sensors on the pelvis and lower-body\nor rely on external visual sensors to obtain global positions of key joints. To\nimprove the practicality of the technology for virtual reality applications, we\nestimate full-body poses using only inertial data obtained from three Inertial\nMeasurement Unit (IMU) sensors worn on the head and wrists, thereby reducing\nthe complexity of the hardware system. In this work, we propose a method called\nProgressive Inertial Poser (ProgIP) for human pose estimation, which combines\nneural network estimation with a human dynamics model, considers the\nhierarchical structure of the kinematic chain, and employs a multi-stage\nprogressive network estimation with increased depth to reconstruct full-body\nmotion in real time. The encoder combines Transformer Encoder and bidirectional\nLSTM (TE-biLSTM) to flexibly capture the temporal dependencies of the inertial\nsequence, while the decoder based on multi-layer perceptrons (MLPs) transforms\nhigh-dimensional features and accurately projects them onto Skinned\nMulti-Person Linear (SMPL) model parameters. Quantitative and qualitative\nexperimental results on multiple public datasets show that our method\noutperforms state-of-the-art methods with the same inputs, and is comparable to\nrecent works using six IMU sensors.\n","authors":["Zunjie Zhu","Yan Zhao","Yihan Hu","Guoxiang Wang","Hai Qiu","Bolun Zheng","Chenggang Yan","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2505.05336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05829v2","updated":"2025-05-08T15:26:57Z","published":"2023-10-09T16:17:42Z","title":"USTEP: Spatio-Temporal Predictive Learning under A Unified View","summary":"  Spatio-temporal predictive learning plays a crucial role in self-supervised\nlearning, with wide-ranging applications across a diverse range of fields.\nPrevious approaches for temporal modeling fall into two categories:\nrecurrent-based and recurrent-free methods. The former, while meticulously\nprocessing frames one by one, neglect short-term spatio-temporal information\nredundancies, leading to inefficiencies. The latter naively stack frames\nsequentially, overlooking the inherent temporal dependencies. In this paper, we\nre-examine the two dominant temporal modeling approaches within the realm of\nspatio-temporal predictive learning, offering a unified perspective. Building\nupon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive\nlearning), an innovative framework that reconciles the recurrent-based and\nrecurrent-free methods by integrating both micro-temporal and macro-temporal\nscales. Extensive experiments on a wide range of spatio-temporal predictive\nlearning demonstrate that USTEP achieves significant improvements over existing\ntemporal modeling approaches, thereby establishing it as a robust solution for\na wide range of spatio-temporal applications.\n","authors":["Cheng Tan","Jue Wang","Zhangyang Gao","Siyuan Li","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2310.05829v2.pdf","comment":"Accepted by TPAMI"},{"id":"http://arxiv.org/abs/2505.05331v1","updated":"2025-05-08T15:22:11Z","published":"2025-05-08T15:22:11Z","title":"Aesthetics Without Semantics","summary":"  While it is easy for human observers to judge an image as beautiful or ugly,\naesthetic decisions result from a combination of entangled perceptual and\ncognitive (semantic) factors, making the understanding of aesthetic judgements\nparticularly challenging from a scientific point of view. Furthermore, our\nresearch shows a prevailing bias in current databases, which include mostly\nbeautiful images, further complicating the study and prediction of aesthetic\nresponses. We address these limitations by creating a database of images with\nminimal semantic content and devising, and next exploiting, a method to\ngenerate images on the ugly side of aesthetic valuations. The resulting Minimum\nSemantic Content (MSC) database consists of a large and balanced collection of\n10,426 images, each evaluated by 100 observers. We next use established image\nmetrics to demonstrate how augmenting an image set biased towards beautiful\nimages with ugly images can modify, or even invert, an observed relationship\nbetween image features and aesthetics valuation. Taken together, our study\nreveals that works in empirical aesthetics attempting to link image content and\naesthetic judgements may magnify, underestimate, or simply miss interesting\neffects due to a limitation of the range of aesthetic values they consider.\n","authors":["C. Alejandro Parraga","Olivier Penacchio","Marcos Muňoz Gonzalez","Bogdan Raducanu","Xavier Otazu"],"pdf_url":"https://arxiv.org/pdf/2505.05331v1.pdf","comment":"Parts of this work were presented in abstract format at the Vision\n  Science of Art Conference (VSAC2016), the Iberian Conference on Perception\n  (CIP2022), and the European Conference on Visual Perception (ECVP2022). See\n  Perception 51, No1 (Suppl.) pp139, 2022)"},{"id":"http://arxiv.org/abs/2505.05321v1","updated":"2025-05-08T15:08:36Z","published":"2025-05-08T15:08:36Z","title":"Feature-Augmented Deep Networks for Multiscale Building Segmentation in\n  High-Resolution UAV and Satellite Imagery","summary":"  Accurate building segmentation from high-resolution RGB imagery remains\nchallenging due to spectral similarity with non-building features, shadows, and\nirregular building geometries. In this study, we present a comprehensive deep\nlearning framework for multiscale building segmentation using RGB aerial and\nsatellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate\na diverse, multi-sensor dataset and introduce feature-augmented inputs by\nderiving secondary representations including Principal Component Analysis\n(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index\n(MBI), and Sobel edge filters from RGB channels. These features guide a\nRes-U-Net architecture in learning complex spatial patterns more effectively.\nWe also propose training policies incorporating layer freezing, cyclical\nlearning rates, and SuperConvergence to reduce training time and resource\nusage. Evaluated on a held-out WorldView-3 image, our model achieves an overall\naccuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of\n0.80, outperforming existing RGB-based benchmarks. This study demonstrates the\neffectiveness of combining multi-resolution imagery, feature augmentation, and\noptimized training strategies for robust building segmentation in remote\nsensing applications.\n","authors":["Chintan B. Maniyar","Minakshi Kumar","Gengchen Mai"],"pdf_url":"https://arxiv.org/pdf/2505.05321v1.pdf","comment":"in preparation for journal submission, 25 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.05318v1","updated":"2025-05-08T15:02:49Z","published":"2025-05-08T15:02:49Z","title":"Mapping User Trust in Vision Language Models: Research Landscape,\n  Challenges, and Prospects","summary":"  The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies.\n","authors":["Agnese Chiatti","Sara Bernardini","Lara Shibelski Godoy Piccolo","Viola Schiaffonati","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2505.05318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05309v1","updated":"2025-05-08T14:57:52Z","published":"2025-05-08T14:57:52Z","title":"Augmented Deep Contexts for Spatially Embedded Video Coding","summary":"  Most Neural Video Codecs (NVCs) only employ temporal references to generate\ntemporal-only contexts and latent prior. These temporal-only NVCs fail to\nhandle large motions or emerging objects due to limited contexts and misaligned\nlatent prior. To relieve the limitations, we propose a Spatially Embedded Video\nCodec (SEVC), in which the low-resolution video is compressed for spatial\nreferences. Firstly, our SEVC leverages both spatial and temporal references to\ngenerate augmented motion vectors and hybrid spatial-temporal contexts.\nSecondly, to address the misalignment issue in latent prior and enrich the\nprior information, we introduce a spatial-guided latent prior augmented by\nmultiple temporal latent representations. At last, we design a joint\nspatial-temporal optimization to learn quality-adaptive bit allocation for\nspatial references, further boosting rate-distortion performance. Experimental\nresults show that our SEVC effectively alleviates the limitations in handling\nlarge motions or emerging objects, and also reduces 11.9% more bitrate than the\nprevious state-of-the-art NVC while providing an additional low-resolution\nbitstream. Our code and model are available at https://github.com/EsakaK/SEVC.\n","authors":["Yifan Bian","Chuanbo Tang","Li Li","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2505.05309v1.pdf","comment":"15 pages,CVPR"},{"id":"http://arxiv.org/abs/2505.05307v1","updated":"2025-05-08T14:52:45Z","published":"2025-05-08T14:52:45Z","title":"PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera\n  Deraining","summary":"  Event cameras excel in high temporal resolution and dynamic range but suffer\nfrom dense noise in rainy conditions. Existing event deraining methods face\ntrade-offs between temporal precision, deraining effectiveness, and\ncomputational efficiency. In this paper, we propose PRE-Mamba, a novel\npoint-based event camera deraining framework that fully exploits the\nspatiotemporal characteristics of raw event and rain. Our framework introduces\na 4D event cloud representation that integrates dual temporal scales to\npreserve high temporal precision, a Spatio-Temporal Decoupling and Fusion\nmodule (STDF) that enhances deraining capability by enabling shallow decoupling\nand interaction of temporal and spatial information, and a Multi-Scale State\nSpace Model (MS3M) that captures deeper rain dynamics across dual-temporal and\nmulti-spatial scales with linear computational complexity. Enhanced by\nfrequency-domain regularization, PRE-Mamba achieves superior performance (0.95\nSR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a\ncomprehensive dataset with labeled synthetic and real-world sequences.\nMoreover, our method generalizes well across varying rain intensities,\nviewpoints, and even snowy conditions.\n","authors":["Ciyu Ruan","Ruishan Guo","Zihang Gong","Jingao Xu","Wenhan Yang","Xinlei Chen"],"pdf_url":"https://arxiv.org/pdf/2505.05307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05291v1","updated":"2025-05-08T14:31:02Z","published":"2025-05-08T14:31:02Z","title":"Benchmarking Ophthalmology Foundation Models for Clinically Significant\n  Age Macular Degeneration Detection","summary":"  Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\nlearn robust representations from large-scale natural image datasets, enhancing\ntheir generalization across domains. In retinal imaging, foundation models\npretrained on either natural or ophthalmic data have shown promise, but the\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\nage-related macular degeneration (AMD) identification. Our results show that\niBOT pretrained on natural images achieves the highest out-of-distribution\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\nfoundation models in improving AMD identification and challenge the assumption\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil.\n","authors":["Benjamin A. Cohen","Jonathan Fhima","Meishar Meisel","Baskin Meital","Luis Filipe Nakayama","Eran Berkowitz","Joachim A. Behar"],"pdf_url":"https://arxiv.org/pdf/2505.05291v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.05288v1","updated":"2025-05-08T14:29:11Z","published":"2025-05-08T14:29:11Z","title":"PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes","summary":"  We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.\n","authors":["Ahmed Abdelreheem","Filippo Aleotti","Jamie Watson","Zawar Qureshi","Abdelrahman Eldesokey","Peter Wonka","Gabriel Brostow","Sara Vicente","Guillermo Garcia-Hernando"],"pdf_url":"https://arxiv.org/pdf/2505.05288v1.pdf","comment":"Tech report. Project page: https://nianticlabs.github.io/placeit3d/"},{"id":"http://arxiv.org/abs/2505.05279v1","updated":"2025-05-08T14:26:00Z","published":"2025-05-08T14:26:00Z","title":"MTL-UE: Learning to Learn Nothing for Multi-Task Learning","summary":"  Most existing unlearnable strategies focus on preventing unauthorized users\nfrom training single-task learning (STL) models with personal data.\nNevertheless, the paradigm has recently shifted towards multi-task data and\nmulti-task learning (MTL), targeting generalist and foundation models that can\nhandle multiple tasks simultaneously. Despite their growing importance, MTL\ndata and models have been largely neglected while pursuing unlearnable\nstrategies. This paper presents MTL-UE, the first unified framework for\ngenerating unlearnable examples for multi-task data and MTL models. Instead of\noptimizing perturbations for each sample, we design a generator-based structure\nthat introduces label priors and class-wise feature embeddings which leads to\nmuch better attacking performance. In addition, MTL-UE incorporates intra-task\nand inter-task embedding regularization to increase inter-class separation and\nsuppress intra-class variance which enhances the attack robustness greatly.\nFurthermore, MTL-UE is versatile with good supports for dense prediction tasks\nin MTL. It is also plug-and-play allowing integrating existing\nsurrogate-dependent unlearnable methods with little adaptation. Extensive\nexperiments show that MTL-UE achieves superior attacking performance\nconsistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5\nMTL task-weighting strategies.\n","authors":["Yi Yu","Song Xia","Siyuan Yang","Chenqi Kong","Wenhan Yang","Shijian Lu","Yap-Peng Tan","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2505.05279v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2412.11039v2","updated":"2025-05-08T14:24:13Z","published":"2024-12-15T03:35:00Z","title":"AirMorph: Topology-Preserving Deep Learning for Pulmonary Airway\n  Analysis","summary":"  Accurate anatomical labeling and analysis of the pulmonary structure and its\nsurrounding anatomy from thoracic CT is getting increasingly important for\nunderstanding the etilogy of abnormalities or supporting targetted therapy and\nearly interventions. Whilst lung and airway cell atlases have been attempted,\nthere is a lack of fine-grained morphological atlases that are clinically\ndeployable. In this work, we introduce AirMorph, a robust, end-to-end deep\nlearning pipeline enabling fully automatic and comprehensive airway anatomical\nlabeling at lobar, segmental, and subsegmental resolutions that can be used to\ncreate digital atlases of the lung. Evaluated across large-scale multi-center\ndatasets comprising diverse pulmonary conditions, the AirMorph consistently\noutperformed existing segmentation and labeling methods in terms of accuracy,\ntopological consistency, and completeness. To simplify clinical interpretation,\nwe further introduce a compact anatomical signature quantifying critical\nmorphological airway features, including stenosis, ectasia, tortuosity,\ndivergence, length, and complexity. When applied to various pulmonary diseases\nsuch as pulmonary fibrosis, emphysema, atelectasis, consolidation, and\nreticular opacities, it demonstrates strong discriminative power, revealing\ndisease-specific morphological patterns with high interpretability and\nexplainability. Additionally, AirMorph supports efficient automated branching\npattern analysis, potentially enhancing bronchoscopic navigation planning and\nprocedural safety, offering a valuable clinical tool for improved diagnosis,\ntargeted treatment, and personalized patient care.\n","authors":["Minghui Zhang","Chenyu Li","Fangfang Xie","Yaoyu Liu","Hanxiao Zhang","Junyang Wu","Chunxi Zhang","Jie Yang","Jiayuan Sun","Guang-Zhong Yang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2412.11039v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2303.12484v5","updated":"2025-05-08T13:51:45Z","published":"2023-03-22T11:51:49Z","title":"Label-Efficient Deep Learning in Medical Image Analysis: Challenges and\n  Future Directions","summary":"  Deep learning has significantly advanced medical imaging analysis (MIA),\nachieving state-of-the-art performance across diverse clinical tasks. However,\nits success largely depends on large-scale, high-quality labeled datasets,\nwhich are costly and time-consuming to obtain due to the need for expert\nannotation. To mitigate this limitation, label-efficient deep learning methods\nhave emerged to improve model performance under limited supervision by\nleveraging labeled, unlabeled, and weakly labeled data. In this survey, we\nsystematically review over 350 peer-reviewed studies and present a\ncomprehensive taxonomy of label-efficient learning methods in MIA. These\nmethods are categorized into four labeling paradigms: no label, insufficient\nlabel, inexact label, and label refinement. For each category, we analyze\nrepresentative techniques across imaging modalities and clinical applications,\nhighlighting shared methodological principles and task-specific adaptations. We\nalso examine the growing role of health foundation models (HFMs) in enabling\nlabel-efficient learning through large-scale pre-training and transfer\nlearning, enhancing the use of limited annotations in downstream tasks.\nFinally, we identify current challenges and future directions to facilitate the\ntranslation of label-efficient learning from research promise to everyday\nclinical care.\n","authors":["Cheng Jin","Zhengrui Guo","Yi Lin","Luyang Luo","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12484v5.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2505.05248v1","updated":"2025-05-08T13:51:00Z","published":"2025-05-08T13:51:00Z","title":"White Light Specular Reflection Data Augmentation for Deep Learning\n  Polyp Detection","summary":"  Colorectal cancer is one of the deadliest cancers today, but it can be\nprevented through early detection of malignant polyps in the colon, primarily\nvia colonoscopies. While this method has saved many lives, human error remains\na significant challenge, as missing a polyp could have fatal consequences for\nthe patient. Deep learning (DL) polyp detectors offer a promising solution.\nHowever, existing DL polyp detectors often mistake white light reflections from\nthe endoscope for polyps, which can lead to false positives.To address this\nchallenge, in this paper, we propose a novel data augmentation approach that\nartificially adds more white light reflections to create harder training\nscenarios. Specifically, we first generate a bank of artificial lights using\nthe training dataset. Then we find the regions of the training images that we\nshould not add these artificial lights on. Finally, we propose a sliding window\nmethod to add the artificial light to the areas that fit of the training\nimages, resulting in augmented images. By providing the model with more\nopportunities to make mistakes, we hypothesize that it will also have more\nchances to learn from those mistakes, ultimately improving its performance in\npolyp detection. Experimental results demonstrate the effectiveness of our new\ndata augmentation method.\n","authors":["Jose Angel Nuñez","Fabian Vazquez","Diego Adame","Xiaoyan Fu","Pengfei Gu","Bin Fu"],"pdf_url":"https://arxiv.org/pdf/2505.05248v1.pdf","comment":"5 pages, 4 Figures, paper accepted by the ISBI (International\n  Symposium on Biomedical Imaging) 2025 Conference"},{"id":"http://arxiv.org/abs/2311.12421v3","updated":"2025-05-08T13:39:24Z","published":"2023-11-21T08:21:55Z","title":"Two Views Are Better than One: Monocular 3D Pose Estimation with\n  Multiview Consistency","summary":"  Deducing a 3D human pose from a single 2D image is inherently challenging\nbecause multiple 3D poses can correspond to the same 2D representation. 3D data\ncan resolve this pose ambiguity, but it is expensive to record and requires an\nintricate setup that is often restricted to controlled lab environments. We\npropose a method that improves the performance of deep learning-based monocular\n3D human pose estimation models by using multiview data only during training,\nbut not during inference. We introduce a novel loss function, consistency loss,\nwhich operates on two synchronized views. This approach is simpler than\nprevious models that require 3D ground truth or intrinsic and extrinsic camera\nparameters. Our consistency loss penalizes differences in two pose sequences\nafter rigid alignment. We also demonstrate that our consistency loss\nsubstantially improves performance for fine-tuning without requiring 3D data.\nFurthermore, we show that using our consistency loss can yield state-of-the-art\nperformance when training models from scratch in a semi-supervised manner. Our\nfindings provide a simple way to capture new data, e.g in a new domain. This\ndata can be added using off-the-shelf cameras with no calibration requirements.\nWe make all our code and data publicly available.\n","authors":["Christian Keilstrup Ingwersen","Rasmus Tirsgaard","Rasmus Nylander","Janus Nørtoft Jensen","Anders Bjorholm Dahl","Morten Rieger Hannemose"],"pdf_url":"https://arxiv.org/pdf/2311.12421v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05240v1","updated":"2025-05-08T13:36:07Z","published":"2025-05-08T13:36:07Z","title":"PADriver: Towards Personalized Autonomous Driving","summary":"  In this paper, we propose PADriver, a novel closed-loop framework for\npersonalized autonomous driving (PAD). Built upon Multi-modal Large Language\nModel (MLLM), PADriver takes streaming frames and personalized textual prompts\nas inputs. It autoaggressively performs scene understanding, danger level\nestimation and action decision. The predicted danger level reflects the risk of\nthe potential action and provides an explicit reference for the final action,\nwhich corresponds to the preset personalized prompt. Moreover, we construct a\nclosed-loop benchmark named PAD-Highway based on Highway-Env simulator to\ncomprehensively evaluate the decision performance under traffic rules. The\ndataset contains 250 hours videos with high-quality annotation to facilitate\nthe development of PAD behavior analysis. Experimental results on the\nconstructed benchmark show that PADriver outperforms state-of-the-art\napproaches on different evaluation metrics, and enables various driving modes.\n","authors":["Genghua Kou","Fan Jia","Weixin Mao","Yingfei Liu","Yucheng Zhao","Ziheng Zhang","Osamu Yoshie","Tiancai Wang","Ying Li","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.05240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16859v2","updated":"2025-05-08T13:34:48Z","published":"2024-08-29T18:49:32Z","title":"Evaluating Deep Learning Models for Breast Cancer Classification: A\n  Comparative Study","summary":"  This study evaluates the effectiveness of deep learning models in classifying\nhistopathological images for early and accurate detection of breast cancer.\nEight advanced models, including ResNet-50, DenseNet-121, ResNeXt-50, Vision\nTransformer (ViT), GoogLeNet (Inception v3), EfficientNet, MobileNet, and\nSqueezeNet, were compared using a dataset of 277,524 image patches. The Vision\nTransformer (ViT) model, with its attention-based mechanisms, achieved the\nhighest validation accuracy of 94%, outperforming conventional CNNs. The study\ndemonstrates the potential of advanced machine learning methods to enhance\nprecision and efficiency in breast cancer diagnosis in clinical settings.\n","authors":["Sania Eskandari","Ali Eslamian","Nusrat Munia","Amjad Alqarni","Qiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.16859v2.pdf","comment":"4 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2208.03571v3","updated":"2025-05-08T13:30:23Z","published":"2022-08-06T19:47:32Z","title":"Transformer-based assignment decision network for multiple object\n  tracking","summary":"  Data association is a crucial component for any multiple object tracking\n(MOT) method that follows the tracking-by-detection paradigm. To generate\ncomplete trajectories such methods employ a data association process to\nestablish assignments between detections and existing targets during each\ntimestep. Recent data association approaches try to solve either a\nmulti-dimensional linear assignment task or a network flow minimization problem\nor tackle it via multiple hypotheses tracking. However, during inference an\noptimization step that computes optimal assignments is required for every\nsequence frame inducing additional complexity to any given solution. To this\nend, in the context of this work we introduce Transformer-based Assignment\nDecision Network (TADN) that tackles data association without the need of any\nexplicit optimization during inference. In particular, TADN can directly infer\nassignment pairs between detections and active targets in a single forward pass\nof the network. We have integrated TADN in a rather simple MOT framework,\ndesigned a novel training strategy for efficient end-to-end training and\ndemonstrated the high potential of our approach for online visual\ntracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and\nUA-DETRAC. Our proposed approach demonstrates strong performance in most\nevaluation metrics despite its simple nature as a tracker lacking significant\nauxiliary components such as occlusion handling or re-identification. The\nimplementation of our method is publicly available at\nhttps://github.com/psaltaath/tadn-mot.\n","authors":["Athena Psalta","Vasileios Tsironis","Konstantinos Karantzalos"],"pdf_url":"https://arxiv.org/pdf/2208.03571v3.pdf","comment":"Preprint version. Under consideration at Computer Vision and Image\n  Understanding"},{"id":"http://arxiv.org/abs/2412.16698v3","updated":"2025-05-08T13:28:57Z","published":"2024-12-21T16:54:28Z","title":"Interact with me: Joint Egocentric Forecasting of Intent to Interact,\n  Attitude and Social Actions","summary":"  For efficient human-agent interaction, an agent should proactively recognize\ntheir target user and prepare for upcoming interactions. We formulate this\nchallenging problem as the novel task of jointly forecasting a person's intent\nto interact with the agent, their attitude towards the agent and the action\nthey will perform, from the agent's (egocentric) perspective. So we propose\n\\emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task\ndependencies through a hierarchical multitask learning approach. SocialEgoNet\nuses whole-body skeletons (keypoints from face, hands and body) extracted from\nonly 1 second of video input for high inference speed. For evaluation, we\naugment an existing egocentric human-agent interaction dataset with new class\nlabels and bounding box annotations. Extensive experiments on this augmented\ndataset, named JPL-Social, demonstrate \\emph{real-time} inference and superior\nperformance (average accuracy across all tasks: 83.15\\%) of our model\noutperforming several competitive baselines. The additional annotations and\ncode will be available upon acceptance.\n","authors":["Tongfei Bian","Yiming Ma","Mathieu Chollet","Victor Sanchez","Tanaya Guha"],"pdf_url":"https://arxiv.org/pdf/2412.16698v3.pdf","comment":"Accepted to ICME, 2025. Camera-ready Version"},{"id":"http://arxiv.org/abs/2505.05229v1","updated":"2025-05-08T13:21:10Z","published":"2025-05-08T13:21:10Z","title":"Does CLIP perceive art the same way we do?","summary":"  CLIP has emerged as a powerful multimodal model capable of connecting images\nand text through joint embeddings, but to what extent does it \"see\" the same\nway humans do - especially when interpreting artworks? In this paper, we\ninvestigate CLIP's ability to extract high-level semantic and stylistic\ninformation from paintings, including both human-created and AI-generated\nimagery. We evaluate its perception across multiple dimensions: content, scene\nunderstanding, artistic style, historical period, and the presence of visual\ndeformations or artifacts. By designing targeted probing tasks and comparing\nCLIP's responses to human annotations and expert benchmarks, we explore its\nalignment with human perceptual and contextual understanding. Our findings\nreveal both strengths and limitations in CLIP's visual representations,\nparticularly in relation to aesthetic cues and artistic intent. We further\ndiscuss the implications of these insights for using CLIP as a guidance\nmechanism during generative processes, such as style transfer or prompt-based\nimage synthesis. Our work highlights the need for deeper interpretability in\nmultimodal systems, especially when applied to creative domains where nuance\nand subjectivity play a central role.\n","authors":["Andrea Asperti","Leonardo Dessì","Maria Chiara Tonetti","Nico Wu"],"pdf_url":"https://arxiv.org/pdf/2505.05229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05223v1","updated":"2025-05-08T13:16:37Z","published":"2025-05-08T13:16:37Z","title":"Multi-Objective Reinforcement Learning for Adaptive Personalized\n  Autonomous Driving","summary":"  Human drivers exhibit individual preferences regarding driving style.\nAdapting autonomous vehicles to these preferences is essential for user trust\nand satisfaction. However, existing end-to-end driving approaches often rely on\npredefined driving styles or require continuous user feedback for adaptation,\nlimiting their ability to support dynamic, context-dependent preferences. We\npropose a novel approach using multi-objective reinforcement learning (MORL)\nwith preference-driven optimization for end-to-end autonomous driving that\nenables runtime adaptation to driving style preferences. Preferences are\nencoded as continuous weight vectors to modulate behavior along interpretable\nstyle objectives$\\unicode{x2013}$including efficiency, comfort, speed, and\naggressiveness$\\unicode{x2013}$without requiring policy retraining. Our\nsingle-policy agent integrates vision-based perception in complex mixed-traffic\nscenarios and is evaluated in diverse urban environments using the CARLA\nsimulator. Experimental results demonstrate that the agent dynamically adapts\nits driving behavior according to changing preferences while maintaining\nperformance in terms of collision avoidance and route completion.\n","authors":["Hendrik Surmann","Jorge de Heuvel","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2505.05223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05215v1","updated":"2025-05-08T13:09:34Z","published":"2025-05-08T13:09:34Z","title":"Diffusion Model Quantization: A Review","summary":"  Recent success of large text-to-image models has empirically underscored the\nexceptional performance of diffusion models in generative tasks. To facilitate\ntheir efficient deployment on resource-constrained edge devices, model\nquantization has emerged as a pivotal technique for both compression and\nacceleration. This survey offers a thorough review of the latest advancements\nin diffusion model quantization, encapsulating and analyzing the current state\nof the art in this rapidly advancing domain. First, we provide an overview of\nthe key challenges encountered in the quantization of diffusion models,\nincluding those based on U-Net architectures and Diffusion Transformers (DiT).\nWe then present a comprehensive taxonomy of prevalent quantization techniques,\nengaging in an in-depth discussion of their underlying principles.\nSubsequently, we perform a meticulous analysis of representative diffusion\nmodel quantization schemes from both qualitative and quantitative perspectives.\nFrom a quantitative standpoint, we rigorously benchmark a variety of methods\nusing widely recognized datasets, delivering an extensive evaluation of the\nmost recent and impactful research in the field. From a qualitative standpoint,\nwe categorize and synthesize the effects of quantization errors, elucidating\nthese impacts through both visual analysis and trajectory examination. In\nconclusion, we outline prospective avenues for future research, proposing novel\ndirections for the quantization of generative models in practical applications.\nThe list of related papers, corresponding codes, pre-trained models and\ncomparison results are publicly available at the survey project homepage\nhttps://github.com/TaylorJocelyn/Diffusion-Model-Quantization.\n","authors":["Qian Zeng","Chenggong Hu","Mingli Song","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2505.05215v1.pdf","comment":"40 pages, 8 figures"},{"id":"http://arxiv.org/abs/2505.05212v1","updated":"2025-05-08T13:05:07Z","published":"2025-05-08T13:05:07Z","title":"HQC-NBV: A Hybrid Quantum-Classical View Planning Approach","summary":"  Efficient view planning is a fundamental challenge in computer vision and\nrobotic perception, critical for tasks ranging from search and rescue\noperations to autonomous navigation. While classical approaches, including\nsampling-based and deterministic methods, have shown promise in planning camera\nviewpoints for scene exploration, they often struggle with computational\nscalability and solution optimality in complex settings. This study introduces\nHQC-NBV, a hybrid quantum-classical framework for view planning that leverages\nquantum properties to efficiently explore the parameter space while maintaining\nrobustness and scalability. We propose a specific Hamiltonian formulation with\nmulti-component cost terms and a parameter-centric variational ansatz with\nbidirectional alternating entanglement patterns that capture the hierarchical\ndependencies between viewpoint parameters. Comprehensive experiments\ndemonstrate that quantum-specific components provide measurable performance\nadvantages. Compared to the classical methods, our approach achieves up to\n49.2% higher exploration efficiency across diverse environments. Our analysis\nof entanglement architecture and coherence-preserving terms provides insights\ninto the mechanisms of quantum advantage in robotic exploration tasks. This\nwork represents a significant advancement in integrating quantum computing into\nrobotic perception systems, offering a paradigm-shifting solution for various\nrobot vision tasks.\n","authors":["Xiaotong Yu","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2505.05212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05209v1","updated":"2025-05-08T13:03:07Z","published":"2025-05-08T13:03:07Z","title":"EAM: Enhancing Anything with Diffusion Transformers for Blind\n  Super-Resolution","summary":"  Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind\nSuper-Resolution (BSR) has become a predominant approach in the field. While\nT2I models have traditionally relied on U-Net architectures, recent\nadvancements have demonstrated that Diffusion Transformers (DiT) achieve\nsignificantly higher performance in this domain. In this work, we introduce\nEnhancing Anything Model (EAM), a novel BSR method that leverages DiT and\noutperforms previous U-Net-based approaches. We introduce a novel block,\n$\\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This\nblock employs a low-resolution latent as a separable flow injection control,\nforming a triple-flow architecture that effectively leverages the prior\nknowledge embedded in the pre-trained DiT. To fully exploit the prior guidance\ncapabilities of T2I models and enhance their generalization in BSR, we\nintroduce a progressive Masked Image Modeling strategy, which also reduces\ntraining costs. Additionally, we propose a subject-aware prompt generation\nstrategy that employs a robust multi-modal model in an in-context learning\nframework. This strategy automatically identifies key image areas, provides\ndetailed descriptions, and optimizes the utilization of T2I diffusion priors.\nOur experiments demonstrate that EAM achieves state-of-the-art results across\nmultiple datasets, outperforming existing methods in both quantitative metrics\nand visual quality.\n","authors":["Haizhen Xie","Kunpeng Du","Qiangyu Yan","Sen Lu","Jianhong Han","Hanting Chen","Hailin Hu","Jie Hu"],"pdf_url":"https://arxiv.org/pdf/2505.05209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05208v1","updated":"2025-05-08T13:02:44Z","published":"2025-05-08T13:02:44Z","title":"Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep\n  Learning","summary":"  Early detection and accurate diagnosis are essential to improving patient\noutcomes. The use of convolutional neural networks (CNNs) for tumor detection\nhas shown promise, but existing models often suffer from overparameterization,\nwhich limits their performance gains. In this study, fuzzy sigmoid convolution\n(FSC) is introduced along with two additional modules: top-of-the-funnel and\nmiddle-of-the-funnel. The proposed methodology significantly reduces the number\nof trainable parameters without compromising classification accuracy. A novel\nconvolutional operator is central to this approach, effectively dilating the\nreceptive field while preserving input data integrity. This enables efficient\nfeature map reduction and enhances the model's tumor detection capability. In\nthe FSC-based model, fuzzy sigmoid activation functions are incorporated within\nconvolutional layers to improve feature extraction and classification. The\ninclusion of fuzzy logic into the architecture improves its adaptability and\nrobustness. Extensive experiments on three benchmark datasets demonstrate the\nsuperior performance and efficiency of the proposed model. The FSC-based\narchitecture achieved classification accuracies of 99.17%, 99.75%, and 99.89%\non three different datasets. The model employs 100 times fewer parameters than\nlarge-scale transfer learning architectures, highlighting its computational\nefficiency and suitability for detecting brain tumors early. This research\noffers lightweight, high-performance deep-learning models for medical imaging\napplications.\n","authors":["Muhammad Irfan","Anum Nawaz","Riku Klen","Abdulhamit Subasi","Tomi Westerlund","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2505.05208v1.pdf","comment":"IEEE IJCNN 2025 has accepted the paper"},{"id":"http://arxiv.org/abs/2505.05195v1","updated":"2025-05-08T12:52:02Z","published":"2025-05-08T12:52:02Z","title":"Concept-Based Unsupervised Domain Adaptation","summary":"  Concept Bottleneck Models (CBMs) enhance interpretability by explaining\npredictions through human-understandable concepts but typically assume that\ntraining and test data share the same distribution. This assumption often fails\nunder domain shifts, leading to degraded performance and poor generalization.\nTo address these limitations and improve the robustness of CBMs, we propose the\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\nto: (1) align concept representations across domains using adversarial\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\ndifferences in concept distributions, thereby preventing performance drop due\nto over-constraints of these distributions, (3) infer concepts directly in the\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\ndiverse domains, and (4) integrate concept learning into conventional domain\nadaptation (DA) with theoretical guarantees, improving interpretability and\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\ndatasets.\n","authors":["Xinyue Xu","Yueying Hu","Hui Tang","Yi Qin","Lu Mi","Hao Wang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.05195v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2504.16612v2","updated":"2025-05-08T12:46:58Z","published":"2025-04-23T10:54:32Z","title":"Federated EndoViT: Pretraining Vision Transformers via Federated\n  Learning on Endoscopic Image Collections","summary":"  Purpose: In this study, we investigate the training of foundation models\nusing federated learning to address data-sharing limitations and enable\ncollaborative model training without data transfer for minimally invasive\nsurgery. Methods: Inspired by the EndoViT study, we adapt the Masked\nAutoencoder for federated learning, enhancing it with adaptive Sharpness-Aware\nMinimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is\npretrained on the Endo700k dataset collection and later fine-tuned and\nevaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,\nand Surgical Phase Recognition. Results: Our findings demonstrate that\nintegrating adaptive FedSAM into the federated MAE approach improves\npretraining, leading to a reduction in reconstruction loss per patch. The\napplication of FL-EndoViT in surgical downstream tasks results in performance\ncomparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over\nCEN-EndoViT in surgical scene segmentation when data is limited and in action\ntriplet recognition when large datasets are used. Conclusion: These findings\nhighlight the potential of federated learning for privacy-preserving training\nof surgical foundation models, offering a robust and generalizable solution for\nsurgical data science. Effective collaboration requires adapting federated\nlearning methods, such as the integration of FedSAM, which can accommodate the\ninherent data heterogeneity across institutions. In future, exploring FL in\nvideo-based models may enhance these capabilities by incorporating\nspatiotemporal dynamics crucial for real-world surgical environments.\n","authors":["Max Kirchner","Alexander C. Jenke","Sebastian Bodenstedt","Fiona R. Kolbinger","Oliver L. Saldanha","Jakob N. Kather","Martin Wagner","Stefanie Speidel"],"pdf_url":"https://arxiv.org/pdf/2504.16612v2.pdf","comment":"Preprint submitted to IEEE TMI"},{"id":"http://arxiv.org/abs/2505.05189v1","updated":"2025-05-08T12:37:51Z","published":"2025-05-08T12:37:51Z","title":"Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language\n  Models","summary":"  Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}.\n","authors":["Wei Peng","Kang Liu","Jianchen Hu","Meng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.05189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05183v1","updated":"2025-05-08T12:33:48Z","published":"2025-05-08T12:33:48Z","title":"PaniCar: Securing the Perception of Advanced Driving Assistance Systems\n  Against Emergency Vehicle Lighting","summary":"  The safety of autonomous cars has come under scrutiny in recent years,\nespecially after 16 documented incidents involving Teslas (with autopilot\nengaged) crashing into parked emergency vehicles (police cars, ambulances, and\nfiretrucks). While previous studies have revealed that strong light sources\noften introduce flare artifacts in the captured image, which degrade the image\nquality, the impact of flare on object detection performance remains unclear.\nIn this research, we unveil PaniCar, a digital phenomenon that causes an object\ndetector's confidence score to fluctuate below detection thresholds when\nexposed to activated emergency vehicle lighting. This vulnerability poses a\nsignificant safety risk, and can cause autonomous vehicles to fail to detect\nobjects near emergency vehicles. In addition, this vulnerability could be\nexploited by adversaries to compromise the security of advanced driving\nassistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3,\n\"manufacturer C\", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors\n(YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle\nlighting to understand the influence of various technical and environmental\nfactors. We also evaluate four SOTA flare removal methods and show that their\nperformance and latency are insufficient for real-time driving constraints. To\nmitigate this risk, we propose Caracetamol, a robust framework designed to\nenhance the resilience of object detectors against the effects of activated\nemergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster\nRCNN, Caracetamol improves the models' average confidence of car detection by\n0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by\n0.33. In addition, Caracetamol is capable of processing frames at a rate of\nbetween 30-50 FPS, enabling real-time ADAS car detection.\n","authors":["Elad Feldman","Jacob Shams","Dudi Biton","Alfred Chen","Shaoyuan Xie","Satoru Koda","Yisroel Mirsky","Asaf Shabtai","Yuval Elovici","Ben Nassi"],"pdf_url":"https://arxiv.org/pdf/2505.05183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16111v3","updated":"2025-05-08T12:26:51Z","published":"2024-09-24T14:19:47Z","title":"CloudTrack: Scalable UAV Tracking with Cloud Semantics","summary":"  Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search and\nrescue scenarios to gather information in the search area. The automatic\nidentification of the person searched for in aerial footage could increase the\nautonomy of such systems, reduce the search time, and thus increase the missed\nperson's chances of survival. In this paper, we present a novel approach to\nperform semantically conditioned open vocabulary object tracking that is\nspecifically designed to cope with the limitations of UAV hardware. Our\napproach has several advantages. It can run with verbal descriptions of the\nmissing person, e.g., the color of the shirt, it does not require dedicated\ntraining to execute the mission and can efficiently track a potentially moving\nperson. Our experimental results demonstrate the versatility and efficacy of\nour approach.\n","authors":["Yannik Blei","Michael Krawez","Nisarga Nilavadi","Tanja Katharina Kaiser","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2409.16111v3.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.04497v2","updated":"2025-05-08T11:59:21Z","published":"2025-05-07T15:20:17Z","title":"Defining and Quantifying Creative Behavior in Popular Image Generators","summary":"  Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition.\n","authors":["Aditi Ramaswamy","Hana Chockler","Melane Navaratnarajah"],"pdf_url":"https://arxiv.org/pdf/2505.04497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05163v1","updated":"2025-05-08T11:57:35Z","published":"2025-05-08T11:57:35Z","title":"Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty\n  Quantification with Gaussian Process Latent Variable Models","summary":"  Vision-Language Models (VLMs) learn joint representations by mapping images\nand text into a shared latent space. However, recent research highlights that\ndeterministic embeddings from standard VLMs often struggle to capture the\nuncertainties arising from the ambiguities in visual and textual descriptions\nand the multiple possible correspondences between images and texts. Existing\napproaches tackle this by learning probabilistic embeddings during VLM\ntraining, which demands large datasets and does not leverage the powerful\nrepresentations already learned by large-scale VLMs like CLIP. In this paper,\nwe propose GroVE, a post-hoc approach to obtaining probabilistic embeddings\nfrom frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model\n(GPLVM) to learn a shared low-dimensional latent space where image and text\ninputs are mapped to a unified representation, optimized through single-modal\nembedding reconstruction and cross-modal alignment objectives. Once trained,\nthe Gaussian Process model generates uncertainty-aware probabilistic\nembeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty\ncalibration across multiple downstream tasks, including cross-modal retrieval,\nvisual question answering, and active learning.\n","authors":["Aishwarya Venkataramanan","Paul Bodesheim","Joachim Denzler"],"pdf_url":"https://arxiv.org/pdf/2505.05163v1.pdf","comment":"UAI 2025, 22 pages"},{"id":"http://arxiv.org/abs/2505.05137v1","updated":"2025-05-08T11:19:08Z","published":"2025-05-08T11:19:08Z","title":"Research on Anomaly Detection Methods Based on Diffusion Models","summary":"  Anomaly detection is a fundamental task in machine learning and data mining,\nwith significant applications in cybersecurity, industrial fault diagnosis, and\nclinical disease monitoring. Traditional methods, such as statistical modeling\nand machine learning-based approaches, often face challenges in handling\ncomplex, high-dimensional data distributions. In this study, we explore the\npotential of diffusion models for anomaly detection, proposing a novel\nframework that leverages the strengths of diffusion probabilistic models (DPMs)\nto effectively identify anomalies in both image and audio data. The proposed\nmethod models the distribution of normal data through a diffusion process and\nreconstructs input data via reverse diffusion, using a combination of\nreconstruction errors and semantic discrepancies as anomaly indicators. To\nenhance the framework's performance, we introduce multi-scale feature\nextraction, attention mechanisms, and wavelet-domain representations, enabling\nthe model to capture fine-grained structures and global dependencies in the\ndata. Extensive experiments on benchmark datasets, including MVTec AD and\nUrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly\ndetection techniques, achieving superior accuracy and robustness across diverse\ndata modalities. This research highlights the effectiveness of diffusion models\nin anomaly detection and provides a robust and efficient solution for\nreal-world applications.\n","authors":["Yi Chen"],"pdf_url":"https://arxiv.org/pdf/2505.05137v1.pdf","comment":"6 pages, 3 table"},{"id":"http://arxiv.org/abs/2505.05136v1","updated":"2025-05-08T11:13:38Z","published":"2025-05-08T11:13:38Z","title":"Automated vision-based assistance tools in bronchoscopy: stenosis\n  severity estimation","summary":"  Purpose: Subglottic stenosis refers to the narrowing of the subglottis, the\nairway between the vocal cords and the trachea. Its severity is typically\nevaluated by estimating the percentage of obstructed airway. This estimation\ncan be obtained from CT data or through visual inspection by experts exploring\nthe region. However, visual inspections are inherently subjective, leading to\nless consistent and robust diagnoses. No public methods or datasets are\ncurrently available for automated evaluation of this condition from\nbronchoscopy video.\n  Methods: We propose a pipeline for automated subglottic stenosis severity\nestimation during the bronchoscopy exploration, without requiring the physician\nto traverse the stenosed region. Our approach exploits the physical effect of\nillumination decline in endoscopy to segment and track the lumen and obtain a\n3D model of the airway. This 3D model is obtained from a single frame and is\nused to measure the airway narrowing.\n  Results: Our pipeline is the first to enable automated and robust subglottic\nstenosis severity measurement using bronchoscopy images. The results show\nconsistency with ground-truth estimations from CT scans and expert estimations,\nand reliable repeatability across multiple estimations on the same patient. Our\nevaluation is performed on our new Subglottic Stenosis Dataset of real\nbronchoscopy procedures data.\n  Conclusion: We demonstrate how to automate evaluation of subglottic stenosis\nseverity using only bronchoscopy. Our approach can assist with and shorten\ndiagnosis and monitoring procedures, with automated and repeatable estimations\nand less exploration time, and save radiation exposure to patients as no CT is\nrequired. Additionally, we release the first public benchmark for subglottic\nstenosis severity assessment.\n","authors":["Clara Tomasini","Javier Rodriguez-Puigvert","Dinora Polanco","Manuel Viñuales","Luis Riazuelo","Ana Cristina Murillo"],"pdf_url":"https://arxiv.org/pdf/2505.05136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05132v1","updated":"2025-05-08T11:09:49Z","published":"2025-05-08T11:09:49Z","title":"An Active Contour Model for Silhouette Vectorization using Bézier\n  Curves","summary":"  In this paper, we propose an active contour model for silhouette\nvectorization using cubic B\\'ezier curves. Among the end points of the B\\'ezier\ncurves, we distinguish between corner and regular points where the orientation\nof the tangent vector is prescribed. By minimizing the distance of the B\\'ezier\ncurves to the silhouette boundary, the active contour model optimizes the\nlocation of the B\\'ezier curves end points, the orientation of the tangent\nvectors in the regular points, and the estimation of the B\\'ezier curve\nparameters. This active contour model can use the silhouette vectorization\nobtained by any method as an initial guess. The proposed method significantly\nreduces the average distance between the silhouette boundary and its\nvectorization obtained by the world-class graphic software Inkscape, Adobe\nIllustrator, and a curvature-based vectorization method, which we introduce for\ncomparison. Our method also allows us to impose additional regularity on the\nB\\'ezier curves by reducing their lengths.\n","authors":["Luis Alvarez","Jean-Michel Morel"],"pdf_url":"https://arxiv.org/pdf/2505.05132v1.pdf","comment":"14 pages, 5 figures and 1 table"},{"id":"http://arxiv.org/abs/2503.10042v2","updated":"2025-05-08T10:40:47Z","published":"2025-03-13T04:48:43Z","title":"How Do Multimodal Large Language Models Handle Complex Multimodal\n  Reasoning? Placing Them in An Extensible Escape Game","summary":"  The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred\ninterest in complex multimodal reasoning tasks in the real-world and virtual\nenvironment, which require coordinating multiple abilities, including visual\nperception, visual reasoning, spatial awareness, and target deduction. However,\nexisting evaluations primarily assess the final task completion, often\ndegrading assessments to isolated abilities such as visual grounding and visual\nquestion answering. Less attention is given to comprehensively and\nquantitatively analyzing reasoning process in multimodal environments, which is\ncrucial for understanding model behaviors and underlying reasoning mechanisms\nbeyond merely task success. To address this, we introduce MM-Escape, an\nextensible benchmark for investigating multimodal reasoning, inspired by\nreal-world escape games. MM-Escape emphasizes intermediate model behaviors\nalongside final task completion. To achieve this, we develop EscapeCraft, a\ncustomizable and open environment that enables models to engage in free-form\nexploration for assessing multimodal reasoning. Extensive experiments show that\nMLLMs, regardless of scale, can successfully complete the simplest room escape\ntasks, with some exhibiting human-like exploration strategies. Yet, performance\ndramatically drops as task difficulty increases. Moreover, we observe that\nperformance bottlenecks vary across models, revealing distinct failure modes\nand limitations in their multimodal reasoning abilities, such as repetitive\ntrajectories without adaptive exploration, getting stuck in corners due to poor\nvisual spatial awareness, and ineffective use of acquired props, such as the\nkey. We hope our work sheds light on new challenges in multimodal reasoning,\nand uncovers potential improvements in MLLMs capabilities.\n","authors":["Ziyue Wang","Yurui Dong","Fuwen Luo","Minyuan Ruan","Zhili Cheng","Chi Chen","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.10042v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05112v1","updated":"2025-05-08T10:27:12Z","published":"2025-05-08T10:27:12Z","title":"MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for\n  PET Denoising","summary":"  Acquiring high-quality Positron Emission Tomography (PET) images requires\nadministering high-dose radiotracers, which increases radiation exposure risks.\nGenerating standard-dose PET (SPET) from low-dose PET (LPET) has become a\npotential solution. However, previous studies have primarily focused on single\nlow-dose PET denoising, neglecting two critical factors: discrepancies in dose\nresponse caused by inter-patient variability, and complementary anatomical\nconstraints derived from CT images. In this work, we propose a novel CT-Guided\nMulti-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for\nmulti-dose PET denoising. Our approach integrates anatomical guidance and\ndose-level adaptation to achieve superior denoising performance under low-dose\nconditions. Specifically, this approach incorporates a CT-Guided High-frequency\nWavelet Attention (HWA) module, which uses wavelet transforms to separate\nhigh-frequency anatomical boundary features from CT images. These extracted\nfeatures are then incorporated into PET imaging through an adaptive weighted\nfusion mechanism to enhance edge details. Additionally, we propose the\nDose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism\nthat dynamically integrates dose levels into channel-spatial attention weight\ncalculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets\ndemonstrate that MDAA-Diff outperforms state-of-the-art approaches in\npreserving diagnostic quality under reduced-dose conditions. Our code is\npublicly available.\n","authors":["Xiaolong Niu","Zanting Ye","Xu Han","Yanchao Huang","Hao Sun","Hubing Wu","Lijun Lu"],"pdf_url":"https://arxiv.org/pdf/2505.05112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09507v2","updated":"2025-05-08T10:03:57Z","published":"2024-12-12T17:55:00Z","title":"Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction","summary":"  Indoor pathloss prediction is a fundamental task in wireless network\nplanning, yet it remains challenging due to environmental complexity and data\nscarcity. In this work, we propose a deep learning-based approach utilizing a\nvision transformer (ViT) architecture with DINO-v2 pretrained weights to model\nindoor radio propagation. Our method processes a floor map with additional\nfeatures of the walls to generate indoor pathloss maps. We systematically\nevaluate the effects of architectural choices, data augmentation strategies,\nand feature engineering techniques. Our findings indicate that extensive\naugmentation significantly improves generalization, while feature engineering\nis crucial in low-data regimes. Through comprehensive experiments, we\ndemonstrate the robustness of our model across different generalization\nscenarios.\n","authors":["Rafayel Mkrtchyan","Edvard Ghukasyan","Khoren Petrosyan","Hrant Khachatrian","Theofanis P. Raptis"],"pdf_url":"https://arxiv.org/pdf/2412.09507v2.pdf","comment":"Work partly supported by the RA Science Committee grant No. 22rl-052\n  (DISTAL) and the EU under Italian National Recovery and Resilience Plan of\n  NextGenerationEU on \"Telecommunications of the Future\" (PE00000001 - program\n  \"RESTART\")"},{"id":"http://arxiv.org/abs/2505.05101v1","updated":"2025-05-08T10:01:14Z","published":"2025-05-08T10:01:14Z","title":"MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via\n  Diffusion Models","summary":"  Multi-object editing aims to modify multiple objects or regions in complex\nscenes while preserving structural coherence. This task faces significant\nchallenges in scenarios involving overlapping or interacting objects: (1)\nInaccurate localization of target objects due to attention misalignment,\nleading to incomplete or misplaced edits; (2) Attribute-object mismatch, where\ncolor or texture changes fail to align with intended regions due to\ncross-attention leakage, creating semantic conflicts (\\textit{e.g.}, color\nbleeding into non-target areas). Existing methods struggle with these\nchallenges: approaches relying on global cross-attention mechanisms suffer from\nattention dilution and spatial interference between objects, while mask-based\nmethods fail to bind attributes to geometrically accurate regions due to\nfeature entanglement in multi-object scenarios. To address these limitations,\nwe propose a training-free, inference-stage optimization approach that enables\nprecise localized image manipulation in complex multi-object scenes, named\nMDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via\ntwo key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention\nwith segmentation masks for precise object positioning, and Color Consistency\nLoss (CCL) amplifies target attribute attention within masks while suppressing\nleakage to adjacent regions. This dual-loss design ensures localized and\ncoherent multi-object edits. Extensive experiments demonstrate that MDE-Edit\noutperforms state-of-the-art methods in editing accuracy and visual quality,\noffering a robust solution for complex multi-object image manipulation tasks.\n","authors":["Hongyang Zhu","Haipeng Liu","Bo Fu","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.05101v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2505.05098v1","updated":"2025-05-08T09:52:55Z","published":"2025-05-08T09:52:55Z","title":"X-Driver: Explainable Autonomous Driving with Vision-Language Models","summary":"  End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving.\n","authors":["Wei Liu","Jiyuan Zhang","Binxiong Zheng","Yufeng Hu","Yingzhan Lin","Zengfeng Zeng"],"pdf_url":"https://arxiv.org/pdf/2505.05098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02393v2","updated":"2025-05-08T09:44:41Z","published":"2025-05-05T06:33:20Z","title":"Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection","summary":"  Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD.\n","authors":["Sungheon Jeong","Jihong Park","Mohsen Imani"],"pdf_url":"https://arxiv.org/pdf/2505.02393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05091v1","updated":"2025-05-08T09:40:17Z","published":"2025-05-08T09:40:17Z","title":"DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions","summary":"  Deep learning (DL) has surpassed human performance on standard benchmarks,\ndriving its widespread adoption in computer vision tasks. One such task is\ndisparity estimation, estimating the disparity between matching pixels in\nstereo image pairs, which is crucial for safety-critical applications like\nmedical surgeries and autonomous navigation. However, DL-based disparity\nestimation methods are highly susceptible to distribution shifts and\nadversarial attacks, raising concerns about their reliability and\ngeneralization. Despite these concerns, a standardized benchmark for evaluating\nthe robustness of disparity estimation methods remains absent, hindering\nprogress in the field.\n  To address this gap, we introduce DispBench, a comprehensive benchmarking\ntool for systematically assessing the reliability of disparity estimation\nmethods. DispBench evaluates robustness against synthetic image corruptions\nsuch as adversarial attacks and out-of-distribution shifts caused by 2D Common\nCorruptions across multiple datasets and diverse corruption scenarios. We\nconduct the most extensive performance and robustness analysis of disparity\nestimation methods to date, uncovering key correlations between accuracy,\nreliability, and generalization. Open-source code for DispBench:\nhttps://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation\n","authors":["Shashank Agnihotri","Amaan Ansari","Annika Dackermann","Fabian Rösch","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2505.05091v1.pdf","comment":"Accepted at CVPR 2025 Workshop on Synthetic Data for Computer Vision"},{"id":"http://arxiv.org/abs/2505.05089v1","updated":"2025-05-08T09:39:19Z","published":"2025-05-08T09:39:19Z","title":"Nonlinear Motion-Guided and Spatio-Temporal Aware Network for\n  Unsupervised Event-Based Optical Flow","summary":"  Event cameras have the potential to capture continuous motion information\nover time and space, making them well-suited for optical flow estimation.\nHowever, most existing learning-based methods for event-based optical flow\nadopt frame-based techniques, ignoring the spatio-temporal characteristics of\nevents. Additionally, these methods assume linear motion between consecutive\nevents within the loss time window, which increases optical flow errors in\nlong-time sequences. In this work, we observe that rich spatio-temporal\ninformation and accurate nonlinear motion between events are crucial for\nevent-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel\nunsupervised event-based optical flow network focusing on long-time sequences.\nWe propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an\nAdaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich\nspatio-temporal information to learn spatio-temporal data associations.\nMeanwhile, we propose a nonlinear motion compensation loss that utilizes the\naccurate nonlinear motion between events to improve the unsupervised learning\nof our network. Extensive experiments demonstrate the effectiveness and\nsuperiority of our method. Remarkably, our method ranks first among\nunsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project\npage is available at https://wynelio.github.io/E-NMSTFlow.\n","authors":["Zuntao Liu","Hao Zhuang","Junjie Jiang","Yuhang Song","Zheng Fang"],"pdf_url":"https://arxiv.org/pdf/2505.05089v1.pdf","comment":"Accepted to ICRA 2025. Project Page:\n  https://wynelio.github.io/E-NMSTFlow"},{"id":"http://arxiv.org/abs/2505.05088v1","updated":"2025-05-08T09:36:49Z","published":"2025-05-08T09:36:49Z","title":"SSH-Net: A Self-Supervised and Hybrid Network for Noisy Image Watermark\n  Removal","summary":"  Visible watermark removal is challenging due to its inherent complexities and\nthe noise carried within images. Existing methods primarily rely on supervised\nlearning approaches that require paired datasets of watermarked and\nwatermark-free images, which are often impractical to obtain in real-world\nscenarios. To address this challenge, we propose SSH-Net, a Self-Supervised and\nHybrid Network specifically designed for noisy image watermark removal. SSH-Net\nsynthesizes reference watermark-free images using the watermark distribution in\na self-supervised manner and adopts a dual-network design to address the task.\nThe upper network, focused on the simpler task of noise removal, employs a\nlightweight CNN-based architecture, while the lower network, designed to handle\nthe more complex task of simultaneously removing watermarks and noise,\nincorporates Transformer blocks to model long-range dependencies and capture\nintricate image features. To enhance the model's effectiveness, a shared\nCNN-based feature encoder is introduced before dual networks to extract common\nfeatures that both networks can leverage. Our code will be available at\nhttps://github.com/wenyang001/SSH-Net.\n","authors":["Wenyang Liu","Jianjun Gao","Kim-Hui Yap"],"pdf_url":"https://arxiv.org/pdf/2505.05088v1.pdf","comment":"Under Review in JVCI"},{"id":"http://arxiv.org/abs/2412.03093v2","updated":"2025-05-08T09:35:41Z","published":"2024-12-04T07:44:58Z","title":"Expanding Event Modality Applications through a Robust CLIP-Based\n  Encoder","summary":"  This paper introduces a powerful encoder that transfers CLIP`s capabilities\nto event-based data, enhancing its utility and expanding its applicability\nacross diverse domains. While large-scale datasets have significantly advanced\nimage-based models, the scarcity of comprehensive event datasets has limited\nperformance potential in event modality. To address this challenge, we adapt\nCLIP`s architecture to align event embeddings with image embeddings, supporting\nzero-shot learning and preserving text alignment while mitigating catastrophic\nforgetting. Our encoder achieves strong performance in object recognition, with\ncompetitive results in zero-shot and few-shot learning tasks. Notably, it\ngeneralizes effectively to events extracted from video data without requiring\nadditional training, highlighting its versatility. Additionally, we integrate\nthis encoder within a cross-modality framework that facilitates interaction\nacross five modalities-Image, Event, Text, Sound, and Depth-expanding the\npossibilities for cross-modal applications. Overall, this work underscores the\ntransformative potential of a robust event encoder, broadening the scope and\nutility of event-based data across various fields.\n","authors":["Sungheon Jeong","Hanning Chen","Sanggeon Yun","Suhyeon Cho","Wenjun Huang","Xiangjian Liu","Mohsen Imani"],"pdf_url":"https://arxiv.org/pdf/2412.03093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05081v1","updated":"2025-05-08T09:26:28Z","published":"2025-05-08T09:26:28Z","title":"PIDiff: Image Customization for Personalized Identities with Diffusion\n  Models","summary":"  Text-to-image generation for personalized identities aims at incorporating\nthe specific identity into images using a text prompt and an identity image.\nBased on the powerful generative capabilities of DDPMs, many previous works\nadopt additional prompts, such as text embeddings and CLIP image embeddings, to\nrepresent the identity information, while they fail to disentangle the identity\ninformation and background information. As a result, the generated images not\nonly lose key identity characteristics but also suffer from significantly\nreduced diversity. To address this issue, previous works have combined the W+\nspace from StyleGAN with diffusion models, leveraging this space to provide a\nmore accurate and comprehensive representation of identity features through\nmulti-level feature extraction. However, the entanglement of identity and\nbackground information in in-the-wild images during training prevents accurate\nidentity localization, resulting in severe semantic interference between\nidentity and background. In this paper, we propose a novel fine-tuning-based\ndiffusion model for personalized identities text-to-image generation, named\nPIDiff, which leverages the W+ space and an identity-tailored fine-tuning\nstrategy to avoid semantic entanglement and achieves accurate feature\nextraction and localization. Style editing can also be achieved by PIDiff\nthrough preserving the characteristics of identity features in the W+ space,\nwhich vary from coarse to fine. Through the combination of the proposed\ncross-attention block and parameter optimization strategy, PIDiff preserves the\nidentity information and maintains the generation capability for in-the-wild\nimages of the pre-trained model during inference. Our experimental results\nvalidate the effectiveness of our method in this task.\n","authors":["Jinyu Gu","Haipeng Liu","Meng Wang","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.05081v1.pdf","comment":"9 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.11895v2","updated":"2025-05-08T09:24:41Z","published":"2025-04-16T09:21:34Z","title":"Search is All You Need for Few-shot Anomaly Detection","summary":"  Few-shot anomaly detection (FSAD) has emerged as a crucial yet challenging\ntask in industrial inspection, where normal distribution modeling must be\naccomplished with only a few normal images. While existing approaches typically\nemploy multi-modal foundation models combining language and vision modalities\nfor prompt-guided anomaly detection, these methods often demand sophisticated\nprompt engineering and extensive manual tuning. In this paper, we demonstrate\nthat a straightforward nearest-neighbor search framework can surpass\nstate-of-the-art performance in both single-class and multi-class FSAD\nscenarios. Our proposed method, VisionAD, consists of four simple yet essential\ncomponents: (1) scalable vision foundation models that extract universal and\ndiscriminative features; (2) dual augmentation strategies - support\naugmentation to enhance feature matching adaptability and query augmentation to\naddress the oversights of single-view prediction; (3) multi-layer feature\nintegration that captures both low-frequency global context and high-frequency\nlocal details with minimal computational overhead; and (4) a class-aware visual\nmemory bank enabling efficient one-for-all multi-class detection. Extensive\nevaluations across MVTec-AD, VisA, and Real-IAD benchmarks demonstrate\nVisionAD's exceptional performance. Using only 1 normal images as support, our\nmethod achieves remarkable image-level AUROC scores of 97.4%, 94.8%, and 70.8%\nrespectively, outperforming current state-of-the-art approaches by significant\nmargins (+1.6%, +3.2%, and +1.4%). The training-free nature and superior\nfew-shot capabilities of VisionAD make it particularly appealing for real-world\napplications where samples are scarce or expensive to obtain. Code is available\nat https://github.com/Qiqigeww/VisionAD.\n","authors":["Qishan Wang","Jia Guo","Shuyong Gao","Haofen Wang","Li Xiong","Junjie Hu","Hanqi Guo","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.11895v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.21699v2","updated":"2025-05-08T09:23:41Z","published":"2025-04-30T14:43:38Z","title":"REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud\n  De-raining","summary":"  Sensor degradation poses a significant challenge in autonomous driving.\nDuring heavy rainfall, the interference from raindrops can adversely affect the\nquality of LiDAR point clouds, resulting in, for instance, inaccurate point\nmeasurements. This, in turn, can potentially lead to safety concerns if\nautonomous driving systems are not weather-aware, i.e., if they are unable to\ndiscern such changes. In this study, we release a new, large-scale, multi-modal\nemulated rain dataset, REHEARSE-3D, to promote research advancements in 3D\npoint cloud de-raining. Distinct from the most relevant competitors, our\ndataset is unique in several respects. First, it is the largest point-wise\nannotated dataset, and second, it is the only one with high-resolution LiDAR\ndata (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and\nnighttime conditions in a controlled weather environment. Furthermore,\nREHEARSE-3D involves rain-characteristic information, which is of significant\nvalue not only for sensor noise modeling but also for analyzing the impact of\nweather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop\ndetection and removal in fused LiDAR and 4D Radar point clouds. Our\ncomprehensive study further evaluates the performance of various statistical\nand deep-learning models. Upon publication, the dataset and benchmark models\nwill be made publicly available at: https://sporsho.github.io/REHEARSE3D.\n","authors":["Abu Mohammed Raisuddin","Jesper Holmblad","Hamed Haghighi","Yuri Poledna","Maikol Funk Drechsler","Valentina Donzella","Eren Erdal Aksoy"],"pdf_url":"https://arxiv.org/pdf/2504.21699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05076v1","updated":"2025-05-08T09:16:01Z","published":"2025-05-08T09:16:01Z","title":"The City that Never Settles: Simulation-based LiDAR Dataset for\n  Long-Term Place Recognition Under Extreme Structural Changes","summary":"  Large-scale construction and demolition significantly challenge long-term\nplace recognition (PR) by drastically reshaping urban and suburban\nenvironments. Existing datasets predominantly reflect limited or indoor-focused\nchanges, failing to adequately represent extensive outdoor transformations. To\nbridge this gap, we introduce the City that Never Settles (CNS) dataset, a\nsimulation-based dataset created using the CARLA simulator, capturing major\nstructural changes-such as building construction and demolition-across diverse\nmaps and sequences. Additionally, we propose TCR_sym, a symmetric version of\nthe original TCR metric, enabling consistent measurement of structural changes\nirrespective of source-target ordering. Quantitative comparisons demonstrate\nthat CNS encompasses more extensive transformations than current real-world\nbenchmarks. Evaluations of state-of-the-art LiDAR-based PR methods on CNS\nreveal substantial performance degradation, underscoring the need for robust\nalgorithms capable of handling significant environmental changes. Our dataset\nis available at https://github.com/Hyunho111/CNS_dataset.\n","authors":["Hyunho Song","Dongjae Lee","Seunghun Oh","Minwoo Jung","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2505.05076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05074v1","updated":"2025-05-08T09:10:05Z","published":"2025-05-08T09:10:05Z","title":"Visual Affordances: Enabling Robots to Understand Object Functionality","summary":"  Human-robot interaction for assistive technologies relies on the prediction\nof affordances, which are the potential actions a robot can perform on objects.\nPredicting object affordances from visual perception is formulated differently\nfor tasks such as grasping detection, affordance classification, affordance\nsegmentation, and hand-object interaction synthesis. In this work, we highlight\nthe reproducibility issue in these redefinitions, making comparative benchmarks\nunfair and unreliable. To address this problem, we propose a unified\nformulation for visual affordance prediction, provide a comprehensive and\nsystematic review of previous works highlighting strengths and limitations of\nmethods and datasets, and analyse what challenges reproducibility. To favour\ntransparency, we introduce the Affordance Sheet, a document to detail the\nproposed solution, the datasets, and the validation. As the physical properties\nof an object influence the interaction with the robot, we present a generic\nframework that links visual affordance prediction to the physical world. Using\nthe weight of an object as an example for this framework, we discuss how\nestimating object mass can affect the affordance prediction. Our approach\nbridges the gap between affordance perception and robot actuation, and accounts\nfor the complete information about objects of interest and how the robot\ninteracts with them to accomplish its task.\n","authors":["Tommaso Apicella","Alessio Xompero","Andrea Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2505.05074v1.pdf","comment":"24 pages, 12 figures, 10 tables. Project website at\n  https://apicis.github.io/aff-survey/"},{"id":"http://arxiv.org/abs/2505.05073v1","updated":"2025-05-08T09:08:58Z","published":"2025-05-08T09:08:58Z","title":"RepSNet: A Nucleus Instance Segmentation model based on Boundary\n  Regression and Structural Re-parameterization","summary":"  Pathological diagnosis is the gold standard for tumor diagnosis, and nucleus\ninstance segmentation is a key step in digital pathology analysis and\npathological diagnosis. However, the computational efficiency of the model and\nthe treatment of overlapping targets are the major challenges in the studies of\nthis problem. To this end, a neural network model RepSNet was designed based on\na nucleus boundary regression and a structural re-parameterization scheme for\nsegmenting and classifying the nuclei in H\\&E-stained histopathological images.\nFirst, RepSNet estimates the boundary position information (BPI) of the parent\nnucleus for each pixel. The BPI estimation incorporates the local information\nof the pixel and the contextual information of the parent nucleus. Then, the\nnucleus boundary is estimated by aggregating the BPIs from a series of pixels\nusing a proposed boundary voting mechanism (BVM), and the instance segmentation\nresults are computed from the estimated nucleus boundary using a connected\ncomponent analysis procedure. The BVM intrinsically achieves a kind of\nsynergistic belief enhancement among the BPIs from various pixels. Therefore,\ndifferent from the methods available in literature that obtain nucleus\nboundaries based on a direct pixel recognition scheme, RepSNet computes its\nboundary decisions based on some guidances from macroscopic information using\nan integration mechanism. In addition, RepSNet employs a re-parametrizable\nencoder-decoder structure. This model can not only aggregate features from some\nreceptive fields with various scales which helps segmentation accuracy\nimprovement, but also reduce the parameter amount and computational burdens in\nthe model inference phase through the structural re-parameterization technique.\nExtensive experiments demonstrated the superiorities of RepSNet compared to\nseveral typical benchmark models.\n","authors":["Shengchun Xiong","Xiangru Li","Yunpeng Zhong","Wanfen Peng"],"pdf_url":"https://arxiv.org/pdf/2505.05073v1.pdf","comment":"25 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2504.21435v2","updated":"2025-05-08T09:08:01Z","published":"2025-04-30T08:48:21Z","title":"SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding","summary":"  With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\nstandalone videos and mainly assess \"visual elements\" like human actions and\nobject states. In reality, contemporary videos often encompass complex and\ncontinuous narratives, typically presented as a series. To address this\nchallenge, we propose SeriesBench, a benchmark consisting of 105 carefully\ncurated narrative-driven series, covering 28 specialized tasks that require\ndeep narrative understanding. Specifically, we first select a diverse set of\ndrama series spanning various genres. Then, we introduce a novel long-span\nnarrative annotation method, combined with a full-information transformation\napproach to convert manual annotations into diverse task formats. To further\nenhance model capacity for detailed analysis of plot structures and character\nrelationships within series, we propose a novel narrative reasoning framework,\nPC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still\nface significant challenges in understanding narrative-driven series, while\nPC-DCoT enables these MLLMs to achieve performance improvements. Overall, our\nSeriesBench and PC-DCoT highlight the critical necessity of advancing model\ncapabilities to understand narrative-driven series, guiding the future\ndevelopment of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025.\n","authors":["Chenkai Zhang","Yiming Lei","Zeming Liu","Haitao Leng","Shaoguo Liu","Tingting Gao","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2504.21435v2.pdf","comment":"29 pages, 15 figures, CVPR 2025"},{"id":"http://arxiv.org/abs/2505.05071v1","updated":"2025-05-08T09:06:53Z","published":"2025-05-08T09:06:53Z","title":"FG-CLIP: Fine-Grained Visual and Textual Alignment","summary":"  Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.\n","authors":["Chunyu Xie","Bin Wang","Fanjing Kong","Jincheng Li","Dawei Liang","Gengshen Zhang","Dawei Leng","Yuhui Yin"],"pdf_url":"https://arxiv.org/pdf/2505.05071v1.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2505.02784v3","updated":"2025-05-08T09:03:34Z","published":"2025-05-05T16:54:04Z","title":"Advances in Automated Fetal Brain MRI Segmentation and Biometry:\n  Insights from the FeTA 2024 Challenge","summary":"  Accurate fetal brain tissue segmentation and biometric analysis are essential\nfor studying brain development in utero. The FeTA Challenge 2024 advanced\nautomated fetal brain MRI analysis by introducing biometry prediction as a new\ntask alongside tissue segmentation. For the first time, our diverse\nmulti-centric test set included data from a new low-field (0.55T) MRI dataset.\nEvaluation metrics were also expanded to include the topology-specific Euler\ncharacteristic difference (ED). Sixteen teams submitted segmentation methods,\nmost of which performed consistently across both high- and low-field scans.\nHowever, longitudinal trends indicate that segmentation accuracy may be\nreaching a plateau, with results now approaching inter-rater variability. The\nED metric uncovered topological differences that were missed by conventional\nmetrics, while the low-field dataset achieved the highest segmentation scores,\nhighlighting the potential of affordable imaging systems when paired with\nhigh-quality reconstruction. Seven teams participated in the biometry task, but\nmost methods failed to outperform a simple baseline that predicted measurements\nbased solely on gestational age, underscoring the challenge of extracting\nreliable biometric estimates from image data alone. Domain shift analysis\nidentified image quality as the most significant factor affecting model\ngeneralization, with super-resolution pipelines also playing a substantial\nrole. Other factors, such as gestational age, pathology, and acquisition site,\nhad smaller, though still measurable, effects. Overall, FeTA 2024 offers a\ncomprehensive benchmark for multi-class segmentation and biometry estimation in\nfetal brain MRI, underscoring the need for data-centric approaches, improved\ntopological evaluation, and greater dataset diversity to enable clinically\nrobust and generalizable AI tools.\n","authors":["Vladyslav Zalevskyi","Thomas Sanchez","Misha Kaandorp","Margaux Roulet","Diego Fajardo-Rojas","Liu Li","Jana Hutter","Hongwei Bran Li","Matthew Barkovich","Hui Ji","Luca Wilhelmi","Aline Dändliker","Céline Steger","Mériam Koob","Yvan Gomez","Anton Jakovčić","Melita Klaić","Ana Adžić","Pavel Marković","Gracia Grabarić","Milan Rados","Jordina Aviles Verdera","Gregor Kasprian","Gregor Dovjak","Raphael Gaubert-Rachmühl","Maurice Aschwanden","Qi Zeng","Davood Karimi","Denis Peruzzo","Tommaso Ciceri","Giorgio Longari","Rachika E. Hamadache","Amina Bouzid","Xavier Lladó","Simone Chiarella","Gerard Martí-Juan","Miguel Ángel González Ballester","Marco Castellaro","Marco Pinamonti","Valentina Visani","Robin Cremese","Keïn Sam","Fleur Gaudfernau","Param Ahir","Mehul Parikh","Maximilian Zenk","Michael Baumgartner","Klaus Maier-Hein","Li Tianhong","Yang Hong","Zhao Longfei","Domen Preloznik","Žiga Špiclin","Jae Won Choi","Muyang Li","Jia Fu","Guotai Wang","Jingwen Jiang","Lyuyang Tong","Bo Du","Andrea Gondova","Sungmin You","Kiho Im","Abdul Qayyum","Moona Mazher","Steven A Niederer","Andras Jakab","Roxane Licandro","Kelly Payette","Meritxell Bach Cuadra"],"pdf_url":"https://arxiv.org/pdf/2505.02784v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07825v3","updated":"2025-05-08T08:59:53Z","published":"2024-12-10T18:55:23Z","title":"3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark","summary":"  3D spatial reasoning is the ability to analyze and interpret the positions,\norientations, and spatial relationships of objects within the 3D space. This\nallows models to develop a comprehensive understanding of the 3D scene,\nenabling their applicability to a broader range of areas, such as autonomous\nnavigation, robotics, and AR/VR. While large multi-modal models (LMMs) have\nachieved remarkable progress in a wide range of image and video understanding\ntasks, their capabilities to perform 3D spatial reasoning on diverse natural\nimages are less studied. In this work we present the first comprehensive 3D\nspatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual\nquestion-answer pairs across 12 question types. We conduct robust and thorough\nevaluation of 3D spatial reasoning capabilities by balancing the data\ndistribution and adopting a novel FlipEval strategy. To further study the\nrobustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench\nincludes two subsets with 3D spatial reasoning questions on paired images with\ncommon and uncommon viewpoints. We benchmark a wide range of open-sourced and\nproprietary LMMs, uncovering their limitations in various aspects of 3D\nawareness, such as height, orientation, location, and multi-object reasoning,\nas well as their degraded performance on images with uncommon camera\nviewpoints. Our 3DSRBench provide valuable findings and insights about the\nfuture development of LMMs with strong 3D reasoning capabilities. Our project\npage and dataset is available https://3dsrbench.github.io.\n","authors":["Wufei Ma","Haoyu Chen","Guofeng Zhang","Yu-Cheng Chou","Celso M de Melo","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2412.07825v3.pdf","comment":"Project page: https://3dsrbench.github.io"},{"id":"http://arxiv.org/abs/2504.21356v2","updated":"2025-05-08T08:58:12Z","published":"2025-04-30T06:30:48Z","title":"Nexus-Gen: A Unified Model for Image Understanding, Generation, and\n  Editing","summary":"  Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field.\n","authors":["Hong Zhang","Zhongjie Duan","Xingjun Wang","Yuze Zhao","Weiyi Lu","Zhipeng Di","Yixuan Xu","Yingda Chen","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.21356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08603v2","updated":"2025-05-08T08:56:29Z","published":"2025-04-11T15:12:05Z","title":"FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot\n  Exploration in Any Environment","summary":"  Geometrically accurate and semantically expressive map representations have\nproven invaluable to facilitate robust and safe mobile robot navigation and\ntask planning. Nevertheless, real-time, open-vocabulary semantic understanding\nof large-scale unknown environments is still an open problem. In this paper we\npresent FindAnything, an open-world mapping and exploration framework that\nincorporates vision-language information into dense volumetric submaps. Thanks\nto the use of vision-language features, FindAnything bridges the gap between\npure geometric and open-vocabulary semantic information for a higher level of\nunderstanding while allowing to explore any environment without the help of any\nexternal source of ground-truth pose information. We represent the environment\nas a series of volumetric occupancy submaps, resulting in a robust and accurate\nmap representation that deforms upon pose updates when the underlying SLAM\nsystem corrects its drift, allowing for a locally consistent representation\nbetween submaps. Pixel-wise vision-language features are aggregated from\nefficient SAM (eSAM)-generated segments, which are in turn integrated into\nobject-centric volumetric submaps, providing a mapping from open-vocabulary\nqueries to 3D geometry that is scalable also in terms of memory usage. The\nopen-vocabulary map representation of FindAnything achieves state-of-the-art\nsemantic accuracy in closed-set evaluations on the Replica dataset. This level\nof scene understanding allows a robot to explore environments based on objects\nor areas of interest selected via natural language queries. Our system is the\nfirst of its kind to be deployed on resource-constrained devices, such as MAVs,\nleveraging vision-language information for real-world robotic tasks.\n","authors":["Sebastián Barbas Laina","Simon Boche","Sotiris Papatheodorou","Simon Schaefer","Jaehyung Jung","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2504.08603v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.05062v1","updated":"2025-05-08T08:54:57Z","published":"2025-05-08T08:54:57Z","title":"ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted\n  Long-Tailed Semi-Supervised Learning","summary":"  Based on the success of large-scale visual foundation models like CLIP in\nvarious downstream tasks, this paper initially attempts to explore their impact\non Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundation\nmodel with three strategies: Linear Probing (LP), Lightweight Fine-Tuning\n(LFT), and Full Fine-Tuning (FFT). Our analysis presents the following\ninsights: i) Compared to LTSSL algorithms trained from scratch, FFT results in\na decline in model performance, whereas LP and LFT, although boosting overall\nmodel performance, exhibit negligible benefits to tail classes. ii) LP produces\nnumerous false pseudo-labels due to \\textit{underlearned} training data, while\nLFT can reduce the number of these false labels but becomes overconfident about\nthem owing to \\textit{biased fitting} training data. This exacerbates the\npseudo-labeled and classifier biases inherent in LTSSL, limiting performance\nimprovement in the tail classes. With these insights, we propose a Unbiased\nLightweight Fine-tuning strategy, \\textbf{ULFine}, which mitigates the\noverconfidence via confidence-aware adaptive fitting of textual prototypes and\ncounteracts the pseudo-labeled and classifier biases via complementary fusion\nof dual logits. Extensive experiments demonstrate that ULFine markedly\ndecreases training costs by over ten times and substantially increases\nprediction accuracies compared to state-of-the-art methods.\n","authors":["Enhao Zhang","Chaohua Li","Chuanxing Geng","Songcan Chen"],"pdf_url":"https://arxiv.org/pdf/2505.05062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12705v5","updated":"2025-05-08T08:46:59Z","published":"2024-10-16T16:11:49Z","title":"WorldCuisines: A Massive-Scale Benchmark for Multilingual and\n  Multicultural Visual Question Answering on Global Cuisines","summary":"  Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.\n","authors":["Genta Indra Winata","Frederikus Hudi","Patrick Amadeus Irawan","David Anugraha","Rifki Afina Putri","Yutong Wang","Adam Nohejl","Ubaidillah Ariq Prathama","Nedjma Ousidhoum","Afifa Amriani","Anar Rzayev","Anirban Das","Ashmari Pramodya","Aulia Adila","Bryan Wilie","Candy Olivia Mawalim","Ching Lam Cheng","Daud Abolade","Emmanuele Chersoni","Enrico Santus","Fariz Ikhwantri","Garry Kuwanto","Hanyang Zhao","Haryo Akbarianto Wibowo","Holy Lovenia","Jan Christian Blaise Cruz","Jan Wira Gotama Putra","Junho Myung","Lucky Susanto","Maria Angelica Riera Machin","Marina Zhukova","Michael Anugraha","Muhammad Farid Adilazuarda","Natasha Santosa","Peerat Limkonchotiwat","Raj Dabre","Rio Alexander Audino","Samuel Cahyawijaya","Shi-Xiong Zhang","Stephanie Yulia Salim","Yi Zhou","Yinxuan Gui","David Ifeoluwa Adelani","En-Shiun Annie Lee","Shogo Okada","Ayu Purwarianti","Alham Fikri Aji","Taro Watanabe","Derry Tanti Wijaya","Alice Oh","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2410.12705v5.pdf","comment":"Best Theme Paper at NAACL 2025"},{"id":"http://arxiv.org/abs/2505.05054v1","updated":"2025-05-08T08:46:28Z","published":"2025-05-08T08:46:28Z","title":"Direct Image Classification from Fourier Ptychographic Microscopy\n  Measurements without Reconstruction","summary":"  The computational imaging technique of Fourier Ptychographic Microscopy (FPM)\nenables high-resolution imaging with a wide field of view and can serve as an\nextremely valuable tool, e.g. in the classification of cells in medical\napplications. However, reconstructing a high-resolution image from tens or even\nhundreds of measurements is computationally expensive, particularly for a wide\nfield of view. Therefore, in this paper, we investigate the idea of classifying\nthe image content in the FPM measurements directly without performing a\nreconstruction step first. We show that Convolutional Neural Networks (CNN) can\nextract meaningful information from measurement sequences, significantly\noutperforming the classification on a single band-limited image (up to 12 %)\nwhile being significantly more efficient than a reconstruction of a\nhigh-resolution image. Furthermore, we demonstrate that a learned multiplexing\nof several raw measurements allows maintaining the classification accuracy\nwhile reducing the amount of data (and consequently also the acquisition time)\nsignificantly.\n","authors":["Navya Sonal Agarwal","Jan Philipp Schneider","Kanchana Vaishnavi Gandikota","Syed Muhammad Kazim","John Meshreki","Ivo Ihrke","Michael Moeller"],"pdf_url":"https://arxiv.org/pdf/2505.05054v1.pdf","comment":"ISCS 2025"},{"id":"http://arxiv.org/abs/2505.04150v2","updated":"2025-05-08T08:45:01Z","published":"2025-05-07T06:02:27Z","title":"Learning from Similarity Proportion Loss for Classifying Skeletal Muscle\n  Recovery Stages","summary":"  Evaluating the regeneration process of damaged muscle tissue is a fundamental\nanalysis in muscle research to measure experimental effect sizes and uncover\nmechanisms behind muscle weakness due to aging and disease. The conventional\napproach to assessing muscle tissue regeneration involves whole-slide imaging\nand expert visual inspection of the recovery stages based on the morphological\ninformation of cells and fibers. There is a need to replace these tasks with\nautomated methods incorporating machine learning techniques to ensure a\nquantitative and objective analysis. Given the limited availability of fully\nlabeled data, a possible approach is Learning from Label Proportions (LLP), a\nweakly supervised learning method using class label proportions. However,\ncurrent LLP methods have two limitations: (1) they cannot adapt the feature\nextractor for muscle tissues, and (2) they treat the classes representing\nrecovery stages and cell morphological changes as nominal, resulting in the\nloss of ordinal information. To address these issues, we propose Ordinal Scale\nLearning from Similarity Proportion (OSLSP), which uses a similarity proportion\nloss derived from two bag combinations. OSLSP can update the feature extractor\nby using class proportion attention to the ordinal scale of the class. Our\nmodel with OSLSP outperforms large-scale pre-trained and fine-tuning models in\nclassification tasks of skeletal muscle recovery stages.\n","authors":["Yu Yamaoka","Weng Ian Chan","Shigeto Seno","Soichiro Fukada","Hideo Matsuda"],"pdf_url":"https://arxiv.org/pdf/2505.04150v2.pdf","comment":"MICCAI2024 workshop ADSMI in Morocco (oral) [Peer-reviewed]"},{"id":"http://arxiv.org/abs/2412.17378v4","updated":"2025-05-08T08:36:42Z","published":"2024-12-23T08:26:30Z","title":"Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained\n  Tiling","summary":"  3D Gaussian Splatting (3DGS) is increasingly attracting attention in both\nacademia and industry owing to its superior visual quality and rendering speed.\nHowever, training a 3DGS model remains a time-intensive task, especially in\nload imbalance scenarios where workload diversity among pixels and Gaussian\nspheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS,\na Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS\ntraining process, perfectly solving load-imbalance issues. First, we\ninnovatively introduce the inter-block dynamic workload distribution technique\nto map workloads to Streaming Multiprocessor(SM) resources within a single GPU\ndynamically, which constitutes the foundation of load balancing. Second, we are\nthe first to propose the Gaussian-wise parallel rendering technique to\nsignificantly reduce workload divergence inside a warp, which serves as a\ncritical component in addressing load imbalance. Based on the above two\nmethods, we further creatively put forward the fine-grained combined load\nbalancing technique to uniformly distribute workload across all SMs, which\nboosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we\npresent a self-adaptive render kernel selection strategy during the 3DGS\ntraining process based on different load-balance situations, which effectively\nimproves training efficiency.\n","authors":["Hao Gui","Lin Hu","Rui Chen","Mingxiao Huang","Yuxin Yin","Jin Yang","Yong Wu","Chen Liu","Zhongxu Sun","Xueyang Zhang","Kun Zhan"],"pdf_url":"https://arxiv.org/pdf/2412.17378v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05049v1","updated":"2025-05-08T08:36:23Z","published":"2025-05-08T08:36:23Z","title":"UncertainSAM: Fast and Efficient Uncertainty Quantification of the\n  Segment Anything Model","summary":"  The introduction of the Segment Anything Model (SAM) has paved the way for\nnumerous semantic segmentation applications. For several tasks, quantifying the\nuncertainty of SAM is of particular interest. However, the ambiguous nature of\nthe class-agnostic foundation model SAM challenges current uncertainty\nquantification (UQ) approaches. This paper presents a theoretically motivated\nuncertainty quantification model based on a Bayesian entropy formulation\njointly respecting aleatoric, epistemic, and the newly introduced task\nuncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ\nmethod. Our model traces the root of uncertainty back to under-parameterised\nmodels, insufficient prompts or image ambiguities. Our proposed deterministic\nUSAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k,\nDAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ\nalternative that can support user-prompting, enhance semi-supervised pipelines,\nor balance the tradeoff between accuracy and cost efficiency.\n","authors":["Timo Kaiser","Thomas Norrenbrock","Bodo Rosenhahn"],"pdf_url":"https://arxiv.org/pdf/2505.05049v1.pdf","comment":"Accepted to ICML'25"},{"id":"http://arxiv.org/abs/2410.03825v2","updated":"2025-05-08T08:32:16Z","published":"2024-10-04T18:00:07Z","title":"MonST3R: A Simple Approach for Estimating Geometry in the Presence of\n  Motion","summary":"  Estimating geometry from dynamic scenes, where objects move and deform over\ntime, remains a core challenge in computer vision. Current approaches often\nrely on multi-stage pipelines or global optimizations that decompose the\nproblem into subtasks, like depth and flow, leading to complex systems prone to\nerrors. In this paper, we present Motion DUSt3R (MonST3R), a novel\ngeometry-first approach that directly estimates per-timestep geometry from\ndynamic scenes. Our key insight is that by simply estimating a pointmap for\neach timestep, we can effectively adapt DUST3R's representation, previously\nonly used for static scenes, to dynamic scenes. However, this approach presents\na significant challenge: the scarcity of suitable training data, namely\ndynamic, posed videos with depth labels. Despite this, we show that by posing\nthe problem as a fine-tuning task, identifying several suitable datasets, and\nstrategically training the model on this limited data, we can surprisingly\nenable the model to handle dynamics, even without an explicit motion\nrepresentation. Based on this, we introduce new optimizations for several\ndownstream video-specific tasks and demonstrate strong performance on video\ndepth and camera pose estimation, outperforming prior work in terms of\nrobustness and efficiency. Moreover, MonST3R shows promising results for\nprimarily feed-forward 4D reconstruction.\n","authors":["Junyi Zhang","Charles Herrmann","Junhwa Hur","Varun Jampani","Trevor Darrell","Forrester Cole","Deqing Sun","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2410.03825v2.pdf","comment":"Accepted by ICLR 25, Project page: https://monst3r-project.github.io/"},{"id":"http://arxiv.org/abs/2505.04512v2","updated":"2025-05-08T08:29:00Z","published":"2025-05-07T15:33:18Z","title":"HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation","summary":"  Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.\n","authors":["Teng Hu","Zhentao Yu","Zhengguang Zhou","Sen Liang","Yuan Zhou","Qin Lin","Qinglin Lu"],"pdf_url":"https://arxiv.org/pdf/2505.04512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05043v1","updated":"2025-05-08T08:27:37Z","published":"2025-05-08T08:27:37Z","title":"xTrace: A Facial Expressive Behaviour Analysis Tool for Continuous\n  Affect Recognition","summary":"  Recognising expressive behaviours in face videos is a long-standing challenge\nin Affective Computing. Despite significant advancements in recent years, it\nstill remains a challenge to build a robust and reliable system for\nnaturalistic and in-the-wild facial expressive behaviour analysis in real time.\nThis paper addresses two key challenges in building such a system: (1). The\npaucity of large-scale labelled facial affect video datasets with extensive\ncoverage of the 2D emotion space, and (2). The difficulty of extracting facial\nvideo features that are discriminative, interpretable, robust, and\ncomputationally efficient. Toward addressing these challenges, we introduce\nxTrace, a robust tool for facial expressive behaviour analysis and predicting\ncontinuous values of dimensional emotions, namely valence and arousal, from\nin-the-wild face videos.\n  To address challenge (1), our affect recognition model is trained on the\nlargest facial affect video data set, containing ~450k videos that cover most\nemotion zones in the dimensional emotion space, making xTrace highly versatile\nin analysing a wide spectrum of naturalistic expressive behaviours. To address\nchallenge (2), xTrace uses facial affect descriptors that are not only\nexplainable, but can also achieve a high degree of accuracy and robustness with\nlow computational complexity. The key components of xTrace are benchmarked\nagainst three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox.\nOn an in-the-wild validation set composed of 50k videos, xTrace achieves 0.86\nmean CCC and 0.13 mean absolute error values. We present a detailed error\nanalysis of affect predictions from xTrace, illustrating (a). its ability to\nrecognise emotions with high accuracy across most bins in the 2D emotion space,\n(b). its robustness to non-frontal head pose angles, and (c). a strong\ncorrelation between its uncertainty estimates and its accuracy.\n","authors":["Mani Kumar Tellamekala","Shashank Jaiswal","Thomas Smith","Timur Alamev","Gary McKeown","Anthony Brown","Michel Valstar"],"pdf_url":"https://arxiv.org/pdf/2505.05043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14423v4","updated":"2025-05-08T08:27:10Z","published":"2024-11-21T18:55:23Z","title":"PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and\n  Video Diffusion for 4D Dynamic Physical Scene Simulation","summary":"  Realistic simulation of dynamic scenes requires accurately capturing diverse\nmaterial properties and modeling complex object interactions grounded in\nphysical principles. However, existing methods are constrained to basic\nmaterial types with limited predictable parameters, making them insufficient to\nrepresent the complexity of real-world materials. We introduce PhysFlow, a\nnovel approach that leverages multi-modal foundation models and video diffusion\nto achieve enhanced 4D dynamic scene simulation. Our method utilizes\nmulti-modal models to identify material types and initialize material\nparameters through image queries, while simultaneously inferring 3D Gaussian\nsplats for detailed scene representation. We further refine these material\nparameters using video diffusion with a differentiable Material Point Method\n(MPM) and optical flow guidance rather than render loss or Score Distillation\nSampling (SDS) loss. This integrated framework enables accurate prediction and\nrealistic simulation of dynamic interactions in real-world scenarios, advancing\nboth accuracy and flexibility in physics-based simulations.\n","authors":["Zhuoman Liu","Weicai Ye","Yan Luximon","Pengfei Wan","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14423v4.pdf","comment":"CVPR 2025. Homepage: https://zhuomanliu.github.io/PhysFlow/"},{"id":"http://arxiv.org/abs/2505.05041v1","updated":"2025-05-08T08:25:44Z","published":"2025-05-08T08:25:44Z","title":"ADNP-15: An Open-Source Histopathological Dataset for Neuritic Plaque\n  Segmentation in Human Brain Whole Slide Images with Frequency Domain Image\n  Enhancement for Stain Normalization","summary":"  Alzheimer's Disease (AD) is a neurodegenerative disorder characterized by\namyloid-beta plaques and tau neurofibrillary tangles, which serve as key\nhistopathological features. The identification and segmentation of these\nlesions are crucial for understanding AD progression but remain challenging due\nto the lack of large-scale annotated datasets and the impact of staining\nvariations on automated image analysis. Deep learning has emerged as a powerful\ntool for pathology image segmentation; however, model performance is\nsignificantly influenced by variations in staining characteristics,\nnecessitating effective stain normalization and enhancement techniques. In this\nstudy, we address these challenges by introducing an open-source dataset\n(ADNP-15) of neuritic plaques (i.e., amyloid deposits combined with a crown of\ndystrophic tau-positive neurites) in human brain whole slide images. We\nestablish a comprehensive benchmark by evaluating five widely adopted deep\nlearning models across four stain normalization techniques, providing deeper\ninsights into their influence on neuritic plaque segmentation. Additionally, we\npropose a novel image enhancement method that improves segmentation accuracy,\nparticularly in complex tissue structures, by enhancing structural details and\nmitigating staining inconsistencies. Our experimental results demonstrate that\nthis enhancement strategy significantly boosts model generalization and\nsegmentation accuracy. All datasets and code are open-source, ensuring\ntransparency and reproducibility while enabling further advancements in the\nfield.\n","authors":["Chenxi Zhao","Jianqiang Li","Qing Zhao","Jing Bai","Susana Boluda","Benoit Delatour","Lev Stimmer","Daniel Racoceanu","Gabriel Jimenez","Guanghui Fu"],"pdf_url":"https://arxiv.org/pdf/2505.05041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04590v2","updated":"2025-05-08T08:24:30Z","published":"2025-05-07T17:32:49Z","title":"TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral\n  Grids for Gradient-Based Mesh Optimization","summary":"  We introduce TetWeave, a novel isosurface representation for gradient-based\nmesh optimization that jointly optimizes the placement of a tetrahedral grid\nused for Marching Tetrahedra and a novel directional signed distance at each\npoint. TetWeave constructs tetrahedral grids on-the-fly via Delaunay\ntriangulation, enabling increased flexibility compared to predefined grids. The\nextracted meshes are guaranteed to be watertight, two-manifold and\nintersection-free. The flexibility of TetWeave enables a resampling strategy\nthat places new points where reconstruction error is high and allows to\nencourage mesh fairness without compromising on reconstruction error. This\nleads to high-quality, adaptive meshes that require minimal memory usage and\nfew parameters to optimize. Consequently, TetWeave exhibits near-linear memory\nscaling relative to the vertex count of the output mesh - a substantial\nimprovement over predefined grids. We demonstrate the applicability of TetWeave\nto a broad range of challenging tasks in computer graphics and vision, such as\nmulti-view 3D reconstruction, mesh compression and geometric texture\ngeneration.\n","authors":["Alexandre Binninger","Ruben Wiersma","Philipp Herholz","Olga Sorkine-Hornung"],"pdf_url":"https://arxiv.org/pdf/2505.04590v2.pdf","comment":"ACM Trans. Graph. 44, 4. SIGGRAPH 2025. 19 pages, 21 figures"},{"id":"http://arxiv.org/abs/2505.05040v1","updated":"2025-05-08T08:23:20Z","published":"2025-05-08T08:23:20Z","title":"Image-Text Relation Prediction for Multilingual Tweets","summary":"  Various social networks have been allowing media uploads for over a decade\nnow. Still, it has not always been clear what is their relation with the posted\ntext or even if there is any at all. In this work, we explore how multilingual\nvision-language models tackle the task of image-text relation prediction in\ndifferent languages, and construct a dedicated balanced benchmark data set from\nTwitter posts in Latvian along with their manual translations into English. We\ncompare our results to previous work and show that the more recently released\nvision-language model checkpoints are becoming increasingly capable at this\ntask, but there is still much room for further improvement.\n","authors":["Matīss Rikters","Edison Marrese-Taylor"],"pdf_url":"https://arxiv.org/pdf/2505.05040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18834v2","updated":"2025-05-08T08:17:43Z","published":"2024-12-25T08:42:23Z","title":"Adaptive Rate Control for Deep Video Compression with Rate-Distortion\n  Prediction","summary":"  Deep video compression has made significant progress in recent years,\nachieving rate-distortion performance that surpasses that of traditional video\ncompression methods. However, rate control schemes tailored for deep video\ncompression have not been well studied. In this paper, we propose a neural\nnetwork-based $\\lambda$-domain rate control scheme for deep video compression,\nwhich determines the coding parameter $\\lambda$ for each to-be-coded frame\nbased on the rate-distortion-$\\lambda$ (R-D-$\\lambda$) relationships directly\nlearned from uncompressed frames, achieving high rate control accuracy\nefficiently without the need for pre-encoding. Moreover, this content-aware\nscheme is able to mitigate inter-frame quality fluctuations and adapt to abrupt\nchanges in video content. Specifically, we introduce two neural network-based\npredictors to estimate the relationship between bitrate and $\\lambda$, as well\nas the relationship between distortion and $\\lambda$ for each frame. Then we\ndetermine the coding parameter $\\lambda$ for each frame to achieve the target\nbitrate. Experimental results demonstrate that our approach achieves high rate\ncontrol accuracy at the mini-GOP level with low time overhead and mitigates\ninter-frame quality fluctuations across video content of varying resolutions.\n","authors":["Bowen Gu","Hao Chen","Ming Lu","Jie Yao","Zhan Ma"],"pdf_url":"https://arxiv.org/pdf/2412.18834v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00626v2","updated":"2025-05-08T08:15:49Z","published":"2024-12-01T00:51:23Z","title":"MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum\n  Learning","summary":"  Harnessing low-light enhancement and domain adaptation, nighttime UAV\ntracking has made substantial strides. However, over-reliance on image\nenhancement, limited high-quality nighttime data, and a lack of integration\nbetween daytime and nighttime trackers hinder the development of an end-to-end\ntrainable framework. Additionally, current ViT-based trackers demand heavy\ncomputational resources due to their reliance on the self-attention mechanism.\nIn this paper, we propose a novel pure Mamba-based tracking framework\n(MambaNUT) that employs a state space model with linear complexity as its\nbackbone, incorporating a single-stream architecture that integrates feature\nlearning and template-search coupling within Vision Mamba. We introduce an\nadaptive curriculum learning (ACL) approach that dynamically adjusts sampling\nstrategies and loss weights, thereby improving the model's ability of\ngeneralization. Our ACL is composed of two levels of curriculum schedulers: (1)\nsampling scheduler that transforms the data distribution from imbalanced to\nbalanced, as well as from easier (daytime) to harder (nighttime) samples; (2)\nloss scheduler that dynamically assigns weights based on the size of the\ntraining data and IoU of individual instances. Exhaustive experiments on\nmultiple nighttime UAV tracking benchmarks demonstrate that the proposed\nMambaNUT achieves state-of-the-art performance while requiring lower\ncomputational costs. The code will be available at\nhttps://github.com/wuyou3474/MambaNUT.\n","authors":["You Wu","Xiangyang Yang","Xucheng Wang","Hengzhou Ye","Dan Zeng","Shuiwang Li"],"pdf_url":"https://arxiv.org/pdf/2412.00626v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06050v5","updated":"2025-05-08T08:15:28Z","published":"2024-06-10T06:38:11Z","title":"Generalizable Human Gaussians from Single-View Image","summary":"  In this work, we tackle the task of learning 3D human Gaussians from a single\nimage, focusing on recovering detailed appearance and geometry including\nunobserved regions. We introduce a single-view generalizable Human Gaussian\nModel (HGM), which employs a novel generate-then-refine pipeline with the\nguidance from human body prior and diffusion prior. Our approach uses a\nControlNet to refine rendered back-view images from coarse predicted human\nGaussians, then uses the refined image along with the input image to\nreconstruct refined human Gaussians. To mitigate the potential generation of\nunrealistic human poses and shapes, we incorporate human priors from the SMPL-X\nmodel as a dual branch, propagating image features from the SMPL-X volume to\nthe image Gaussians using sparse convolution and attention mechanisms. Given\nthat the initial SMPL-X estimation might be inaccurate, we gradually refine it\nwith our HGM model. We validate our approach on several publicly available\ndatasets. Our method surpasses previous methods in both novel view synthesis\nand surface reconstruction. Our approach also exhibits strong generalization\nfor cross-dataset evaluation and in-the-wild images.\n","authors":["Jinnan Chen","Chen Li","Jianfeng Zhang","Lingting Zhu","Buzhen Huang","Hanlin Chen","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2406.06050v5.pdf","comment":"ICLR 2025: https://jinnan-chen.github.io/projects/HGM/"},{"id":"http://arxiv.org/abs/2408.17237v3","updated":"2025-05-08T08:08:28Z","published":"2024-08-30T12:27:22Z","title":"A nonlinear elasticity model in computer vision","summary":"  The purpose of this paper is to analyze a nonlinear elasticity model\nintroduced by the authors for comparing two images, regarded as bounded open\nsubsets of $\\R^n$ together with associated vector-valued intensity maps.\nOptimal transformations between the images are sought as minimisers of an\nintegral functional among orientation-preserving homeomorphisms. The existence\nof minimisers is proved under natural coercivity and polyconvexity conditions,\nassuming only that the intensity functions are bounded measurable. Variants of\nthe existence theorem are also proved, first under the constraint that finite\nsets of landmark points in the two images are mapped one to the other, and\nsecond when one image is to be compared to an unknown part of another.\n  The question is studied as to whether for images related by an affine mapping\nthe unique minimiser is given by that affine mapping. For a natural class of\nfunctional integrands an example is given guaranteeing that this property holds\nfor pairs of images in which the second is a scaling of the first by a constant\nfactor. However for the property to hold for arbitrary pairs of affinely\nrelated images it is shown that the integrand has to depend on the gradient of\nthe transformation as a convex function of its determinant alone. This suggests\na new model in which the integrand depends also on second derivatives of the\ntransformation, and an example is given for which both existence of minimisers\nis assured and the above property holds for all pairs of affinely related\nimages.\n","authors":["John M. Ball","Christopher L. Horner"],"pdf_url":"https://arxiv.org/pdf/2408.17237v3.pdf","comment":"The paper has been substantially revised. In particular the section\n  on metrics has been rewritten to correct an error, and a new result added on\n  the existence of discrete morphing sequences in the mass-conserving case. In\n  the mass-conserving case there is a new formulation of the question\n  concerning whether the minimizing deformation for affinely related images is\n  the corresponding affine map"},{"id":"http://arxiv.org/abs/2503.05423v2","updated":"2025-05-08T08:00:37Z","published":"2025-03-07T13:50:29Z","title":"Semantic Shift Estimation via Dual-Projection and Classifier\n  Reconstruction for Exemplar-Free Class-Incremental Learning","summary":"  Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn\nfrom distinct categories without retaining exemplars but easily suffers from\ncatastrophic forgetting of learned knowledge. While existing EFCIL methods\nleverage knowledge distillation to alleviate forgetting, they still face two\ncritical challenges: semantic shift and decision bias. Specifically, the\nembeddings of old tasks shift in the embedding space after learning new tasks,\nand the classifier becomes biased towards new tasks due to training solely with\nnew data, thereby hindering the balance between old and new knowledge. To\naddress these issues, we propose the Dual-Projection Shift Estimation and\nClassifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates\nsemantic shift through a dual-projection, which combines a learnable\ntransformation with a row-space projection to capture both task-wise and\ncategory-wise shifts. Furthermore, to mitigate decision bias, DPCR employs\nridge regression to reformulate classifier training as a reconstruction\nprocess. This reconstruction exploits previous information encoded in\ncovariance and prototype of each class after calibration with estimated shift,\nthereby reducing decision bias. Extensive experiments demonstrate that, across\nvarious datasets, DPCR effectively balances old and new tasks, outperforming\nstate-of-the-art EFCIL methods.\n","authors":["Run He","Di Fang","Yicheng Xu","Yawen Cui","Ming Li","Cen Chen","Ziqian Zeng","Huiping Zhuang"],"pdf_url":"https://arxiv.org/pdf/2503.05423v2.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2504.21487v2","updated":"2025-05-08T07:57:04Z","published":"2025-04-30T10:12:48Z","title":"DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling\n  for Image Restoration","summary":"  Diffusion models have achieved remarkable progress in universal image\nrestoration. While existing methods speed up inference by reducing sampling\nsteps, substantial step intervals often introduce cumulative errors. Moreover,\nthey struggle to balance the commonality of degradation representations and\nrestoration quality. To address these challenges, we introduce\n\\textbf{DGSolver}, a diffusion generalist solver with universal posterior\nsampling. We first derive the exact ordinary differential equations for\ngeneralist diffusion models and tailor high-order solvers with a queue-based\naccelerated sampling strategy to improve both accuracy and efficiency. We then\nintegrate universal posterior sampling to better approximate\nmanifold-constrained gradients, yielding a more accurate noise estimation and\ncorrecting errors in inverse inference. Extensive experiments show that\nDGSolver outperforms state-of-the-art methods in restoration accuracy,\nstability, and scalability, both qualitatively and quantitatively. Code and\nmodels will be available at https://github.com/MiliLab/DGSolver.\n","authors":["Hebaixu Wang","Jing Zhang","Haonan Guo","Di Wang","Jiayi Ma","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2504.21487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05023v1","updated":"2025-05-08T07:56:30Z","published":"2025-05-08T07:56:30Z","title":"Split Matching for Inductive Zero-shot Semantic Segmentation","summary":"  Zero-shot Semantic Segmentation (ZSS) aims to segment categories that are not\nannotated during training. While fine-tuning vision-language models has\nachieved promising results, these models often overfit to seen categories due\nto the lack of supervision for unseen classes. As an alternative to fully\nsupervised approaches, query-based segmentation has shown great latent in ZSS,\nas it enables object localization without relying on explicit labels. However,\nconventional Hungarian matching, a core component in query-based frameworks,\nneeds full supervision and often misclassifies unseen categories as background\nin the setting of ZSS. To address this issue, we propose Split Matching (SM), a\nnovel assignment strategy that decouples Hungarian matching into two\ncomponents: one for seen classes in annotated regions and another for latent\nclasses in unannotated regions (referred to as unseen candidates).\nSpecifically, we partition the queries into seen and candidate groups, enabling\neach to be optimized independently according to its available supervision. To\ndiscover unseen candidates, we cluster CLIP dense features to generate pseudo\nmasks and extract region-level embeddings using CLS tokens. Matching is then\nconducted separately for the two groups based on both class-level similarity\nand mask-level consistency. Additionally, we introduce a Multi-scale Feature\nEnhancement (MFE) module that refines decoder features through residual\nmulti-scale aggregation, improving the model's ability to capture spatial\ndetails across resolutions. SM is the first to introduce decoupled Hungarian\nmatching under the inductive ZSS setting, and achieves state-of-the-art\nperformance on two standard benchmarks.\n","authors":["Jialei Chen","Xu Zheng","Dongyue Li","Chong Yi","Seigo Ito","Danda Pani Paudel","Luc Van Gool","Hiroshi Murase","Daisuke Deguchi"],"pdf_url":"https://arxiv.org/pdf/2505.05023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05022v1","updated":"2025-05-08T07:56:16Z","published":"2025-05-08T07:56:16Z","title":"SOAP: Style-Omniscient Animatable Portraits","summary":"  Creating animatable 3D avatars from a single image remains challenging due to\nstyle limitations (realistic, cartoon, anime) and difficulties in handling\naccessories or hairstyles. While 3D diffusion models advance single-view\nreconstruction for general objects, outputs often lack animation controls or\nsuffer from artifacts because of the domain gap. We propose SOAP, a\nstyle-omniscient framework to generate rigged, topology-consistent avatars from\nany portrait. Our method leverages a multiview diffusion model trained on 24K\n3D heads with multiple styles and an adaptive optimization pipeline to deform\nthe FLAME mesh while maintaining topology and rigging via differentiable\nrendering. The resulting textured avatars support FACS-based animation,\nintegrate with eyeballs and teeth, and preserve details like braided hair or\naccessories. Extensive experiments demonstrate the superiority of our method\nover state-of-the-art techniques for both single-view head modeling and\ndiffusion-based generation of Image-to-3D. Our code and data are publicly\navailable for research purposes at https://github.com/TingtingLiao/soap.\n","authors":["Tingting Liao","Yujian Zheng","Adilbek Karmanov","Liwen Hu","Leyang Jin","Yuliang Xiu","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2505.05022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02060v2","updated":"2025-05-08T07:41:06Z","published":"2025-05-04T10:36:58Z","title":"Transforming faces into video stories -- VideoFace2.0","summary":"  Face detection and face recognition have been in the focus of vision\ncommunity since the very beginnings. Inspired by the success of the original\nVideoface digitizer, a pioneering device that allowed users to capture video\nsignals from any source, we have designed an advanced video analytics tool to\nefficiently create structured video stories, i.e. identity-based information\ncatalogs. VideoFace2.0 is the name of the developed system for spatial and\ntemporal localization of each unique face in the input video, i.e. face\nre-identification (ReID), which also allows their cataloging, characterization\nand creation of structured video outputs for later downstream tasks. Developed\nnear real-time solution is primarily designed to be utilized in application\nscenarios involving TV production, media analysis, and as an efficient tool for\ncreating large video datasets necessary for training machine learning (ML)\nmodels in challenging vision tasks such as lip reading and multimodal speech\nrecognition. Conducted experiments confirm applicability of the proposed face\nReID algorithm that is combining the concepts of face detection, face\nrecognition and passive tracking-by-detection in order to achieve robust and\nefficient face ReID. The system is envisioned as a compact and modular\nextensions of the existing video production equipment. Presented results are\nbased on test implementation that achieves between 18-25 fps on consumer type\nnotebook. Ablation experiments also confirmed that the proposed algorithm\nbrings relative gain in the reduction of number of false identities in the\nrange of 73%-93%. We hope that the presented work and shared code\nimplementation will stimulate further interest in development of similar,\napplication specific video analysis tools, and lower the entry barrier for\nproduction of high-quality multi-modal datasets in the future.\n","authors":["Branko Brkljač","Vladimir Kalušev","Branislav Popović","Milan Sečujski"],"pdf_url":"https://arxiv.org/pdf/2505.02060v2.pdf","comment":"4 Pages, 2 Figures, 1 Table, 1 Algorithm; Associated VideoFace2.0\n  code, test videos and results visualizations are available at\n  https://github.com/brkljac/VideoFace2.0 ; Preprint accepted for publication\n  at the 14th Mediterranean Conference on Embedded Computing (MECO), 10-14 June\n  2025, Budva, Montenegro"},{"id":"http://arxiv.org/abs/2505.05008v1","updated":"2025-05-08T07:25:42Z","published":"2025-05-08T07:25:42Z","title":"Adaptive Contextual Embedding for Robust Far-View Borehole Detection","summary":"  In controlled blasting operations, accurately detecting densely distributed\ntiny boreholes from far-view imagery is critical for operational safety and\nefficiency. However, existing detection methods often struggle due to small\nobject scales, highly dense arrangements, and limited distinctive visual\nfeatures of boreholes. To address these challenges, we propose an adaptive\ndetection approach that builds upon existing architectures (e.g., YOLO) by\nexplicitly leveraging consistent embedding representations derived through\nexponential moving average (EMA)-based statistical updates.\n  Our method introduces three synergistic components: (1) adaptive augmentation\nutilizing dynamically updated image statistics to robustly handle illumination\nand texture variations; (2) embedding stabilization to ensure consistent and\nreliable feature extraction; and (3) contextual refinement leveraging spatial\ncontext for improved detection accuracy. The pervasive use of EMA in our method\nis particularly advantageous given the limited visual complexity and small\nscale of boreholes, allowing stable and robust representation learning even\nunder challenging visual conditions. Experiments on a challenging proprietary\nquarry-site dataset demonstrate substantial improvements over baseline\nYOLO-based architectures, highlighting our method's effectiveness in realistic\nand complex industrial scenarios.\n","authors":["Xuesong Liu","Tianyu Hao","Emmett J. Ientilucci"],"pdf_url":"https://arxiv.org/pdf/2505.05008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05007v1","updated":"2025-05-08T07:24:52Z","published":"2025-05-08T07:24:52Z","title":"Driving with Context: Online Map Matching for Complex Roads Using Lane\n  Markings and Scenario Recognition","summary":"  Accurate online map matching is fundamental to vehicle navigation and the\nactivation of intelligent driving functions. Current online map matching\nmethods are prone to errors in complex road networks, especially in multilevel\nroad area. To address this challenge, we propose an online Standard Definition\n(SD) map matching method by constructing a Hidden Markov Model (HMM) with\nmultiple probability factors. Our proposed method can achieve accurate map\nmatching even in complex road networks by carefully leveraging lane markings\nand scenario recognition in the designing of the probability factors. First,\nthe lane markings are generated by a multi-lane tracking method and associated\nwith the SD map using HMM to build an enriched SD map. In areas covered by the\nenriched SD map, the vehicle can re-localize itself by performing Iterative\nClosest Point (ICP) registration for the lane markings. Then, the probability\nfactor accounting for the lane marking detection can be obtained using the\nassociation probability between adjacent lanes and roads. Second, the driving\nscenario recognition model is applied to generate the emission probability\nfactor of scenario recognition, which improves the performance of map matching\non elevated roads and ordinary urban roads underneath them. We validate our\nmethod through extensive road tests in Europe and China, and the experimental\nresults show that our proposed method effectively improves the online map\nmatching accuracy as compared to other existing methods, especially in\nmultilevel road area. Specifically, the experiments show that our proposed\nmethod achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset\nand test data of multilevel road areas in Shanghai respectively, significantly\noutperforming benchmark methods. The implementation is available at\nhttps://github.com/TRV-Lab/LMSR-OMM.\n","authors":["Xin Bi","Zhichao Li","Yuxuan Xia","Panpan Tong","Lijuan Zhang","Yang Chen","Junsheng Fu"],"pdf_url":"https://arxiv.org/pdf/2505.05007v1.pdf","comment":"9 pages and 12 figures. Under review at IEEE RA-L"},{"id":"http://arxiv.org/abs/2505.05004v1","updated":"2025-05-08T07:21:12Z","published":"2025-05-08T07:21:12Z","title":"Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT\n  Cohort","summary":"  Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar\ntransitional vertebrae or enumeration anomalies. While some studies manually\nassess these anomalies and describe the ribs qualitatively, this study aims to\nautomate thoracolumbar stump rib detection and analyze their morphology\nquantitatively. To this end, we train a high-resolution deep-learning model for\nrib segmentation and show significant improvements compared to existing models\n(Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative\nalgorithm and piece-wise linear interpolation to assess the length of the ribs,\nshowing a success rate of 98.2%. When analyzing morphological features, we show\nthat stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs\n-13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1,\np-value < 0.01), and are oriented more downwards and sideways within the first\ncentimeters in contrast to full-length ribs. We show that with partially\nvisible ribs, these features can achieve an F1-score of 0.84 in differentiating\nstump ribs from regular ones. We publish the model weights and masks for public\nuse.\n","authors":["Hendrik Möller","Hanna Schön","Alina Dima","Benjamin Keinert-Weth","Robert Graf","Matan Atad","Johannes Paetzold","Friederike Jungmann","Rickmer Braren","Florian Kofler","Bjoern Menze","Daniel Rueckert","Jan S. Kirschke"],"pdf_url":"https://arxiv.org/pdf/2505.05004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08777v4","updated":"2025-05-08T07:18:17Z","published":"2024-11-13T17:02:46Z","title":"LUDO: Low-Latency Understanding of Deformable Objects using Point Cloud\n  Occupancy Functions","summary":"  Accurately determining the shape of objects and the location of their\ninternal structures within deformable objects is crucial for medical tasks that\nrequire precise targeting, such as robotic biopsies. We introduce LUDO, a\nmethod for accurate low-latency understanding of deformable objects. LUDO\nreconstructs objects in their deformed state, including their internal\nstructures, from a single-view point cloud observation in under 30 ms using\noccupancy networks. LUDO provides uncertainty estimates for its predictions.\nAdditionally, it provides explainability by highlighting key features in its\ninput observations. Both uncertainty and explainability are important for\nsafety-critical applications such as surgical interventions. We demonstrate\nLUDO's abilities for autonomous targeting of internal regions of interest\n(ROIs) in deformable objects. We evaluate LUDO in real-world robotic\nexperiments, achieving a success rate of 98.9% for puncturing various ROIs\ninside deformable objects. LUDO demonstrates the potential to interact with\ndeformable objects without the need for deformable registration methods.\n","authors":["Pit Henrich","Franziska Mathis-Ullrich","Paul Maria Scheikl"],"pdf_url":"https://arxiv.org/pdf/2411.08777v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05001v1","updated":"2025-05-08T07:12:23Z","published":"2025-05-08T07:12:23Z","title":"StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal\n  Bidirectional Warps","summary":"  We retarget video stitching to an emerging issue, named warping shake, which\nunveils the temporal content shakes induced by sequentially unsmooth warps when\nextending image stitching to video stitching. Even if the input videos are\nstable, the stitched video can inevitably cause undesired warping shakes and\naffect the visual experience. To address this issue, we propose StabStitch++, a\nnovel video stitching framework to realize spatial stitching and temporal\nstabilization with unsupervised learning simultaneously. First, different from\nexisting learning-based image stitching solutions that typically warp one image\nto align with another, we suppose a virtual midplane between original image\nplanes and project them onto it. Concretely, we design a differentiable\nbidirectional decomposition module to disentangle the homography transformation\nand incorporate it into our spatial warp, evenly spreading alignment burdens\nand projective distortions across two views. Then, inspired by camera paths in\nvideo stabilization, we derive the mathematical expression of stitching\ntrajectories in video stitching by elaborately integrating spatial and temporal\nwarps. Finally, a warp smoothing model is presented to produce stable stitched\nvideos with a hybrid loss to simultaneously encourage content alignment,\ntrajectory smoothness, and online collaboration. Compared with StabStitch that\nsacrifices alignment for stabilization, StabStitch++ makes no compromise and\noptimizes both of them simultaneously, especially in the online mode. To\nestablish an evaluation benchmark and train the learning framework, we build a\nvideo stitching dataset with a rich diversity in camera motions and scenes.\nExperiments exhibit that StabStitch++ surpasses current solutions in stitching\nperformance, robustness, and efficiency, offering compelling advancements in\nthis field by building a real-time online video stitching system.\n","authors":["Lang Nie","Chunyu Lin","Kang Liao","Yun Zhang","Shuaicheng Liu","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.05001v1.pdf","comment":"TPAMI2025; https://github.com/nie-lang/StabStitch2. arXiv admin note:\n  text overlap with arXiv:2403.06378"},{"id":"http://arxiv.org/abs/2409.07967v3","updated":"2025-05-08T07:03:12Z","published":"2024-09-12T11:54:25Z","title":"Locality-aware Cross-modal Correspondence Learning for Dense\n  Audio-Visual Events Localization","summary":"  Dense-localization Audio-Visual Events (DAVE) aims to identify time\nboundaries and corresponding categories for events that are both audible and\nvisible in a long video, where events may co-occur and exhibit varying\ndurations. However, complex audio-visual scenes often involve asynchronization\nbetween modalities, making accurate localization challenging. Existing DAVE\nsolutions extract audio and visual features through unimodal encoders, and fuse\nthem via dense cross-modal interaction. However, independent unimodal encoding\nstruggles to emphasize shared semantics between modalities without cross-modal\nguidance, while dense cross-modal attention may over-attend to semantically\nunrelated audio-visual features. To address these problems, we present LoCo, a\nLocality-aware cross-modal Correspondence learning framework for DAVE. LoCo\nleverages the local temporal continuity of audio-visual events as important\nguidance to filter irrelevant cross-modal signals and enhance cross-modal\nalignment throughout both unimodal and cross-modal encoding stages. i)\nSpecifically, LoCo applies Local Correspondence Feature (LCF) Modulation to\nenforce unimodal encoders to focus on modality-shared semantics by modulating\nagreement between audio and visual features based on local cross-modal\ncoherence. ii) To better aggregate cross-modal relevant features, we further\ncustomize Local Adaptive Cross-modal (LAC) Interaction, which dynamically\nadjusts attention regions in a data-driven manner. This adaptive mechanism\nfocuses attention on local event boundaries and accommodates varying event\ndurations. By incorporating LCF and LAC, LoCo provides solid performance gains\nand outperforms existing DAVE methods.\n","authors":["Ling Xing","Hongyu Qu","Rui Yan","Xiangbo Shu","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2409.07967v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04996v1","updated":"2025-05-08T07:00:58Z","published":"2025-05-08T07:00:58Z","title":"Inter-Diffusion Generation Model of Speakers and Listeners for Effective\n  Communication","summary":"  Full-body gestures play a pivotal role in natural interactions and are\ncrucial for achieving effective communication. Nevertheless, most existing\nstudies primarily focus on the gesture generation of speakers, overlooking the\nvital role of listeners in the interaction process and failing to fully explore\nthe dynamic interaction between them. This paper innovatively proposes an\nInter-Diffusion Generation Model of Speakers and Listeners for Effective\nCommunication. For the first time, we integrate the full-body gestures of\nlisteners into the generation framework. By devising a novel inter-diffusion\nmechanism, this model can accurately capture the complex interaction patterns\nbetween speakers and listeners during communication. In the model construction\nprocess, based on the advanced diffusion model architecture, we innovatively\nintroduce interaction conditions and the GAN model to increase the denoising\nstep size. As a result, when generating gesture sequences, the model can not\nonly dynamically generate based on the speaker's speech information but also\nrespond in realtime to the listener's feedback, enabling synergistic\ninteraction between the two. Abundant experimental results demonstrate that\ncompared with the current state-of-the-art gesture generation methods, the\nmodel we proposed has achieved remarkable improvements in the naturalness,\ncoherence, and speech-gesture synchronization of the generated gestures. In the\nsubjective evaluation experiments, users highly praised the generated\ninteraction scenarios, believing that they are closer to real life human\ncommunication situations. Objective index evaluations also show that our model\noutperforms the baseline methods in multiple key indicators, providing more\npowerful support for effective communication.\n","authors":["Jinhe Huang","Yongkang Cheng","Yuming Hang","Gaoge Han","Jinewei Li","Jing Zhang","Xingjian Gu"],"pdf_url":"https://arxiv.org/pdf/2505.04996v1.pdf","comment":"accepted by ICMR 2025"},{"id":"http://arxiv.org/abs/2505.04485v2","updated":"2025-05-08T06:43:49Z","published":"2025-05-07T14:58:04Z","title":"FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame\n  Averaging","summary":"  We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural\nnetwork architecture built on top of the well-known KPConv, a widely adopted\nbackbone for 3D point cloud analysis. Even though invariance and/or\nequivariance to Euclidean transformations are required for many common tasks,\nKPConv-based networks can only approximately achieve such properties when\ntraining on large datasets or with significant data augmentations. Using Frame\nAveraging, we allow to flexibly customize point cloud neural networks built\nwith KPConv layers, by making them exactly invariant and/or equivariant to\ntranslations, rotations and/or reflections of the input point clouds. By simply\nwrapping around an existing KPConv-based network, FA-KPConv embeds geometrical\nprior knowledge into it while preserving the number of learnable parameters and\nnot compromising any input information. We showcase the benefit of such an\nintroduced bias for point cloud classification and point cloud registration,\nespecially in challenging cases such as scarce training data or randomly\nrotated test data.\n","authors":["Ali Alawieh","Alexandru P. Condurache"],"pdf_url":"https://arxiv.org/pdf/2505.04485v2.pdf","comment":"8 pages, 2 figures, accepted at IJCNN 2025"},{"id":"http://arxiv.org/abs/2505.04979v1","updated":"2025-05-08T06:32:59Z","published":"2025-05-08T06:32:59Z","title":"Federated Deconfounding and Debiasing Learning for Out-of-Distribution\n  Generalization","summary":"  Attribute bias in federated learning (FL) typically leads local models to\noptimize inconsistently due to the learning of non-causal associations,\nresulting degraded performance. Existing methods either use data augmentation\nfor increasing sample diversity or knowledge distillation for learning\ninvariant representations to address this problem. However, they lack a\ncomprehensive analysis of the inference paths, and the interference from\nconfounding factors limits their performance. To address these limitations, we\npropose the \\underline{Fed}erated \\underline{D}econfounding and\n\\underline{D}ebiasing \\underline{L}earning (FedDDL) method. It constructs a\nstructured causal graph to analyze the model inference process, and performs\nbackdoor adjustment to eliminate confounding paths. Specifically, we design an\nintra-client deconfounding learning module for computer vision tasks to\ndecouple background and objects, generating counterfactual samples that\nestablish a connection between the background and any label, which stops the\nmodel from using the background to infer the label. Moreover, we design an\ninter-client debiasing learning module to construct causal prototypes to reduce\nthe proportion of the background in prototype components. Notably, it bridges\nthe gap between heterogeneous representations via causal prototypical\nregularization. Extensive experiments on 2 benchmarking datasets demonstrate\nthat \\methodname{} significantly enhances the model capability to focus on main\nobjects in unseen data, leading to 4.5\\% higher Top-1 Accuracy on average over\n9 state-of-the-art existing methods.\n","authors":["Zhuang Qi","Sijin Zhou","Lei Meng","Han Hu","Han Yu","Xiangxu Meng"],"pdf_url":"https://arxiv.org/pdf/2505.04979v1.pdf","comment":"IJCAI-25 Accepted"},{"id":"http://arxiv.org/abs/2505.04974v1","updated":"2025-05-08T06:19:18Z","published":"2025-05-08T06:19:18Z","title":"ReAlign: Bilingual Text-to-Motion Generation via Step-Aware\n  Reward-Guided Alignment","summary":"  Bilingual text-to-motion generation, which synthesizes 3D human motions from\nbilingual text inputs, holds immense potential for cross-linguistic\napplications in gaming, film, and robotics. However, this task faces critical\nchallenges: the absence of bilingual motion-language datasets and the\nmisalignment between text and motion distributions in diffusion models, leading\nto semantically inconsistent or low-quality motions. To address these\nchallenges, we propose BiHumanML3D, a novel bilingual human motion dataset,\nwhich establishes a crucial benchmark for bilingual text-to-motion generation\nmodels. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD),\nwhich leverages cross-lingual aligned representations to capture semantics,\nthereby achieving a unified bilingual model. Building upon this, we propose\nReward-guided sampling Alignment (ReAlign) method, comprising a step-aware\nreward model to assess alignment quality during sampling and a reward-guided\nstrategy that directs the diffusion process toward an optimally aligned\ndistribution. This reward model integrates step-aware tokens and combines a\ntext-aligned module for semantic consistency and a motion-aligned module for\nrealism, refining noisy motions at each timestep to balance probability density\nand alignment. Experiments demonstrate that our approach significantly improves\ntext-motion alignment and motion quality compared to existing state-of-the-art\nmethods. Project page: https://wengwanjiang.github.io/ReAlign-page/.\n","authors":["Wanjiang Weng","Xiaofeng Tan","Hongsong Wang","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.04974v1.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2505.04594v2","updated":"2025-05-08T06:18:31Z","published":"2025-05-07T17:37:23Z","title":"MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection","summary":"  Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets.\n","authors":["Zhihao Zhang","Abhinav Kumar","Girish Chandar Ganesan","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2505.04594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04972v1","updated":"2025-05-08T06:16:36Z","published":"2025-05-08T06:16:36Z","title":"AI and Vision based Autonomous Navigation of Nano-Drones in\n  Partially-Known Environments","summary":"  The miniaturisation of sensors and processors, the advancements in connected\nedge intelligence, and the exponential interest in Artificial Intelligence are\nboosting the affirmation of autonomous nano-size drones in the Internet of\nRobotic Things ecosystem. However, achieving safe autonomous navigation and\nhigh-level tasks such as exploration and surveillance with these tiny platforms\nis extremely challenging due to their limited resources. This work focuses on\nenabling the safe and autonomous flight of a pocket-size, 30-gram platform\ncalled Crazyflie 2.1 in a partially known environment. We propose a novel\nAI-aided, vision-based reactive planning method for obstacle avoidance under\nthe ambit of Integrated Sensing, Computing and Communication paradigm. We deal\nwith the constraints of the nano-drone by splitting the navigation task into\ntwo parts: a deep learning-based object detector runs on the edge (external\nhardware) while the planning algorithm is executed onboard. The results show\nthe ability to command the drone at $\\sim8$ frames-per-second and a model\nperformance reaching a COCO mean-average-precision of $60.8$. Field experiments\ndemonstrate the feasibility of the solution with the drone flying at a top\nspeed of $1$ m/s while steering away from an obstacle placed in an unknown\nposition and reaching the target destination. The outcome highlights the\ncompatibility of the communication delay and the model performance with the\nrequirements of the real-time navigation task. We provide a feasible\nalternative to a fully onboard implementation that can be extended to\nautonomous exploration with nano-drones.\n","authors":["Mattia Sartori","Chetna Singhal","Neelabhro Roy","Davide Brunelli","James Gross"],"pdf_url":"https://arxiv.org/pdf/2505.04972v1.pdf","comment":"in DCOSS-IoT 2025, Wi-DroIT 2025"},{"id":"http://arxiv.org/abs/2505.04969v1","updated":"2025-05-08T06:01:11Z","published":"2025-05-08T06:01:11Z","title":"General Transform: A Unified Framework for Adaptive Transform to Enhance\n  Representations","summary":"  Discrete transforms, such as the discrete Fourier transform, are widely used\nin machine learning to improve model performance by extracting meaningful\nfeatures. However, with numerous transforms available, selecting an appropriate\none often depends on understanding the dataset's properties, making the\napproach less effective when such knowledge is unavailable. In this work, we\npropose General Transform (GT), an adaptive transform-based representation\ndesigned for machine learning applications. Unlike conventional transforms, GT\nlearns data-driven mapping tailored to the dataset and task of interest. Here,\nwe demonstrate that models incorporating GT outperform conventional\ntransform-based approaches across computer vision and natural language\nprocessing tasks, highlighting its effectiveness in diverse learning scenarios.\n","authors":["Gekko Budiutama","Shunsuke Daimon","Hirofumi Nishi","Yu-ichiro Matsushita"],"pdf_url":"https://arxiv.org/pdf/2505.04969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03577v2","updated":"2025-05-08T05:49:30Z","published":"2024-10-04T16:30:54Z","title":"Look Twice Before You Answer: Memory-Space Visual Retracing for\n  Hallucination Mitigation in Multimodal Large Language Models","summary":"  Despite their impressive capabilities, multimodal large language models\n(MLLMs) are prone to hallucinations, i.e., the generated content that is\nnonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in\nMLLMs often stem from the sensitivity of text decoder to visual tokens, leading\nto a phenomenon akin to \"amnesia\" about visual information. To address this\nissue, we propose MemVR, a novel decoding paradigm inspired by common\ncognition: when the memory of an image seen the moment before is forgotten,\npeople will look at it again for factual answers. Following this principle, we\ntreat visual tokens as supplementary evidence, re-injecting them into the MLLM\nthrough Feed Forward Network (FFN) as \"key-value memory\" at the middle trigger\nlayer. This \"look-twice\" mechanism occurs when the model exhibits high\nuncertainty during inference, effectively enhancing factual alignment.\nComprehensive experimental evaluations demonstrate that MemVR significantly\nmitigates hallucination across various MLLMs and excels in general benchmarks\nwithout incurring additional time overhead. The implementation is available\nfrom https://github.com/1zhou-Wang/MemVR\n","authors":["Xin Zou","Yizhou Wang","Yibo Yan","Yuanhuiyi Lyu","Kening Zheng","Sirui Huang","Junkai Chen","Peijie Jiang","Jia Liu","Chang Tang","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2410.03577v2.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.04965v1","updated":"2025-05-08T05:49:06Z","published":"2025-05-08T05:49:06Z","title":"DenseGrounding: Improving Dense Language-Vision Semantics for\n  Ego-Centric 3D Visual Grounding","summary":"  Enabling intelligent agents to comprehend and interact with 3D environments\nthrough natural language is crucial for advancing robotics and human-computer\ninteraction. A fundamental task in this field is ego-centric 3D visual\ngrounding, where agents locate target objects in real-world 3D spaces based on\nverbal descriptions. However, this task faces two significant challenges: (1)\nloss of fine-grained visual semantics due to sparse fusion of point clouds with\nego-centric multi-view images, (2) limited textual semantic context due to\narbitrary language descriptions. We propose DenseGrounding, a novel approach\ndesigned to address these issues by enhancing both visual and textual\nsemantics. For visual features, we introduce the Hierarchical Scene Semantic\nEnhancer, which retains dense semantics by capturing fine-grained global scene\nfeatures and facilitating cross-modal alignment. For text descriptions, we\npropose a Language Semantic Enhancer that leverages large language models to\nprovide rich context and diverse language descriptions with additional context\nduring model training. Extensive experiments show that DenseGrounding\nsignificantly outperforms existing methods in overall accuracy, with\nimprovements of 5.81% and 7.56% when trained on the comprehensive full dataset\nand smaller mini subset, respectively, further advancing the SOTA in egocentric\n3D visual grounding. Our method also achieves 1st place and receives the\nInnovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D\nVisual Grounding Track, validating its effectiveness and robustness.\n","authors":["Henry Zheng","Hao Shi","Qihang Peng","Yong Xien Chng","Rui Huang","Yepeng Weng","Zhongchao Shi","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2505.04965v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2505.04964v1","updated":"2025-05-08T05:44:52Z","published":"2025-05-08T05:44:52Z","title":"CAG-VLM: Fine-Tuning of a Large-Scale Model to Recognize Angiographic\n  Images for Next-Generation Diagnostic Systems","summary":"  Coronary angiography (CAG) is the gold-standard imaging modality for\nevaluating coronary artery disease, but its interpretation and subsequent\ntreatment planning rely heavily on expert cardiologists. To enable AI-based\ndecision support, we introduce a two-stage, physician-curated pipeline and a\nbilingual (Japanese/English) CAG image-report dataset. First, we sample 14,686\nframes from 539 exams and annotate them for key-frame detection and left/right\nlaterality; a ConvNeXt-Base CNN trained on this data achieves 0.96 F1 on\nlaterality classification, even on low-contrast frames. Second, we apply the\nCNN to 243 independent exams, extract 1,114 key frames, and pair each with its\npre-procedure report and expert-validated diagnostic and treatment summary,\nyielding a parallel corpus. We then fine-tune three open-source VLMs\n(PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3) via LoRA and evaluate\nthem using VLScore and cardiologist review. Although PaliGemma2 w/LoRA attains\nthe highest VLScore, Gemma3 w/LoRA achieves the top clinician rating (mean\n7.20/10); we designate this best-performing model as CAG-VLM. These results\ndemonstrate that specialized, fine-tuned VLMs can effectively assist\ncardiologists in generating clinical reports and treatment recommendations from\nCAG images.\n","authors":["Yuto Nakamura","Satoshi Kodera","Haruki Settai","Hiroki Shinohara","Masatsugu Tamura","Tomohiro Noguchi","Tatsuki Furusawa","Ryo Takizawa","Tempei Kabayama","Norihiko Takeda"],"pdf_url":"https://arxiv.org/pdf/2505.04964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04963v1","updated":"2025-05-08T05:44:16Z","published":"2025-05-08T05:44:16Z","title":"ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis","summary":"  Synthesizing medical images remains challenging due to limited annotated\npathological data, modality domain gaps, and the complexity of representing\ndiffuse pathologies such as liver cirrhosis. Existing methods often struggle to\nmaintain anatomical fidelity while accurately modeling pathological features,\nfrequently relying on priors derived from natural images or inefficient\nmulti-step sampling. In this work, we introduce ViCTr (Vital Consistency\nTransfer), a novel two-stage framework that combines a rectified flow\ntrajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,\npathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k\ndataset using Elastic Weight Consolidation (EWC) to preserve critical\nanatomical structures. We then fine-tune the model adversarially with Low-Rank\nAdaptation (LoRA) modules for precise control over pathology severity. By\nreformulating Tweedie's formula within a linear trajectory framework, ViCTr\nsupports one-step sampling, reducing inference from 50 steps to just 4, without\nsacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and\nCirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art\nperformance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for\ncirrhosis synthesis 28% lower than existing approaches and improving nnUNet\nsegmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews\nindicate that ViCTr-generated liver cirrhosis MRIs are clinically\nindistinguishable from real scans. To our knowledge, ViCTr is the first method\nto provide fine-grained, pathology-aware MRI synthesis with graded severity\ncontrol, closing a critical gap in AI-driven medical imaging research.\n","authors":["Onkar Susladkar","Gayatri Deshmukh","Yalcin Tur","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2505.04963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04962v1","updated":"2025-05-08T05:43:31Z","published":"2025-05-08T05:43:31Z","title":"An Efficient Method for Accurate Pose Estimation and Error Correction of\n  Cuboidal Objects","summary":"  The proposed system outlined in this paper is a solution to a use case that\nrequires the autonomous picking of cuboidal objects from an organized or\nunorganized pile with high precision. This paper presents an efficient method\nfor precise pose estimation of cuboid-shaped objects, which aims to reduce\nerrors in target pose in a time-efficient manner. Typical pose estimation\nmethods like global point cloud registrations are prone to minor pose errors\nfor which local registration algorithms are generally used to improve pose\naccuracy. However, due to the execution time overhead and uncertainty in the\nerror of the final achieved pose, an alternate, linear time approach is\nproposed for pose error estimation and correction. This paper presents an\noverview of the solution followed by a detailed description of individual\nmodules of the proposed algorithm.\n","authors":["Utsav Rai","Hardik Mehta","Vismay Vakharia","Aditya Choudhary","Amit Parmar","Rolif Lima","Kaushik Das"],"pdf_url":"https://arxiv.org/pdf/2505.04962v1.pdf","comment":"Accepted in IEEE/RSJ IROS 2022 Workshop on Mobile Manipulation and\n  Embodied Intelligence (MOMA)"},{"id":"http://arxiv.org/abs/2505.04961v1","updated":"2025-05-08T05:42:33Z","published":"2025-05-08T05:42:33Z","title":"ADD: Physics-Based Motion Imitation with Adversarial Differential\n  Discriminators","summary":"  Multi-objective optimization problems, which require the simultaneous\noptimization of multiple terms, are prevalent across numerous applications.\nExisting multi-objective optimization methods often rely on manually tuned\naggregation functions to formulate a joint optimization target. The performance\nof such hand-tuned methods is heavily dependent on careful weight selection, a\ntime-consuming and laborious process. These limitations also arise in the\nsetting of reinforcement-learning-based motion tracking for physically\nsimulated characters, where intricately crafted reward functions are typically\nused to achieve high-fidelity results. Such solutions not only require domain\nexpertise and significant manual adjustment, but also limit the applicability\nof the resulting reward function across diverse skills. To bridge this gap, we\npresent a novel adversarial multi-objective optimization technique that is\nbroadly applicable to a range of multi-objective optimization problems,\nincluding motion tracking. The proposed adversarial differential discriminator\nreceives a single positive sample, yet is still effective at guiding the\noptimization process. We demonstrate that our technique can enable characters\nto closely replicate a variety of acrobatic and agile behaviors, achieving\ncomparable quality to state-of-the-art motion-tracking methods, without relying\non manually tuned reward functions. Results are best visualized through\nhttps://youtu.be/rz8BYCE9E2w.\n","authors":["Ziyu Zhang","Sergey Bashkirov","Dun Yang","Michael Taylor","Xue Bin Peng"],"pdf_url":"https://arxiv.org/pdf/2505.04961v1.pdf","comment":"19 pages, 15 figures"},{"id":"http://arxiv.org/abs/2505.04959v1","updated":"2025-05-08T05:41:46Z","published":"2025-05-08T05:41:46Z","title":"MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing\n  pulmonary MRI based on 3D Gaussian representation","summary":"  This study presents an unsupervised, motion-resolved reconstruction framework\nfor high-resolution, free-breathing pulmonary magnetic resonance imaging (MRI),\nutilizing a three-dimensional Gaussian representation (3DGS). The proposed\nmethod leverages 3DGS to address the challenges of motion-resolved 3D isotropic\npulmonary MRI reconstruction by enabling data smoothing between voxels for\ncontinuous spatial representation. Pulmonary MRI data acquisition is performed\nusing a golden-angle radial sampling trajectory, with respiratory motion\nsignals extracted from the center of k-space in each radial spoke. Based on the\nestimated motion signal, the k-space data is sorted into multiple respiratory\nphases. A 3DGS framework is then applied to reconstruct a reference image\nvolume from the first motion state. Subsequently, a patient-specific\nconvolutional neural network is trained to estimate the deformation vector\nfields (DVFs), which are used to generate the remaining motion states through\nspatial transformation of the reference volume. The proposed reconstruction\npipeline is evaluated on six datasets from six subjects and bench-marked\nagainst three state-of-the-art reconstruction methods. The experimental\nfindings demonstrate that the proposed reconstruction framework effectively\nreconstructs high-resolution, motion-resolved pulmonary MR images. Compared\nwith existing approaches, it achieves superior image quality, reflected by\nhigher signal-to-noise ratio and contrast-to-noise ratio. The proposed\nunsupervised 3DGS-based reconstruction method enables accurate motion-resolved\npulmonary MRI with isotropic spatial resolution. Its superior performance in\nimage quality metrics over state-of-the-art methods highlights its potential as\na robust solution for clinical pulmonary MR imaging.\n","authors":["Tengya Peng","Ruyi Zha","Qing Zou"],"pdf_url":"https://arxiv.org/pdf/2505.04959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09049v3","updated":"2025-05-08T05:24:19Z","published":"2024-10-11T17:59:58Z","title":"SceneCraft: Layout-Guided 3D Scene Generation","summary":"  The creation of complex 3D scenes tailored to user specifications has been a\ntedious and challenging task with traditional 3D modeling tools. Although some\npioneering methods have achieved automatic text-to-3D generation, they are\ngenerally limited to small-scale scenes with restricted control over the shape\nand texture. We introduce SceneCraft, a novel method for generating detailed\nindoor scenes that adhere to textual descriptions and spatial layout\npreferences provided by users. Central to our method is a rendering-based\ntechnique, which converts 3D semantic layouts into multi-view 2D proxy maps.\nFurthermore, we design a semantic and depth conditioned diffusion model to\ngenerate multi-view images, which are used to learn a neural radiance field\n(NeRF) as the final scene representation. Without the constraints of panorama\nimage generation, we surpass previous methods in supporting complicated indoor\nspace generation beyond a single room, even as complicated as a whole\nmulti-bedroom apartment with irregular shapes and layouts. Through experimental\nanalysis, we demonstrate that our method significantly outperforms existing\napproaches in complex indoor scene generation with diverse textures, consistent\ngeometry, and realistic visual quality. Code and more results are available at:\nhttps://orangesodahub.github.io/SceneCraft\n","authors":["Xiuyu Yang","Yunze Man","Jun-Kun Chen","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09049v3.pdf","comment":"NeurIPS 2024. Code: https://github.com/OrangeSodahub/SceneCraft\n  Project Page: https://orangesodahub.github.io/SceneCraft"},{"id":"http://arxiv.org/abs/2409.03757v3","updated":"2025-05-08T05:10:27Z","published":"2024-09-05T17:59:56Z","title":"Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding","summary":"  Complex 3D scene understanding has gained increasing attention, with scene\nencoding strategies playing a crucial role in this success. However, the\noptimal scene encoding strategies for various scenarios remain unclear,\nparticularly compared to their image-based counterparts. To address this issue,\nwe present a comprehensive study that probes various visual encoding models for\n3D scene understanding, identifying the strengths and limitations of each model\nacross different scenarios. Our evaluation spans seven vision foundation\nencoders, including image-based, video-based, and 3D foundation models. We\nevaluate these models in four tasks: Vision-Language Scene Reasoning, Visual\nGrounding, Segmentation, and Registration, each focusing on different aspects\nof scene understanding. Our evaluations yield key findings: DINOv2 demonstrates\nsuperior performance, video models excel in object-level tasks, diffusion\nmodels benefit geometric tasks, and language-pretrained models show unexpected\nlimitations in language-related tasks. These insights challenge some\nconventional understandings, provide novel perspectives on leveraging visual\nfoundation models, and highlight the need for more flexible encoder selection\nin future vision-language and scene-understanding tasks. Code:\nhttps://github.com/YunzeMan/Lexicon3D\n","authors":["Yunze Man","Shuhong Zheng","Zhipeng Bao","Martial Hebert","Liang-Yan Gui","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2409.03757v3.pdf","comment":"NeurIPS 2024. Project page: https://yunzeman.github.io/lexicon3d\n  Github: https://github.com/YunzeMan/Lexicon3D"},{"id":"http://arxiv.org/abs/2505.00735v2","updated":"2025-05-08T04:58:46Z","published":"2025-04-29T21:19:29Z","title":"Leveraging Depth Maps and Attention Mechanisms for Enhanced Image\n  Inpainting","summary":"  Existing deep learning-based image inpainting methods typically rely on\nconvolutional networks with RGB images to reconstruct images. However, relying\nexclusively on RGB images may neglect important depth information, which plays\na critical role in understanding the spatial and structural context of a scene.\nJust as human vision leverages stereo cues to perceive depth, incorporating\ndepth maps into the inpainting process can enhance the model's ability to\nreconstruct images with greater accuracy and contextual awareness. In this\npaper, we propose a novel approach that incorporates both RGB and depth images\nfor enhanced image inpainting. Our models employ a dual encoder architecture,\nwhere one encoder processes the RGB image and the other handles the depth\nimage. The encoded features from both encoders are then fused in the decoder\nusing an attention mechanism, effectively integrating the RGB and depth\nrepresentations. We use two different masking strategies, line and square, to\ntest the robustness of the model under different types of occlusions. To\nfurther analyze the effectiveness of our approach, we use Gradient-weighted\nClass Activation Mapping (Grad-CAM) visualizations to examine the regions of\ninterest the model focuses on during inpainting. We show that incorporating\ndepth information alongside the RGB image significantly improves the\nreconstruction quality. Through both qualitative and quantitative comparisons,\nwe demonstrate that the depth-integrated model outperforms the baseline, with\nattention mechanisms further enhancing inpainting performance, as evidenced by\nmultiple evaluation metrics and visualization.\n","authors":["Jin Hyun Park","Harine Choi","Praewa Pitiphat"],"pdf_url":"https://arxiv.org/pdf/2505.00735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04946v1","updated":"2025-05-08T04:49:52Z","published":"2025-05-08T04:49:52Z","title":"T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video\n  Generation Models","summary":"  Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis.\n","authors":["Xuyang Guo","Jiayan Huo","Zhenmei Shi","Zhao Song","Jiahao Zhang","Jiale Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.04946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08821v2","updated":"2025-05-08T04:42:55Z","published":"2025-02-12T22:24:49Z","title":"DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with\n  Saliency Maps","summary":"  The recent surge in advanced generative models, such as diffusion models and\ngenerative adversarial networks (GANs), has led to an alarming rise in\nAI-generated images across various domains on the web. While such technologies\noffer benefits such as democratizing artistic creation, they also pose\nchallenges in misinformation, digital forgery, and authenticity verification.\nAdditionally, the uncredited use of AI-generated images in media and marketing\nhas sparked significant backlash from online communities. In response to this,\nwe introduce DejAIvu, a Chrome Web extension that combines real-time\nAI-generated image detection with saliency-based explainability while users\nbrowse the web. Using an ONNX-optimized deep learning model, DejAIvu\nautomatically analyzes images on websites such as Google Images, identifies\nAI-generated content using model inference, and overlays a saliency heatmap to\nhighlight AI-related artifacts. Our approach integrates efficient in-browser\ninference, gradient-based saliency analysis, and a seamless user experience,\nensuring that AI detection is both transparent and interpretable. We also\nevaluate DejAIvu across multiple pretrained architectures and benchmark\ndatasets, demonstrating high accuracy and low latency, making it a practical\nand deployable tool for enhancing AI image accountability. The code for this\nsystem can be found at https://github.com/Noodulz/dejAIvu.\n","authors":["Jocelyn Dzuong"],"pdf_url":"https://arxiv.org/pdf/2502.08821v2.pdf","comment":"5 pages, 3 figures. Accepted to IJCAI 2025 Demo Track. Revised\n  version will be uploaded soon"},{"id":"http://arxiv.org/abs/2505.04941v1","updated":"2025-05-08T04:37:12Z","published":"2025-05-08T04:37:12Z","title":"Building-Guided Pseudo-Label Learning for Cross-Modal Building Damage\n  Mapping","summary":"  Accurate building damage assessment using bi-temporal multi-modal remote\nsensing images is essential for effective disaster response and recovery\nplanning. This study proposes a novel Building-Guided Pseudo-Label Learning\nFramework to address the challenges of mapping building damage from\npre-disaster optical and post-disaster SAR images. First, we train a series of\nbuilding extraction models using pre-disaster optical images and building\nlabels. To enhance building segmentation, we employ multi-model fusion and\ntest-time augmentation strategies to generate pseudo-probabilities, followed by\na low-uncertainty pseudo-label training method for further refinement. Next, a\nchange detection model is trained on bi-temporal cross-modal images and damaged\nbuilding labels. To improve damage classification accuracy, we introduce a\nbuilding-guided low-uncertainty pseudo-label refinement strategy, which\nleverages building priors from the previous step to guide pseudo-label\ngeneration for damaged buildings, reducing uncertainty and enhancing\nreliability. Experimental results on the 2025 IEEE GRSS Data Fusion Contest\ndataset demonstrate the effectiveness of our approach, which achieved the\nhighest mIoU score (54.28%) and secured first place in the competition.\n","authors":["Jiepan Li","He Huang","Yu Sheng","Yujun Guo","Wei He"],"pdf_url":"https://arxiv.org/pdf/2505.04941v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04938v1","updated":"2025-05-08T04:27:11Z","published":"2025-05-08T04:27:11Z","title":"FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image\n  Registration","summary":"  In recent years, deformable medical image registration techniques have made\nsignificant progress. However, existing models still lack efficiency in\nparallel extraction of coarse and fine-grained features. To address this, we\nconstruct a new pyramid registration network based on feature and deformation\nfield (FF-PNet). For coarse-grained feature extraction, we design a Residual\nFeature Fusion Module (RFFM), for fine-grained image deformation, we propose a\nResidual Deformation Field Fusion Module (RDFFM). Through the parallel\noperation of these two modules, the model can effectively handle complex image\ndeformations. It is worth emphasizing that the encoding stage of FF-PNet only\nemploys traditional convolutional neural networks without any attention\nmechanisms or multilayer perceptrons, yet it still achieves remarkable\nimprovements in registration accuracy, fully demonstrating the superior feature\ndecoding capabilities of RFFM and RDFFM. We conducted extensive experiments on\nthe LPBA and OASIS datasets. The results show our network consistently\noutperforms popular methods in metrics like the Dice Similarity Coefficient.\n","authors":["Ying Zhang","Shuai Guo","Chenxi Sun","Yuchen Zhu","Jinhai Xiang"],"pdf_url":"https://arxiv.org/pdf/2505.04938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04105v2","updated":"2025-05-08T04:09:55Z","published":"2025-05-07T03:44:28Z","title":"MAISY: Motion-Aware Image SYnthesis for Medical Image Motion Correction","summary":"  Patient motion during medical image acquisition causes blurring, ghosting,\nand distorts organs, which makes image interpretation challenging. Current\nstate-of-the-art algorithms using Generative Adversarial Network (GAN)-based\nmethods with their ability to learn the mappings between corrupted images and\ntheir ground truth via Structural Similarity Index Measure (SSIM) loss\neffectively generate motion-free images. However, we identified the following\nlimitations: (i) they mainly focus on global structural characteristics and\ntherefore overlook localized features that often carry critical pathological\ninformation, and (ii) the SSIM loss function struggles to handle images with\nvarying pixel intensities, luminance factors, and variance. In this study, we\npropose Motion-Aware Image SYnthesis (MAISY) which initially characterize\nmotion and then uses it for correction by: (a) leveraging the foundation model\nSegment Anything Model (SAM), to dynamically learn spatial patterns along\nanatomical boundaries where motion artifacts are most pronounced and, (b)\nintroducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively\nemphasizes spatial regions with high pixel variance to preserve essential\nanatomical details during artifact correction. Experiments on chest and head CT\ndatasets demonstrate that our model outperformed the state-of-the-art\ncounterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by\n10%, and Dice by 16%.\n","authors":["Andrew Zhang","Hao Wang","Shuchang Ye","Michael Fulham","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2505.04105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01894v2","updated":"2025-05-08T03:50:30Z","published":"2025-02-27T19:18:37Z","title":"LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces","summary":"  We introduce the Local Intersectional Visual Spaces (LIVS) dataset, a\nbenchmark for multi-criteria alignment, developed through a two-year\nparticipatory process with 30 community organizations to support the\npluralistic alignment of text-to-image (T2I) models in inclusive urban\nplanning. The dataset encodes 37,710 pairwise comparisons across 13,462 images,\nstructured along six criteria - Accessibility, Safety, Comfort, Invitingness,\nInclusivity, and Diversity - derived from 634 community-defined concepts. Using\nDirect Preference Optimization (DPO), we fine-tune Stable Diffusion XL to\nreflect multi-criteria spatial preferences and evaluate the LIVS dataset and\nthe fine-tuned model through four case studies: (1) DPO increases alignment\nwith annotated preferences, particularly when annotation volume is high; (2)\npreference patterns vary across participant identities, underscoring the need\nfor intersectional data; (3) human-authored prompts generate more distinctive\nvisual outputs than LLM-generated ones, influencing annotation decisiveness;\nand (4) intersectional groups assign systematically different ratings across\ncriteria, revealing the limitations of single-objective alignment. While DPO\nimproves alignment under specific conditions, the prevalence of neutral ratings\nindicates that community values are heterogeneous and often ambiguous. LIVS\nprovides a benchmark for developing T2I models that incorporate local,\nstakeholder-driven preferences, offering a foundation for context-aware\nalignment in spatial design.\n","authors":["Rashid Mushkani","Shravan Nayak","Hugo Berard","Allison Cohen","Shin Koseki","Hadrien Bertrand"],"pdf_url":"https://arxiv.org/pdf/2503.01894v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.04922v1","updated":"2025-05-08T03:37:07Z","published":"2025-05-08T03:37:07Z","title":"Canny2Palm: Realistic and Controllable Palmprint Generation for\n  Large-scale Pre-training","summary":"  Palmprint recognition is a secure and privacy-friendly method of biometric\nidentification. One of the major challenges to improve palmprint recognition\naccuracy is the scarcity of palmprint data. Recently, a popular line of\nresearch revolves around the synthesis of virtual palmprints for large-scale\npre-training purposes. In this paper, we propose a novel synthesis method named\nCanny2Palm that extracts palm textures with Canny edge detector and uses them\nto condition a Pix2Pix network for realistic palmprint generation. By\nre-assembling palmprint textures from different identities, we are able to\ncreate new identities by seeding the generator with new assemblies. Canny2Palm\nnot only synthesizes realistic data following the distribution of real\npalmprints but also enables controllable diversity to generate large-scale new\nidentities. On open-set palmprint recognition benchmarks, models pre-trained\nwith Canny2Palm synthetic data outperform the state-of-the-art with up to 7.2%\nhigher identification accuracy. Moreover, the performance of models pre-trained\nwith Canny2Palm continues to improve given 10,000 synthetic IDs while those\nwith existing methods already saturate, demonstrating the potential of our\nmethod for large-scale pre-training.\n","authors":["Xingzeng Lan","Xing Duan","Chen Chen","Weiyu Lin","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2505.04922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04921v1","updated":"2025-05-08T03:35:23Z","published":"2025-05-08T03:35:23Z","title":"Perception, Reason, Think, and Plan: A Survey on Large Multimodal\n  Reasoning Models","summary":"  Reasoning lies at the heart of intelligence, shaping the ability to make\ndecisions, draw conclusions, and generalize across domains. In artificial\nintelligence, as systems increasingly operate in open, uncertain, and\nmultimodal environments, reasoning becomes essential for enabling robust and\nadaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a\npromising paradigm, integrating modalities such as text, images, audio, and\nvideo to support complex reasoning capabilities and aiming to achieve\ncomprehensive perception, precise understanding, and deep reasoning. As\nresearch advances, multimodal reasoning has rapidly evolved from modular,\nperception-driven pipelines to unified, language-centric frameworks that offer\nmore coherent cross-modal understanding. While instruction tuning and\nreinforcement learning have improved model reasoning, significant challenges\nremain in omni-modal generalization, reasoning depth, and agentic behavior. To\naddress these issues, we present a comprehensive and structured survey of\nmultimodal reasoning research, organized around a four-stage developmental\nroadmap that reflects the field's shifting design philosophies and emerging\ncapabilities. First, we review early efforts based on task-specific modules,\nwhere reasoning was implicitly embedded across stages of representation,\nalignment, and fusion. Next, we examine recent approaches that unify reasoning\ninto multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT)\nand multimodal reinforcement learning enabling richer and more structured\nreasoning chains. Finally, drawing on empirical insights from challenging\nbenchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the\nconceptual direction of native large multimodal reasoning models (N-LMRMs),\nwhich aim to support scalable, agentic, and adaptive reasoning and planning in\ncomplex, real-world environments.\n","authors":["Yunxin Li","Zhenyu Liu","Zitao Li","Xuanyu Zhang","Zhenran Xu","Xinyu Chen","Haoyuan Shi","Shenyuan Jiang","Xintong Wang","Jifang Wang","Shouzheng Huang","Xinping Zhao","Borui Jiang","Lanqing Hong","Longyue Wang","Zhuotao Tian","Baoxing Huai","Wenhan Luo","Weihua Luo","Zheng Zhang","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.04921v1.pdf","comment":"75 Pages,10 figures; Project:\n  https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models"},{"id":"http://arxiv.org/abs/2505.04917v1","updated":"2025-05-08T03:16:03Z","published":"2025-05-08T03:16:03Z","title":"A Simple Detector with Frame Dynamics is a Strong Tracker","summary":"  Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle\n(Anti-UAV) applications. Existing trackers often depend on cropped template\nregions and have limited motion modeling capabilities, which pose challenges\nwhen dealing with tiny targets. To address this, we propose a simple yet\neffective infrared tiny-object tracker that enhances tracking performance by\nintegrating global detection and motion-aware learning with temporal priors.\nOur method is based on object detection and achieves significant improvements\nthrough two key innovations. First, we introduce frame dynamics, leveraging\nframe difference and optical flow to encode both prior target features and\nmotion characteristics at the input level, enabling the model to better\ndistinguish the target from background clutter. Second, we propose a trajectory\nconstraint filtering strategy in the post-processing stage, utilizing\nspatio-temporal priors to suppress false positives and enhance tracking\nrobustness. Extensive experiments show that our method consistently outperforms\nexisting approaches across multiple metrics in challenging infrared UAV\ntracking scenarios. Notably, we achieve state-of-the-art performance in the 4th\nAnti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.\n","authors":["Chenxu Peng","Chenxu Wang","Minrui Zou","Danyang Li","Zhengpeng Yang","Yimian Dai","Ming-Ming Cheng","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2505.04917v1.pdf","comment":"2025 CVPR Anti-UAV Workshop"},{"id":"http://arxiv.org/abs/2505.04915v1","updated":"2025-05-08T03:11:58Z","published":"2025-05-08T03:11:58Z","title":"GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing","summary":"  Scene text editing, a subfield of image editing, requires modifying texts in\nimages while preserving style consistency and visual coherence with the\nsurrounding environment. While diffusion-based methods have shown promise in\ntext generation, they still struggle to produce high-quality results. These\nmethods often generate distorted or unrecognizable characters, particularly\nwhen dealing with complex characters like Chinese. In such systems, characters\nare composed of intricate stroke patterns and spatial relationships that must\nbe precisely maintained. We present GlyphMastero, a specialized glyph encoder\ndesigned to guide the latent diffusion model for generating texts with\nstroke-level precision. Our key insight is that existing methods, despite using\npretrained OCR models for feature extraction, fail to capture the hierarchical\nnature of text structures - from individual strokes to stroke-level\ninteractions to overall character-level structure. To address this, our glyph\nencoder explicitly models and captures the cross-level interactions between\nlocal-level individual characters and global-level text lines through our novel\nglyph attention module. Meanwhile, our model implements a feature pyramid\nnetwork to fuse the multi-scale OCR backbone features at the global-level.\nThrough these cross-level and multi-scale fusions, we obtain more detailed\nglyph-aware guidance, enabling precise control over the scene text generation\nprocess. Our method achieves an 18.02\\% improvement in sentence accuracy over\nthe state-of-the-art multi-lingual scene text editing baseline, while\nsimultaneously reducing the text-region Fr\\'echet inception distance by\n53.28\\%.\n","authors":["Tong Wang","Ting Liu","Xiaochao Qu","Chengjing Wu","Luoqi Liu","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2505.04915v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2505.04913v1","updated":"2025-05-08T03:09:27Z","published":"2025-05-08T03:09:27Z","title":"Advanced 3D Imaging Approach to TSV/TGV Metrology and Inspection Using\n  Only Optical Microscopy","summary":"  This paper introduces an innovative approach to silicon and glass via\ninspection, which combines hybrid field microscopy with photometric stereo.\nConventional optical microscopy techniques are generally limited to superficial\ninspections and struggle to effectively visualize the internal structures of\nsilicon and glass vias. By utilizing various lighting conditions for 3D\nreconstruction, the proposed method surpasses these limitations. By integrating\nphotometric stereo to the traditional optical microscopy, the proposed method\nnot only enhances the capability to detect micro-scale defects but also\nprovides a detailed visualization of depth and edge abnormality, which are\ntypically not visible with conventional optical microscopy inspection. The\nexperimental results demonstrated that the proposed method effectively captures\nintricate surface details and internal structures. Quantitative comparisons\nbetween the reconstructed models and actual measurements present the capability\nof the proposed method to significantly improve silicon and glass via\ninspection process. As a result, the proposed method achieves enhanced\ncost-effectiveness while maintaining high accuracy and repeatability,\nsuggesting substantial advancements in silicon and glass via inspection\ntechniques\n","authors":["Gugeong Sung"],"pdf_url":"https://arxiv.org/pdf/2505.04913v1.pdf","comment":"6 pages, 6 figures, Submitted to arXiv for preprint"},{"id":"http://arxiv.org/abs/2505.04911v1","updated":"2025-05-08T02:59:01Z","published":"2025-05-08T02:59:01Z","title":"SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with\n  Off-the-Shelf Multimodal Large Language Models","summary":"  This study introduces SpatialPrompting, a novel framework that harnesses the\nemergent reasoning capabilities of off-the-shelf multimodal large language\nmodels to achieve zero-shot spatial reasoning in three-dimensional (3D)\nenvironments. Unlike existing methods that rely on expensive 3D-specific\nfine-tuning with specialized 3D inputs such as point clouds or voxel-based\nfeatures, SpatialPrompting employs a keyframe-driven prompt generation\nstrategy. This framework uses metrics such as vision-language similarity,\nMahalanobis distance, field of view, and image sharpness to select a diverse\nand informative set of keyframes from image sequences and then integrates them\nwith corresponding camera pose data to effectively abstract spatial\nrelationships and infer complex 3D structures. The proposed framework not only\nestablishes a new paradigm for flexible spatial reasoning that utilizes\nintuitive visual and positional cues but also achieves state-of-the-art\nzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across\nseveral metrics. The proposed method effectively eliminates the need for\nspecialized 3D inputs and fine-tuning, offering a simpler and more scalable\nalternative to conventional approaches.\n","authors":["Shun Taguchi","Hideki Deguchi","Takumi Hamazaki","Hiroyuki Sakai"],"pdf_url":"https://arxiv.org/pdf/2505.04911v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.04905v1","updated":"2025-05-08T02:44:53Z","published":"2025-05-08T02:44:53Z","title":"Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised\n  Object Localization","summary":"  Weakly Supervised Object Localization (WSOL), which aims to localize objects\nby only using image-level labels, has attracted much attention because of its\nlow annotation cost in real applications. Current studies focus on the Class\nActivation Map (CAM) of CNN and the self-attention map of transformer to\nidentify the region of objects. However, both CAM and self-attention maps can\nnot learn pixel-level fine-grained information on the foreground objects, which\nhinders the further advance of WSOL. To address this problem, we initiatively\nleverage the capability of zero-shot generalization and fine-grained\nsegmentation in Segment Anything Model (SAM) to boost the activation of\nintegral object regions. Further, to alleviate the semantic ambiguity issue\naccrued in single point prompt-based SAM, we propose an innovative mask prompt\nto SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a\nGlobal Token Transformer (GTFormer) to generate a coarse-grained foreground map\nas a flexible mask prompt, where the GTFormer jointly embeds patch tokens and\nnovel global tokens to learn foreground semantics. Secondly, we deliver grid\npoints as dense prompts into SAM to maximize the probability of foreground\nmask, which avoids the lack of objects caused by a single point/box prompt.\nFinally, we propose a pixel-level similarity metric to come true the mask\nmatching from mask prompt to SAM, where the mask with the highest score is\nviewed as the final localization map. Experiments show that the proposed\nPro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC,\nwith 84.03\\% and 66.85\\% Top-1 Loc, respectively.\n","authors":["Xi Yang","Songsong Duan","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2505.04905v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2505.04899v1","updated":"2025-05-08T02:30:44Z","published":"2025-05-08T02:30:44Z","title":"OWT: A Foundational Organ-Wise Tokenization Framework for Medical\n  Imaging","summary":"  Recent advances in representation learning often rely on holistic, black-box\nembeddings that entangle multiple semantic components, limiting\ninterpretability and generalization. These issues are especially critical in\nmedical imaging. To address these limitations, we propose an Organ-Wise\nTokenization (OWT) framework with a Token Group-based Reconstruction (TGR)\ntraining paradigm. Unlike conventional approaches that produce holistic\nfeatures, OWT explicitly disentangles an image into separable token groups,\neach corresponding to a distinct organ or semantic entity. Our design ensures\neach token group encapsulates organ-specific information, boosting\ninterpretability, generalization, and efficiency while allowing fine-grained\ncontrol in downstream tasks. Experiments on CT and MRI datasets demonstrate the\neffectiveness of OWT in not only achieving strong image reconstruction and\nsegmentation performance, but also enabling novel semantic-level generation and\nretrieval applications that are out of reach for standard holistic embedding\nmethods. These findings underscore the potential of OWT as a foundational\nframework for semantically disentangled representation learning, offering broad\nscalability and applicability to real-world medical imaging scenarios and\nbeyond.\n","authors":["Sifan Song","Siyeop Yoon","Pengfei Jin","Sekeun Kim","Matthew Tivnan","Yujin Oh","Runqi Meng","Ling Chen","Zhiliang Lyu","Dufan Wu","Ning Guo","Xiang Li","Quanzheng Li"],"pdf_url":"https://arxiv.org/pdf/2505.04899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02180v2","updated":"2025-05-08T02:26:55Z","published":"2025-01-04T04:09:57Z","title":"Quaternionic Reweighted Amplitude Flow for Phase Retrieval in Image\n  Reconstruction","summary":"  Quaternionic signal processing provides powerful tools for efficiently\nmanaging color signals by preserving the intrinsic correlations among signal\ndimensions through quaternion algebra. In this paper, we address the\nquaternionic phase retrieval problem by systematically developing novel\nalgorithms based on an amplitude-based model. Specifically, we propose the\nQuaternionic Reweighted Amplitude Flow (QRAF) algorithm, which is further\nenhanced by three of its variants: incremental, accelerated, and adapted QRAF\nalgorithms. In addition, we introduce the Quaternionic Perturbed Amplitude Flow\n(QPAF) algorithm, which has linear convergence. Extensive numerical experiments\non both synthetic data and real images, demonstrate that our proposed methods\nsignificantly improve recovery performance and computational efficiency\ncompared to state-of-the-art approaches.\n","authors":["Ren Hu","Pan Lian"],"pdf_url":"https://arxiv.org/pdf/2501.02180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05956v3","updated":"2025-05-08T02:20:07Z","published":"2024-08-12T07:13:08Z","title":"Boosting Adverse Weather Crowd Counting via Multi-queue Contrastive\n  Learning","summary":"  Currently, most crowd counting methods have outstanding performance under\nnormal weather conditions. However, our experimental validation reveals two key\nobstacles limiting the accuracy improvement of crowd counting models: 1) the\ndomain gap between the adverse weather and the normal weather images; 2) the\nweather class imbalance in the training set. To address the problems, we\npropose a two-stage crowd counting method named Multi-queue Contrastive\nLearning (MQCL). Specifically, in the first stage, our target is to equip the\nbackbone network with weather-awareness capabilities. In this process, a\ncontrastive learning method named multi-queue MoCo designed by us is employed\nto enable representation learning under weather class imbalance. After the\nfirst stage is completed, the backbone model is \"mature\" enough to extract\nweather-related representations. On this basis, we proceed to the second stage,\nin which we propose to refine the representations under the guidance of\ncontrastive learning, enabling the conversion of the weather-aware\nrepresentations to the normal weather domain. Through such representation and\nconversion, the model achieves robust counting performance under both normal\nand adverse weather conditions. Extensive experimental results show that,\ncompared to the baseline, MQCL reduces the counting error under adverse weather\nconditions by 22%, while introducing only about 13% increase in computational\nburden, which achieves state-of-the-art performance.\n","authors":["Tianhang Pan","Xiuyi Jia"],"pdf_url":"https://arxiv.org/pdf/2408.05956v3.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.04888v1","updated":"2025-05-08T01:49:53Z","published":"2025-05-08T01:49:53Z","title":"Cross-Branch Orthogonality for Improved Generalization in Face Deepfake\n  Detection","summary":"  Remarkable advancements in generative AI technology have given rise to a\nspectrum of novel deepfake categories with unprecedented leaps in their\nrealism, and deepfakes are increasingly becoming a nuisance to law enforcement\nauthorities and the general public. In particular, we observe alarming levels\nof confusion, deception, and loss of faith regarding multimedia content within\nsociety caused by face deepfakes, and existing deepfake detectors are\nstruggling to keep up with the pace of improvements in deepfake generation.\nThis is primarily due to their reliance on specific forgery artifacts, which\nlimits their ability to generalise and detect novel deepfake types. To combat\nthe spread of malicious face deepfakes, this paper proposes a new strategy that\nleverages coarse-to-fine spatial information, semantic information, and their\ninteractions while ensuring feature distinctiveness and reducing the redundancy\nof the modelled features. A novel feature orthogonality-based disentanglement\nstrategy is introduced to ensure branch-level and cross-branch feature\ndisentanglement, which allows us to integrate multiple feature vectors without\nadding complexity to the feature space or compromising generalisation.\nComprehensive experiments on three public benchmarks: FaceForensics++,\nCeleb-DF, and the Deepfake Detection Challenge (DFDC) show that these design\nchoices enable the proposed approach to outperform current state-of-the-art\nmethods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a\ncross-dataset evaluation setting.\n","authors":["Tharindu Fernando","Clinton Fookes","Sridha Sridharan","Simon Denman"],"pdf_url":"https://arxiv.org/pdf/2505.04888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11470v5","updated":"2025-05-08T01:21:27Z","published":"2023-07-21T10:10:18Z","title":"Semi-supervised Underwater Image Enhancement Using A Physics-Aware\n  Triple-Stream Network","summary":"  Underwater images normally suffer from degradation due to the transmission\nmedium of water bodies. Both traditional prior-based approaches and deep\nlearning-based methods have been used to address this problem. However, the\ninflexible assumption of the former often impairs their effectiveness in\nhandling diverse underwater scenes, while the generalization of the latter to\nunseen images is usually weakened by insufficient data. In this study, we\nleverage both the physics-based Image Formation Model (IFM) and deep learning\ntechniques for Underwater Image Enhancement (UIE). To this end, we propose a\nnovel Physics-Aware Triple-Stream Underwater Image Enhancement Network, i.e.,\nPATS-UIENet, which comprises a Direct Signal Transmission Estimation Steam\n(D-Stream), a Backscatter Signal Transmission Estimation Steam (B-Stream) and\nan Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE\ntask by explicitly estimating the degradation parameters of a revised IFM. We\nalso adopt an IFM-inspired semi-supervised learning framework, which exploits\nboth the labeled and unlabeled images, to address the issue of insufficient\ndata. To our knowledge, such a physics-aware deep network and the IFM-inspired\nsemi-supervised learning framework have not been used for the UIE task before.\nOur method performs better than, or at least comparably to, sixteen baselines\nacross six testing sets in the degradation estimation and UIE tasks. These\npromising results should be due to the fact that the proposed method can not\nonly model the degradation but also learn the characteristics of diverse\nunderwater scenes.\n","authors":["Shixuan Xu","Hao Qi","Xinghui Dong"],"pdf_url":"https://arxiv.org/pdf/2307.11470v5.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2505.03838v2","updated":"2025-05-08T01:21:21Z","published":"2025-05-05T04:09:31Z","title":"IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation\n  and Classification","summary":"  Precise and effective processing of cardiac imaging data is critical for the\nidentification and management of the cardiovascular diseases. We introduce\nIntelliCardiac, a comprehensive, web-based medical image processing platform\nfor the automatic segmentation of 4D cardiac images and disease classification,\nutilizing an AI model trained on the publicly accessible ACDC dataset. The\nsystem, intended for patients, cardiologists, and healthcare professionals,\noffers an intuitive interface and uses deep learning models to identify\nessential heart structures and categorize cardiac diseases. The system supports\nanalysis of both the right and left ventricles as well as myocardium, and then\nclassifies patient's cardiac images into five diagnostic categories: dilated\ncardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right\nventricular abnormality, and no disease. IntelliCardiac combines a deep\nlearning-based segmentation model with a two-step classification pipeline. The\nsegmentation module gains an overall accuracy of 92.6%. The classification\nmodule, trained on characteristics taken from segmented heart structures,\nachieves 98% accuracy in five categories. These results exceed the performance\nof the existing state-of-the-art methods that integrate both segmentation and\nclassification models. IntelliCardiac, which supports real-time visualization,\nworkflow integration, and AI-assisted diagnostics, has great potential as a\nscalable, accurate tool for clinical decision assistance in cardiac imaging and\ndiagnosis.\n","authors":["Ting Yu Tsai","An Yu","Meghana Spurthi Maadugundu","Ishrat Jahan Mohima","Umme Habiba Barsha","Mei-Hwa F. Chen","Balakrishnan Prabhakaran","Ming-Ching Chang"],"pdf_url":"https://arxiv.org/pdf/2505.03838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04877v1","updated":"2025-05-08T01:20:24Z","published":"2025-05-08T01:20:24Z","title":"Learning from Loss Landscape: Generalizable Mixed-Precision Quantization\n  via Adaptive Sharpness-Aware Gradient Aligning","summary":"  Mixed Precision Quantization (MPQ) has become an essential technique for\noptimizing neural network by determining the optimal bitwidth per layer.\nExisting MPQ methods, however, face a major hurdle: they require a\ncomputationally expensive search for quantization policies on large-scale\ndatasets. To resolve this issue, we introduce a novel approach that first\nsearches for quantization policies on small datasets and then generalizes them\nto large-scale datasets. This approach simplifies the process, eliminating the\nneed for large-scale quantization fine-tuning and only necessitating model\nweight adjustment. Our method is characterized by three key techniques:\nsharpness-aware minimization for enhanced quantization generalization, implicit\ngradient direction alignment to handle gradient conflicts among different\noptimization objectives, and an adaptive perturbation radius to accelerate\noptimization. Both theoretical analysis and experimental results validate our\napproach. Using the CIFAR10 dataset (just 0.5\\% the size of ImageNet training\ndata) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a\nsignificantly lower computational cost, while improving efficiency by up to\n150% over the baselines.\n","authors":["Lianbo Ma","Jianlun Ma","Yuee Zhou","Guoyang Xie","Qiang He","Zhichao Lu"],"pdf_url":"https://arxiv.org/pdf/2505.04877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09085v2","updated":"2025-05-08T01:17:59Z","published":"2024-09-11T05:28:52Z","title":"HESSO: Towards Automatic Efficient and User Friendly Any Neural Network\n  Training and Pruning","summary":"  Structured pruning is one of the most popular approaches to effectively\ncompress the heavy deep neural networks (DNNs) into compact sub-networks while\nretaining performance. The existing methods suffer from multi-stage procedures\nalong with significant engineering efforts and human expertise. The\nOnly-Train-Once (OTO) series has been recently proposed to resolve the many\npain points by streamlining the workflow by automatically conducting (i) search\nspace generation, (ii) structured sparse optimization, and (iii) sub-network\nconstruction. However, the built-in sparse optimizers in the OTO series, i.e.,\nthe Half-Space Projected Gradient (HSPG) family, have limitations that require\nhyper-parameter tuning and the implicit controls of the sparsity exploration,\nconsequently requires intervening by human expertise. To address such\nlimitations, we propose a Hybrid Efficient Structured Sparse Optimizer (HESSO).\nHESSO could automatically and efficiently train a DNN to produce a\nhigh-performing subnetwork. Meanwhile, it is almost tuning-free and enjoys\nuser-friendly integration for generic training applications. To address another\ncommon issue of irreversible performance collapse observed in pruning DNNs, we\nfurther propose a Corrective Redundant Identification Cycle (CRIC) for reliably\nidentifying indispensable structures. We numerically demonstrate the efficacy\nof HESSO and its enhanced version HESSO-CRIC on a variety of applications\nranging from computer vision to natural language processing, including large\nlanguage model. The numerical results showcase that HESSO can achieve\ncompetitive even superior performance to varying state-of-the-arts and support\nmost DNN architectures. Meanwhile, CRIC can effectively prevent the\nirreversible performance collapse and further enhance the performance of HESSO\non certain applications.\n","authors":["Tianyi Chen","Xiaoyi Qu","David Aponte","Colby Banbury","Jongwoo Ko","Tianyu Ding","Yong Ma","Vladimir Lyapunov","Ilya Zharkov","Luming Liang"],"pdf_url":"https://arxiv.org/pdf/2409.09085v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.08142v2","updated":"2025-05-08T00:48:48Z","published":"2024-03-13T00:04:07Z","title":"FieldNet: Efficient Real-Time Shadow Removal for Enhanced Vision in\n  Field Robotics","summary":"  Shadows significantly hinder computer vision tasks in outdoor environments,\nparticularly in field robotics, where varying lighting conditions complicate\nobject detection and localisation. We present FieldNet, a novel deep learning\nframework for real-time shadow removal, optimised for resource-constrained\nhardware. FieldNet introduces a probabilistic enhancement module and a novel\nloss function to address challenges of inconsistent shadow boundary supervision\nand artefact generation, achieving enhanced accuracy and simplicity without\nrequiring shadow masks during inference. Trained on a dataset of 10,000 natural\nimages augmented with synthetic shadows, FieldNet outperforms state-of-the-art\nmethods on benchmark datasets (ISTD, ISTD+, SRD), with up to $9$x speed\nimprovements (66 FPS on Nvidia 2080Ti) and superior shadow removal quality\n(PSNR: 38.67, SSIM: 0.991). Real-world case studies in precision agriculture\nrobotics demonstrate the practical impact of FieldNet in enhancing weed\ndetection accuracy. These advancements establish FieldNet as a robust,\nefficient solution for real-time vision tasks in field robotics and beyond.\n","authors":["Alzayat Saleh","Alex Olsen","Jake Wood","Bronson Philippa","Mostafa Rahimi Azghadi"],"pdf_url":"https://arxiv.org/pdf/2403.08142v2.pdf","comment":"22 pages, 9 figures, 8 tables. Published at Expert Systems with\n  Applications"},{"id":"http://arxiv.org/abs/2505.04864v1","updated":"2025-05-08T00:28:31Z","published":"2025-05-08T00:28:31Z","title":"Auto-regressive transformation for image alignment","summary":"  Existing methods for image alignment struggle in cases involving\nfeature-sparse regions, extreme scale and field-of-view differences, and large\ndeformations, often resulting in suboptimal accuracy. Robustness to these\nchallenges improves through iterative refinement of the transformation field\nwhile focusing on critical regions in multi-scale image representations. We\nthus propose Auto-Regressive Transformation (ART), a novel method that\niteratively estimates the coarse-to-fine transformations within an\nauto-regressive framework. Leveraging hierarchical multi-scale features, our\nnetwork refines the transformations using randomly sampled points at each\nscale. By incorporating guidance from the cross-attention layer, the model\nfocuses on critical regions, ensuring accurate alignment even in challenging,\nfeature-limited conditions. Extensive experiments across diverse datasets\ndemonstrate that ART significantly outperforms state-of-the-art methods,\nestablishing it as a powerful new method for precise image alignment with broad\napplicability.\n","authors":["Kanggeon Lee","Soochahn Lee","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2505.04864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04861v1","updated":"2025-05-08T00:08:31Z","published":"2025-05-08T00:08:31Z","title":"Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model","summary":"  The Segment Anything Model (SAM) is a popular vision foundation model;\nhowever, its high computational and memory demands make deployment on\nresource-constrained devices challenging. While Post-Training Quantization\n(PTQ) is a practical approach for reducing computational overhead, existing PTQ\nmethods rely on fixed bit-width quantization, leading to suboptimal accuracy\nand efficiency. To address this limitation, we propose Mix-QSAM, a\nmixed-precision PTQ framework for SAM. First, we introduce a layer-wise\nimportance score, derived using Kullback-Leibler (KL) divergence, to quantify\neach layer's contribution to the model's output. Second, we introduce\ncross-layer synergy, a novel metric based on causal mutual information, to\ncapture dependencies between adjacent layers. This ensures that highly\ninterdependent layers maintain similar bit-widths, preventing abrupt precision\nmismatches that degrade feature propagation and numerical stability. Using\nthese metrics, we formulate an Integer Quadratic Programming (IQP) problem to\ndetermine optimal bit-width allocation under model size and bit-operation\nconstraints, assigning higher precision to critical layers while minimizing\nbit-width in less influential layers. Experimental results demonstrate that\nMix-QSAM consistently outperforms existing PTQ methods on instance segmentation\nand object detection tasks, achieving up to 20% higher average precision under\n6-bit and 4-bit mixed-precision settings, while maintaining computational\nefficiency.\n","authors":["Navin Ranjan","Andreas Savakis"],"pdf_url":"https://arxiv.org/pdf/2505.04861v1.pdf","comment":"12 pages, 2 Figures"},{"id":"http://arxiv.org/abs/2505.04860v1","updated":"2025-05-08T00:03:04Z","published":"2025-05-08T00:03:04Z","title":"D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation","summary":"  Learning bimanual manipulation is challenging due to its high dimensionality\nand tight coordination required between two arms. Eye-in-hand imitation\nlearning, which uses wrist-mounted cameras, simplifies perception by focusing\non task-relevant views. However, collecting diverse demonstrations remains\ncostly, motivating the need for scalable data augmentation. While prior work\nhas explored visual augmentation in single-arm settings, extending these\napproaches to bimanual manipulation requires generating viewpoint-consistent\nobservations across both arms and producing corresponding action labels that\nare both valid and feasible. In this work, we propose Diffusion for COordinated\nDual-arm Data Augmentation (D-CODA), a method for offline data augmentation\ntailored to eye-in-hand bimanual imitation learning that trains a diffusion\nmodel to synthesize novel, viewpoint-consistent wrist-camera images for both\narms while simultaneously generating joint-space action labels. It employs\nconstrained optimization to ensure that augmented states involving\ngripper-to-object contacts adhere to constraints suitable for bimanual\ncoordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our\nresults across 2250 simulation trials and 300 real-world trials demonstrate\nthat it outperforms baselines and ablations, showing its potential for scalable\ndata augmentation in eye-in-hand bimanual manipulation. Our project website is\nat: https://dcodaaug.github.io/D-CODA/.\n","authors":["I-Chun Arthur Liu","Jason Chen","Gaurav Sukhatme","Daniel Seita"],"pdf_url":"https://arxiv.org/pdf/2505.04860v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2505.05470v1","updated":"2025-05-08T17:58:45Z","published":"2025-05-08T17:58:45Z","title":"Flow-GRPO: Training Flow Matching Models via Online RL","summary":"  We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.\n","authors":["Jie Liu","Gongye Liu","Jiajun Liang","Yangguang Li","Jiaheng Liu","Xintao Wang","Pengfei Wan","Di Zhang","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2505.05470v1.pdf","comment":"Code: https://github.com/yifan123/flow_grpo"},{"id":"http://arxiv.org/abs/2505.05467v1","updated":"2025-05-08T17:57:40Z","published":"2025-05-08T17:57:40Z","title":"StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant","summary":"  We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.\n","authors":["Haibo Wang","Bo Feng","Zhengfeng Lai","Mingze Xu","Shiyu Li","Weifeng Ge","Afshin Dehghan","Meng Cao","Ping Huang"],"pdf_url":"https://arxiv.org/pdf/2505.05467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05465v1","updated":"2025-05-08T17:56:57Z","published":"2025-05-08T17:56:57Z","title":"ComPO: Preference Alignment via Comparison Oracles","summary":"  Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}.\n","authors":["Peter Chen","Xi Chen","Wotao Yin","Tianyi Lin"],"pdf_url":"https://arxiv.org/pdf/2505.05465v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2505.05453v1","updated":"2025-05-08T17:44:45Z","published":"2025-05-08T17:44:45Z","title":"Conversational Process Model Redesign","summary":"  With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria.\n","authors":["Nataliia Klievtsova","Timotheus Kampik","Juergen Mangler","Stefanie Rinderle-Ma"],"pdf_url":"https://arxiv.org/pdf/2505.05453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18039v2","updated":"2025-05-08T17:34:57Z","published":"2025-04-25T03:12:43Z","title":"MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind","summary":"  Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains.\n","authors":["Zheng Zhang","Nuoqian Xiao","Qi Chai","Deheng Ye","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2504.18039v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05440v1","updated":"2025-05-08T17:31:20Z","published":"2025-05-08T17:31:20Z","title":"EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation","summary":"  Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation.\n","authors":["Biao Yi","Xavier Hu","Yurun Chen","Shengyu Zhang","Hongxia Yang","Fan Wu","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2505.05440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11686v3","updated":"2025-05-08T17:23:39Z","published":"2024-09-18T03:56:56Z","title":"Automated detection of underdiagnosed medical conditions via\n  opportunistic imaging","summary":"  Abdominal computed tomography (CT) scans are frequently performed in clinical\nsettings. Opportunistic CT involves repurposing routine CT images to extract\ndiagnostic information and is an emerging tool for detecting underdiagnosed\nconditions such as sarcopenia, hepatic steatosis, and ascites. This study\nutilizes deep learning methods to promote accurate diagnosis and clinical\ndocumentation. We analyze 2,674 inpatient CT scans to identify discrepancies\nbetween imaging phenotypes (characteristics derived from opportunistic CT\nscans) and their corresponding documentation in radiology reports and ICD\ncoding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans\ndiagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)\nthrough either opportunistic imaging or radiology reports were ICD-coded. Our\nfindings demonstrate opportunistic CT's potential to enhance diagnostic\nprecision and accuracy of risk adjustment models, offering advancements in\nprecision medicine.\n","authors":["Asad Aali","Andrew Johnston","Louis Blankemeier","Dave Van Veen","Laura T Derry","David Svec","Jason Hom","Robert D. Boutin","Akshay S. Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2409.11686v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05423v1","updated":"2025-05-08T17:12:56Z","published":"2025-05-08T17:12:56Z","title":"TransProQA: an LLM-based literary Translation evaluation metric with\n  Professional Question Answering","summary":"  The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.\n","authors":["Ran Zhang","Wei Zhao","Lieve Macken","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2505.05423v1.pdf","comment":"WIP"},{"id":"http://arxiv.org/abs/2505.05422v1","updated":"2025-05-08T17:12:19Z","published":"2025-05-08T17:12:19Z","title":"TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and\n  Generation","summary":"  Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP.\n","authors":["Haokun Lin","Teng Wang","Yixiao Ge","Yuying Ge","Zhichao Lu","Ying Wei","Qingfu Zhang","Zhenan Sun","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2505.05422v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2505.03989v2","updated":"2025-05-08T16:52:28Z","published":"2025-05-06T21:53:44Z","title":"An alignment safety case sketch based on debate","summary":"  If AI systems match or exceed human capabilities on a wide range of tasks, it\nmay become difficult for humans to efficiently judge their actions -- making it\nhard to use human feedback to steer them towards desirable traits. One proposed\nsolution is to leverage another superhuman system to point out flaws in the\nsystem's outputs via a debate. This paper outlines the value of debate for AI\nsafety, as well as the assumptions and further research required to make debate\nwork. It does so by sketching an ``alignment safety case'' -- an argument that\nan AI system will not autonomously take actions which could lead to egregious\nharm, despite being able to do so. The sketch focuses on the risk of an AI R\\&D\nagent inside an AI company sabotaging research, for example by producing false\nresults. To prevent this, the agent is trained via debate, subject to\nexploration guarantees, to teach the system to be honest. Honesty is maintained\nthroughout deployment via online training. The safety case rests on four key\nclaims: (1) the agent has become good at the debate game, (2) good performance\nin the debate game implies that the system is mostly honest, (3) the system\nwill not become significantly less honest during deployment, and (4) the\ndeployment context is tolerant of some errors. We identify open research\nproblems that, if solved, could render this a compelling argument that an AI\nsystem is safe.\n","authors":["Marie Davidsen Buhl","Jacob Pfau","Benjamin Hilton","Geoffrey Irving"],"pdf_url":"https://arxiv.org/pdf/2505.03989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05410v1","updated":"2025-05-08T16:51:43Z","published":"2025-05-08T16:51:43Z","title":"Reasoning Models Don't Always Say What They Think","summary":"  Chain-of-thought (CoT) offers a potential boon for AI safety as it allows\nmonitoring a model's CoT to try to understand its intentions and reasoning\nprocesses. However, the effectiveness of such monitoring hinges on CoTs\nfaithfully representing models' actual reasoning processes. We evaluate CoT\nfaithfulness of state-of-the-art reasoning models across 6 reasoning hints\npresented in the prompts and find: (1) for most settings and models tested,\nCoTs reveal their usage of hints in at least 1% of examples where they use the\nhint, but the reveal rate is often below 20%, (2) outcome-based reinforcement\nlearning initially improves faithfulness but plateaus without saturating, and\n(3) when reinforcement learning increases how frequently hints are used (reward\nhacking), the propensity to verbalize them does not increase, even without\ntraining against a CoT monitor. These results suggest that CoT monitoring is a\npromising way of noticing undesired behaviors during training and evaluations,\nbut that it is not sufficient to rule them out. They also suggest that in\nsettings like ours where CoT reasoning is not necessary, test-time monitoring\nof CoTs is unlikely to reliably catch rare and catastrophic unexpected\nbehaviors.\n","authors":["Yanda Chen","Joe Benton","Ansh Radhakrishnan","Jonathan Uesato","Carson Denison","John Schulman","Arushi Somani","Peter Hase","Misha Wagner","Fabien Roger","Vlad Mikulik","Samuel R. Bowman","Jan Leike","Jared Kaplan","Ethan Perez"],"pdf_url":"https://arxiv.org/pdf/2505.05410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05408v1","updated":"2025-05-08T16:50:06Z","published":"2025-05-08T16:50:06Z","title":"Crosslingual Reasoning through Test-Time Scaling","summary":"  Reasoning capabilities of large language models are primarily studied for\nEnglish, even when pretrained models are multilingual. In this work, we\ninvestigate to what extent English reasoning finetuning with long\nchain-of-thoughts (CoTs) can generalize across languages. First, we find that\nscaling up inference compute for English-centric reasoning language models\n(RLMs) improves multilingual mathematical reasoning across many languages\nincluding low-resource languages, to an extent where they outperform models\ntwice their size. Second, we reveal that while English-centric RLM's CoTs are\nnaturally predominantly English, they consistently follow a quote-and-think\npattern to reason about quoted non-English inputs. Third, we discover an\neffective strategy to control the language of long CoT reasoning, and we\nobserve that models reason better and more efficiently in high-resource\nlanguages. Finally, we observe poor out-of-domain reasoning generalization, in\nparticular from STEM to cultural commonsense knowledge, even for English.\nOverall, we demonstrate the potentials, study the mechanisms and outline the\nlimitations of crosslingual generalization of English reasoning test-time\nscaling. We conclude that practitioners should let English-centric RLMs reason\nin high-resource languages, while further work is needed to improve reasoning\nin low-resource languages and out-of-domain contexts.\n","authors":["Zheng-Xin Yong","M. Farid Adilazuarda","Jonibek Mansurov","Ruochen Zhang","Niklas Muennighoff","Carsten Eickhoff","Genta Indra Winata","Julia Kreutzer","Stephen H. Bach","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2505.05408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05402v1","updated":"2025-05-08T16:42:13Z","published":"2025-05-08T16:42:13Z","title":"CART-ELC: Oblique Decision Tree Induction via Exhaustive Search","summary":"  Oblique decision trees have attracted attention due to their potential for\nimproved classification performance over traditional axis-aligned decision\ntrees. However, methods that rely on exhaustive search to find oblique splits\nface computational challenges. As a result, they have not been widely explored.\nWe introduce a novel algorithm, Classification and Regression Tree - Exhaustive\nLinear Combinations (CART-ELC), for inducing oblique decision trees that\nperforms an exhaustive search on a restricted set of hyperplanes. We then\ninvestigate the algorithm's computational complexity and its predictive\ncapabilities. Our results demonstrate that CART-ELC consistently achieves\ncompetitive performance on small datasets, often yielding statistically\nsignificant improvements in classification accuracy relative to existing\ndecision tree induction algorithms, while frequently producing shallower,\nsimpler, and thus more interpretable trees.\n","authors":["Andrew D. Laack"],"pdf_url":"https://arxiv.org/pdf/2505.05402v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.20500v3","updated":"2025-05-08T16:41:56Z","published":"2025-03-26T12:39:56Z","title":"Novel Deep Neural OFDM Receiver Architectures for LLR Estimation","summary":"  Neural receivers have recently become a popular topic, where the received\nsignals can be directly decoded by data driven mechanisms such as machine\nlearning and deep learning. In this paper, we propose two novel neural network\nbased orthogonal frequency division multiplexing (OFDM) receivers performing\nchannel estimation and equalization tasks and directly predicting log\nlikelihood ratios (LLRs) from the received in phase and quadrature phase (IQ)\nsignals. The first network, the Dual Attention Transformer (DAT), employs a\nstate of the art (SOTA) transformer architecture with an attention mechanism.\nThe second network, the Residual Dual Non Local Attention Network (RDNLA),\nutilizes a parallel residual architecture with a non local attention block. The\nbit error rate (BER) and block error rate (BLER) performance of various SOTA\nneural receiver architectures is compared with our proposed methods across\ndifferent signal to noise ratio (SNR) levels. The simulation results show that\nDAT and RDNLA outperform both traditional communication systems and existing\nneural receiver models.\n","authors":["Erhan Karakoca","Hüseyin Çevik","İbrahim Hökelek","Ali Görçin"],"pdf_url":"https://arxiv.org/pdf/2503.20500v3.pdf","comment":"Submitted to IEEE Globecom 2025"},{"id":"http://arxiv.org/abs/2405.13325v3","updated":"2025-05-08T16:33:22Z","published":"2024-05-22T03:56:55Z","title":"DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event\n  Argument Extraction with Slot Querying","summary":"  Recent advancements in event argument extraction (EAE) involve incorporating\nuseful auxiliary information into models during training and inference, such as\nretrieved instances and event templates. These methods face two challenges: (1)\nthe retrieval results may be irrelevant and (2) templates are developed\nindependently for each event without considering their possible relationship.\nIn this work, we propose DEGAP to address these challenges through a simple yet\neffective components: dual prefixes, i.e. learnable prompt vectors, where the\ninstance-oriented prefix and template-oriented prefix are trained to learn\ninformation from different event instances and templates. Additionally, we\npropose an event-guided adaptive gating mechanism, which can adaptively\nleverage possible connections between different events and thus capture\nrelevant information from the prefix. Finally, these event-guided prefixes\nprovide relevant information as cues to EAE model without retrieval. Extensive\nexperiments demonstrate that our method achieves new state-of-the-art\nperformance on four datasets (ACE05, RAMS, WIKIEVENTS, and MLEE). Further\nanalysis shows the impact of different components.\n","authors":["Guanghui Wang","Dexi Liu","Jian-Yun Nie","Qizhi Wan","Rong Hu","Xiping Liu","Wanlong Liu","Jiaming Liu"],"pdf_url":"https://arxiv.org/pdf/2405.13325v3.pdf","comment":"Published as a conference paper in COLING 2025"},{"id":"http://arxiv.org/abs/2505.05396v1","updated":"2025-05-08T16:32:55Z","published":"2025-05-08T16:32:55Z","title":"A Pain Assessment Framework based on multimodal data and Deep Machine\n  Learning methods","summary":"  From the original abstract:\n  This thesis initially aims to study the pain assessment process from a\nclinical-theoretical perspective while exploring and examining existing\nautomatic approaches. Building on this foundation, the primary objective of\nthis Ph.D. project is to develop innovative computational methods for automatic\npain assessment that achieve high performance and are applicable in real\nclinical settings. A primary goal is to thoroughly investigate and assess\nsignificant factors, including demographic elements that impact pain\nperception, as recognized in pain research, through a computational standpoint.\nWithin the limits of the available data in this research area, our goal was to\ndesign, develop, propose, and offer automatic pain assessment pipelines for\nunimodal and multimodal configurations that are applicable to the specific\nrequirements of different scenarios. The studies published in this Ph.D. thesis\nshowcased the effectiveness of the proposed methods, achieving state-of-the-art\nresults. Additionally, they paved the way for exploring new approaches in\nartificial intelligence, foundation models, and generative artificial\nintelligence.\n","authors":["Stefanos Gkikas"],"pdf_url":"https://arxiv.org/pdf/2505.05396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05375v1","updated":"2025-05-08T16:09:40Z","published":"2025-05-08T16:09:40Z","title":"Threshold Modulation for Online Test-Time Adaptation of Spiking Neural\n  Networks","summary":"  Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.\n","authors":["Kejie Zhao","Wenjia Hua","Aiersi Tuerhong","Luziwei Leng","Yuxin Ma","Qinghua Guo"],"pdf_url":"https://arxiv.org/pdf/2505.05375v1.pdf","comment":"Accepted by IJCNN 2025. \\c{opyright} 2025 IEEE. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, including reprinting/republishing this material for advertising or\n  promotional purposes, collecting new collected works for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2504.16381v4","updated":"2025-05-08T15:53:07Z","published":"2025-04-23T03:02:29Z","title":"PINN-MEP: Continuous Neural Representations for Minimum-Energy Path\n  Discovery in Molecular Systems","summary":"  Characterizing conformational transitions in physical systems remains a\nfundamental challenge in the computational sciences. Traditional sampling\nmethods like molecular dynamics (MD) or MCMC often struggle with the\nhigh-dimensional nature of molecular systems and the high energy barriers of\ntransitions between stable states. While these transitions are rare events in\nsimulation timescales, they often represent the most biologically significant\nprocesses - for example, the conformational change of an ion channel protein\nfrom its closed to open state, which controls cellular ion flow and is crucial\nfor neural signaling. Such transitions in real systems may take milliseconds to\nseconds but could require months or years of continuous simulation to observe\neven once. We present a method that reformulates transition path generation as\na continuous optimization problem solved through physics-informed neural\nnetworks (PINNs) inspired by string methods for minimum-energy path (MEP)\ngeneration. By representing transition paths as implicit neural functions and\nleveraging automatic differentiation with differentiable molecular dynamics\nforce fields, our method enables the efficient discovery of physically\nrealistic transition pathways without requiring expensive path sampling. We\ndemonstrate our method's effectiveness on two proteins, including an explicitly\nhydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300\natoms.\n","authors":["Magnus Petersen","Roberto Covino"],"pdf_url":"https://arxiv.org/pdf/2504.16381v4.pdf","comment":"Update 08.05.2025: Added some intermediate steps in the derivation of\n  the loss to add clarity. Update 28.04.2025: Added citation and reference to\n  just-released work \"Action-Minimization Meets Generative Modeling: Efficient\n  Transition Path Sampling with the Onsager-Machlup Functional\" by Sanjeev Raja\n  et al. and added an appendix section clarifying some loss derivation steps"},{"id":"http://arxiv.org/abs/2406.07944v2","updated":"2025-05-08T15:48:00Z","published":"2024-06-12T07:06:38Z","title":"Enhancing Differential Testing With LLMs For Testing Deep Learning\n  Libraries","summary":"  Differential testing offers a promising strategy to alleviate the test oracle\nproblem by comparing the test results between alternative implementations.\nHowever, existing differential testing techniques for deep learning (DL)\nlibraries are limited by the key challenges of finding alternative\nimplementations (called counterparts) for a given API and subsequently\ngenerating diverse test inputs. To address the two challenges, this paper\nintroduces DLLens, an LLM-enhanced differential testing technique for DL\nlibraries. To address the first challenge, DLLens incorporates an LLM-based\ncounterpart synthesis workflow, with the insight that the counterpart of a\ngiven DL library API's computation could be successfully synthesized through\ncertain composition and adaptation of the APIs from another DL library. To\naddress the second challenge, DLLens incorporates a static analysis technique\nthat extracts the path constraints from the implementations of a given API and\nits counterpart to guide diverse test input generation. The extraction is\nfacilitated by LLM's knowledge of the concerned DL library and its upstream\nlibraries.\n  We evaluate DLLens on two popular DL libraries, TensorFlow and PyTorch. Our\nevaluation shows that DLLens synthesizes counterparts for 1.84 times as many\nAPIs as those found by state-of-the-art techniques on these libraries.\nMoreover, under the same time budget, DLLens covers 7.23% more branches and\ndetects 1.88 times as many bugs as state-of-the-art techniques on 200 randomly\nsampled APIs. DLLens has successfully detected 71 bugs in recent TensorFlow and\nPyTorch libraries. Among them, 59 are confirmed by developers, including 46\nconfirmed as previously unknown bugs, and 10 of these previously unknown bugs\nhave been fixed in the latest version of TensorFlow and PyTorch.\n","authors":["Meiziniu Li","Dongze Li","Jianmeng Liu","Jialun Cao","Yongqiang Tian","Shing-Chi Cheung"],"pdf_url":"https://arxiv.org/pdf/2406.07944v2.pdf","comment":"This work has been accepted by ACM TOSEM. Manuscript under final\n  preparation"},{"id":"http://arxiv.org/abs/2505.05356v1","updated":"2025-05-08T15:45:53Z","published":"2025-05-08T15:45:53Z","title":"Time of the Flight of the Gaussians: Optimizing Depth Indirectly in\n  Dynamic Radiance Fields","summary":"  We present a method to reconstruct dynamic scenes from monocular\ncontinuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that\nachieves similar or better accuracy than neural volumetric approaches and is\n100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a\nsingle viewpoint is a significant challenge in computer vision. In C-ToF\nradiance field reconstruction, the property of interest-depth-is not directly\nmeasured, causing an additional challenge. This problem has a large and\nunderappreciated impact upon the optimization when using a fast primitive-based\nscene representation like 3D Gaussian splatting, which is commonly used with\nmulti-view data to produce satisfactory results and is brittle in its\noptimization otherwise. We incorporate two heuristics into the optimization to\nimprove the accuracy of scene geometry represented by Gaussians. Experimental\nresults show that our approach produces accurate reconstructions under\nconstrained C-ToF sensing conditions, including for fast motions like swinging\nbaseball bats. https://visual.cs.brown.edu/gftorf\n","authors":["Runfeng Li","Mikhail Okunev","Zixuan Guo","Anh Ha Duong","Christian Richardt","Matthew O'Toole","James Tompkin"],"pdf_url":"https://arxiv.org/pdf/2505.05356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05354v1","updated":"2025-05-08T15:43:40Z","published":"2025-05-08T15:43:40Z","title":"High-fidelity Grain Growth Modeling: Leveraging Deep Learning for Fast\n  Computations","summary":"  Grain growth simulation is crucial for predicting metallic material\nmicrostructure evolution during annealing and resulting final mechanical\nproperties, but traditional partial differential equation-based methods are\ncomputationally expensive, creating bottlenecks in materials design and\nmanufacturing. In this work, we introduce a machine learning framework that\ncombines a Convolutional Long Short-Term Memory networks with an Autoencoder to\nefficiently predict grain growth evolution. Our approach captures both spatial\nand temporal aspects of grain evolution while encoding high-dimensional grain\nstructure data into a compact latent space for pattern learning, enhanced by a\nnovel composite loss function combining Mean Squared Error, Structural\nSimilarity Index Measurement, and Boundary Preservation to maintain structural\nintegrity of grain boundary topology of the prediction. Results demonstrated\nthat our machine learning approach accelerates grain growth prediction by up to\n\\SI{89}{\\times} faster, reducing computation time from \\SI{10}{\\minute} to\napproximately \\SI{10}{\\second} while maintaining high-fidelity predictions. The\nbest model (S-30-30) achieving a structural similarity score of\n\\SI{86.71}{\\percent} and mean grain size error of just \\SI{0.07}{\\percent}. All\nmodels accurately captured grain boundary topology, morphology, and size\ndistributions. This approach enables rapid microstructural prediction for\napplications where conventional simulations are prohibitively time-consuming,\npotentially accelerating innovation in materials science and manufacturing.\n","authors":["Pungponhavoan Tep","Marc Bernacki"],"pdf_url":"https://arxiv.org/pdf/2505.05354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05321v1","updated":"2025-05-08T15:08:36Z","published":"2025-05-08T15:08:36Z","title":"Feature-Augmented Deep Networks for Multiscale Building Segmentation in\n  High-Resolution UAV and Satellite Imagery","summary":"  Accurate building segmentation from high-resolution RGB imagery remains\nchallenging due to spectral similarity with non-building features, shadows, and\nirregular building geometries. In this study, we present a comprehensive deep\nlearning framework for multiscale building segmentation using RGB aerial and\nsatellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate\na diverse, multi-sensor dataset and introduce feature-augmented inputs by\nderiving secondary representations including Principal Component Analysis\n(PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index\n(MBI), and Sobel edge filters from RGB channels. These features guide a\nRes-U-Net architecture in learning complex spatial patterns more effectively.\nWe also propose training policies incorporating layer freezing, cyclical\nlearning rates, and SuperConvergence to reduce training time and resource\nusage. Evaluated on a held-out WorldView-3 image, our model achieves an overall\naccuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of\n0.80, outperforming existing RGB-based benchmarks. This study demonstrates the\neffectiveness of combining multi-resolution imagery, feature augmentation, and\noptimized training strategies for robust building segmentation in remote\nsensing applications.\n","authors":["Chintan B. Maniyar","Minakshi Kumar","Gengchen Mai"],"pdf_url":"https://arxiv.org/pdf/2505.05321v1.pdf","comment":"in preparation for journal submission, 25 pages, 11 figures"},{"id":"http://arxiv.org/abs/2505.05318v1","updated":"2025-05-08T15:02:49Z","published":"2025-05-08T15:02:49Z","title":"Mapping User Trust in Vision Language Models: Research Landscape,\n  Challenges, and Prospects","summary":"  The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies.\n","authors":["Agnese Chiatti","Sara Bernardini","Lara Shibelski Godoy Piccolo","Viola Schiaffonati","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2505.05318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05315v1","updated":"2025-05-08T15:01:06Z","published":"2025-05-08T15:01:06Z","title":"Scalable Chain of Thoughts via Elastic Reasoning","summary":"  Large reasoning models (LRMs) have achieved remarkable progress on complex\ntasks by generating extended chains of thought (CoT). However, their\nuncontrolled output lengths pose significant challenges for real-world\ndeployment, where inference-time budgets on tokens, latency, or compute are\nstrictly constrained. We propose Elastic Reasoning, a novel framework for\nscalable chain of thoughts that explicitly separates reasoning into two\nphases--thinking and solution--with independently allocated budgets. At test\ntime, Elastic Reasoning prioritize that completeness of solution segments,\nsignificantly improving reliability under tight resource constraints. To train\nmodels that are robust to truncated thinking, we introduce a lightweight\nbudget-constrained rollout strategy, integrated into GRPO, which teaches the\nmodel to reason adaptively when the thinking process is cut short and\ngeneralizes effectively to unseen budget constraints without additional\ntraining. Empirical results on mathematical (AIME, MATH500) and programming\n(LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning\nperforms robustly under strict budget constraints, while incurring\nsignificantly lower training cost than baseline methods. Remarkably, our\napproach also produces more concise and efficient reasoning even in\nunconstrained settings. Elastic Reasoning offers a principled and practical\nsolution to the pressing challenge of controllable reasoning at scale.\n","authors":["Yuhui Xu","Hanze Dong","Lei Wang","Doyen Sahoo","Junnan Li","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2505.05315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10146v2","updated":"2025-05-08T14:36:20Z","published":"2025-04-14T11:56:55Z","title":"GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and\n  Problem Solutions","summary":"  We propose GeoUni, the first unified geometry expert model capable of\ngenerating problem solutions and diagrams within a single framework in a way\nthat enables the creation of unique and individualized geometry problems.\nTraditionally, solving geometry problems and generating diagrams have been\ntreated as separate tasks in machine learning, with no models successfully\nintegrating both to support problem creation. However, we believe that mastery\nin geometry requires frictionless integration of all of these skills, from\nsolving problems to visualizing geometric relationships, and finally, crafting\ntailored problems. Our extensive experiments demonstrate that GeoUni, with only\n1.5B parameters, achieves performance comparable to larger models such as\nDeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also\nexcels in generating precise geometric diagrams, surpassing both text-to-image\nmodels and unified models, including the GPT-4o image generation. Most\nimportantly, GeoUni is the only model capable of successfully generating\ntextual problems with matching diagrams based on specific knowledge points,\nthus offering a wider range of capabilities that extend beyond current models.\n","authors":["Jo-Ku Cheng","Zeren Zhang","Ran Chen","Jingyang Deng","Ziran Qin","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2504.10146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05291v1","updated":"2025-05-08T14:31:02Z","published":"2025-05-08T14:31:02Z","title":"Benchmarking Ophthalmology Foundation Models for Clinically Significant\n  Age Macular Degeneration Detection","summary":"  Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to\nlearn robust representations from large-scale natural image datasets, enhancing\ntheir generalization across domains. In retinal imaging, foundation models\npretrained on either natural or ophthalmic data have shown promise, but the\nbenefits of in-domain pretraining remain uncertain. To investigate this, we\nbenchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets\ntotaling 70,000 expert-annotated images for the task of moderate-to-late\nage-related macular degeneration (AMD) identification. Our results show that\niBOT pretrained on natural images achieves the highest out-of-distribution\ngeneralization, with AUROCs of 0.80-0.97, outperforming domain-specific models,\nwhich achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining,\nwhich achieved AUROCs of 0.68-0.91. These findings highlight the value of\nfoundation models in improving AMD identification and challenge the assumption\nthat in-domain pretraining is necessary. Furthermore, we release BRAMD, an\nopen-access dataset (n=587) of DFIs with AMD labels from Brazil.\n","authors":["Benjamin A. Cohen","Jonathan Fhima","Meishar Meisel","Baskin Meital","Luis Filipe Nakayama","Eran Berkowitz","Joachim A. Behar"],"pdf_url":"https://arxiv.org/pdf/2505.05291v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.05288v1","updated":"2025-05-08T14:29:11Z","published":"2025-05-08T14:29:11Z","title":"PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes","summary":"  We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.\n","authors":["Ahmed Abdelreheem","Filippo Aleotti","Jamie Watson","Zawar Qureshi","Abdelrahman Eldesokey","Peter Wonka","Gabriel Brostow","Sara Vicente","Guillermo Garcia-Hernando"],"pdf_url":"https://arxiv.org/pdf/2505.05288v1.pdf","comment":"Tech report. Project page: https://nianticlabs.github.io/placeit3d/"},{"id":"http://arxiv.org/abs/2505.05283v1","updated":"2025-05-08T14:27:45Z","published":"2025-05-08T14:27:45Z","title":"Software Development Life Cycle Perspective: A Survey of Benchmarks for\n  CodeLLMs and Agents","summary":"  Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios.\n","authors":["Kaixin Wang","Tianlin Li","Xiaoyu Zhang","Chong Wang","Weisong Sun","Yang Liu","Bin Shi"],"pdf_url":"https://arxiv.org/pdf/2505.05283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05271v1","updated":"2025-05-08T14:17:27Z","published":"2025-05-08T14:17:27Z","title":"T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet\n  Extraction","summary":"  Aspect sentiment triplet extraction (ASTE) aims to extract triplets composed\nof aspect terms, opinion terms, and sentiment polarities from given sentences.\nThe table tagging method is a popular approach to addressing this task, which\nencodes a sentence into a 2-dimensional table, allowing for the tagging of\nrelations between any two words. Previous efforts have focused on designing\nvarious downstream relation learning modules to better capture interactions\nbetween tokens in the table, revealing that a stronger capability to capture\nrelations can lead to greater improvements in the model. Motivated by this, we\nattempt to directly utilize transformer layers as downstream relation learning\nmodules. Due to the powerful semantic modeling capability of transformers, it\nis foreseeable that this will lead to excellent improvement. However, owing to\nthe quadratic relation between the length of the table and the length of the\ninput sentence sequence, using transformers directly faces two challenges:\noverly long table sequences and unfair local attention interaction. To address\nthese challenges, we propose a novel Table-Transformer (T-T) for the\ntagging-based ASTE method. Specifically, we introduce a stripe attention\nmechanism with a loop-shift strategy to tackle these challenges. The former\nmodifies the global attention mechanism to only attend to a 2-dimensional local\nattention window, while the latter facilitates interaction between different\nattention windows. Extensive and comprehensive experiments demonstrate that the\nT-T, as a downstream relation learning module, achieves state-of-the-art\nperformance with lower computational costs.\n","authors":["Kun Peng","Chaodong Tong","Cong Cao","Hao Peng","Qian Li","Guanlin Wu","Lei Jiang","Yanbing Liu","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2505.05271v1.pdf","comment":"Accepted by IJCAI2025"},{"id":"http://arxiv.org/abs/2505.03694v2","updated":"2025-05-08T14:12:19Z","published":"2025-05-06T16:59:54Z","title":"Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and\n  Avoid","summary":"  Assured safe-separation is essential for achieving seamless high-density\noperation of airborne vehicles in a shared airspace. To equip\nresource-constrained aerial systems with this safety-critical capability, we\npresent ViSafe, a high-speed vision-only airborne collision avoidance system.\nViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by\ntightly integrating a learning-based edge-AI framework with a custom\nmulti-camera hardware prototype designed under SWaP-C constraints. By\nleveraging perceptual input-focused control barrier functions (CBF) to design,\nencode, and enforce safety thresholds, ViSafe can provide provably safe runtime\nguarantees for self-separation in high-speed aerial operations. We evaluate\nViSafe's performance through an extensive test campaign involving both\nsimulated digital twins and real-world flight scenarios. By independently\nvarying agent types, closure rates, interaction geometries, and environmental\nconditions (e.g., weather and lighting), we demonstrate that ViSafe\nconsistently ensures self-separation across diverse scenarios. In\nfirst-of-its-kind real-world high-speed collision avoidance tests with closure\nrates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous\ncollision avoidance, establishing a new standard for safety in high-speed\naerial navigation.\n","authors":["Parv Kapoor","Ian Higgins","Nikhil Keetha","Jay Patrikar","Brady Moon","Zelin Ye","Yao He","Ivan Cisneros","Yaoyu Hu","Changliu Liu","Eunsuk Kang","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2505.03694v2.pdf","comment":"13 pages, RSS 2025 Demo track, https://theairlab.org/visafe/"},{"id":"http://arxiv.org/abs/2505.05262v1","updated":"2025-05-08T14:07:20Z","published":"2025-05-08T14:07:20Z","title":"Enhancing Cooperative Multi-Agent Reinforcement Learning with State\n  Modelling and Adversarial Exploration","summary":"  Learning to cooperate in distributed partially observable environments with\nno communication abilities poses significant challenges for multi-agent deep\nreinforcement learning (MARL). This paper addresses key concerns in this\ndomain, focusing on inferring state representations from individual agent\nobservations and leveraging these representations to enhance agents'\nexploration and collaborative task execution policies. To this end, we propose\na novel state modelling framework for cooperative MARL, where agents infer\nmeaningful belief representations of the non-observable state, with respect to\noptimizing their own policies, while filtering redundant and less informative\njoint state information. Building upon this framework, we propose the MARL SMPE\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\nunder partial observability, explicitly by incorporating their beliefs into the\npolicy network, and implicitly by adopting an adversarial type of exploration\npolicies which encourages agents to discover novel, high-value states while\nimproving the discriminative abilities of others. Experimentally, we show that\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\ntasks from the MPE, LBF, and RWARE benchmarks.\n","authors":["Andreas Kontogiannis","Konstantinos Papathanasiou","Yi Shen","Giorgos Stamou","Michael M. Zavlanos","George Vouros"],"pdf_url":"https://arxiv.org/pdf/2505.05262v1.pdf","comment":"Accepted (Poster) at ICML 2025"},{"id":"http://arxiv.org/abs/2502.18218v2","updated":"2025-05-08T13:59:42Z","published":"2025-02-25T14:03:15Z","title":"FLARE: A Framework for Stellar Flare Forecasting using Stellar Physical\n  Properties and Historical Records","summary":"  Stellar flare events are critical observational samples for astronomical\nresearch; however, recorded flare events remain limited. Stellar flare\nforecasting can provide additional flare event samples to support research\nefforts. Despite this potential, no specialized models for stellar flare\nforecasting have been proposed to date. In this paper, we present extensive\nexperimental evidence demonstrating that both stellar physical properties and\nhistorical flare records are valuable inputs for flare forecasting tasks. We\nthen introduce FLARE (Forecasting Light-curve-based Astronomical Records via\nfeatures Ensemble), the first-of-its-kind large model specifically designed for\nstellar flare forecasting. FLARE integrates stellar physical properties and\nhistorical flare records through a novel Soft Prompt Module and Residual Record\nFusion Module. Our experiments on the publicly available Kepler light curve\ndataset demonstrate that FLARE achieves superior performance compared to other\nmethods across all evaluation metrics. Finally, we validate the forecast\ncapability of our model through a comprehensive case study.\n","authors":["Bingke Zhu","Xiaoxiao Wang","Minghui Jia","Yihan Tao","Xiao Kong","Ali Luo","Yingying Chen","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.18218v2.pdf","comment":"Accepted by IJCAI 2025"},{"id":"http://arxiv.org/abs/2303.12484v5","updated":"2025-05-08T13:51:45Z","published":"2023-03-22T11:51:49Z","title":"Label-Efficient Deep Learning in Medical Image Analysis: Challenges and\n  Future Directions","summary":"  Deep learning has significantly advanced medical imaging analysis (MIA),\nachieving state-of-the-art performance across diverse clinical tasks. However,\nits success largely depends on large-scale, high-quality labeled datasets,\nwhich are costly and time-consuming to obtain due to the need for expert\nannotation. To mitigate this limitation, label-efficient deep learning methods\nhave emerged to improve model performance under limited supervision by\nleveraging labeled, unlabeled, and weakly labeled data. In this survey, we\nsystematically review over 350 peer-reviewed studies and present a\ncomprehensive taxonomy of label-efficient learning methods in MIA. These\nmethods are categorized into four labeling paradigms: no label, insufficient\nlabel, inexact label, and label refinement. For each category, we analyze\nrepresentative techniques across imaging modalities and clinical applications,\nhighlighting shared methodological principles and task-specific adaptations. We\nalso examine the growing role of health foundation models (HFMs) in enabling\nlabel-efficient learning through large-scale pre-training and transfer\nlearning, enhancing the use of limited annotations in downstream tasks.\nFinally, we identify current challenges and future directions to facilitate the\ntranslation of label-efficient learning from research promise to everyday\nclinical care.\n","authors":["Cheng Jin","Zhengrui Guo","Yi Lin","Luyang Luo","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12484v5.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2502.04728v2","updated":"2025-05-08T13:42:18Z","published":"2025-02-07T07:52:25Z","title":"Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models","summary":"  Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domains, achieving over 50\\%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks.\n","authors":["Zhouliang Yu","Yuhuan Yuan","Tim Z. Xiao","Fuxiang Frank Xia","Jie Fu","Ge Zhang","Ge Lin","Weiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.04728v2.pdf","comment":"Accepted by TMLR2025 (32 pages, 6 figures)"},{"id":"http://arxiv.org/abs/2410.15236v2","updated":"2025-05-08T13:35:24Z","published":"2024-10-20T00:00:56Z","title":"Jailbreaking and Mitigation of Vulnerabilities in Large Language Models","summary":"  Large Language Models (LLMs) have transformed artificial intelligence by\nadvancing natural language understanding and generation, enabling applications\nacross fields beyond healthcare, software engineering, and conversational\nsystems. Despite these advancements in the past few years, LLMs have shown\nconsiderable vulnerabilities, particularly to prompt injection and jailbreaking\nattacks. This review analyzes the state of research on these vulnerabilities\nand presents available defense strategies. We roughly categorize attack\napproaches into prompt-based, model-based, multimodal, and multilingual,\ncovering techniques such as adversarial prompting, backdoor injections, and\ncross-modality exploits. We also review various defense mechanisms, including\nprompt filtering, transformation, alignment techniques, multi-agent defenses,\nand self-regulation, evaluating their strengths and shortcomings. We also\ndiscuss key metrics and benchmarks used to assess LLM safety and robustness,\nnoting challenges like the quantification of attack success in interactive\ncontexts and biases in existing datasets. Identifying current research gaps, we\nsuggest future directions for resilient alignment strategies, advanced defenses\nagainst evolving attacks, automation of jailbreak detection, and consideration\nof ethical and societal impacts. This review emphasizes the need for continued\nresearch and cooperation within the AI community to enhance LLM security and\nensure their safe deployment.\n","authors":["Benji Peng","Keyu Chen","Qian Niu","Ziqian Bi","Ming Liu","Pohsun Feng","Tianyang Wang","Lawrence K. Q. Yan","Yizhu Wen","Yichao Zhang","Caitlyn Heqi Yin"],"pdf_url":"https://arxiv.org/pdf/2410.15236v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.03571v3","updated":"2025-05-08T13:30:23Z","published":"2022-08-06T19:47:32Z","title":"Transformer-based assignment decision network for multiple object\n  tracking","summary":"  Data association is a crucial component for any multiple object tracking\n(MOT) method that follows the tracking-by-detection paradigm. To generate\ncomplete trajectories such methods employ a data association process to\nestablish assignments between detections and existing targets during each\ntimestep. Recent data association approaches try to solve either a\nmulti-dimensional linear assignment task or a network flow minimization problem\nor tackle it via multiple hypotheses tracking. However, during inference an\noptimization step that computes optimal assignments is required for every\nsequence frame inducing additional complexity to any given solution. To this\nend, in the context of this work we introduce Transformer-based Assignment\nDecision Network (TADN) that tackles data association without the need of any\nexplicit optimization during inference. In particular, TADN can directly infer\nassignment pairs between detections and active targets in a single forward pass\nof the network. We have integrated TADN in a rather simple MOT framework,\ndesigned a novel training strategy for efficient end-to-end training and\ndemonstrated the high potential of our approach for online visual\ntracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and\nUA-DETRAC. Our proposed approach demonstrates strong performance in most\nevaluation metrics despite its simple nature as a tracker lacking significant\nauxiliary components such as occlusion handling or re-identification. The\nimplementation of our method is publicly available at\nhttps://github.com/psaltaath/tadn-mot.\n","authors":["Athena Psalta","Vasileios Tsironis","Konstantinos Karantzalos"],"pdf_url":"https://arxiv.org/pdf/2208.03571v3.pdf","comment":"Preprint version. Under consideration at Computer Vision and Image\n  Understanding"},{"id":"http://arxiv.org/abs/2505.05235v1","updated":"2025-05-08T13:29:46Z","published":"2025-05-08T13:29:46Z","title":"Advancing Neural Network Verification through Hierarchical Safety\n  Abstract Interpretation","summary":"  Traditional methods for formal verification (FV) of deep neural networks\n(DNNs) are constrained by a binary encoding of safety properties, where a model\nis classified as either safe or unsafe (robust or not robust). This binary\nencoding fails to capture the nuanced safety levels within a model, often\nresulting in either overly restrictive or too permissive requirements. In this\npaper, we introduce a novel problem formulation called Abstract\nDNN-Verification, which verifies a hierarchical structure of unsafe outputs,\nproviding a more granular analysis of the safety aspect for a given DNN.\nCrucially, by leveraging abstract interpretation and reasoning about output\nreachable sets, our approach enables assessing multiple safety levels during\nthe FV process, requiring the same (in the worst case) or even potentially less\ncomputational effort than the traditional binary verification approach.\nSpecifically, we demonstrate how this formulation allows rank adversarial\ninputs according to their abstract safety level violation, offering a more\ndetailed evaluation of the model's safety and robustness. Our contributions\ninclude a theoretical exploration of the relationship between our novel\nabstract safety formulation and existing approaches that employ abstract\ninterpretation for robustness verification, complexity analysis of the novel\nproblem introduced, and an empirical evaluation considering both a complex deep\nreinforcement learning task (based on Habitat 3.0) and standard\nDNN-Verification benchmarks.\n","authors":["Luca Marzari","Isabella Mastroeni","Alessandro Farinelli"],"pdf_url":"https://arxiv.org/pdf/2505.05235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05232v1","updated":"2025-05-08T13:26:33Z","published":"2025-05-08T13:26:33Z","title":"ChemRxivQuest: A Curated Chemistry Question-Answer Database Extracted\n  from ChemRxiv Preprints","summary":"  The rapid expansion of chemistry literature poses significant challenges for\nresearchers seeking to efficiently access domain-specific knowledge. To support\nadvancements in chemistry-focused natural language processing (NLP), we present\nChemRxivQuest, a curated dataset of 970 high-quality question-answer (QA) pairs\nderived from 155 ChemRxiv preprints across 17 subfields of chemistry. Each QA\npair is explicitly linked to its source text segment to ensure traceability and\ncontextual accuracy. ChemRxivQuest was constructed using an automated pipeline\nthat combines optical character recognition (OCR), GPT-4o-based QA generation,\nand a fuzzy matching technique for answer verification. The dataset emphasizes\nconceptual, mechanistic, applied, and experimental questions, enabling\napplications in retrieval-based QA systems, search engine development, and\nfine-tuning of domain-adapted large language models. We analyze the dataset's\nstructure, coverage, and limitations, and outline future directions for\nexpansion and expert validation. ChemRxivQuest provides a foundational resource\nfor chemistry NLP research, education, and tool development.\n","authors":["Mahmoud Amiri","Thomas Bocklitz"],"pdf_url":"https://arxiv.org/pdf/2505.05232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05226v1","updated":"2025-05-08T13:18:05Z","published":"2025-05-08T13:18:05Z","title":"Put CASH on Bandits: A Max K-Armed Problem for Automated Machine\n  Learning","summary":"  The Combined Algorithm Selection and Hyperparameter optimization (CASH) is a\nchallenging resource allocation problem in the field of AutoML. We propose\nMaxUCB, a max $k$-armed bandit method to trade off exploring different model\nclasses and conducting hyperparameter optimization. MaxUCB is specifically\ndesigned for the light-tailed and bounded reward distributions arising in this\nsetting and, thus, provides an efficient alternative compared to classic max\n$k$-armed bandit methods assuming heavy-tailed reward distributions. We\ntheoretically and empirically evaluate our method on four standard AutoML\nbenchmarks, demonstrating superior performance over prior approaches.\n","authors":["Amir Rezaei Balef","Claire Vernade","Katharina Eggensperger"],"pdf_url":"https://arxiv.org/pdf/2505.05226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.20784v2","updated":"2025-05-08T13:18:00Z","published":"2025-04-29T14:01:10Z","title":"Approximate Lifted Model Construction","summary":"  Probabilistic relational models such as parametric factor graphs enable\nefficient (lifted) inference by exploiting the indistinguishability of objects.\nIn lifted inference, a representative of indistinguishable objects is used for\ncomputations. To obtain a relational (i.e., lifted) representation, the\nAdvanced Colour Passing (ACP) algorithm is the state of the art. The ACP\nalgorithm, however, requires underlying distributions, encoded as\npotential-based factorisations, to exactly match to identify and exploit\nindistinguishabilities. Hence, ACP is unsuitable for practical applications\nwhere potentials learned from data inevitably deviate even if associated\nobjects are indistinguishable. To mitigate this problem, we introduce the\n$\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which\nallows for a deviation of potentials depending on a hyperparameter\n$\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits\nindistinguishabilities that are not exact. We prove that the approximation\nerror induced by $\\varepsilon$-ACP is strictly bounded and our experiments show\nthat the approximation error is close to zero in practice.\n","authors":["Malte Luttermann","Jan Speller","Marcel Gehrke","Tanya Braun","Ralf Möller","Mattis Hartwig"],"pdf_url":"https://arxiv.org/pdf/2504.20784v2.pdf","comment":"Extended version of paper accepted to the Proceedings of the 34th\n  International Joint Conference on Artificial Intelligence (IJCAI-2025)"},{"id":"http://arxiv.org/abs/2505.05211v1","updated":"2025-05-08T13:04:32Z","published":"2025-05-08T13:04:32Z","title":"Incentive-Aware Machine Learning; Robustness, Fairness, Improvement &\n  Causality","summary":"  The article explores the emerging domain of incentive-aware machine learning\n(ML), which focuses on algorithmic decision-making in contexts where\nindividuals can strategically modify their inputs to influence outcomes. It\ncategorizes the research into three perspectives: robustness, aiming to design\nmodels resilient to \"gaming\"; fairness, analyzing the societal impacts of such\nsystems; and improvement/causality, recognizing situations where strategic\nactions lead to genuine personal or societal improvement. The paper introduces\na unified framework encapsulating models for these perspectives, including\noffline, online, and causal settings, and highlights key challenges such as\ndifferentiating between gaming and improvement and addressing heterogeneity\namong agents. By synthesizing findings from diverse works, we outline\ntheoretical advancements and practical solutions for robust, fair, and\ncausally-informed incentive-aware ML systems.\n","authors":["Chara Podimata"],"pdf_url":"https://arxiv.org/pdf/2505.05211v1.pdf","comment":"This literature review was published in SIGEcom Exchanges in 2025"},{"id":"http://arxiv.org/abs/2505.05203v1","updated":"2025-05-08T13:00:24Z","published":"2025-05-08T13:00:24Z","title":"LAPSO: A Unified Optimization View for Learning-Augmented Power System\n  Operations","summary":"  With the high penetration of renewables, traditional model-based power system\noperation is challenged to deliver economic, stable, and robust decisions.\nMachine learning has emerged as a powerful modeling tool for capturing complex\ndynamics to address these challenges. However, its separate design often lacks\nsystematic integration with existing methods. To fill the gap, this paper\nproposes a holistic framework of Learning-Augmented Power System Operations\n(LAPSO, pronounced as Lap-So). Adopting a native optimization perspective,\nLAPSO is centered on the operation stage and aims to break the boundary between\ntemporally siloed power system tasks, such as forecast, operation and control,\nwhile unifying the objectives of machine learning and model-based optimizations\nat both training and inference stages. Systematic analysis and simulations\ndemonstrate the effectiveness of applying LAPSO in designing new integrated\nalgorithms, such as stability-constrained optimization (SCO) and\nobjective-based forecasting (OBF), while enabling end-to-end tracing of\ndifferent sources of uncertainties. In addition, a dedicated Python\npackage-lapso is introduced to automatically augment existing power system\noptimization models with learnable components. All code and data are available\nat https://github.com/xuwkk/lapso_exp.\n","authors":["Wangkun Xu","Zhongda Chu","Fei Teng"],"pdf_url":"https://arxiv.org/pdf/2505.05203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05197v1","updated":"2025-05-08T12:55:07Z","published":"2025-05-08T12:55:07Z","title":"Societal and technological progress as sewing an ever-growing,\n  ever-changing, patchy, and polychrome quilt","summary":"  Artificial Intelligence (AI) systems are increasingly placed in positions\nwhere their decisions have real consequences, e.g., moderating online spaces,\nconducting research, and advising on policy. Ensuring they operate in a safe\nand ethically acceptable fashion is thus critical. However, most solutions have\nbeen a form of one-size-fits-all \"alignment\". We are worried that such systems,\nwhich overlook enduring moral diversity, will spark resistance, erode trust,\nand destabilize our institutions. This paper traces the underlying problem to\nan often-unstated Axiom of Rational Convergence: the idea that under ideal\nconditions, rational agents will converge in the limit of conversation on a\nsingle ethics. Treating that premise as both optional and doubtful, we propose\nwhat we call the appropriateness framework: an alternative approach grounded in\nconflict theory, cultural evolution, multi-agent systems, and institutional\neconomics. The appropriateness framework treats persistent disagreement as the\nnormal case and designs for it by applying four principles: (1) contextual\ngrounding, (2) community customization, (3) continual adaptation, and (4)\npolycentric governance. We argue here that adopting these design principles is\na good way to shift the main alignment metaphor from moral unification to a\nmore productive metaphor of conflict management, and that taking this step is\nboth desirable and urgent.\n","authors":["Joel Z. Leibo","Alexander Sasha Vezhnevets","William A. Cunningham","Sébastien Krier","Manfred Diaz","Simon Osindero"],"pdf_url":"https://arxiv.org/pdf/2505.05197v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2505.05195v1","updated":"2025-05-08T12:52:02Z","published":"2025-05-08T12:52:02Z","title":"Concept-Based Unsupervised Domain Adaptation","summary":"  Concept Bottleneck Models (CBMs) enhance interpretability by explaining\npredictions through human-understandable concepts but typically assume that\ntraining and test data share the same distribution. This assumption often fails\nunder domain shifts, leading to degraded performance and poor generalization.\nTo address these limitations and improve the robustness of CBMs, we propose the\nConcept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed\nto: (1) align concept representations across domains using adversarial\ntraining, (2) introduce a relaxation threshold to allow minor domain-specific\ndifferences in concept distributions, thereby preventing performance drop due\nto over-constraints of these distributions, (3) infer concepts directly in the\ntarget domain without requiring labeled concept data, enabling CBMs to adapt to\ndiverse domains, and (4) integrate concept learning into conventional domain\nadaptation (DA) with theoretical guarantees, improving interpretability and\nestablishing new benchmarks for DA. Experiments demonstrate that our approach\nsignificantly outperforms the state-of-the-art CBM and DA methods on real-world\ndatasets.\n","authors":["Xinyue Xu","Yueying Hu","Hui Tang","Yi Qin","Lu Mi","Hao Wang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2505.05195v1.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2403.16101v3","updated":"2025-05-08T12:50:03Z","published":"2024-03-24T11:33:18Z","title":"Public Perceptions of Fairness Metrics Across Borders","summary":"  Which fairness metrics are appropriately applicable in your contexts? There\nmay be instances of discordance regarding the perception of fairness, even when\nthe outcomes comply with established fairness metrics. Several\nquestionnaire-based surveys have been conducted to evaluate fairness metrics\nwith human perceptions of fairness. However, these surveys were limited in\nscope, including only a few hundred participants within a single country. In\nthis study, we conduct an international survey to evaluate public perceptions\nof various fairness metrics in decision-making scenarios. We collected\nresponses from 1,000 participants in each of China, France, Japan, and the\nUnited States, amassing a total of 4,000 participants, to analyze the\npreferences of fairness metrics. Our survey consists of three distinct\nscenarios paired with four fairness metrics. This investigation explores the\nrelationship between personal attributes and the choice of fairness metrics,\nuncovering a significant influence of national context on these preferences.\n","authors":["Yuya Sasaki","Sohei Tokuno","Haruka Maeda","Kazuki Nakajima","Osamu Sakura","George Fletcher","Mykola Pechenizkiy","Panagiotis Karras","Irina Shklovski"],"pdf_url":"https://arxiv.org/pdf/2403.16101v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09609v2","updated":"2025-05-08T12:44:32Z","published":"2025-04-13T14:57:11Z","title":"A highly maneuverable flying squirrel drone with agility-improving\n  foldable wings","summary":"  Drones, like most airborne aerial vehicles, face inherent disadvantages in\nachieving agile flight due to their limited thrust capabilities. These physical\nconstraints cannot be fully addressed through advancements in control\nalgorithms alone. Drawing inspiration from the winged flying squirrel, this\npaper proposes a highly maneuverable drone equipped with agility-enhancing\nfoldable wings. By leveraging collaborative control between the conventional\npropeller system and the foldable wings-coordinated through the Thrust-Wing\nCoordination Control (TWCC) framework-the controllable acceleration set is\nexpanded, enabling the generation of abrupt vertical forces that are\nunachievable with traditional wingless drones. The complex aerodynamics of the\nfoldable wings are modeled using a physics-assisted recurrent neural network\n(paRNN), which calibrates the angle of attack (AOA) to align with the real\naerodynamic behavior of the wings. The additional air resistance generated by\nappropriately deploying these wings significantly improves the tracking\nperformance of the proposed \"flying squirrel\" drone. The model is trained on\nreal flight data and incorporates flat-plate aerodynamic principles.\nExperimental results demonstrate that the proposed flying squirrel drone\nachieves a 13.1% improvement in tracking performance, as measured by root mean\nsquare error (RMSE), compared to a conventional wingless drone. A demonstration\nvideo is available on YouTube: https://youtu.be/O8nrip18azY.\n","authors":["Dohyeon Lee","Jun-Gill Kang","Soohee Han"],"pdf_url":"https://arxiv.org/pdf/2504.09609v2.pdf","comment":"Accepted to IEEE Robotics and Automation Letters. Project Page :\n  https://jgkang1210.github.io/fsdrone_ral/ , Video :\n  https://www.youtube.com/watch?v=tckIF3KCJig , Dohyeon Lee and Jun-Gill Kang\n  are co-authors"},{"id":"http://arxiv.org/abs/2505.05190v1","updated":"2025-05-08T12:39:00Z","published":"2025-05-08T12:39:00Z","title":"Revealing Weaknesses in Text Watermarking Through Self-Information\n  Rewrite Attacks","summary":"  Text watermarking aims to subtly embed statistical signals into text by\ncontrolling the Large Language Model (LLM)'s sampling process, enabling\nwatermark detectors to verify that the output was generated by the specified\nmodel. The robustness of these watermarking algorithms has become a key factor\nin evaluating their effectiveness. Current text watermarking algorithms embed\nwatermarks in high-entropy tokens to ensure text quality. In this paper, we\nreveal that this seemingly benign design can be exploited by attackers, posing\na significant risk to the robustness of the watermark. We introduce a generic\nefficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA),\nwhich leverages the vulnerability by calculating the self-information of each\ntoken to identify potential pattern tokens and perform targeted attack. Our\nwork exposes a widely prevalent vulnerability in current watermarking\nalgorithms. The experimental results show SIRA achieves nearly 100% attack\nsuccess rates on seven recent watermarking methods with only 0.88 USD per\nmillion tokens cost. Our approach does not require any access to the watermark\nalgorithms or the watermarked LLM and can seamlessly transfer to any LLM as the\nattack model, even mobile-level models. Our findings highlight the urgent need\nfor more robust watermarking.\n","authors":["Yixin Cheng","Hongcheng Guo","Yangming Li","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2505.05190v1.pdf","comment":"ICML 2025 Accpeted"},{"id":"http://arxiv.org/abs/2505.05189v1","updated":"2025-05-08T12:37:51Z","published":"2025-05-08T12:37:51Z","title":"Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language\n  Models","summary":"  Prompt learning is one of the most effective paradigms for adapting\npre-trained vision-language models (VLMs) to the biomedical image\nclassification tasks in few shot scenarios. However, most of the current prompt\nlearning methods only used the text prompts and ignored the particular\nstructures (such as the complex anatomical structures and subtle pathological\nfeatures) in the biomedical images. In this work, we propose Biomed-DPT, a\nknowledge-enhanced dual modality prompt tuning technique. In designing the text\nprompt, Biomed-DPT constructs a dual prompt including the template-driven\nclinical prompts and the large language model (LLM)-driven domain-adapted\nprompts, then extracts the clinical knowledge from the domain-adapted prompts\nthrough the knowledge distillation technique. In designing the vision prompt,\nBiomed-DPT introduces the zero vector as a soft prompt to leverage attention\nre-weighting so that the focus on non-diagnostic regions and the recognition of\nnon-critical pathological features are avoided. Biomed-DPT achieves an average\nclassification accuracy of 66.14\\% across 11 biomedical image datasets covering\n9 modalities and 10 organs, with performance reaching 78.06\\% in base classes\nand 75.97\\% in novel classes, surpassing the Context Optimization (CoOp) method\nby 6.20\\%, 3.78\\%, and 8.04\\%, respectively. Our code are available at\n\\underline{https://github.com/Kanyooo/Biomed-DPT}.\n","authors":["Wei Peng","Kang Liu","Jianchen Hu","Meng Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.05189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05181v1","updated":"2025-05-08T12:32:29Z","published":"2025-05-08T12:32:29Z","title":"Stochastic Variational Propagation: Local, Scalable and Efficient\n  Alternative to Backpropagation","summary":"  Backpropagation (BP) is the cornerstone of deep learning, but its reliance on\nglobal gradient synchronization limits scalability and imposes significant\nmemory overhead. We propose Stochastic Variational Propagation (SVP), a\nscalable alternative that reframes training as hierarchical variational\ninference. SVP treats layer activations as latent variables and optimizes local\nEvidence Lower Bounds (ELBOs), enabling independent, local updates while\npreserving global coherence. However, directly applying KL divergence in\nlayer-wise ELBOs risks inter-layer's representation collapse due to excessive\ncompression. To prevent this, SVP projects activations into low-dimensional\nspaces via fixed random matrices, ensuring information preservation and\nrepresentational diversity. Combined with a feature alignment loss for\ninter-layer consistency, SVP achieves competitive accuracy with BP across\ndiverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to\nImageNet), reduces memory usage by up to 4x, and significantly improves\nscalability. More broadly, SVP introduces a probabilistic perspective to deep\nrepresentation learning, opening pathways toward more modular and interpretable\nneural network design.\n","authors":["Bojian Yin","Federico Corradi"],"pdf_url":"https://arxiv.org/pdf/2505.05181v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.05177v1","updated":"2025-05-08T12:28:00Z","published":"2025-05-08T12:28:00Z","title":"MARK: Memory Augmented Refinement of Knowledge","summary":"  Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time.\n","authors":["Anish Ganguli","Prabal Deb","Debleena Banerjee"],"pdf_url":"https://arxiv.org/pdf/2505.05177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05170v1","updated":"2025-05-08T12:13:16Z","published":"2025-05-08T12:13:16Z","title":"Dukawalla: Voice Interfaces for Small Businesses in Africa","summary":"  Small and medium sized businesses often struggle with data driven decision\nmaking do to a lack of advanced analytics tools, especially in African\ncountries where they make up a majority of the workforce. Though many tools\nexist they are not designed to fit into the ways of working of SMB workers who\nare mobile first, have limited time to learn new workflows, and for whom social\nand business are tightly coupled. To address this, the Dukawalla prototype was\ncreated. This intelligent assistant bridges the gap between raw business data,\nand actionable insights by leveraging voice interaction and the power of\ngenerative AI. Dukawalla provides an intuitive way for business owners to\ninteract with their data, aiding in informed decision making. This paper\nexamines Dukawalla's deployment across SMBs in Nairobi, focusing on their\nexperiences using this voice based assistant to streamline data collection and\nprovide business insights\n","authors":["Elizabeth Ankrah","Stephanie Nyairo","Mercy Muchai","Kagonya Awori","Millicent Ochieng","Mark Kariuki","Jacki O'Neill"],"pdf_url":"https://arxiv.org/pdf/2505.05170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04497v2","updated":"2025-05-08T11:59:21Z","published":"2025-05-07T15:20:17Z","title":"Defining and Quantifying Creative Behavior in Popular Image Generators","summary":"  Creativity of generative AI models has been a subject of scientific debate in\nthe last years, without a conclusive answer. In this paper, we study creativity\nfrom a practical perspective and introduce quantitative measures that help the\nuser to choose a suitable AI model for a given task. We evaluated our measures\non a number of popular image-to-image generation models, and the results of\nthis suggest that our measures conform to human intuition.\n","authors":["Aditi Ramaswamy","Hana Chockler","Melane Navaratnarajah"],"pdf_url":"https://arxiv.org/pdf/2505.04497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15169v2","updated":"2025-05-08T11:58:28Z","published":"2025-03-19T12:51:52Z","title":"Benchmarking Open-Source Large Language Models on Healthcare Text\n  Classification Tasks","summary":"  The application of large language models (LLMs) to healthcare information\nextraction has emerged as a promising approach. This study evaluates the\nclassification performance of five open-source LLMs: GEMMA-3-27B-IT,\nLLAMA3-70B, LLAMA4-109B, DEEPSEEK-R1-DISTILL-LLAMA-70B, and\nDEEPSEEK-V3-0324-UD-Q2_K_XL, across six healthcare-related classification tasks\ninvolving both social media data (breast cancer, changes in medication regimen,\nadverse pregnancy outcomes, potential COVID-19 cases) and clinical data (stigma\nlabeling, medication change discussion). We report precision, recall, and F1\nscores with 95% confidence intervals for all model-task combinations. Our\nfindings reveal significant performance variability between LLMs, with\nDeepSeekV3 emerging as the strongest overall performer, achieving the highest\nF1 scores in four tasks. Notably, models generally performed better on social\nmedia tasks compared to clinical data tasks, suggesting potential\ndomain-specific challenges. GEMMA-3-27B-IT demonstrated exceptionally high\nrecall despite its smaller parameter count, while LLAMA4-109B showed\nsurprisingly underwhelming performance compared to its predecessor LLAMA3-70B,\nindicating that larger parameter counts do not guarantee improved\nclassification results. We observed distinct precision-recall trade-offs across\nmodels, with some favoring sensitivity over specificity and vice versa. These\nfindings highlight the importance of task-specific model selection for\nhealthcare applications, considering the particular data domain and\nprecision-recall requirements rather than model size alone. As healthcare\nincreasingly integrates AI-driven text classification tools, this comprehensive\nbenchmarking provides valuable guidance for model selection and implementation\nwhile underscoring the need for continued evaluation and domain adaptation of\nLLMs in healthcare contexts.\n","authors":["Yuting Guo","Abeed Sarker"],"pdf_url":"https://arxiv.org/pdf/2503.15169v2.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2502.07503v4","updated":"2025-05-08T11:40:01Z","published":"2025-02-11T12:11:40Z","title":"Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems","summary":"  Inspired by recent findings on the fractal geometry of language, we introduce\nRecursive INference Scaling (RINS) as a complementary, plug-in recipe for\nscaling inference time in language and multimodal systems. RINS is a particular\nform of recursive depth that significantly outperforms +55 other variants,\nincluding the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et\nal., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior\nworks, we carry out our comparisons on a compute-matched regime, and\ndemonstrate that for a fixed model size and training compute budget, RINS\nsubstantially improves language modeling performance. It also generalizes\nbeyond pure language tasks, delivering gains in multimodal systems, including a\n+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by\nderiving data scaling laws, we show that RINS improves both the asymptotic\nperformance limits and the scaling exponents. More importantly, with\nlight-weight (linear) adapters (comprising <1% of model parameters) and\nstochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled\npretraining improves performance in language modeling even when recursive depth\nis not applied at inference time. This corresponds to improving performance on\na training compute-, parameter-, and inference-matched regime, suggesting its\npotential as a viable component of LLM pretraining!\n","authors":["Ibrahim Alabdulmohsin","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.07503v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05145v1","updated":"2025-05-08T11:32:46Z","published":"2025-05-08T11:32:46Z","title":"Understanding In-context Learning of Addition via Activation Subspaces","summary":"  To perform in-context learning, language models must extract signals from\nindividual few-shot examples, aggregate these into a learned prediction rule,\nand then apply this rule to new examples. How is this implemented in the\nforward pass of modern transformer models? To study this, we consider a\nstructured family of few-shot learning tasks for which the true prediction rule\nis to add an integer $k$ to the input. We find that Llama-3-8B attains high\naccuracy on this task for a range of $k$, and localize its few-shot ability to\njust three attention heads via a novel optimization approach. We further show\nthe extracted signals lie in a six-dimensional subspace, where four of the\ndimensions track the unit digit and the other two dimensions track overall\nmagnitude. We finally examine how these heads extract information from\nindividual few-shot examples, identifying a self-correction mechanism in which\nmistakes from earlier examples are suppressed by later examples. Our results\ndemonstrate how tracking low-dimensional subspaces across a forward pass can\nprovide insight into fine-grained computational structures.\n","authors":["Xinyan Hu","Kayo Yin","Michael I. Jordan","Jacob Steinhardt","Lijie Chen"],"pdf_url":"https://arxiv.org/pdf/2505.05145v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2505.05138v1","updated":"2025-05-08T11:21:29Z","published":"2025-05-08T11:21:29Z","title":"Guiding Evolutionary AutoEncoder Training with Activation-Based Pruning\n  Operators","summary":"  This study explores a novel approach to neural network pruning using\nevolutionary computation, focusing on simultaneously pruning the encoder and\ndecoder of an autoencoder. We introduce two new mutation operators that use\nlayer activations to guide weight pruning. Our findings reveal that one of\nthese activation-informed operators outperforms random pruning, resulting in\nmore efficient autoencoders with comparable performance to canonically trained\nmodels. Prior work has established that autoencoder training is effective and\nscalable with a spatial coevolutionary algorithm that cooperatively coevolves a\npopulation of encoders with a population of decoders, rather than one\nautoencoder. We evaluate how the same activity-guided mutation operators\ntransfer to this context. We find that random pruning is better than guided\npruning, in the coevolutionary setting. This suggests activation-based guidance\nproves more effective in low-dimensional pruning environments, where\nconstrained sample spaces can lead to deviations from true uniformity in\nrandomization. Conversely, population-driven strategies enhance robustness by\nexpanding the total pruning dimensionality, achieving statistically uniform\nrandomness that better preserves system dynamics. We experiment with pruning\naccording to different schedules and present best combinations of operator and\nschedule for the canonical and coevolving populations cases.\n","authors":["Steven Jorgensen","Erik Hemberg","Jamal Toutouh","Una-May O'Reilly"],"pdf_url":"https://arxiv.org/pdf/2505.05138v1.pdf","comment":"Accepted to The Genetic and Evolutionary Computation Conference\n  (GECCO 2025)"},{"id":"http://arxiv.org/abs/2502.18635v2","updated":"2025-05-08T10:58:09Z","published":"2025-02-25T20:52:06Z","title":"Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for\n  LLM and RAG Systems","summary":"  While Retrieval Augmented Generation (RAG) has emerged as a popular technique\nfor improving Large Language Model (LLM) systems, it introduces a large number\nof choices, parameters and hyperparameters that must be made or tuned. This\nincludes the LLM, embedding, and ranker models themselves, as well as\nhyperparameters governing individual RAG components. Yet, collectively\noptimizing the entire configuration in a RAG or LLM system remains\nunder-explored - especially in multi-objective settings - due to intractably\nlarge solution spaces, noisy objective evaluations, and the high cost of\nevaluations. In this work, we introduce the first approach for multi-objective\nparameter optimization of cost, latency, safety and alignment over entire LLM\nand RAG systems. We find that Bayesian optimization methods significantly\noutperform baseline approaches, obtaining a superior Pareto front on two new\nRAG benchmark tasks. We conclude our work with important considerations for\npractitioners who are designing multi-objective RAG systems, highlighting\nnuances such as how optimal configurations may not generalize across tasks and\nobjectives.\n","authors":["Matthew Barker","Andrew Bell","Evan Thomas","James Carr","Thomas Andrews","Umang Bhatt"],"pdf_url":"https://arxiv.org/pdf/2502.18635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.00654v3","updated":"2025-05-08T10:52:25Z","published":"2025-05-01T16:55:44Z","title":"Large Language Models Understanding: an Inherent Ambiguity Barrier","summary":"  A lively ongoing debate is taking place, since the extraordinary emergence of\nLarge Language Models (LLMs) with regards to their capability to understand the\nworld and capture the meaning of the dialogues in which they are involved.\nArguments and counter-arguments have been proposed based upon thought\nexperiments, anecdotal conversations between LLMs and humans, statistical\nlinguistic analysis, philosophical considerations, and more. In this brief\npaper we present a counter-argument based upon a thought experiment and\nsemi-formal considerations leading to an inherent ambiguity barrier which\nprevents LLMs from having any understanding of what their amazingly fluent\ndialogues mean.\n","authors":["Daniel N. Nissani"],"pdf_url":"https://arxiv.org/pdf/2505.00654v3.pdf","comment":"submitted to NEURAL COMPUTATION"},{"id":"http://arxiv.org/abs/2307.02075v3","updated":"2025-05-08T10:46:52Z","published":"2023-07-05T07:32:34Z","title":"Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for\n  Entity Alignment","summary":"  Entity alignment (EA) aims at identifying equivalent entity pairs across\ndifferent knowledge graphs (KGs) that refer to the same real-world identity. To\ncircumvent the shortage of seed alignments provided for training, recent EA\nmodels utilize pseudo-labeling strategies to iteratively add unaligned entity\npairs predicted with high confidence to the seed alignments for model training.\nHowever, the adverse impact of confirmation bias during pseudo-labeling has\nbeen largely overlooked, thus hindering entity alignment performance. To\nsystematically combat confirmation bias for pseudo-labeling-based entity\nalignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment\n(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the\naccuracy of entity alignment. UPL-EA consists of two complementary components:\n(1) Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling as\nan effective means to determine entity correspondences and reduce erroneous\nmatches across two KGs. An effective criterion is derived to infer\npseudo-labeled alignments that satisfy one-to-one correspondences; (2) Parallel\npseudo-label ensembling refines pseudo-labeled alignments by combining\npredictions over multiple models independently trained in parallel. The\nensembled pseudo-labeled alignments are thereafter used to augment seed\nalignments to reinforce subsequent model training for alignment inference. The\neffectiveness of UPL-EA in eliminating pseudo-labeling errors is both\ntheoretically supported and experimentally validated. Our extensive results and\nin-depth analyses demonstrate the superiority of UPL-EA over 15 competitive\nbaselines and its utility as a general pseudo-labeling framework for entity\nalignment.\n","authors":["Qijie Ding","Jie Yin","Daokun Zhang","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2307.02075v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05115v1","updated":"2025-05-08T10:31:03Z","published":"2025-05-08T10:31:03Z","title":"Is there a half-life for the success rates of AI agents?","summary":"  Building on the recent empirical work of Kwa et al. (2025), I show that\nwithin their suite of research-engineering tasks the performance of AI agents\non longer-duration tasks can be explained by an extremely simple mathematical\nmodel -- a constant rate of failing during each minute a human would take to do\nthe task. This implies an exponentially declining success rate with the length\nof the task and that each agent could be characterised by its own half-life.\nThis empirical regularity allows us to estimate the success rate for an agent\nat different task lengths. And the fact that this model is a good fit for the\ndata is suggestive of the underlying causes of failure on longer tasks -- that\nthey involve increasingly large sets of subtasks where failing any one fails\nthe task. Whether this model applies more generally on other suites of tasks is\nunknown and an important subject for further work.\n","authors":["Toby Ord"],"pdf_url":"https://arxiv.org/pdf/2505.05115v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2505.05108v1","updated":"2025-05-08T10:13:53Z","published":"2025-05-08T10:13:53Z","title":"Multi-agent Embodied AI: Advances and Future Directions","summary":"  Embodied artificial intelligence (Embodied AI) plays a pivotal role in the\napplication of advanced technologies in the intelligent era, where AI systems\nare integrated with physical bodies that enable them to perceive, reason, and\ninteract with their environments. Through the use of sensors for input and\nactuators for action, these systems can learn and adapt based on real-world\nfeedback, allowing them to perform tasks effectively in dynamic and\nunpredictable environments. As techniques such as deep learning (DL),\nreinforcement learning (RL), and large language models (LLMs) mature, embodied\nAI has become a leading field in both academia and industry, with applications\nspanning robotics, healthcare, transportation, and manufacturing. However, most\nresearch has focused on single-agent systems that often assume static, closed\nenvironments, whereas real-world embodied AI must navigate far more complex\nscenarios. In such settings, agents must not only interact with their\nsurroundings but also collaborate with other agents, necessitating\nsophisticated mechanisms for adaptation, real-time learning, and collaborative\nproblem-solving. Despite increasing interest in multi-agent systems, existing\nresearch remains narrow in scope, often relying on simplified models that fail\nto capture the full complexity of dynamic, open environments for multi-agent\nembodied AI. Moreover, no comprehensive survey has systematically reviewed the\nadvancements in this area. As embodied AI rapidly evolves, it is crucial to\ndeepen our understanding of multi-agent embodied AI to address the challenges\npresented by real-world applications. To fill this gap and foster further\ndevelopment in the field, this paper reviews the current state of research,\nanalyzes key contributions, and identifies challenges and future directions,\nproviding insights to guide innovation and progress in this field.\n","authors":["Zhaohan Feng","Ruiqi Xue","Lei Yuan","Yang Yu","Ning Ding","Meiqin Liu","Bingzhao Gao","Jian Sun","Gang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.05108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05106v1","updated":"2025-05-08T10:10:00Z","published":"2025-05-08T10:10:00Z","title":"A Neuro-Symbolic Framework for Sequence Classification with Relational\n  and Temporal Knowledge","summary":"  One of the goals of neuro-symbolic artificial intelligence is to exploit\nbackground knowledge to improve the performance of learning tasks. However,\nmost of the existing frameworks focus on the simplified scenario where\nknowledge does not change over time and does not cover the temporal dimension.\nIn this work we consider the much more challenging problem of knowledge-driven\nsequence classification where different portions of knowledge must be employed\nat different timesteps, and temporal relations are available. Our experimental\nevaluation compares multi-stage neuro-symbolic and neural-only architectures,\nand it is conducted on a newly-introduced benchmarking framework. Results\ndemonstrate the challenging nature of this novel setting, and also highlight\nunder-explored shortcomings of neuro-symbolic methods, representing a precious\nreference for future research.\n","authors":["Luca Salvatore Lorello","Marco Lippi","Stefano Melacci"],"pdf_url":"https://arxiv.org/pdf/2505.05106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09507v2","updated":"2025-05-08T10:03:57Z","published":"2024-12-12T17:55:00Z","title":"Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction","summary":"  Indoor pathloss prediction is a fundamental task in wireless network\nplanning, yet it remains challenging due to environmental complexity and data\nscarcity. In this work, we propose a deep learning-based approach utilizing a\nvision transformer (ViT) architecture with DINO-v2 pretrained weights to model\nindoor radio propagation. Our method processes a floor map with additional\nfeatures of the walls to generate indoor pathloss maps. We systematically\nevaluate the effects of architectural choices, data augmentation strategies,\nand feature engineering techniques. Our findings indicate that extensive\naugmentation significantly improves generalization, while feature engineering\nis crucial in low-data regimes. Through comprehensive experiments, we\ndemonstrate the robustness of our model across different generalization\nscenarios.\n","authors":["Rafayel Mkrtchyan","Edvard Ghukasyan","Khoren Petrosyan","Hrant Khachatrian","Theofanis P. Raptis"],"pdf_url":"https://arxiv.org/pdf/2412.09507v2.pdf","comment":"Work partly supported by the RA Science Committee grant No. 22rl-052\n  (DISTAL) and the EU under Italian National Recovery and Resilience Plan of\n  NextGenerationEU on \"Telecommunications of the Future\" (PE00000001 - program\n  \"RESTART\")"},{"id":"http://arxiv.org/abs/2505.05086v1","updated":"2025-05-08T09:34:15Z","published":"2025-05-08T09:34:15Z","title":"Beyond Low-rank Decomposition: A Shortcut Approach for Efficient\n  On-Device Learning","summary":"  On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks.\n","authors":["Le-Trung Nguyen","Ael Quelennec","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2505.05086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08292v2","updated":"2025-05-08T09:33:59Z","published":"2025-03-11T11:05:42Z","title":"Large Language Models for Outpatient Referral: Problem Definition,\n  Benchmarking and Challenges","summary":"  Large language models (LLMs) are increasingly applied to outpatient referral\ntasks across healthcare systems. However, there is a lack of standardized\nevaluation criteria to assess their effectiveness, particularly in dynamic,\ninteractive scenarios. In this study, we systematically examine the\ncapabilities and limitations of LLMs in managing tasks within Intelligent\nOutpatient Referral (IOR) systems and propose a comprehensive evaluation\nframework specifically designed for such systems. This framework comprises two\ncore tasks: static evaluation, which focuses on evaluating the ability of\npredefined outpatient referrals, and dynamic evaluation, which evaluates\ncapabilities of refining outpatient referral recommendations through iterative\ndialogues. Our findings suggest that LLMs offer limited advantages over\nBERT-like models, but show promise in asking effective questions during\ninteractive dialogues.\n","authors":["Xiaoxiao Liu","Qingying Xiao","Junying Chen","Xiangyi Feng","Xiangbo Wu","Bairui Zhang","Xiang Wan","Jian Chang","Guangjun Yu","Yan Hu","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2503.08292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16149v5","updated":"2025-05-08T09:23:42Z","published":"2024-03-24T13:43:43Z","title":"Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a\n  Comprehensive Survey","summary":"  The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to traditional network traffic analysis in fields like\nmobile apps and websites, CIoT introduces unique characteristics that pose new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for assessing CIoT security and privacy risks, this\nsurvey reviews 310 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs.\n","authors":["Yan Jia","Yuxin Song","Zihou Liu","Qingyin Tan","Yang Song","Yu Zhang","Zheli Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16149v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15699v2","updated":"2025-05-08T09:12:22Z","published":"2025-04-22T08:34:35Z","title":"Advancing Embodied Agent Security: From Safety Benchmarks to Input\n  Moderation","summary":"  Embodied agents exhibit immense potential across a multitude of domains,\nmaking the assurance of their behavioral safety a fundamental prerequisite for\ntheir widespread deployment. However, existing research predominantly\nconcentrates on the security of general large language models, lacking\nspecialized methodologies for establishing safety benchmarks and input\nmoderation tailored to embodied agents. To bridge this gap, this paper\nintroduces a novel input moderation framework, meticulously designed to\nsafeguard embodied agents. This framework encompasses the entire pipeline,\nincluding taxonomy definition, dataset curation, moderator architecture, model\ntraining, and rigorous evaluation. Notably, we introduce EAsafetyBench, a\nmeticulously crafted safety benchmark engineered to facilitate both the\ntraining and stringent assessment of moderators specifically designed for\nembodied agents. Furthermore, we propose Pinpoint, an innovative\nprompt-decoupled input moderation scheme that harnesses a masked attention\nmechanism to effectively isolate and mitigate the influence of functional\nprompts on moderation tasks. Extensive experiments conducted on diverse\nbenchmark datasets and models validate the feasibility and efficacy of the\nproposed approach. The results demonstrate that our methodologies achieve an\nimpressive average detection accuracy of 94.58%, surpassing the performance of\nexisting state-of-the-art techniques, alongside an exceptional moderation\nprocessing time of merely 0.002 seconds per instance.\n","authors":["Ning Wang","Zihan Yan","Weiyang Li","Chuan Ma","He Chen","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2504.15699v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2504.21435v2","updated":"2025-05-08T09:08:01Z","published":"2025-04-30T08:48:21Z","title":"SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding","summary":"  With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\nstandalone videos and mainly assess \"visual elements\" like human actions and\nobject states. In reality, contemporary videos often encompass complex and\ncontinuous narratives, typically presented as a series. To address this\nchallenge, we propose SeriesBench, a benchmark consisting of 105 carefully\ncurated narrative-driven series, covering 28 specialized tasks that require\ndeep narrative understanding. Specifically, we first select a diverse set of\ndrama series spanning various genres. Then, we introduce a novel long-span\nnarrative annotation method, combined with a full-information transformation\napproach to convert manual annotations into diverse task formats. To further\nenhance model capacity for detailed analysis of plot structures and character\nrelationships within series, we propose a novel narrative reasoning framework,\nPC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still\nface significant challenges in understanding narrative-driven series, while\nPC-DCoT enables these MLLMs to achieve performance improvements. Overall, our\nSeriesBench and PC-DCoT highlight the critical necessity of advancing model\ncapabilities to understand narrative-driven series, guiding the future\ndevelopment of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025.\n","authors":["Chenkai Zhang","Yiming Lei","Zeming Liu","Haitao Leng","Shaoguo Liu","Tingting Gao","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2504.21435v2.pdf","comment":"29 pages, 15 figures, CVPR 2025"},{"id":"http://arxiv.org/abs/2505.05071v1","updated":"2025-05-08T09:06:53Z","published":"2025-05-08T09:06:53Z","title":"FG-CLIP: Fine-Grained Visual and Textual Alignment","summary":"  Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks\nsuch as image-text retrieval and zero-shot classification but struggles with\nfine-grained understanding due to its focus on coarse-grained short captions.\nTo address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances\nfine-grained understanding through three key innovations. First, we leverage\nlarge multimodal models to generate 1.6 billion long caption-image pairs for\ncapturing global-level semantic details. Second, a high-quality dataset is\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned with detailed captions to ensure precise, context-rich\nrepresentations. Third, 10 million hard fine-grained negative samples are\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincluding fine-grained understanding, open-vocabulary object detection,\nimage-text retrieval, and general multimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.\n","authors":["Chunyu Xie","Bin Wang","Fanjing Kong","Jincheng Li","Dawei Liang","Gengshen Zhang","Dawei Leng","Yuhui Yin"],"pdf_url":"https://arxiv.org/pdf/2505.05071v1.pdf","comment":"Accepted at ICML 2025"},{"id":"http://arxiv.org/abs/2504.21356v2","updated":"2025-05-08T08:58:12Z","published":"2025-04-30T06:30:48Z","title":"Nexus-Gen: A Unified Model for Image Understanding, Generation, and\n  Editing","summary":"  Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field.\n","authors":["Hong Zhang","Zhongjie Duan","Xingjun Wang","Yuze Zhao","Weiyi Lu","Zhipeng Di","Yixuan Xu","Yingda Chen","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.21356v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08603v2","updated":"2025-05-08T08:56:29Z","published":"2025-04-11T15:12:05Z","title":"FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot\n  Exploration in Any Environment","summary":"  Geometrically accurate and semantically expressive map representations have\nproven invaluable to facilitate robust and safe mobile robot navigation and\ntask planning. Nevertheless, real-time, open-vocabulary semantic understanding\nof large-scale unknown environments is still an open problem. In this paper we\npresent FindAnything, an open-world mapping and exploration framework that\nincorporates vision-language information into dense volumetric submaps. Thanks\nto the use of vision-language features, FindAnything bridges the gap between\npure geometric and open-vocabulary semantic information for a higher level of\nunderstanding while allowing to explore any environment without the help of any\nexternal source of ground-truth pose information. We represent the environment\nas a series of volumetric occupancy submaps, resulting in a robust and accurate\nmap representation that deforms upon pose updates when the underlying SLAM\nsystem corrects its drift, allowing for a locally consistent representation\nbetween submaps. Pixel-wise vision-language features are aggregated from\nefficient SAM (eSAM)-generated segments, which are in turn integrated into\nobject-centric volumetric submaps, providing a mapping from open-vocabulary\nqueries to 3D geometry that is scalable also in terms of memory usage. The\nopen-vocabulary map representation of FindAnything achieves state-of-the-art\nsemantic accuracy in closed-set evaluations on the Replica dataset. This level\nof scene understanding allows a robot to explore environments based on objects\nor areas of interest selected via natural language queries. Our system is the\nfirst of its kind to be deployed on resource-constrained devices, such as MAVs,\nleveraging vision-language information for real-world robotic tasks.\n","authors":["Sebastián Barbas Laina","Simon Boche","Sotiris Papatheodorou","Simon Schaefer","Jaehyung Jung","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2504.08603v2.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.11055v3","updated":"2025-05-08T08:51:19Z","published":"2024-09-17T10:31:37Z","title":"Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and\n  Model Size in Large Language Models From Edge to Giant","summary":"  Quantization has gained attention as a promising solution for the\ncost-effective deployment of large and small language models. However, most\nprior work has been limited to perplexity or basic knowledge tasks and lacks a\ncomprehensive evaluation of recent models like Llama-3.3. In this paper, we\nconduct a comprehensive evaluation of instruction-tuned models spanning 1B to\n405B parameters, applying four quantization methods across 13 datasets. Our\nfindings reveal that (1) quantized models generally surpass smaller FP16\nbaselines, yet they often struggle with instruction-following and hallucination\ndetection; (2) FP8 consistently emerges as the most robust option across tasks,\nand AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller\nmodels can suffer severe accuracy drops at 4-bit quantization, while 70B-scale\nmodels maintain stable performance; (4) notably, \\textit{hard} tasks do not\nalways experience the largest accuracy losses, indicating that quantization\nmagnifies a model's inherent weaknesses rather than simply correlating with\ntask difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant\nperformance declines in coding and STEM tasks, though reasoning may sometimes\nimprove.\n","authors":["Jemin Lee","Sihyeong Park","Jinse Kwon","Jihun Oh","Yongin Kwon"],"pdf_url":"https://arxiv.org/pdf/2409.11055v3.pdf","comment":"Accepted in IJCAI 2025, 21 pages, 2 figure"},{"id":"http://arxiv.org/abs/2505.05059v1","updated":"2025-05-08T08:50:32Z","published":"2025-05-08T08:50:32Z","title":"Enhancing Reinforcement Learning for the Floorplanning of Analog ICs\n  with Beam Search","summary":"  The layout of analog ICs requires making complex trade-offs, while addressing\ndevice physics and variability of the circuits. This makes full automation with\nlearning-based solutions hard to achieve. However, reinforcement learning (RL)\nhas recently reached significant results, particularly in solving the\nfloorplanning problem. This paper presents a hybrid method that combines RL\nwith a beam (BS) strategy. The BS algorithm enhances the agent's inference\nprocess, allowing for the generation of flexible floorplans by accomodating\nvarious objective weightings, and addressing congestion without without the\nneed for policy retraining or fine-tuning. Moreover, the RL agent's\ngeneralization ability stays intact, along with its efficient handling of\ncircuit features and constraints. Experimental results show approx. 5-85%\nimprovement in area, dead space and half-perimeter wire length compared to a\nstandard RL application, along with higher rewards for the agent. Moreover,\nperformance and efficiency align closely with those of existing\nstate-of-the-art techniques.\n","authors":["Sandro Junior Della Rovere","Davide Basso","Luca Bortolussi","Mirjana Videnovic-Misic","Husni Habal"],"pdf_url":"https://arxiv.org/pdf/2505.05059v1.pdf","comment":"Published in Proceedings of the 21st International Conference on\n  Synthesis, Modeling, Analysis and Simulation Methods, and Applications to\n  Circuit Design (SMACD 2025). 4 pages, 3 figures"},{"id":"http://arxiv.org/abs/2505.05056v1","updated":"2025-05-08T08:47:11Z","published":"2025-05-08T08:47:11Z","title":"Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic\n  Annotations","summary":"  This paper reports the construction of the Teochew-Wild, a speech corpus of\nthe Teochew dialect. The corpus includes 18.9 hours of in-the-wild Teochew\nspeech data from multiple speakers, covering both formal and colloquial\nexpressions, with precise orthographic and pinyin annotations. Additionally, we\nprovide supplementary text processing tools and resources to propel research\nand applications in speech tasks for this low-resource language, such as\nautomatic speech recognition (ASR) and text-to-speech (TTS). To the best of our\nknowledge, this is the first publicly available Teochew dataset with accurate\northographic annotations. We conduct experiments on the corpus, and the results\nvalidate its effectiveness in ASR and TTS tasks.\n","authors":["Linrong Pan","Chenglong Jiang","Gaoze Hou","Ying Gao"],"pdf_url":"https://arxiv.org/pdf/2505.05056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12705v5","updated":"2025-05-08T08:46:59Z","published":"2024-10-16T16:11:49Z","title":"WorldCuisines: A Massive-Scale Benchmark for Multilingual and\n  Multicultural Visual Question Answering on Global Cuisines","summary":"  Vision Language Models (VLMs) often struggle with culture-specific knowledge,\nparticularly in languages other than English and in underrepresented cultural\ncontexts. To evaluate their understanding of such knowledge, we introduce\nWorldCuisines, a massive-scale benchmark for multilingual and multicultural,\nvisually grounded language understanding. This benchmark includes a visual\nquestion answering (VQA) dataset with text-image pairs across 30 languages and\ndialects, spanning 9 language families and featuring over 1 million data\npoints, making it the largest multicultural VQA benchmark to date. It includes\ntasks for identifying dish names and their origins. We provide evaluation\ndatasets in two sizes (12k and 60k instances) alongside a training dataset (1\nmillion instances). Our findings show that while VLMs perform better with\ncorrect location context, they struggle with adversarial contexts and\npredicting specific regional cuisines and languages. To support future\nresearch, we release a knowledge base with annotated food entries and images\nalong with the VQA data.\n","authors":["Genta Indra Winata","Frederikus Hudi","Patrick Amadeus Irawan","David Anugraha","Rifki Afina Putri","Yutong Wang","Adam Nohejl","Ubaidillah Ariq Prathama","Nedjma Ousidhoum","Afifa Amriani","Anar Rzayev","Anirban Das","Ashmari Pramodya","Aulia Adila","Bryan Wilie","Candy Olivia Mawalim","Ching Lam Cheng","Daud Abolade","Emmanuele Chersoni","Enrico Santus","Fariz Ikhwantri","Garry Kuwanto","Hanyang Zhao","Haryo Akbarianto Wibowo","Holy Lovenia","Jan Christian Blaise Cruz","Jan Wira Gotama Putra","Junho Myung","Lucky Susanto","Maria Angelica Riera Machin","Marina Zhukova","Michael Anugraha","Muhammad Farid Adilazuarda","Natasha Santosa","Peerat Limkonchotiwat","Raj Dabre","Rio Alexander Audino","Samuel Cahyawijaya","Shi-Xiong Zhang","Stephanie Yulia Salim","Yi Zhou","Yinxuan Gui","David Ifeoluwa Adelani","En-Shiun Annie Lee","Shogo Okada","Ayu Purwarianti","Alham Fikri Aji","Taro Watanabe","Derry Tanti Wijaya","Alice Oh","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2410.12705v5.pdf","comment":"Best Theme Paper at NAACL 2025"},{"id":"http://arxiv.org/abs/2505.05054v1","updated":"2025-05-08T08:46:28Z","published":"2025-05-08T08:46:28Z","title":"Direct Image Classification from Fourier Ptychographic Microscopy\n  Measurements without Reconstruction","summary":"  The computational imaging technique of Fourier Ptychographic Microscopy (FPM)\nenables high-resolution imaging with a wide field of view and can serve as an\nextremely valuable tool, e.g. in the classification of cells in medical\napplications. However, reconstructing a high-resolution image from tens or even\nhundreds of measurements is computationally expensive, particularly for a wide\nfield of view. Therefore, in this paper, we investigate the idea of classifying\nthe image content in the FPM measurements directly without performing a\nreconstruction step first. We show that Convolutional Neural Networks (CNN) can\nextract meaningful information from measurement sequences, significantly\noutperforming the classification on a single band-limited image (up to 12 %)\nwhile being significantly more efficient than a reconstruction of a\nhigh-resolution image. Furthermore, we demonstrate that a learned multiplexing\nof several raw measurements allows maintaining the classification accuracy\nwhile reducing the amount of data (and consequently also the acquisition time)\nsignificantly.\n","authors":["Navya Sonal Agarwal","Jan Philipp Schneider","Kanchana Vaishnavi Gandikota","Syed Muhammad Kazim","John Meshreki","Ivo Ihrke","Michael Moeller"],"pdf_url":"https://arxiv.org/pdf/2505.05054v1.pdf","comment":"ISCS 2025"},{"id":"http://arxiv.org/abs/2505.02417v2","updated":"2025-05-08T08:30:12Z","published":"2025-05-05T07:22:54Z","title":"T2S: High-resolution Time Series Generation with Text-to-Series\n  Diffusion Models","summary":"  Text-to-Time Series generation holds significant potential to address\nchallenges such as data sparsity, imbalance, and limited availability of\nmultimodal time series datasets across domains. While diffusion models have\nachieved remarkable success in Text-to-X (e.g., vision and audio data)\ngeneration, their use in time series generation remains in its nascent stages.\nExisting approaches face two critical limitations: (1) the lack of systematic\nexploration of general-proposed time series captions, which are often\ndomain-specific and struggle with generalization; and (2) the inability to\ngenerate time series of arbitrary lengths, limiting their applicability to\nreal-world scenarios. In this work, we first categorize time series captions\ninto three levels: point-level, fragment-level, and instance-level.\nAdditionally, we introduce a new fragment-level dataset containing over 600,000\nhigh-resolution time series-text pairs. Second, we propose Text-to-Series\n(T2S), a diffusion-based framework that bridges the gap between natural\nlanguage and time series in a domain-agnostic manner. T2S employs a\nlength-adaptive variational autoencoder to encode time series of varying\nlengths into consistent latent embeddings. On top of that, T2S effectively\naligns textual representations with latent embeddings by utilizing Flow\nMatching and employing Diffusion Transformer as the denoiser. We train T2S in\nan interleaved paradigm across multiple lengths, allowing it to generate\nsequences of any desired length. Extensive evaluations demonstrate that T2S\nachieves state-of-the-art performance across 13 datasets spanning 12 domains.\n","authors":["Yunfeng Ge","Jiawei Li","Yiji Zhao","Haomin Wen","Zhao Li","Meikang Qiu","Hongyan Li","Ming Jin","Shirui Pan"],"pdf_url":"https://arxiv.org/pdf/2505.02417v2.pdf","comment":"Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2505.03961v2","updated":"2025-05-08T08:29:29Z","published":"2025-05-06T20:23:25Z","title":"The Power of Stories: Narrative Priming Shapes How LLM Agents\n  Collaborate and Compete","summary":"  According to Yuval Noah Harari, large-scale human cooperation is driven by\nshared narratives that encode common beliefs and values. This study explores\nwhether such narratives can similarly nudge LLM agents toward collaboration. We\nuse a finitely repeated public goods game in which LLM agents choose either\ncooperative or egoistic spending strategies. We prime agents with stories\nhighlighting teamwork to different degrees and test how this influences\nnegotiation outcomes. Our experiments explore four questions:(1) How do\nnarratives influence negotiation behavior? (2) What differs when agents share\nthe same story versus different ones? (3) What happens when the agent numbers\ngrow? (4) Are agents resilient against self-serving negotiators? We find that\nstory-based priming significantly affects negotiation strategies and success\nrates. Common stories improve collaboration, benefiting each agent. By\ncontrast, priming agents with different stories reverses this effect, and those\nagents primed toward self-interest prevail. We hypothesize that these results\ncarry implications for multi-agent system design and AI alignment.\n","authors":["Gerrit Großmann","Larisa Ivanova","Sai Leela Poduru","Mohaddeseh Tabrizian","Islam Mesabah","David A. Selby","Sebastian J. Vollmer"],"pdf_url":"https://arxiv.org/pdf/2505.03961v2.pdf","comment":"16 pages, 8 figures. Code available at\n  https://github.com/storyagents25/story-agents"},{"id":"http://arxiv.org/abs/2505.05040v1","updated":"2025-05-08T08:23:20Z","published":"2025-05-08T08:23:20Z","title":"Image-Text Relation Prediction for Multilingual Tweets","summary":"  Various social networks have been allowing media uploads for over a decade\nnow. Still, it has not always been clear what is their relation with the posted\ntext or even if there is any at all. In this work, we explore how multilingual\nvision-language models tackle the task of image-text relation prediction in\ndifferent languages, and construct a dedicated balanced benchmark data set from\nTwitter posts in Latvian along with their manual translations into English. We\ncompare our results to previous work and show that the more recently released\nvision-language model checkpoints are becoming increasingly capable at this\ntask, but there is still much room for further improvement.\n","authors":["Matīss Rikters","Edison Marrese-Taylor"],"pdf_url":"https://arxiv.org/pdf/2505.05040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04165v2","updated":"2025-05-08T08:17:59Z","published":"2025-05-07T06:34:34Z","title":"TS-SNN: Temporal Shift Module for Spiking Neural Networks","summary":"  Spiking Neural Networks (SNNs) are increasingly recognized for their\nbiological plausibility and energy efficiency, positioning them as strong\nalternatives to Artificial Neural Networks (ANNs) in neuromorphic computing\napplications. SNNs inherently process temporal information by leveraging the\nprecise timing of spikes, but balancing temporal feature utilization with low\nenergy consumption remains a challenge. In this work, we introduce Temporal\nShift module for Spiking Neural Networks (TS-SNN), which incorporates a novel\nTemporal Shift (TS) module to integrate past, present, and future spike\nfeatures within a single timestep via a simple yet effective shift operation. A\nresidual combination method prevents information loss by integrating shifted\nand original features. The TS module is lightweight, requiring only one\nadditional learnable parameter, and can be seamlessly integrated into existing\narchitectures with minimal additional computational cost. TS-SNN achieves\nstate-of-the-art performance on benchmarks like CIFAR-10 (96.72\\%), CIFAR-100\n(80.28\\%), and ImageNet (70.61\\%) with fewer timesteps, while maintaining low\nenergy consumption. This work marks a significant step forward in developing\nefficient and accurate SNN architectures.\n","authors":["Kairong Yu","Tianqing Zhang","Qi Xu","Gang Pan","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2505.04165v2.pdf","comment":"Accepted by ICML2025"},{"id":"http://arxiv.org/abs/2504.00762v4","updated":"2025-05-08T08:07:55Z","published":"2025-04-01T13:13:43Z","title":"Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling\n  Efficiently Scales Test-Time Compute","summary":"  This paper presents a simple, effective, and cost-efficient strategy to\nimprove LLM performance by scaling test-time compute. Our strategy builds upon\nthe repeated-sampling-then-voting framework, with a novel twist: incorporating\nmultiple models, even weaker ones, to leverage their complementary strengths\nthat potentially arise from diverse training data and paradigms. By using\nconsistency as a signal, our strategy dynamically switches between models.\nTheoretical analysis highlights the efficiency and performance advantages of\nour strategy. Extensive experiments on six datasets demonstrate that our\nstrategy not only outperforms self-consistency and state-of-the-art multi-agent\ndebate approaches, but also significantly reduces inference costs.\nAdditionally, ModelSwitch requires only a few comparable LLMs to achieve\noptimal performance and can be extended with verification methods,\ndemonstrating the potential of leveraging multiple LLMs in the\ngeneration-verification paradigm.\n","authors":["Jianhao Chen","Zishuo Xun","Bocheng Zhou","Han Qi","Hangfan Zhang","Qiaosheng Zhang","Yang Chen","Wei Hu","Yuzhong Qu","Wanli Ouyang","Shuyue Hu"],"pdf_url":"https://arxiv.org/pdf/2504.00762v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05029v1","updated":"2025-05-08T08:02:20Z","published":"2025-05-08T08:02:20Z","title":"A Reputation System for Large Language Model-based Multi-agent Systems\n  to Avoid the Tragedy of the Commons","summary":"  The tragedy of the commons, where individual self-interest leads to\ncollectively disastrous outcomes, is a pervasive challenge in human society.\nRecent studies have demonstrated that similar phenomena can arise in generative\nmulti-agent systems (MASs). To address this challenge, this paper explores the\nuse of reputation systems as a remedy. We propose RepuNet, a dynamic,\ndual-level reputation framework that models both agent-level reputation\ndynamics and system-level network evolution. Specifically, driven by direct\ninteractions and indirect gossip, agents form reputations for both themselves\nand their peers, and decide whether to connect or disconnect other agents for\nfuture interactions. Through two distinct scenarios, we show that RepuNet\neffectively mitigates the 'tragedy of the commons', promoting and sustaining\ncooperation in generative MASs. Moreover, we find that reputation systems can\ngive rise to rich emergent behaviors in generative MASs, such as the formation\nof cooperative clusters, the social isolation of exploitative agents, and the\npreference for sharing positive gossip rather than negative ones.\n","authors":["Siyue Ren","Wanli Fu","Xinkun Zou","Chen Shen","Yi Cai","Chen Chu","Zhen Wang","Shuyue Hu"],"pdf_url":"https://arxiv.org/pdf/2505.05029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.05423v2","updated":"2025-05-08T08:00:37Z","published":"2025-03-07T13:50:29Z","title":"Semantic Shift Estimation via Dual-Projection and Classifier\n  Reconstruction for Exemplar-Free Class-Incremental Learning","summary":"  Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn\nfrom distinct categories without retaining exemplars but easily suffers from\ncatastrophic forgetting of learned knowledge. While existing EFCIL methods\nleverage knowledge distillation to alleviate forgetting, they still face two\ncritical challenges: semantic shift and decision bias. Specifically, the\nembeddings of old tasks shift in the embedding space after learning new tasks,\nand the classifier becomes biased towards new tasks due to training solely with\nnew data, thereby hindering the balance between old and new knowledge. To\naddress these issues, we propose the Dual-Projection Shift Estimation and\nClassifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates\nsemantic shift through a dual-projection, which combines a learnable\ntransformation with a row-space projection to capture both task-wise and\ncategory-wise shifts. Furthermore, to mitigate decision bias, DPCR employs\nridge regression to reformulate classifier training as a reconstruction\nprocess. This reconstruction exploits previous information encoded in\ncovariance and prototype of each class after calibration with estimated shift,\nthereby reducing decision bias. Extensive experiments demonstrate that, across\nvarious datasets, DPCR effectively balances old and new tasks, outperforming\nstate-of-the-art EFCIL methods.\n","authors":["Run He","Di Fang","Yicheng Xu","Yawen Cui","Ming Li","Cen Chen","Ziqian Zeng","Huiping Zhuang"],"pdf_url":"https://arxiv.org/pdf/2503.05423v2.pdf","comment":"Accepted by ICML 2025"},{"id":"http://arxiv.org/abs/2505.05019v1","updated":"2025-05-08T07:51:36Z","published":"2025-05-08T07:51:36Z","title":"Generating Reliable Synthetic Clinical Trial Data: The Role of\n  Hyperparameter Optimization and Domain Constraints","summary":"  The generation of synthetic clinical trial data offers a promising approach\nto mitigating privacy concerns and data accessibility limitations in medical\nresearch. However, ensuring that synthetic datasets maintain high fidelity,\nutility, and adherence to domain-specific constraints remains a key challenge.\nWhile hyperparameter optimization (HPO) has been shown to improve generative\nmodel performance, the effectiveness of different optimization strategies for\nsynthetic clinical data remains unclear. This study systematically evaluates\nfour HPO strategies across eight generative models, comparing single-metric\noptimization against compound metric optimization approaches. Our results\ndemonstrate that HPO consistently improves synthetic data quality, with TVAE,\nCTGAN, and CTAB-GAN+ achieving improvements of up to 60%, 39%, and 38%,\nrespectively. Compound metric optimization outperformed single-metric\nstrategies, producing more balanced and generalizable synthetic datasets.\nInterestingly, HPO alone is insufficient to ensure clinically valid synthetic\ndata, as all models exhibited violations of fundamental survival constraints.\nPreprocessing and postprocessing played a crucial role in reducing these\nviolations, as models lacking robust processing steps produced invalid data in\nup to 61% of cases. These findings underscore the necessity of integrating\nexplicit domain knowledge alongside HPO to create high quality synthetic\ndatasets. Our study provides actionable recommendations for improving synthetic\ndata generation, with future research needed to refine metric selection and\nvalidate these findings on larger datasets to enhance clinical applicability.\n","authors":["Waldemar Hahn","Jan-Niklas Eckardt","Christoph Röllig","Martin Sedlmayr","Jan Moritz Middeke","Markus Wolfien"],"pdf_url":"https://arxiv.org/pdf/2505.05019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05015v1","updated":"2025-05-08T07:42:05Z","published":"2025-05-08T07:42:05Z","title":"An Agent-Based Modeling Approach to Free-Text Keyboard Dynamics for\n  Continuous Authentication","summary":"  Continuous authentication systems leveraging free-text keyboard dynamics\noffer a promising additional layer of security in a multifactor authentication\nsetup that can be used in a transparent way with no impact on user experience.\nThis study investigates the efficacy of behavioral biometrics by employing an\nAgent-Based Model (ABM) to simulate diverse typing profiles across mechanical\nand membrane keyboards. Specifically, we generated synthetic keystroke data\nfrom five unique agents, capturing features related to dwell time, flight time,\nand error rates within sliding 5-second windows updated every second. Two\nmachine learning approaches, One-Class Support Vector Machine (OC-SVM) and\nRandom Forest (RF), were evaluated for user verification. Results revealed a\nstark contrast in performance: while One-Class SVM failed to differentiate\nindividual users within each group, Random Forest achieved robust\nintra-keyboard user recognition (Accuracy > 0.7) but struggled to generalize\nacross keyboards for the same user, highlighting the significant impact of\nkeyboard hardware on typing behavior. These findings suggest that: (1)\nkeyboard-specific user profiles may be necessary for reliable authentication,\nand (2) ensemble methods like RF outperform One-Class SVM in capturing\nfine-grained user-specific patterns.\n","authors":["Roberto Dillon"," Arushi"],"pdf_url":"https://arxiv.org/pdf/2505.05015v1.pdf","comment":"16 pages, 5 figures, 12 tables"},{"id":"http://arxiv.org/abs/2309.04522v3","updated":"2025-05-08T07:33:46Z","published":"2023-09-08T18:00:01Z","title":"Connecting NTK and NNGP: A Unified Theoretical Framework for Wide Neural\n  Network Learning Dynamics","summary":"  Artificial neural networks have revolutionized machine learning in recent\nyears, but a complete theoretical framework for their learning process is still\nlacking. Substantial advances were achieved for wide networks, within two\ndisparate theoretical frameworks: the Neural Tangent Kernel (NTK), which\nassumes linearized gradient descent dynamics, and the Bayesian Neural Network\nGaussian Process (NNGP). We unify these two theories using gradient descent\nlearning with an additional noise in an ensemble of wide deep networks. We\nconstruct an analytical theory for the network input-output function and\nintroduce a new time-dependent Neural Dynamical Kernel (NDK) from which both\nNTK and NNGP kernels are derived. We identify two learning phases: a\ngradient-driven learning phase, dominated by loss minimization, in which the\ntime scale is governed by the initialization variance. It is followed by a slow\ndiffusive learning stage, where the parameters sample the solution space, with\na time constant decided by the noise and the Bayesian prior variance. The two\nvariance parameters strongly affect the performance in the two regimes,\nespecially in sigmoidal neurons. In contrast to the exponential convergence of\nthe mean predictor in the initial phase, the convergence to the equilibrium is\nmore complex and may behave nonmonotonically. By characterizing the diffusive\nphase, our work sheds light on representational drift in the brain, explaining\nhow neural activity changes continuously without degrading performance, either\nby ongoing gradient signals that synchronize the drifts of different synapses\nor by architectural biases that generate task-relevant information that is\nrobust against the drift process. This work closes the gap between the NTK and\nNNGP theories, providing a comprehensive framework for the learning process of\ndeep wide neural networks and for analyzing dynamics in biological circuits.\n","authors":["Yehonatan Avidan","Qianyi Li","Haim Sompolinsky"],"pdf_url":"https://arxiv.org/pdf/2309.04522v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00437v2","updated":"2025-05-08T07:29:10Z","published":"2024-11-01T08:02:09Z","title":"E2E-AFG: An End-to-End Model with Adaptive Filtering for\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation methods often neglect the quality of content\nretrieved from external knowledge bases, resulting in irrelevant information or\npotential misinformation that negatively affects the generation results of\nlarge language models. In this paper, we propose an end-to-end model with\nadaptive filtering for retrieval-augmented generation (E2E-AFG), which\nintegrates answer existence judgment and text generation into a single\nend-to-end framework. This enables the model to focus more effectively on\nrelevant content while reducing the influence of irrelevant information and\ngenerating accurate answers. We evaluate E2E-AFG on six representative\nknowledge-intensive language datasets, and the results show that it\nconsistently outperforms baseline models across all tasks, demonstrating the\neffectiveness and robustness of the proposed approach.\n","authors":["Yun Jiang","Zilong Xie","Wei Zhang","Yun Fang","Shuai Pan"],"pdf_url":"https://arxiv.org/pdf/2411.00437v2.pdf","comment":"13 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2505.05001v1","updated":"2025-05-08T07:12:23Z","published":"2025-05-08T07:12:23Z","title":"StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal\n  Bidirectional Warps","summary":"  We retarget video stitching to an emerging issue, named warping shake, which\nunveils the temporal content shakes induced by sequentially unsmooth warps when\nextending image stitching to video stitching. Even if the input videos are\nstable, the stitched video can inevitably cause undesired warping shakes and\naffect the visual experience. To address this issue, we propose StabStitch++, a\nnovel video stitching framework to realize spatial stitching and temporal\nstabilization with unsupervised learning simultaneously. First, different from\nexisting learning-based image stitching solutions that typically warp one image\nto align with another, we suppose a virtual midplane between original image\nplanes and project them onto it. Concretely, we design a differentiable\nbidirectional decomposition module to disentangle the homography transformation\nand incorporate it into our spatial warp, evenly spreading alignment burdens\nand projective distortions across two views. Then, inspired by camera paths in\nvideo stabilization, we derive the mathematical expression of stitching\ntrajectories in video stitching by elaborately integrating spatial and temporal\nwarps. Finally, a warp smoothing model is presented to produce stable stitched\nvideos with a hybrid loss to simultaneously encourage content alignment,\ntrajectory smoothness, and online collaboration. Compared with StabStitch that\nsacrifices alignment for stabilization, StabStitch++ makes no compromise and\noptimizes both of them simultaneously, especially in the online mode. To\nestablish an evaluation benchmark and train the learning framework, we build a\nvideo stitching dataset with a rich diversity in camera motions and scenes.\nExperiments exhibit that StabStitch++ surpasses current solutions in stitching\nperformance, robustness, and efficiency, offering compelling advancements in\nthis field by building a real-time online video stitching system.\n","authors":["Lang Nie","Chunyu Lin","Kang Liao","Yun Zhang","Shuaicheng Liu","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.05001v1.pdf","comment":"TPAMI2025; https://github.com/nie-lang/StabStitch2. arXiv admin note:\n  text overlap with arXiv:2403.06378"},{"id":"http://arxiv.org/abs/2505.04997v1","updated":"2025-05-08T07:05:51Z","published":"2025-05-08T07:05:51Z","title":"Foam-Agent: Towards Automated Intelligent CFD Workflows","summary":"  Computational Fluid Dynamics (CFD) is an essential simulation tool in various\nengineering disciplines, but it often requires substantial domain expertise and\nmanual configuration, creating barriers to entry. We present Foam-Agent, a\nmulti-agent framework that automates complex OpenFOAM-based CFD simulation\nworkflows from natural language inputs. Our innovation includes (1) a\nhierarchical multi-index retrieval system with specialized indices for\ndifferent simulation aspects, (2) a dependency-aware file generation system\nthat provides consistency management across configuration files, and (3) an\niterative error correction mechanism that diagnoses and resolves simulation\nfailures without human intervention. Through comprehensive evaluation on the\ndataset of 110 simulation tasks, Foam-Agent achieves an 83.6% success rate with\nClaude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for\nMetaOpenFOAM and 37.3% for OpenFOAM-GPT). Ablation studies demonstrate the\ncritical contribution of each system component, with the specialized error\ncorrection mechanism providing a 36.4% performance improvement. Foam-Agent\nsubstantially lowers the CFD expertise threshold while maintaining modeling\naccuracy, demonstrating the potential of specialized multi-agent systems to\ndemocratize access to complex scientific simulation tools. The code is public\nat https://github.com/csml-rpi/Foam-Agent\n","authors":["Ling Yue","Nithin Somasekharan","Yadi Cao","Shaowu Pan"],"pdf_url":"https://arxiv.org/pdf/2505.04997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04994v1","updated":"2025-05-08T06:59:14Z","published":"2025-05-08T06:59:14Z","title":"Rethinking Invariance in In-context Learning","summary":"  In-Context Learning (ICL) has emerged as a pivotal capability of\nauto-regressive large language models, yet it is hindered by a notable\nsensitivity to the ordering of context examples regardless of their mutual\nindependence. To address this issue, recent studies have introduced several\nvariant algorithms of ICL that achieve permutation invariance. However, many of\nthese do not exhibit comparable performance with the standard auto-regressive\nICL algorithm. In this work, we identify two crucial elements in the design of\nan invariant ICL algorithm: information non-leakage and context\ninterdependence, which are not simultaneously achieved by any of the existing\nmethods. These investigations lead us to the proposed Invariant ICL (InvICL), a\nmethodology designed to achieve invariance in ICL while ensuring the two\nproperties. Empirically, our findings reveal that InvICL surpasses previous\nmodels, both invariant and non-invariant, in most benchmark datasets,\nshowcasing superior generalization capabilities across varying input lengths.\nCode is available at https://github.com/PKU-ML/InvICL.\n","authors":["Lizhe Fang","Yifei Wang","Khashayar Gatmiry","Lei Fang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2505.04994v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06600v4","updated":"2025-05-08T06:57:11Z","published":"2024-06-06T13:44:57Z","title":"HORAE: A Domain-Agnostic Language for Automated Service Regulation","summary":"  Artificial intelligence is rapidly encroaching on the field of service\nregulation. However, existing AI-based regulation techniques are often tailored\nto specific application domains and thus are difficult to generalize in an\nautomated manner. This paper presents Horae, a unified specification language\nfor modeling (multimodal) regulation rules across a diverse set of domains. We\nshowcase how Horae facilitates an intelligent service regulation pipeline by\nfurther exploiting a fine-tuned large language model named RuleGPT that\nautomates the Horae modeling process, thereby yielding an end-to-end framework\nfor fully automated intelligent service regulation. The feasibility and\neffectiveness of our framework are demonstrated over a benchmark of various\nreal-world regulation domains. In particular, we show that our open-sourced,\nfine-tuned RuleGPT with 7B parameters suffices to outperform GPT-3.5 and\nperform on par with GPT-4o.\n","authors":["Yutao Sun","Mingshuai Chen","Tiancheng Zhao","Kangjia Zhao","He Li","Jintao Chen","Zhongyi Wang","Liqiang Lu","Xinkui Zhao","Shuiguang Deng","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2406.06600v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04983v1","updated":"2025-05-08T06:40:17Z","published":"2025-05-08T06:40:17Z","title":"Decomposition of Probabilities of Causation with Two Mediators","summary":"  Mediation analysis for probabilities of causation (PoC) provides a\nfundamental framework for evaluating the necessity and sufficiency of treatment\nin provoking an event through different causal pathways. One of the primary\nobjectives of causal mediation analysis is to decompose the total effect into\npath-specific components. In this study, we investigate the path-specific\nprobability of necessity and sufficiency (PNS) to decompose the total PNS into\npath-specific components along distinct causal pathways between treatment and\noutcome, incorporating two mediators. We define the path-specific PNS for\ndecomposition and provide an identification theorem. Furthermore, we conduct\nnumerical experiments to assess the properties of the proposed estimators from\nfinite samples and demonstrate their practical application using a real-world\neducational dataset.\n","authors":["Yuta Kawakami","Jin Tian"],"pdf_url":"https://arxiv.org/pdf/2505.04983v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.14491"},{"id":"http://arxiv.org/abs/2504.08837v3","updated":"2025-05-08T06:35:06Z","published":"2025-04-10T17:41:56Z","title":"VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning","summary":"  Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks like MathVista, MathVerse, and MathVision is similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt the\nGRPO algorithm with a novel technique called Selective Sample Replay (SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a rethinking trigger token to the end of rollouts in\nRL training, explicitly enforcing a self-reflection reasoning step. By\ncombining these two techniques, our model, VL-Rethinker, advances\nstate-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5%\nrespectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary\nbenchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the\ngap with OpenAI-o1. Our empirical results show the effectiveness of our\napproaches.\n","authors":["Haozhe Wang","Chao Qu","Zuming Huang","Wei Chu","Fangzhen Lin","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2504.08837v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2505.04977v1","updated":"2025-05-08T06:30:46Z","published":"2025-05-08T06:30:46Z","title":"ChainMarks: Securing DNN Watermark with Cryptographic Chain","summary":"  With the widespread deployment of deep neural network (DNN) models, dynamic\nwatermarking techniques are being used to protect the intellectual property of\nmodel owners. However, recent studies have shown that existing watermarking\nschemes are vulnerable to watermark removal and ambiguity attacks. Besides, the\nvague criteria for determining watermark presence further increase the\nlikelihood of such attacks. In this paper, we propose a secure DNN watermarking\nscheme named ChainMarks, which generates secure and robust watermarks by\nintroducing a cryptographic chain into the trigger inputs and utilizes a\ntwo-phase Monte Carlo method for determining watermark presence. First,\nChainMarks generates trigger inputs as a watermark dataset by repeatedly\napplying a hash function over a secret key, where the target labels associated\nwith trigger inputs are generated from the digital signature of model owner.\nThen, the watermarked model is produced by training a DNN over both the\noriginal and watermark datasets. To verify watermarks, we compare the predicted\nlabels of trigger inputs with the target labels and determine ownership with a\nmore accurate decision threshold that considers the classification probability\nof specific models. Experimental results show that ChainMarks exhibits higher\nlevels of robustness and security compared to state-of-the-art watermarking\nschemes. With a better marginal utility, ChainMarks provides a higher\nprobability guarantee of watermark presence in DNN models with the same level\nof watermark accuracy.\n","authors":["Brian Choi","Shu Wang","Isabelle Choi","Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2505.04977v1.pdf","comment":"Accepted In ACM ASIA Conference on Computer and Communications\n  Security (ASIA CCS '25), August 25-29, 2025, Ha Noi, Vietnam"},{"id":"http://arxiv.org/abs/2505.04972v1","updated":"2025-05-08T06:16:36Z","published":"2025-05-08T06:16:36Z","title":"AI and Vision based Autonomous Navigation of Nano-Drones in\n  Partially-Known Environments","summary":"  The miniaturisation of sensors and processors, the advancements in connected\nedge intelligence, and the exponential interest in Artificial Intelligence are\nboosting the affirmation of autonomous nano-size drones in the Internet of\nRobotic Things ecosystem. However, achieving safe autonomous navigation and\nhigh-level tasks such as exploration and surveillance with these tiny platforms\nis extremely challenging due to their limited resources. This work focuses on\nenabling the safe and autonomous flight of a pocket-size, 30-gram platform\ncalled Crazyflie 2.1 in a partially known environment. We propose a novel\nAI-aided, vision-based reactive planning method for obstacle avoidance under\nthe ambit of Integrated Sensing, Computing and Communication paradigm. We deal\nwith the constraints of the nano-drone by splitting the navigation task into\ntwo parts: a deep learning-based object detector runs on the edge (external\nhardware) while the planning algorithm is executed onboard. The results show\nthe ability to command the drone at $\\sim8$ frames-per-second and a model\nperformance reaching a COCO mean-average-precision of $60.8$. Field experiments\ndemonstrate the feasibility of the solution with the drone flying at a top\nspeed of $1$ m/s while steering away from an obstacle placed in an unknown\nposition and reaching the target destination. The outcome highlights the\ncompatibility of the communication delay and the model performance with the\nrequirements of the real-time navigation task. We provide a feasible\nalternative to a fully onboard implementation that can be extended to\nautonomous exploration with nano-drones.\n","authors":["Mattia Sartori","Chetna Singhal","Neelabhro Roy","Davide Brunelli","James Gross"],"pdf_url":"https://arxiv.org/pdf/2505.04972v1.pdf","comment":"in DCOSS-IoT 2025, Wi-DroIT 2025"},{"id":"http://arxiv.org/abs/2505.04971v1","updated":"2025-05-08T06:09:05Z","published":"2025-05-08T06:09:05Z","title":"Moments of Causal Effects","summary":"  The moments of random variables are fundamental statistical measures for\ncharacterizing the shape of a probability distribution, encompassing metrics\nsuch as mean, variance, skewness, and kurtosis. Additionally, the product\nmoments, including covariance and correlation, reveal the relationships between\nmultiple random variables. On the other hand, the primary focus of causal\ninference is the evaluation of causal effects, which are defined as the\ndifference between two potential outcomes. While traditional causal effect\nassessment focuses on the average causal effect, this work provides\ndefinitions, identification theorems, and bounds for moments and product\nmoments of causal effects to analyze their distribution and relationships. We\nconduct experiments to illustrate the estimation of the moments of causal\neffects from finite samples and demonstrate their practical application using a\nreal-world medical dataset.\n","authors":["Yuta Kawakami","Jin Tian"],"pdf_url":"https://arxiv.org/pdf/2505.04971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03750v2","updated":"2025-05-08T06:08:59Z","published":"2025-04-17T05:56:14Z","title":"AI-Powered Agile Analog Circuit Design and Optimization","summary":"  Artificial intelligence (AI) techniques are transforming analog circuit\ndesign by automating device-level tuning and enabling system-level\nco-optimization. This paper integrates two approaches: (1) AI-assisted\ntransistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct\ncircuit parameter optimization, demonstrated on a linearly tunable\ntransconductor; and (2) AI-integrated circuit transfer function modeling for\nsystem-level optimization in a keyword spotting (KWS) application, demonstrated\nby optimizing an analog bandpass filter within a machine learning training\nloop. The combined insights highlight how AI can improve analog performance,\nreduce design iteration effort, and jointly optimize analog components and\napplication-level metrics.\n","authors":["Jinhai Hu","Wang Ling Goh","Yuan Gao"],"pdf_url":"https://arxiv.org/pdf/2505.03750v2.pdf","comment":"3 pages, 5 figures, AI4X, 2025"},{"id":"http://arxiv.org/abs/2505.02309v2","updated":"2025-05-08T05:55:48Z","published":"2025-05-05T01:27:47Z","title":"Optimizing LLMs for Resource-Constrained Environments: A Survey of Model\n  Compression Techniques","summary":"  Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment.\n","authors":["Sanjay Surendranath Girija","Shashank Kapoor","Lakshit Arora","Dipen Pradhan","Aman Raj","Ankit Shetgaonkar"],"pdf_url":"https://arxiv.org/pdf/2505.02309v2.pdf","comment":"Accepted to IEEE COMPSAC 2025"},{"id":"http://arxiv.org/abs/2505.04966v1","updated":"2025-05-08T05:51:48Z","published":"2025-05-08T05:51:48Z","title":"Position: The AI Conference Peer Review Crisis Demands Author Feedback\n  and Reviewer Rewards","summary":"  The peer review process in major artificial intelligence (AI) conferences\nfaces unprecedented challenges with the surge of paper submissions (exceeding\n10,000 submissions per venue), accompanied by growing concerns over review\nquality and reviewer responsibility. This position paper argues for the need to\ntransform the traditional one-way review system into a bi-directional feedback\nloop where authors evaluate review quality and reviewers earn formal\naccreditation, creating an accountability framework that promotes a\nsustainable, high-quality peer review system. The current review system can be\nviewed as an interaction between three parties: the authors, reviewers, and\nsystem (i.e., conference), where we posit that all three parties share\nresponsibility for the current problems. However, issues with authors can only\nbe addressed through policy enforcement and detection tools, and ethical\nconcerns can only be corrected through self-reflection. As such, this paper\nfocuses on reforming reviewer accountability with systematic rewards through\ntwo key mechanisms: (1) a two-stage bi-directional review system that allows\nauthors to evaluate reviews while minimizing retaliatory behavior, (2)a\nsystematic reviewer reward system that incentivizes quality reviewing. We ask\nfor the community's strong interest in these problems and the reforms that are\nneeded to enhance the peer review process.\n","authors":["Jaeho Kim","Yunseok Lee","Seulki Lee"],"pdf_url":"https://arxiv.org/pdf/2505.04966v1.pdf","comment":"ICML2025 Position Track Oral"},{"id":"http://arxiv.org/abs/2504.21218v2","updated":"2025-05-08T05:47:40Z","published":"2025-04-29T23:10:07Z","title":"Theoretical Foundations for Semantic Cognition in Artificial\n  Intelligence","summary":"  This monograph presents a modular cognitive architecture for artificial\nintelligence grounded in the formal modeling of belief as structured semantic\nstate. Belief states are defined as dynamic ensembles of linguistic expressions\nembedded within a navigable manifold, where operators enable assimilation,\nabstraction, nullification, memory, and introspection. Drawing from philosophy,\ncognitive science, and neuroscience, we develop a layered framework that\nenables self-regulating epistemic agents capable of reflective, goal-directed\nthought. At the core of this framework is the epistemic vacuum: a class of\nsemantically inert cognitive states that serves as the conceptual origin of\nbelief space. From this foundation, the Null Tower arises as a generative\nstructure recursively built through internal representational capacities. The\ntheoretical constructs are designed to be implementable in both symbolic and\nneural systems, including large language models, hybrid agents, and adaptive\nmemory architectures. This work offers a foundational substrate for\nconstructing agents that reason, remember, and regulate their beliefs in\nstructured, interpretable ways.\n","authors":["Sebastian Dumbrava"],"pdf_url":"https://arxiv.org/pdf/2504.21218v2.pdf","comment":"248 pages, 77 figures"},{"id":"http://arxiv.org/abs/2505.04961v1","updated":"2025-05-08T05:42:33Z","published":"2025-05-08T05:42:33Z","title":"ADD: Physics-Based Motion Imitation with Adversarial Differential\n  Discriminators","summary":"  Multi-objective optimization problems, which require the simultaneous\noptimization of multiple terms, are prevalent across numerous applications.\nExisting multi-objective optimization methods often rely on manually tuned\naggregation functions to formulate a joint optimization target. The performance\nof such hand-tuned methods is heavily dependent on careful weight selection, a\ntime-consuming and laborious process. These limitations also arise in the\nsetting of reinforcement-learning-based motion tracking for physically\nsimulated characters, where intricately crafted reward functions are typically\nused to achieve high-fidelity results. Such solutions not only require domain\nexpertise and significant manual adjustment, but also limit the applicability\nof the resulting reward function across diverse skills. To bridge this gap, we\npresent a novel adversarial multi-objective optimization technique that is\nbroadly applicable to a range of multi-objective optimization problems,\nincluding motion tracking. The proposed adversarial differential discriminator\nreceives a single positive sample, yet is still effective at guiding the\noptimization process. We demonstrate that our technique can enable characters\nto closely replicate a variety of acrobatic and agile behaviors, achieving\ncomparable quality to state-of-the-art motion-tracking methods, without relying\non manually tuned reward functions. Results are best visualized through\nhttps://youtu.be/rz8BYCE9E2w.\n","authors":["Ziyu Zhang","Sergey Bashkirov","Dun Yang","Michael Taylor","Xue Bin Peng"],"pdf_url":"https://arxiv.org/pdf/2505.04961v1.pdf","comment":"19 pages, 15 figures"},{"id":"http://arxiv.org/abs/2505.04956v1","updated":"2025-05-08T05:38:19Z","published":"2025-05-08T05:38:19Z","title":"Graffe: Graph Representation Learning via Diffusion Probabilistic Models","summary":"  Diffusion probabilistic models (DPMs), widely recognized for their potential\nto generate high-quality samples, tend to go unnoticed in representation\nlearning. While recent progress has highlighted their potential for capturing\nvisual semantics, adapting DPMs to graph representation learning remains in its\ninfancy. In this paper, we introduce Graffe, a self-supervised diffusion model\nproposed for graph representation learning. It features a graph encoder that\ndistills a source graph into a compact representation, which, in turn, serves\nas the condition to guide the denoising process of the diffusion decoder. To\nevaluate the effectiveness of our model, we first explore the theoretical\nfoundations of applying diffusion models to representation learning, proving\nthat the denoising objective implicitly maximizes the conditional mutual\ninformation between data and its representation. Specifically, we prove that\nthe negative logarithm of the denoising score matching loss is a tractable\nlower bound for the conditional mutual information. Empirically, we conduct a\nseries of case studies to validate our theoretical insights. In addition,\nGraffe delivers competitive results under the linear probing setting on node\nand graph classification tasks, achieving state-of-the-art performance on 9 of\nthe 11 real-world datasets. These findings indicate that powerful generative\nmodels, especially diffusion models, serve as an effective tool for graph\nrepresentation learning.\n","authors":["Dingshuo Chen","Shuchen Xue","Liuji Chen","Yingheng Wang","Qiang Liu","Shu Wu","Zhi-Ming Ma","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2505.04956v1.pdf","comment":"16 pages, 4 figures, under review"},{"id":"http://arxiv.org/abs/2503.17656v2","updated":"2025-05-08T05:33:56Z","published":"2025-03-22T05:32:03Z","title":"NaFM: Pre-training a Foundation Model for Small-Molecule Natural\n  Products","summary":"  Natural products, as metabolites from microorganisms, animals, or plants,\nexhibit diverse biological activities, making them crucial for drug discovery.\nNowadays, existing deep learning methods for natural products research\nprimarily rely on supervised learning approaches designed for specific\ndownstream tasks. However, such one-model-for-a-task paradigm often lacks\ngeneralizability and leaves significant room for performance improvement.\nAdditionally, existing molecular characterization methods are not well-suited\nfor the unique tasks associated with natural products. To address these\nlimitations, we have pre-trained a foundation model for natural products based\non their unique properties. Our approach employs a novel pretraining strategy\nthat is especially tailored to natural products. By incorporating contrastive\nlearning and masked graph learning objectives, we emphasize evolutional\ninformation from molecular scaffolds while capturing side-chain information.\nOur framework achieves state-of-the-art (SOTA) results in various downstream\ntasks related to natural product mining and drug discovery. We first compare\ntaxonomy classification with synthesized molecule-focused baselines to\ndemonstrate that current models are inadequate for understanding natural\nsynthesis. Furthermore, by diving into a fine-grained analysis at both the gene\nand microbial levels, NaFM demonstrates the ability to capture evolutionary\ninformation. Eventually, our method is experimented with virtual screening,\nillustrating informative natural product representations that can lead to more\neffective identification of potential drug candidates.\n","authors":["Yuheng Ding","Yusong Wang","Bo Qiang","Jie Yu","Qi Li","Yiran Zhou","Zhenmin Liu"],"pdf_url":"https://arxiv.org/pdf/2503.17656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04955v1","updated":"2025-05-08T05:32:36Z","published":"2025-05-08T05:32:36Z","title":"Chain-of-Thought Tokens are Computer Program Variables","summary":"  Chain-of-thoughts (CoT) requires large language models (LLMs) to generate\nintermediate steps before reaching the final answer, and has been proven\neffective to help LLMs solve complex reasoning tasks. However, the inner\nmechanism of CoT still remains largely unclear. In this paper, we empirically\nstudy the role of CoT tokens in LLMs on two compositional tasks: multi-digit\nmultiplication and dynamic programming. While CoT is essential for solving\nthese problems, we find that preserving only tokens that store intermediate\nresults would achieve comparable performance. Furthermore, we observe that\nstoring intermediate results in an alternative latent form will not affect\nmodel performance. We also randomly intervene some values in CoT, and notice\nthat subsequent CoT tokens and the final answer would change correspondingly.\nThese findings suggest that CoT tokens may function like variables in computer\nprograms but with potential drawbacks like unintended shortcuts and\ncomputational complexity limits between tokens. The code and data are available\nat https://github.com/solitaryzero/CoTs_are_Variables.\n","authors":["Fangwei Zhu","Peiyi Wang","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2505.04955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.02665v2","updated":"2025-05-08T05:27:18Z","published":"2025-05-05T14:14:59Z","title":"A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning\n  and Inference-time Scaling Law","summary":"  This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems.\n","authors":["Qianjun Pan","Wenkai Ji","Yuyang Ding","Junsong Li","Shilian Chen","Junyi Wang","Jie Zhou","Qin Chen","Min Zhang","Yulan Wu","Liang He"],"pdf_url":"https://arxiv.org/pdf/2505.02665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12261v4","updated":"2025-05-08T05:13:20Z","published":"2024-10-16T05:58:55Z","title":"CATCH: Channel-Aware multivariate Time Series Anomaly Detection via\n  Frequency Patching","summary":"  Anomaly detection in multivariate time series is challenging as heterogeneous\nsubsequence anomalies may occur. Reconstruction-based methods, which focus on\nlearning normal patterns in the frequency domain to detect diverse abnormal\nsubsequences, achieve promising results, while still falling short on capturing\nfine-grained frequency characteristics and channel correlations. To contend\nwith the limitations, we introduce CATCH, a framework based on frequency\npatching. We propose to patchify the frequency domain into frequency bands,\nwhich enhances its ability to capture fine-grained frequency characteristics.\nTo perceive appropriate channel correlations, we propose a Channel Fusion\nModule (CFM), which features a patch-wise mask generator and a masked-attention\nmechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM\nis encouraged to iteratively discover appropriate patch-wise channel\ncorrelations, and to cluster relevant channels while isolating adverse effects\nfrom irrelevant channels. Extensive experiments on 10 real-world datasets and\n12 synthetic datasets demonstrate that CATCH achieves state-of-the-art\nperformance. We make our code and datasets available at\nhttps://github.com/decisionintelligence/CATCH.\n","authors":["Xingjian Wu","Xiangfei Qiu","Zhengyu Li","Yihang Wang","Jilin Hu","Chenjuan Guo","Hui Xiong","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12261v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2504.13199v3","updated":"2025-05-08T05:10:46Z","published":"2025-04-14T21:10:25Z","title":"Building Trustworthy Multimodal AI: A Review of Fairness, Transparency,\n  and Ethics in Vision-Language Tasks","summary":"  Objective: This review explores the trustworthiness of multimodal artificial\nintelligence (AI) systems, specifically focusing on vision-language tasks. It\naddresses critical challenges related to fairness, transparency, and ethical\nimplications in these systems, providing a comparative analysis of key tasks\nsuch as Visual Question Answering (VQA), image captioning, and visual dialogue.\nBackground: Multimodal models, particularly vision-language models, enhance\nartificial intelligence (AI) capabilities by integrating visual and textual\ndata, mimicking human learning processes. Despite significant advancements, the\ntrustworthiness of these models remains a crucial concern, particularly as AI\nsystems increasingly confront issues regarding fairness, transparency, and\nethics. Methods: This review examines research conducted from 2017 to 2024\nfocusing on forenamed core vision-language tasks. It employs a comparative\napproach to analyze these tasks through the lens of trustworthiness,\nunderlining fairness, explainability, and ethics. This study synthesizes\nfindings from recent literature to identify trends, challenges, and\nstate-of-the-art solutions. Results: Several key findings were highlighted.\nTransparency: Explainability of vision language tasks is important for user\ntrust. Techniques, such as attention maps and gradient-based methods, have\nsuccessfully addressed this issue. Fairness: Bias mitigation in VQA and visual\ndialogue systems is essential for ensuring unbiased outcomes across diverse\ndemographic groups. Ethical Implications: Addressing biases in multilingual\nmodels and ensuring ethical data handling is critical for the responsible\ndeployment of vision-language systems. Conclusion: This study underscores the\nimportance of integrating fairness, transparency, and ethical considerations in\ndeveloping vision-language models within a unified framework.\n","authors":["Mohammad Saleh","Azadeh Tabatabaei"],"pdf_url":"https://arxiv.org/pdf/2504.13199v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04950v1","updated":"2025-05-08T05:10:38Z","published":"2025-05-08T05:10:38Z","title":"Position: Epistemic Artificial Intelligence is Essential for Machine\n  Learning Models to Know When They Do Not Know","summary":"  Despite the impressive achievements of AI, including advancements in\ngenerative models and large language models, there remains a significant gap in\nthe ability of AI to handle uncertainty and generalize beyond the training\ndata. We argue that AI models, especially in autonomous systems, fail to make\nrobust predictions when faced with unfamiliar or adversarial data, as evidenced\nby incidents with autonomous vehicles. Traditional machine learning approaches\nstruggle to address these issues due to an overemphasis on data fitting and\ndomain adaptation. This position paper posits a paradigm shift towards\nepistemic artificial intelligence, emphasizing the need for models to learn not\nonly from what they know but also from their ignorance. This approach, which\nfocuses on recognizing and managing uncertainty, offers a potential solution to\nimprove the resilience and robustness of AI systems, ensuring that they can\nbetter handle unpredictable real-world environments.\n","authors":["Shireen Kudukkil Manchingal","Fabio Cuzzolin"],"pdf_url":"https://arxiv.org/pdf/2505.04950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03757v3","updated":"2025-05-08T05:10:27Z","published":"2024-09-05T17:59:56Z","title":"Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding","summary":"  Complex 3D scene understanding has gained increasing attention, with scene\nencoding strategies playing a crucial role in this success. However, the\noptimal scene encoding strategies for various scenarios remain unclear,\nparticularly compared to their image-based counterparts. To address this issue,\nwe present a comprehensive study that probes various visual encoding models for\n3D scene understanding, identifying the strengths and limitations of each model\nacross different scenarios. Our evaluation spans seven vision foundation\nencoders, including image-based, video-based, and 3D foundation models. We\nevaluate these models in four tasks: Vision-Language Scene Reasoning, Visual\nGrounding, Segmentation, and Registration, each focusing on different aspects\nof scene understanding. Our evaluations yield key findings: DINOv2 demonstrates\nsuperior performance, video models excel in object-level tasks, diffusion\nmodels benefit geometric tasks, and language-pretrained models show unexpected\nlimitations in language-related tasks. These insights challenge some\nconventional understandings, provide novel perspectives on leveraging visual\nfoundation models, and highlight the need for more flexible encoder selection\nin future vision-language and scene-understanding tasks. Code:\nhttps://github.com/YunzeMan/Lexicon3D\n","authors":["Yunze Man","Shuhong Zheng","Zhipeng Bao","Martial Hebert","Liang-Yan Gui","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2409.03757v3.pdf","comment":"NeurIPS 2024. Project page: https://yunzeman.github.io/lexicon3d\n  Github: https://github.com/YunzeMan/Lexicon3D"},{"id":"http://arxiv.org/abs/2503.22688v2","updated":"2025-05-08T04:56:05Z","published":"2025-03-05T09:47:02Z","title":"CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large\n  Language Models in Interactive Code Generation","summary":"  Large Language Models (LLMs) have demonstrated exceptional performance in\ncode generation tasks and have become indispensable programming assistants for\ndevelopers. However, existing code generation benchmarks primarily assess the\nfunctional correctness of code generated by LLMs in single-turn interactions,\noffering limited insight into their capabilities to generate code that strictly\nfollows users' instructions, especially in multi-turn interaction scenarios. In\nthis paper, we introduce CodeIF-Bench, a benchmark for evaluating LLMs'\ninstruction-following capabilities in interactive code generation.\nSpecifically, CodeIF-Bench incorporates nine types of verifiable instructions\naligned with the real-world software development requirements, which can be\nindependently and objectively validated through specified test cases,\nfacilitating the evaluation of instruction-following capability in multi-turn\ninteractions. We evaluate nine prominent LLMs using CodeIF-Bench, and the\nexperimental results reveal a significant disparity between their basic\nprogramming capability and instruction-following capability, particularly as\ntask complexity, context length, and the number of dialogue rounds increase.\n","authors":["Peiding Wang","Li Zhang","Fang Liu","Lin Shi","Minxiao Li","Bo Shen","An Fu"],"pdf_url":"https://arxiv.org/pdf/2503.22688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04946v1","updated":"2025-05-08T04:49:52Z","published":"2025-05-08T04:49:52Z","title":"T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video\n  Generation Models","summary":"  Thanks to recent advancements in scalable deep architectures and large-scale\npretraining, text-to-video generation has achieved unprecedented capabilities\nin producing high-fidelity, instruction-following content across a wide range\nof styles, enabling applications in advertising, entertainment, and education.\nHowever, these models' ability to render precise on-screen text, such as\ncaptions or mathematical formulas, remains largely untested, posing significant\nchallenges for applications requiring exact textual accuracy. In this work, we\nintroduce T2VTextBench, the first human-evaluation benchmark dedicated to\nevaluating on-screen text fidelity and temporal consistency in text-to-video\nmodels. Our suite of prompts integrates complex text strings with dynamic scene\nchanges, testing each model's ability to maintain detailed instructions across\nframes. We evaluate ten state-of-the-art systems, ranging from open-source\nsolutions to commercial offerings, and find that most struggle to generate\nlegible, consistent text. These results highlight a critical gap in current\nvideo generators and provide a clear direction for future research aimed at\nenhancing textual manipulation in video synthesis.\n","authors":["Xuyang Guo","Jiayan Huo","Zhenmei Shi","Zhao Song","Jiahao Zhang","Jiale Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.04946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08884v2","updated":"2025-05-08T04:42:58Z","published":"2024-10-26T15:55:21Z","title":"Quantifying Risk Propensities of Large Language Models: Ethical Focus\n  and Bias Detection through Role-Play","summary":"  As Large Language Models (LLMs) become more prevalent, concerns about their\nsafety, ethics, and potential biases have risen. Systematically evaluating\nLLMs' risk decision-making tendencies and attitudes, particularly in the\nethical domain, has become crucial. This study innovatively applies the\nDomain-Specific Risk-Taking (DOSPERT) scale from cognitive science to LLMs and\nproposes a novel Ethical Decision-Making Risk Attitude Scale (EDRAS) to assess\nLLMs' ethical risk attitudes in depth. We further propose a novel approach\nintegrating risk scales and role-playing to quantitatively evaluate systematic\nbiases in LLMs. Through systematic evaluation and analysis of multiple\nmainstream LLMs, we assessed the \"risk personalities\" of LLMs across multiple\ndomains, with a particular focus on the ethical domain, and revealed and\nquantified LLMs' systematic biases towards different groups. This research\nhelps understand LLMs' risk decision-making and ensure their safe and reliable\napplication. Our approach provides a tool for identifying and mitigating\nbiases, contributing to fairer and more trustworthy AI systems. The code and\ndata are available.\n","authors":["Yifan Zeng","Liang Kairong","Fangzhou Dong","Peijia Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.08884v2.pdf","comment":"Accepted by CogSci 2025"},{"id":"http://arxiv.org/abs/2502.08821v2","updated":"2025-05-08T04:42:55Z","published":"2025-02-12T22:24:49Z","title":"DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with\n  Saliency Maps","summary":"  The recent surge in advanced generative models, such as diffusion models and\ngenerative adversarial networks (GANs), has led to an alarming rise in\nAI-generated images across various domains on the web. While such technologies\noffer benefits such as democratizing artistic creation, they also pose\nchallenges in misinformation, digital forgery, and authenticity verification.\nAdditionally, the uncredited use of AI-generated images in media and marketing\nhas sparked significant backlash from online communities. In response to this,\nwe introduce DejAIvu, a Chrome Web extension that combines real-time\nAI-generated image detection with saliency-based explainability while users\nbrowse the web. Using an ONNX-optimized deep learning model, DejAIvu\nautomatically analyzes images on websites such as Google Images, identifies\nAI-generated content using model inference, and overlays a saliency heatmap to\nhighlight AI-related artifacts. Our approach integrates efficient in-browser\ninference, gradient-based saliency analysis, and a seamless user experience,\nensuring that AI detection is both transparent and interpretable. We also\nevaluate DejAIvu across multiple pretrained architectures and benchmark\ndatasets, demonstrating high accuracy and low latency, making it a practical\nand deployable tool for enhancing AI image accountability. The code for this\nsystem can be found at https://github.com/Noodulz/dejAIvu.\n","authors":["Jocelyn Dzuong"],"pdf_url":"https://arxiv.org/pdf/2502.08821v2.pdf","comment":"5 pages, 3 figures. Accepted to IJCAI 2025 Demo Track. Revised\n  version will be uploaded soon"},{"id":"http://arxiv.org/abs/2505.04939v1","updated":"2025-05-08T04:27:15Z","published":"2025-05-08T04:27:15Z","title":"Structural Alignment in Link Prediction","summary":"  While Knowledge Graphs (KGs) have become increasingly popular across various\nscientific disciplines for their ability to model and interlink huge quantities\nof data, essentially all real-world KGs are known to be incomplete. As such,\nwith the growth of KG use has been a concurrent development of machine learning\ntools designed to predict missing information in KGs, which is referred to as\nthe Link Prediction Task. The majority of state-of-the-art link predictors to\ndate have followed an embedding-based paradigm. In this paradigm, it is assumed\nthat the information content of a KG is best represented by the (individual)\nvector representations of its nodes and edges, and that therefore node and edge\nembeddings are particularly well-suited to performing link prediction.\n  This thesis proposes an alternative perspective on the field's approach to\nlink prediction and KG data modelling. Specifically, this work re-analyses KGs\nand state-of-the-art link predictors from a graph-structure-first perspective\nthat models the information content of a KG in terms of whole triples, rather\nthan individual nodes and edges.\n  Following a literature review and two core sets of experiments, this thesis\nconcludes that a structure-first perspective on KGs and link prediction is both\nviable and useful for understanding KG learning and for enabling cross-KG\ntransfer learning for the link prediction task. This observation is used to\ncreate and propose the Structural Alignment Hypothesis, which postulates that\nlink prediction can be understood and modelled as a structural task.\n  All code and data used for this thesis are open-sourced. This thesis was\nwritten bilingually, with the main document in English and an informal extended\nsummary in Irish. An Irish-language translation dictionary of machine learning\nterms (the Focl\\'oir Tr\\'achtais) created for this work is open-sourced as\nwell.\n","authors":["Jeffrey Seathrún Sardina"],"pdf_url":"https://arxiv.org/pdf/2505.04939v1.pdf","comment":"Ph.D. thesis submitted to Trinity College Dublin"},{"id":"http://arxiv.org/abs/2505.04931v1","updated":"2025-05-08T04:09:36Z","published":"2025-05-08T04:09:36Z","title":"Fair Uncertainty Quantification for Depression Prediction","summary":"  Trustworthy depression prediction based on deep learning, incorporating both\npredictive reliability and algorithmic fairness across diverse demographic\ngroups, is crucial for clinical application. Recently, achieving reliable\ndepression predictions through uncertainty quantification has attracted\nincreasing attention. However, few studies have focused on the fairness of\nuncertainty quantification (UQ) in depression prediction. In this work, we\ninvestigate the algorithmic fairness of UQ, namely Equal Opportunity Coverage\n(EOC) fairness, and propose Fair Uncertainty Quantification (FUQ) for\ndepression prediction. FUQ pursues reliable and fair depression predictions\nthrough group-based analysis. Specifically, we first group all the participants\nby different sensitive attributes and leverage conformal prediction to quantify\nuncertainty within each demographic group, which provides a theoretically\nguaranteed and valid way to quantify uncertainty for depression prediction and\nfacilitates the investigation of fairness across different demographic groups.\nFurthermore, we propose a fairness-aware optimization strategy that formulates\nfairness as a constrained optimization problem under EOC constraints. This\nenables the model to preserve predictive reliability while adapting to the\nheterogeneous uncertainty levels across demographic groups, thereby achieving\noptimal fairness. Through extensive evaluations on several visual and audio\ndepression datasets, our approach demonstrates its effectiveness.\n","authors":["Yonghong Li","Xiuzhuang Zhou"],"pdf_url":"https://arxiv.org/pdf/2505.04931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04927v1","updated":"2025-05-08T03:52:43Z","published":"2025-05-08T03:52:43Z","title":"Belief Filtering for Epistemic Control in Linguistic State Space","summary":"  We examine belief filtering as a mechanism for the epistemic control of\nartificial agents, focusing on the regulation of internal cognitive states\nrepresented as linguistic expressions. This mechanism is developed within the\nSemantic Manifold framework, where belief states are dynamic, structured\nensembles of natural language fragments. Belief filters act as content-aware\noperations on these fragments across various cognitive transitions. This paper\nillustrates how the inherent interpretability and modularity of such a\nlinguistically-grounded cognitive architecture directly enable belief\nfiltering, offering a principled approach to agent regulation. The study\nhighlights the potential for enhancing AI safety and alignment through\nstructured interventions in an agent's internal semantic space and points to\nnew directions for architecturally embedded cognitive governance.\n","authors":["Sebastian Dumbrava"],"pdf_url":"https://arxiv.org/pdf/2505.04927v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2503.01894v2","updated":"2025-05-08T03:50:30Z","published":"2025-02-27T19:18:37Z","title":"LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces","summary":"  We introduce the Local Intersectional Visual Spaces (LIVS) dataset, a\nbenchmark for multi-criteria alignment, developed through a two-year\nparticipatory process with 30 community organizations to support the\npluralistic alignment of text-to-image (T2I) models in inclusive urban\nplanning. The dataset encodes 37,710 pairwise comparisons across 13,462 images,\nstructured along six criteria - Accessibility, Safety, Comfort, Invitingness,\nInclusivity, and Diversity - derived from 634 community-defined concepts. Using\nDirect Preference Optimization (DPO), we fine-tune Stable Diffusion XL to\nreflect multi-criteria spatial preferences and evaluate the LIVS dataset and\nthe fine-tuned model through four case studies: (1) DPO increases alignment\nwith annotated preferences, particularly when annotation volume is high; (2)\npreference patterns vary across participant identities, underscoring the need\nfor intersectional data; (3) human-authored prompts generate more distinctive\nvisual outputs than LLM-generated ones, influencing annotation decisiveness;\nand (4) intersectional groups assign systematically different ratings across\ncriteria, revealing the limitations of single-objective alignment. While DPO\nimproves alignment under specific conditions, the prevalence of neutral ratings\nindicates that community values are heterogeneous and often ambiguous. LIVS\nprovides a benchmark for developing T2I models that incorporate local,\nstakeholder-driven preferences, offering a foundation for context-aware\nalignment in spatial design.\n","authors":["Rashid Mushkani","Shravan Nayak","Hugo Berard","Allison Cohen","Shin Koseki","Hadrien Bertrand"],"pdf_url":"https://arxiv.org/pdf/2503.01894v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2502.14281v3","updated":"2025-05-08T03:34:20Z","published":"2025-02-20T05:41:52Z","title":"Correcting Noisy Multilabel Predictions: Modeling Label Noise through\n  Latent Space Shifts","summary":"  Noise in data appears to be inevitable in most real-world machine learning\napplications and would cause severe overfitting problems. Not only can data\nfeatures contain noise, but labels are also prone to be noisy due to human\ninput. In this paper, rather than noisy label learning in multiclass\nclassifications, we instead focus on the less explored area of noisy label\nlearning for multilabel classifications. Specifically, we investigate the\npost-correction of predictions generated from classifiers learned with noisy\nlabels. The reasons are two-fold. Firstly, this approach can directly work with\nthe trained models to save computational resources. Secondly, it could be\napplied on top of other noisy label correction techniques to achieve further\nimprovements. To handle this problem, we appeal to deep generative approaches\nthat are possible for uncertainty estimation. Our model posits that label noise\narises from a stochastic shift in the latent variable, providing a more robust\nand beneficial means for noisy learning. We develop both unsupervised and\nsemi-supervised learning methods for our model. The extensive empirical study\npresents solid evidence to that our approach is able to consistently improve\nthe independent models and performs better than a number of existing methods\nacross various noisy label settings. Moreover, a comprehensive empirical\nanalysis of the proposed method is carried out to validate its robustness,\nincluding sensitivity analysis and an ablation study, among other elements.\n","authors":["Weipeng Huang","Qin Li","Yang Xiao","Cheng Qiao","Tie Cai","Junwei Liang","Neil J. Hurley","Guangyuan Piao"],"pdf_url":"https://arxiv.org/pdf/2502.14281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12613v2","updated":"2025-05-08T03:26:00Z","published":"2025-03-16T18:55:54Z","title":"Negotiative Alignment: Embracing Disagreement to Achieve Fairer Outcomes\n  -- Insights from Urban Studies","summary":"  Cities are not monolithic; they are arenas of negotiation among groups that\nhold varying needs, values, and experiences. Conventional methods of urban\nassessment -- from standardized surveys to AI-driven evaluations -- frequently\nrely on a single consensus metric (e.g., an average measure of inclusivity or\nsafety). Although such aggregations simplify design decisions, they risk\nobscuring the distinct perspectives of marginalized populations. In this paper,\nwe present findings from a community-centered study in Montreal involving 35\nresidents with diverse demographic and social identities, particularly\nwheelchair users, seniors, and LGBTQIA2+ individuals. Using rating and ranking\ntasks on 20 urban sites, we observe that disagreements are systematic rather\nthan random, reflecting structural inequalities, differing cultural values, and\npersonal experiences of safety and accessibility.\n  Based on these empirical insights, we propose negotiative alignment, an AI\nframework that treats disagreement as an essential input to be preserved,\nanalyzed, and addressed. Negotiative alignment builds on pluralistic models by\ndynamically updating stakeholder preferences through multi-agent negotiation\nmechanisms, ensuring no single perspective is marginalized. We outline how this\nframework can be integrated into urban analytics -- and other decision-making\ncontexts -- to retain minority viewpoints, adapt to changing stakeholder\nconcerns, and enhance fairness and accountability. The study demonstrates that\npreserving and engaging with disagreement, rather than striving for an\nartificial consensus, can produce more equitable and responsive AI-driven\noutcomes in urban design.\n","authors":["Rashid Mushkani","Hugo Berard","Shin Koseki"],"pdf_url":"https://arxiv.org/pdf/2503.12613v2.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2505.04918v1","updated":"2025-05-08T03:25:55Z","published":"2025-05-08T03:25:55Z","title":"Physics-Assisted and Topology-Informed Deep Learning for Weather\n  Prediction","summary":"  Although deep learning models have demonstrated remarkable potential in\nweather prediction, most of them overlook either the \\textbf{physics} of the\nunderlying weather evolution or the \\textbf{topology} of the Earth's surface.\nIn light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted\nAnd Topology-informed deep learning model for weather prediction. PASSAT\nattributes the weather evolution to two key factors: (i) the advection process\nthat can be characterized by the advection equation and the Navier-Stokes\nequation; (ii) the Earth-atmosphere interaction that is difficult to both model\nand calculate. PASSAT also takes the topology of the Earth's surface into\nconsideration, other than simply treating it as a plane. With these\nconsiderations, PASSAT numerically solves the advection equation and the\nNavier-Stokes equation on the spherical manifold, utilizes a spherical graph\nneural network to capture the Earth-atmosphere interaction, and generates the\ninitial velocity fields that are critical to solving the advection equation\nfrom the same spherical graph neural network. In the $5.625^\\circ$-resolution\nERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based\nweather prediction models and the operational numerical weather prediction\nmodel IFS T42. Code and checkpoint are available at\nhttps://github.com/Yumenomae/PASSAT_5p625.\n","authors":["Jiaqi Zheng","Qing Ling","Yerong Feng"],"pdf_url":"https://arxiv.org/pdf/2505.04918v1.pdf","comment":"International Joint Conferences on Artificial Intelligence (IJCAI\n  2025)"},{"id":"http://arxiv.org/abs/2505.04916v1","updated":"2025-05-08T03:14:14Z","published":"2025-05-08T03:14:14Z","title":"An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in\n  Higher Education","summary":"  Recent advances in AI have catalyzed the adoption of intelligent educational\ntools, yet many semantic retrieval systems remain ill-suited to the unique\nlinguistic and structural characteristics of academic content. This study\npresents two open-source embedding models fine-tuned for educational question\nanswering, particularly in the context of course syllabi. A synthetic dataset\nof 3,197 sentence pairs, spanning synonymous terminology, paraphrased\nquestions, and implicit-explicit mappings, was constructed through a\ncombination of manual curation and large language model (LLM)-assisted\ngeneration. Two training strategies were evaluated: (1) a baseline model\nfine-tuned using MultipleNegativesRankingLoss (MNRL), and (2) a dual-loss model\nthat combines MNRL with CosineSimilarityLoss to improve both semantic ranking\nand similarity calibration. Evaluations were conducted on 28 university course\nsyllabi using a fixed set of natural language questions categorized into\ncourse, faculty, and teaching assistant information. Results demonstrate that\nboth fine-tuned models outperform strong open-source baselines, including\nall-MiniLM-L6-v2 and multi-qa-MiniLM-L6-cos-v1, and that the dual-loss model\nnarrows the performance gap with high-performing proprietary embeddings such as\nOpenAI's text-embedding-3 series. This work contributes reusable,\ndomain-aligned embedding models and provides a replicable framework for\neducational semantic retrieval, supporting downstream applications such as\nacademic chatbots, retrieval-augmented generation (RAG) systems, and learning\nmanagement system (LMS) integrations.\n","authors":["Ramteja Sajja","Yusuf Sermet","Ibrahim Demir"],"pdf_url":"https://arxiv.org/pdf/2505.04916v1.pdf","comment":"17 pages, 3 Tables"},{"id":"http://arxiv.org/abs/2505.04914v1","updated":"2025-05-08T03:09:57Z","published":"2025-05-08T03:09:57Z","title":"Enigme: Generative Text Puzzles for Evaluating Reasoning in Language\n  Models","summary":"  Transformer-decoder language models are a core innovation in text based\ngenerative artificial intelligence. These models are being deployed as\ngeneral-purpose intelligence systems in many applications. Central to their\nutility is the capacity to understand natural language commands and exploit the\nreasoning embedded in human text corpora to apply some form of reasoning\nprocess to a wide variety of novel tasks. To understand the limitations of this\napproach to generating reasoning we argue that we need to consider the\narchitectural constraints of these systems. Consideration of the latent\nvariable structure of transformer-decoder models allows us to design reasoning\ntasks that should probe the boundary of their capacity to reason. We present\nenigme, an open-source library for generating text-based puzzles to be used in\ntraining and evaluating reasoning skills within transformer-decoder models and\nfuture AI architectures.\n","authors":["John Hawkins"],"pdf_url":"https://arxiv.org/pdf/2505.04914v1.pdf","comment":"To be published in the proceedings of The 2025 11th International\n  Conference on Engineering, Applied Sciences, and Technology (ICEAST)"},{"id":"http://arxiv.org/abs/2505.02581v2","updated":"2025-05-08T03:02:00Z","published":"2025-05-05T11:33:18Z","title":"Agentic Neurodivergence as a Contingent Solution to the AI Alignment\n  Problem","summary":"  The AI alignment problem, which focusses on ensuring that artificial\nintelligence (AI), including AGI and ASI, systems act according to human\nvalues, presents profound challenges. With the progression from narrow AI to\nArtificial General Intelligence (AGI) and Superintelligence, fears about\ncontrol and existential risk have escalated. Here, we investigate whether\nembracing inevitable AI misalignment can be a contingent strategy to foster a\ndynamic ecosystem of competing agents as a viable path to steer them in more\nhuman-aligned trends and mitigate risks. We explore how misalignment may serve\nand should be promoted as a counterbalancing mechanism to team up with\nwhichever agents are most aligned to human interests, ensuring that no single\nsystem dominates destructively. The main premise of our contribution is that\nmisalignment is inevitable because full AI-human alignment is a mathematical\nimpossibility from Turing-complete systems, which we also offer as a proof in\nthis contribution, a feature then inherited to AGI and ASI systems. We\nintroduce and test change-of-opinion attacks based on this kind of perturbation\nand intervention analysis to study how agents may neutralise friendly or\nunfriendly AIs through cooperation and competition. We show that open models\nare more diverse and that most likely guardrails implemented in proprietary\nmodels are successful at steering and controlling to some extent the agents'\nrange of opinion and sentiment change with possible positive and negative\nconsequences in what we believe are signs of a neuro-symbolic approach even if\nshallow.\n","authors":["Alberto Hernández-Espinosa","Felipe S. Abrahão","Olaf Witkowski","Hector Zenil"],"pdf_url":"https://arxiv.org/pdf/2505.02581v2.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2505.04911v1","updated":"2025-05-08T02:59:01Z","published":"2025-05-08T02:59:01Z","title":"SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with\n  Off-the-Shelf Multimodal Large Language Models","summary":"  This study introduces SpatialPrompting, a novel framework that harnesses the\nemergent reasoning capabilities of off-the-shelf multimodal large language\nmodels to achieve zero-shot spatial reasoning in three-dimensional (3D)\nenvironments. Unlike existing methods that rely on expensive 3D-specific\nfine-tuning with specialized 3D inputs such as point clouds or voxel-based\nfeatures, SpatialPrompting employs a keyframe-driven prompt generation\nstrategy. This framework uses metrics such as vision-language similarity,\nMahalanobis distance, field of view, and image sharpness to select a diverse\nand informative set of keyframes from image sequences and then integrates them\nwith corresponding camera pose data to effectively abstract spatial\nrelationships and infer complex 3D structures. The proposed framework not only\nestablishes a new paradigm for flexible spatial reasoning that utilizes\nintuitive visual and positional cues but also achieves state-of-the-art\nzero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across\nseveral metrics. The proposed method effectively eliminates the need for\nspecialized 3D inputs and fine-tuning, offering a simpler and more scalable\nalternative to conventional approaches.\n","authors":["Shun Taguchi","Hideki Deguchi","Takumi Hamazaki","Hiroyuki Sakai"],"pdf_url":"https://arxiv.org/pdf/2505.04911v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2501.17899v2","updated":"2025-05-08T02:28:58Z","published":"2025-01-29T04:32:41Z","title":"The Right to AI","summary":"  This paper proposes a Right to AI, which asserts that individuals and\ncommunities should meaningfully participate in the development and governance\nof the AI systems that shape their lives. Motivated by the increasing\ndeployment of AI in critical domains and inspired by Henri Lefebvre's concept\nof the Right to the City, we reconceptualize AI as a societal infrastructure,\nrather than merely a product of expert design. In this paper, we critically\nevaluate how generative agents, large-scale data extraction, and diverse\ncultural values bring new complexities to AI oversight. The paper proposes that\ngrassroots participatory methodologies can mitigate biased outcomes and enhance\nsocial responsiveness. It asserts that data is socially produced and should be\nmanaged and owned collectively. Drawing on Sherry Arnstein's Ladder of Citizen\nParticipation and analyzing nine case studies, the paper develops a four-tier\nmodel for the Right to AI that situates the current paradigm and envisions an\naspirational future. It proposes recommendations for inclusive data ownership,\ntransparent design processes, and stakeholder-driven oversight. We also discuss\nmarket-led and state-centric alternatives and argue that participatory\napproaches offer a better balance between technical efficiency and democratic\nlegitimacy.\n","authors":["Rashid Mushkani","Hugo Berard","Allison Cohen","Shin Koeski"],"pdf_url":"https://arxiv.org/pdf/2501.17899v2.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2505.04898v1","updated":"2025-05-08T02:19:39Z","published":"2025-05-08T02:19:39Z","title":"Precise gradient descent training dynamics for finite-width multi-layer\n  neural networks","summary":"  In this paper, we provide the first precise distributional characterization\nof gradient descent iterates for general multi-layer neural networks under the\ncanonical single-index regression model, in the `finite-width proportional\nregime' where the sample size and feature dimension grow proportionally while\nthe network width and depth remain bounded. Our non-asymptotic state evolution\ntheory captures Gaussian fluctuations in first-layer weights and concentration\nin deeper-layer weights, and remains valid for non-Gaussian features.\n  Our theory differs from existing neural tangent kernel (NTK), mean-field (MF)\ntheories and tensor program (TP) in several key aspects. First, our theory\noperates in the finite-width regime whereas these existing theories are\nfundamentally infinite-width. Second, our theory allows weights to evolve from\nindividual initializations beyond the lazy training regime, whereas NTK and MF\nare either frozen at or only weakly sensitive to initialization, and TP relies\non special initialization schemes. Third, our theory characterizes both\ntraining and generalization errors for general multi-layer neural networks\nbeyond the uniform convergence regime, whereas existing theories study\ngeneralization almost exclusively in two-layer settings.\n  As a statistical application, we show that vanilla gradient descent can be\naugmented to yield consistent estimates of the generalization error at each\niteration, which can be used to guide early stopping and hyperparameter tuning.\nAs a further theoretical implication, we show that despite model\nmisspecification, the model learned by gradient descent retains the structure\nof a single-index function with an effective signal determined by a linear\ncombination of the true signal and the initialization.\n","authors":["Qiyang Han","Masaaki Imaizumi"],"pdf_url":"https://arxiv.org/pdf/2505.04898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04891v1","updated":"2025-05-08T01:53:36Z","published":"2025-05-08T01:53:36Z","title":"Clustering with Communication: A Variational Framework for Single Cell\n  Representation Learning","summary":"  Single-cell RNA sequencing (scRNA-seq) has revealed complex cellular\nheterogeneity, but recent studies emphasize that understanding biological\nfunction also requires modeling cell-cell communication (CCC), the signaling\ninteractions mediated by ligand-receptor pairs that coordinate cellular\nbehavior. Tools like CellChat have demonstrated that CCC plays a critical role\nin processes such as cell differentiation, tissue regeneration, and immune\nresponse, and that transcriptomic data inherently encodes rich information\nabout intercellular signaling. We propose CCCVAE, a novel variational\nautoencoder framework that incorporates CCC signals into single-cell\nrepresentation learning. By leveraging a communication-aware kernel derived\nfrom ligand-receptor interactions and a sparse Gaussian process, CCCVAE encodes\nbiologically informed priors into the latent space. Unlike conventional VAEs\nthat treat each cell independently, CCCVAE encourages latent embeddings to\nreflect both transcriptional similarity and intercellular signaling context.\nEmpirical results across four scRNA-seq datasets show that CCCVAE improves\nclustering performance, achieving higher evaluation scores than standard VAE\nbaselines. This work demonstrates the value of embedding biological priors into\ndeep generative models for unsupervised single-cell analysis.\n","authors":["Cong Qi","Yeqing Chen","Jie Zhang","Wei Zhi"],"pdf_url":"https://arxiv.org/pdf/2505.04891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04888v1","updated":"2025-05-08T01:49:53Z","published":"2025-05-08T01:49:53Z","title":"Cross-Branch Orthogonality for Improved Generalization in Face Deepfake\n  Detection","summary":"  Remarkable advancements in generative AI technology have given rise to a\nspectrum of novel deepfake categories with unprecedented leaps in their\nrealism, and deepfakes are increasingly becoming a nuisance to law enforcement\nauthorities and the general public. In particular, we observe alarming levels\nof confusion, deception, and loss of faith regarding multimedia content within\nsociety caused by face deepfakes, and existing deepfake detectors are\nstruggling to keep up with the pace of improvements in deepfake generation.\nThis is primarily due to their reliance on specific forgery artifacts, which\nlimits their ability to generalise and detect novel deepfake types. To combat\nthe spread of malicious face deepfakes, this paper proposes a new strategy that\nleverages coarse-to-fine spatial information, semantic information, and their\ninteractions while ensuring feature distinctiveness and reducing the redundancy\nof the modelled features. A novel feature orthogonality-based disentanglement\nstrategy is introduced to ensure branch-level and cross-branch feature\ndisentanglement, which allows us to integrate multiple feature vectors without\nadding complexity to the feature space or compromising generalisation.\nComprehensive experiments on three public benchmarks: FaceForensics++,\nCeleb-DF, and the Deepfake Detection Challenge (DFDC) show that these design\nchoices enable the proposed approach to outperform current state-of-the-art\nmethods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a\ncross-dataset evaluation setting.\n","authors":["Tharindu Fernando","Clinton Fookes","Sridha Sridharan","Simon Denman"],"pdf_url":"https://arxiv.org/pdf/2505.04888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04883v1","updated":"2025-05-08T01:43:21Z","published":"2025-05-08T01:43:21Z","title":"QBR: A Question-Bank-Based Approach to Fine-Grained Legal Knowledge\n  Retrieval for the General Public","summary":"  Retrieval of legal knowledge by the general public is a challenging problem\ndue to the technicality of the professional knowledge and the lack of\nfundamental understanding by laypersons on the subject. Traditional information\nretrieval techniques assume that users are capable of formulating succinct and\nprecise queries for effective document retrieval. In practice, however, the\nwide gap between the highly technical contents and untrained users makes legal\nknowledge retrieval very difficult. We propose a methodology, called QBR, which\nemploys a Questions Bank (QB) as an effective medium for bridging the knowledge\ngap. We show how the QB is used to derive training samples to enhance the\nembedding of knowledge units within documents, which leads to effective\nfine-grained knowledge retrieval. We discuss and evaluate through experiments\nvarious advantages of QBR over traditional methods. These include more\naccurate, efficient, and explainable document retrieval, better comprehension\nof retrieval results, and highly effective fine-grained knowledge retrieval. We\nalso present some case studies and show that QBR achieves social impact by\nassisting citizens to resolve everyday legal concerns.\n","authors":["Mingruo Yuan","Ben Kao","Tien-Hsuan Wu"],"pdf_url":"https://arxiv.org/pdf/2505.04883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04881v1","updated":"2025-05-08T01:40:40Z","published":"2025-05-08T01:40:40Z","title":"ConCISE: Confidence-guided Compression in Step-by-step Efficient\n  Reasoning","summary":"  Large Reasoning Models (LRMs) perform strongly in complex reasoning tasks via\nChain-of-Thought (CoT) prompting, but often suffer from verbose outputs caused\nby redundant content, increasing computational overhead, and degrading user\nexperience. Existing compression methods either operate post-hoc pruning,\nrisking disruption to reasoning coherence, or rely on sampling-based selection,\nwhich fails to intervene effectively during generation. In this work, we\nintroduce a confidence-guided perspective to explain the emergence of redundant\nreflection in LRMs, identifying two key patterns: Confidence Deficit, where the\nmodel reconsiders correct steps due to low internal confidence, and Termination\nDelay, where reasoning continues even after reaching a confident answer. Based\non this analysis, we propose ConCISE (Confidence-guided Compression In\nStep-by-step Efficient Reasoning), a framework that simplifies reasoning chains\nby reinforcing the model's confidence during inference, thus preventing the\ngeneration of redundant reflection steps. It integrates Confidence Injection to\nstabilize intermediate steps and Early Stopping to terminate reasoning when\nconfidence is sufficient. Extensive experiments demonstrate that fine-tuning\nLRMs on ConCISE-generated data yields significantly shorter outputs, reducing\nlength by up to approximately 50% under SimPO, while maintaining high task\naccuracy. ConCISE consistently outperforms existing baselines across multiple\nreasoning benchmarks.\n","authors":["Ziqing Qiao","Yongheng Deng","Jiali Zeng","Dong Wang","Lai Wei","Fandong Meng","Jie Zhou","Ju Ren","Yaoxue Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.04881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04880v1","updated":"2025-05-08T01:38:12Z","published":"2025-05-08T01:38:12Z","title":"GroverGPT-2: Simulating Grover's Algorithm via Chain-of-Thought\n  Reasoning and Quantum-Native Tokenization","summary":"  Quantum computing offers theoretical advantages over classical computing for\nspecific tasks, yet the boundary of practical quantum advantage remains an open\nquestion. To investigate this boundary, it is crucial to understand whether,\nand how, classical machines can learn and simulate quantum algorithms. Recent\nprogress in large language models (LLMs) has demonstrated strong reasoning\nabilities, prompting exploration into their potential for this challenge. In\nthis work, we introduce GroverGPT-2, an LLM-based method for simulating\nGrover's algorithm using Chain-of-Thought (CoT) reasoning and quantum-native\ntokenization. Building on its predecessor, GroverGPT-2 performs simulation\ndirectly from quantum circuit representations while producing logically\nstructured and interpretable outputs. Our results show that GroverGPT-2 can\nlearn and internalize quantum circuit logic through efficient processing of\nquantum-native tokens, providing direct evidence that classical models like\nLLMs can capture the structure of quantum algorithms. Furthermore, GroverGPT-2\noutputs interleave circuit data with natural language, embedding explicit\nreasoning into the simulation. This dual capability positions GroverGPT-2 as a\nprototype for advancing machine understanding of quantum algorithms and\nmodeling quantum circuit logic. We also identify an empirical scaling law for\nGroverGPT-2 with increasing qubit numbers, suggesting a path toward scalable\nclassical simulation. These findings open new directions for exploring the\nlimits of classical simulatability, enhancing quantum education and research,\nand laying groundwork for future foundation models in quantum computing.\n","authors":["Min Chen","Jinglei Cheng","Pingzhi Li","Haoran Wang","Tianlong Chen","Junyu Liu"],"pdf_url":"https://arxiv.org/pdf/2505.04880v1.pdf","comment":"26 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.11137v3","updated":"2025-05-08T01:35:18Z","published":"2025-02-16T14:05:54Z","title":"Safety Evaluation of DeepSeek Models in Chinese Contexts","summary":"  Recently, the DeepSeek series of models, leveraging their exceptional\nreasoning capabilities and open-source strategy, is reshaping the global AI\nlandscape. Despite these advantages, they exhibit significant safety\ndeficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco,\nin collaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nhas a 100\\% attack success rate when processing harmful prompts. Additionally,\nmultiple safety companies and research institutions have confirmed critical\nsafety vulnerabilities in this model. As models demonstrating robust\nperformance in Chinese and English, DeepSeek models require equally crucial\nsafety assessments in both language contexts. However, current research has\npredominantly focused on safety evaluations in English environments, leaving a\ngap in comprehensive assessments of their safety performance in Chinese\ncontexts. In response to this gap, this study introduces CHiSafetyBench, a\nChinese-specific safety evaluation benchmark. This benchmark systematically\nevaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts,\nrevealing their performance across safety categories. The experimental results\nquantify the deficiencies of these two models in Chinese contexts, providing\nkey insights for subsequent improvements. It should be noted that, despite our\nefforts to establish a comprehensive, objective, and authoritative evaluation\nbenchmark, the selection of test samples, characteristics of data distribution,\nand the setting of evaluation criteria may inevitably introduce certain biases\ninto the evaluation results. We will continuously optimize the evaluation\nbenchmark and periodically update this report to provide more comprehensive and\naccurate assessment outcomes. Please refer to the latest version of the paper\nfor the most recent evaluation results and conclusions.\n","authors":["Wenjing Zhang","Xuejiao Lei","Zhaoxiang Liu","Ning Wang","Zhenhong Long","Peijun Yang","Jiaojiao Zhao","Minjie Hua","Chaoyang Ma","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2502.11137v3.pdf","comment":"12 pages, 2 tables, 7 figures"},{"id":"http://arxiv.org/abs/2505.03838v2","updated":"2025-05-08T01:21:21Z","published":"2025-05-05T04:09:31Z","title":"IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation\n  and Classification","summary":"  Precise and effective processing of cardiac imaging data is critical for the\nidentification and management of the cardiovascular diseases. We introduce\nIntelliCardiac, a comprehensive, web-based medical image processing platform\nfor the automatic segmentation of 4D cardiac images and disease classification,\nutilizing an AI model trained on the publicly accessible ACDC dataset. The\nsystem, intended for patients, cardiologists, and healthcare professionals,\noffers an intuitive interface and uses deep learning models to identify\nessential heart structures and categorize cardiac diseases. The system supports\nanalysis of both the right and left ventricles as well as myocardium, and then\nclassifies patient's cardiac images into five diagnostic categories: dilated\ncardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right\nventricular abnormality, and no disease. IntelliCardiac combines a deep\nlearning-based segmentation model with a two-step classification pipeline. The\nsegmentation module gains an overall accuracy of 92.6%. The classification\nmodule, trained on characteristics taken from segmented heart structures,\nachieves 98% accuracy in five categories. These results exceed the performance\nof the existing state-of-the-art methods that integrate both segmentation and\nclassification models. IntelliCardiac, which supports real-time visualization,\nworkflow integration, and AI-assisted diagnostics, has great potential as a\nscalable, accurate tool for clinical decision assistance in cardiac imaging and\ndiagnosis.\n","authors":["Ting Yu Tsai","An Yu","Meghana Spurthi Maadugundu","Ishrat Jahan Mohima","Umme Habiba Barsha","Mei-Hwa F. Chen","Balakrishnan Prabhakaran","Ming-Ching Chang"],"pdf_url":"https://arxiv.org/pdf/2505.03838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04877v1","updated":"2025-05-08T01:20:24Z","published":"2025-05-08T01:20:24Z","title":"Learning from Loss Landscape: Generalizable Mixed-Precision Quantization\n  via Adaptive Sharpness-Aware Gradient Aligning","summary":"  Mixed Precision Quantization (MPQ) has become an essential technique for\noptimizing neural network by determining the optimal bitwidth per layer.\nExisting MPQ methods, however, face a major hurdle: they require a\ncomputationally expensive search for quantization policies on large-scale\ndatasets. To resolve this issue, we introduce a novel approach that first\nsearches for quantization policies on small datasets and then generalizes them\nto large-scale datasets. This approach simplifies the process, eliminating the\nneed for large-scale quantization fine-tuning and only necessitating model\nweight adjustment. Our method is characterized by three key techniques:\nsharpness-aware minimization for enhanced quantization generalization, implicit\ngradient direction alignment to handle gradient conflicts among different\noptimization objectives, and an adaptive perturbation radius to accelerate\noptimization. Both theoretical analysis and experimental results validate our\napproach. Using the CIFAR10 dataset (just 0.5\\% the size of ImageNet training\ndata) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a\nsignificantly lower computational cost, while improving efficiency by up to\n150% over the baselines.\n","authors":["Lianbo Ma","Jianlun Ma","Yuee Zhou","Guoyang Xie","Qiang He","Zhichao Lu"],"pdf_url":"https://arxiv.org/pdf/2505.04877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04873v1","updated":"2025-05-08T01:17:15Z","published":"2025-05-08T01:17:15Z","title":"Federated Learning for Cyber Physical Systems: A Comprehensive Survey","summary":"  The integration of machine learning (ML) in cyber physical systems (CPS) is a\ncomplex task due to the challenges that arise in terms of real-time decision\nmaking, safety, reliability, device heterogeneity, and data privacy. There are\nalso open research questions that must be addressed in order to fully realize\nthe potential of ML in CPS. Federated learning (FL), a distributed approach to\nML, has become increasingly popular in recent years. It allows models to be\ntrained using data from decentralized sources. This approach has been gaining\npopularity in the CPS field, as it integrates computer, communication, and\nphysical processes. Therefore, the purpose of this work is to provide a\ncomprehensive analysis of the most recent developments of FL-CPS, including the\nnumerous application areas, system topologies, and algorithms developed in\nrecent years. The paper starts by discussing recent advances in both FL and\nCPS, followed by their integration. Then, the paper compares the application of\nFL in CPS with its applications in the internet of things (IoT) in further\ndepth to show their connections and distinctions. Furthermore, the article\nscrutinizes how FL is utilized in critical CPS applications, e.g., intelligent\ntransportation systems, cybersecurity services, smart cities, and smart\nhealthcare solutions. The study also includes critical insights and lessons\nlearned from various FL-CPS implementations. The paper's concluding section\ndelves into significant concerns and suggests avenues for further research in\nthis fast-paced and dynamic era.\n","authors":["Minh K. Quan","Pubudu N. Pathirana","Mayuri Wijayasundara","Sujeeva Setunge","Dinh C. Nguyen","Christopher G. Brinton","David J. Love","H. Vincent Poor"],"pdf_url":"https://arxiv.org/pdf/2505.04873v1.pdf","comment":"This work has been accepted by IEEE Communications Surveys &\n  Tutorials"},{"id":"http://arxiv.org/abs/2501.01031v3","updated":"2025-05-08T01:07:15Z","published":"2025-01-02T03:26:13Z","title":"ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented\n  Contextual Learning","summary":"  Ensuring cultural values alignment in Large Language Models (LLMs) remains a\ncritical challenge, as these models often embed Western-centric biases from\ntheir training data, leading to misrepresentations and fairness concerns in\ncross-cultural applications. Existing approaches such as role assignment and\nfew-shot learning struggle to address these limitations effectively due to\ntheir reliance on pre-trained knowledge, limited scalability, and inability to\ncapture nuanced cultural values. To address these issues, we propose ValuesRAG,\na novel and effective framework that applies Retrieval-Augmented Generation\n(RAG) with In-Context Learning (ICL) to integrate cultural and demographic\nknowledge dynamically during text generation. Leveraging the World Values\nSurvey (WVS) dataset, ValuesRAG first generates summaries of values for each\nindividual. We subsequently curate several representative regional datasets to\nserve as test datasets and retrieve relevant summaries of values based on\ndemographic features, followed by a reranking step to select the top-k relevant\nsummaries. We evaluate ValuesRAG using 6 diverse regional datasets and show\nthat it consistently outperforms baselines: including zero-shot,\nrole-assignment, few-shot, and hybrid methods, both in main experiments and\nablation settings. Notably, ValuesRAG achieves the best overall performance\nover prior methods, demonstrating its effectiveness in fostering culturally\naligned and inclusive AI systems. Our findings underscore the potential of\ndynamic retrieval-based methods to bridge the gap between global LLM\ncapabilities and localized cultural values.\n","authors":["Wonduk Seo","Zonghao Yuan","Yi Bu"],"pdf_url":"https://arxiv.org/pdf/2501.01031v3.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2505.04864v1","updated":"2025-05-08T00:28:31Z","published":"2025-05-08T00:28:31Z","title":"Auto-regressive transformation for image alignment","summary":"  Existing methods for image alignment struggle in cases involving\nfeature-sparse regions, extreme scale and field-of-view differences, and large\ndeformations, often resulting in suboptimal accuracy. Robustness to these\nchallenges improves through iterative refinement of the transformation field\nwhile focusing on critical regions in multi-scale image representations. We\nthus propose Auto-Regressive Transformation (ART), a novel method that\niteratively estimates the coarse-to-fine transformations within an\nauto-regressive framework. Leveraging hierarchical multi-scale features, our\nnetwork refines the transformations using randomly sampled points at each\nscale. By incorporating guidance from the cross-attention layer, the model\nfocuses on critical regions, ensuring accurate alignment even in challenging,\nfeature-limited conditions. Extensive experiments across diverse datasets\ndemonstrate that ART significantly outperforms state-of-the-art methods,\nestablishing it as a powerful new method for precise image alignment with broad\napplicability.\n","authors":["Kanggeon Lee","Soochahn Lee","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2505.04864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04860v1","updated":"2025-05-08T00:03:04Z","published":"2025-05-08T00:03:04Z","title":"D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation","summary":"  Learning bimanual manipulation is challenging due to its high dimensionality\nand tight coordination required between two arms. Eye-in-hand imitation\nlearning, which uses wrist-mounted cameras, simplifies perception by focusing\non task-relevant views. However, collecting diverse demonstrations remains\ncostly, motivating the need for scalable data augmentation. While prior work\nhas explored visual augmentation in single-arm settings, extending these\napproaches to bimanual manipulation requires generating viewpoint-consistent\nobservations across both arms and producing corresponding action labels that\nare both valid and feasible. In this work, we propose Diffusion for COordinated\nDual-arm Data Augmentation (D-CODA), a method for offline data augmentation\ntailored to eye-in-hand bimanual imitation learning that trains a diffusion\nmodel to synthesize novel, viewpoint-consistent wrist-camera images for both\narms while simultaneously generating joint-space action labels. It employs\nconstrained optimization to ensure that augmented states involving\ngripper-to-object contacts adhere to constraints suitable for bimanual\ncoordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our\nresults across 2250 simulation trials and 300 real-world trials demonstrate\nthat it outperforms baselines and ablations, showing its potential for scalable\ndata augmentation in eye-in-hand bimanual manipulation. Our project website is\nat: https://dcodaaug.github.io/D-CODA/.\n","authors":["I-Chun Arthur Liu","Jason Chen","Gaurav Sukhatme","Daniel Seita"],"pdf_url":"https://arxiv.org/pdf/2505.04860v1.pdf","comment":null}]}}